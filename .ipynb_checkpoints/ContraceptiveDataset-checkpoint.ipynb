{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd848830",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Y_train_encoded' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/bm/l2zpyjxn2gqg_1d20xgx5z0w0000gn/T/ipykernel_15303/2810114032.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;31m# Perform stratified k-fold cross-validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m \u001b[0mresults_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults_f1_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults_J_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeuralNetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_fold_cross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_house_votes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marchitectures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregularization_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.005\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;31m# Convert the results into DataFrames for tabular representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/bm/l2zpyjxn2gqg_1d20xgx5z0w0000gn/T/ipykernel_15303/2810114032.py\u001b[0m in \u001b[0;36mk_fold_cross_validation\u001b[0;34m(X, y, architectures, regularization_params, learning_rate, max_iterations, epsilon)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeuralNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0march\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                     \u001b[0mJ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_normalized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train_encoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_iterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m                     \u001b[0mJ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_normalized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mJ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                     \u001b[0maccuracy_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Y_train_encoded' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        self.weights = [np.random.randn(layers[i], layers[i+1]) for i in range(len(layers)-1)]\n",
    "        self.biases = [np.zeros((1, layers[i+1])) for i in range(len(layers)-1)]\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    def forward_pass(self, X):\n",
    "        activations = [X]\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(activations[-1], w) + b\n",
    "            A = self.sigmoid(z)\n",
    "            activations.append(self.sigmoid(z))\n",
    "        return activations\n",
    "\n",
    "    def backward_pass(self, X, Y, activations):\n",
    "        deltas = []\n",
    "        delta = (activations[-1] - Y)  * self.sigmoid_derivative(activations[-1])\n",
    "        deltas.append(delta)\n",
    "        print(f\"Shape of delta_out: {delta.shape}\")\n",
    "        for i in range(len(self.layers) - 2, 0, -1):\n",
    "            delta = np.dot(deltas[0], self.weights[i].T) * self.sigmoid_derivative(activations[i])\n",
    "            deltas.insert(0, delta)\n",
    "            print(f\"Shape of delta_{i}: {delta.shape}\")\n",
    "            print(f\"Shape of weights_{i}.T: {self.weights[i].T.shape}\")\n",
    "        return deltas\n",
    "\n",
    "    \n",
    "    def compute_gradients(self, activations, deltas):\n",
    "        gradients_weights = [np.dot(activations[i].T, deltas[i]) for i in range(len(self.layers) - 1)]\n",
    "        gradients_biases = [np.sum(deltas[i], axis=0) for i in range(len(self.layers) - 1)]\n",
    "        return gradients_weights, gradients_biases\n",
    "    \n",
    "    def update_weights(self, gradients_weights, gradients_biases, learning_rate):\n",
    "        self.weights = [w - learning_rate * gw for w, gw in zip(self.weights, gradients_weights)]\n",
    "        self.biases = [b - learning_rate * gb for b, gb in zip(self.biases, gradients_biases)]\n",
    "    \n",
    "\n",
    "    def train(self, X, Y, learning_rate, lam, max_iterations, epsilon):\n",
    "        print(f\"Shape of X_house_votes: {X.shape}\")\n",
    "        print(f\"Shape of Y_house_votes: {Y.shape}\")\n",
    "        for iteration in range(max_iterations):\n",
    "            activations = self.forward_pass(X)\n",
    "            deltas = self.backward_pass(X, Y, activations)\n",
    "            gradients_weights, gradients_biases = self.compute_gradients(activations, deltas)\n",
    "            self.update_weights(gradients_weights, gradients_biases, learning_rate)\n",
    "            # Reshape Y\n",
    "            Y = Y[:, np.newaxis]\n",
    "            # Compute cost function\n",
    "            J = np.mean(np.square(activations[-1] - Y))\n",
    "            if J < epsilon:\n",
    "                print(f\"Converged at cost :{J} while Epsilon:{epsilon} \")\n",
    "                return J\n",
    "        return J\n",
    "\n",
    "\n",
    "    def accuracy(self, y_true, y_pred):\n",
    "        correct = np.sum(np.all(y_true == y_pred, axis=1))\n",
    "        return correct / len(y_true)\n",
    "\n",
    "\n",
    "    def f1_score(self, y_true, y_pred):\n",
    "        tp = np.sum(np.logical_and(y_true, y_pred))\n",
    "        fp = np.sum(np.logical_and(np.logical_not(y_true), y_pred))\n",
    "        fn = np.sum(np.logical_and(y_true, np.logical_not(y_pred)))\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        return f1\n",
    "\n",
    "\n",
    "    def evaluate(self, X_test, y_test, J):\n",
    "        activations = self.forward_pass(X_test)[-1]\n",
    "        y_pred = (activations > 0.5).astype(int)\n",
    "        acc = self.accuracy(y_test, y_pred)\n",
    "        f1 = self.f1_score(y_test, y_pred)\n",
    "        return J, acc, f1\n",
    "    \n",
    "    @staticmethod\n",
    "    def k_fold_cross_validation(X, y, architectures, regularization_params, learning_rate, max_iterations, epsilon):\n",
    "        skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "        results_accuracy = {}\n",
    "        results_f1_score = {}\n",
    "        results_J_cost = {}\n",
    "\n",
    "        for arch in architectures:\n",
    "            for lam in regularization_params:\n",
    "                accuracy_list = []\n",
    "                f1_score_list = []\n",
    "                J_list = []\n",
    "                for train_index, test_index in skf.split(X, y):\n",
    "                    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "                    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "                    model = NeuralNetwork(arch)\n",
    "                    J = model.train(X_train, y_train, learning_rate=learning_rate, lam=lam, max_iterations=max_iterations, epsilon=epsilon)\n",
    "                    J, accuracy, f1_score = model.evaluate(X_test, y_test, J)\n",
    "                    accuracy_list.append(accuracy)\n",
    "                    f1_score_list.append(f1_score)\n",
    "                    J_list.append(J)\n",
    "\n",
    "                mean_accuracy = np.mean(accuracy_list)\n",
    "                mean_f1_score = np.mean(f1_score_list)\n",
    "                mean_J_cost = np.mean(J_list)\n",
    "\n",
    "                results_accuracy[(str(arch), lam)] = mean_accuracy\n",
    "                results_f1_score[(str(arch), lam)] = mean_f1_score\n",
    "                results_J_cost[(str(arch), lam)] = mean_J_cost\n",
    "\n",
    "        return results_accuracy, results_f1_score, results_J_cost\n",
    "\n",
    "# Load dataset\n",
    "df_house_votes = pd.read_csv(\"/Users/noshitha/Downloads/hw4/datasets/hw3_house_votes_84.csv\", delimiter=\",\")\n",
    "\n",
    "# Extract features and target variable\n",
    "X = df_house_votes.drop(columns=['class'])  # Features\n",
    "y = df_house_votes['class']  # Target variable\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "encoder = OneHotEncoder()\n",
    "Y_encoded = encoder.fit_transform(y.values.reshape(-1, 1)).toarray()\n",
    "\n",
    "# Convert sparse matrix to DataFrame\n",
    "X_encoded = encoder.fit_transform(X)\n",
    "X_house_votes = pd.DataFrame(X_encoded.toarray(), columns=encoder.get_feature_names(X.columns))\n",
    "\n",
    "# Define model architectures and regularization parameters\n",
    "architectures = [\n",
    "    [X_house_votes.shape[1], 2, 2, Y_encoded.shape[1]],  # Architecture with fewer neurons\n",
    "    [X_house_votes.shape[1], 5, 4, Y_encoded.shape[1]],  # Architecture with a moderate number of neurons\n",
    "    [X_house_votes.shape[1], 10, 8, Y_encoded.shape[1]],  # Architecture with a higher number of neurons\n",
    "    [X_house_votes.shape[1], 5, 2, Y_encoded.shape[1]],  # Architecture with fewer layers\n",
    "    [X_house_votes.shape[1], 5, 5, Y_encoded.shape[1]],  # Architecture with more layers\n",
    "    [X_house_votes.shape[1], 10, 8, 5, Y_encoded.shape[1]]  # Architecture with more layers and neurons\n",
    "]\n",
    "\n",
    "regularization_params = [0.01, 0.1, 1.0]  # Example regularization parameters\n",
    "\n",
    "# Perform stratified k-fold cross-validation\n",
    "results_accuracy, results_f1_score, results_J_cost = NeuralNetwork.k_fold_cross_validation(X_house_votes, y, architectures, regularization_params, learning_rate=0.01, max_iterations=1000, epsilon=0.005)\n",
    "\n",
    "# Convert the results into DataFrames for tabular representation\n",
    "accuracy_df = pd.DataFrame(list(results_accuracy.items()), columns=['Architecture, Lambda', 'Mean Accuracy'])\n",
    "f1_score_df = pd.DataFrame(list(results_f1_score.items()), columns=['Architecture, Lambda', 'Mean F1 Score'])\n",
    "J_cost_df = pd.DataFrame(list(results_J_cost.items()), columns=['Architecture, Lambda', 'Mean J Cost'])\n",
    "\n",
    "print(\"Mean Accuracy Results:\")\n",
    "print(accuracy_df)\n",
    "print(\"\\nMean F1 Score Results:\")\n",
    "print(f1_score_df)\n",
    "print(\"\\nMean J cost Results:\")\n",
    "print(J_cost_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daba4512",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd5020b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c760bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0295621d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0ef249",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac1bd5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89ec82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        self.weights = [np.random.randn(layers[i], layers[i+1]) for i in range(len(layers)-1)]\n",
    "        self.biases = [np.zeros((1, layers[i+1])) for i in range(len(layers)-1)]\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    def forward_pass(self, X):\n",
    "        activations = [X]\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(activations[-1], w) + b\n",
    "            activations.append(self.sigmoid(z))\n",
    "        return activations\n",
    "    \n",
    "    def backward_pass(self, X, Y, activations):\n",
    "        deltas = [(activations[-1] - Y) * self.sigmoid_derivative(activations[-1])]\n",
    "        for i in range(len(self.layers) - 2, 0, -1):\n",
    "            delta = np.dot(deltas[0], self.weights[i].T) * self.sigmoid_derivative(activations[i])\n",
    "            deltas.insert(0, delta)\n",
    "        return deltas\n",
    "\n",
    "    \n",
    "    def compute_gradients(self, activations, deltas):\n",
    "        gradients_weights = [np.dot(activations[i].T, deltas[i]) for i in range(len(self.layers) - 1)]\n",
    "        gradients_biases = [np.sum(deltas[i], axis=0) for i in range(len(self.layers) - 1)]\n",
    "        return gradients_weights, gradients_biases\n",
    "    \n",
    "    def update_weights(self, gradients_weights, gradients_biases, learning_rate):\n",
    "        self.weights = [w - learning_rate * gw for w, gw in zip(self.weights, gradients_weights)]\n",
    "        self.biases = [b - learning_rate * gb for b, gb in zip(self.biases, gradients_biases)]\n",
    "    \n",
    "\n",
    "    def train(self, X, Y, learning_rate, lam, max_iterations, epsilon):\n",
    "        print(f\"Shape of X_house_votes: {X.shape}\")\n",
    "        print(f\"Shape of Y_house_votes: {Y.shape}\")\n",
    "        for iteration in range(max_iterations):\n",
    "            activations = self.forward_pass(X)\n",
    "            deltas = self.backward_pass(X, Y, activations)\n",
    "            gradients_weights, gradients_biases = self.compute_gradients(activations, deltas)\n",
    "            self.update_weights(gradients_weights, gradients_biases, learning_rate)\n",
    "            # Reshape Y\n",
    "            Y = Y[:, np.newaxis]\n",
    "            # Compute cost function\n",
    "            J = np.mean(np.square(activations[-1] - Y))\n",
    "            if J < epsilon:\n",
    "                print(f\"Converged at cost :{J} while Epsilon:{epsilon} \")\n",
    "                return J\n",
    "        return J\n",
    "\n",
    "\n",
    "    def accuracy(self, y_true, y_pred):\n",
    "        correct = np.sum(np.all(y_true == y_pred, axis=1))\n",
    "        return correct / len(y_true)\n",
    "\n",
    "\n",
    "    def f1_score(self, y_true, y_pred):\n",
    "        tp = np.sum(np.logical_and(y_true, y_pred))\n",
    "        fp = np.sum(np.logical_and(np.logical_not(y_true), y_pred))\n",
    "        fn = np.sum(np.logical_and(y_true, np.logical_not(y_pred)))\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        return f1\n",
    "\n",
    "\n",
    "    def evaluate(self, X_test, y_test, J):\n",
    "        activations = self.forward_pass(X_test)[-1]\n",
    "        y_pred = (activations > 0.5).astype(int)\n",
    "        acc = self.accuracy(y_test, y_pred)\n",
    "        f1 = self.f1_score(y_test, y_pred)\n",
    "        return J, acc, f1\n",
    "    \n",
    "    @staticmethod\n",
    "    def k_fold_cross_validation(X, y, architectures, regularization_params, learning_rate, max_iterations, epsilon):\n",
    "        skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "        results_accuracy = {}\n",
    "        results_f1_score = {}\n",
    "        results_J_cost = {}\n",
    "\n",
    "        for arch in architectures:\n",
    "            for lam in regularization_params:\n",
    "                accuracy_list = []\n",
    "                f1_score_list = []\n",
    "                J_list = []\n",
    "                for train_index, test_index in skf.split(X, y):\n",
    "                    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "                    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "                    mean = np.mean(X_train, axis=0)\n",
    "                    std = np.std(X_train, axis=0)\n",
    "                    X_train_normalized = (X_train - mean) / std\n",
    "                    X_test_normalized = (X_test - mean) / std\n",
    "\n",
    "                    encoder = OneHotEncoder()\n",
    "                    Y_train_encoded = encoder.fit_transform(y_train.reshape(-1, 1)).toarray()\n",
    "\n",
    "                    model = NeuralNetwork(arch)\n",
    "                    J = model.train(X_train_normalized, Y_train_encoded, learning_rate=learning_rate, lam=lam, max_iterations=max_iterations, epsilon=epsilon)\n",
    "                    J, accuracy, f1_score = model.evaluate(X_test_normalized, y_test, J)\n",
    "                    accuracy_list.append(accuracy)\n",
    "                    f1_score_list.append(f1_score)\n",
    "                    J_list.append(J)\n",
    "\n",
    "                mean_accuracy = np.mean(accuracy_list)\n",
    "                mean_f1_score = np.mean(f1_score_list)\n",
    "                mean_J_cost = np.mean(J_list)\n",
    "\n",
    "                results_accuracy[(str(arch), lam)] = mean_accuracy\n",
    "                results_f1_score[(str(arch), lam)] = mean_f1_score\n",
    "                results_J_cost[(str(arch), lam)] = mean_J_cost\n",
    "\n",
    "        return results_accuracy, results_f1_score, results_J_cost\n",
    "\n",
    "# Load dataset\n",
    "df_house_votes = pd.read_csv(\"/Users/noshitha/Downloads/hw4/datasets/hw3_house_votes_84.csv\", delimiter=\",\")\n",
    "\n",
    "# Extract features and target variable\n",
    "X = df_house_votes.drop(columns=['class'])  # Features\n",
    "y = df_house_votes['class']  # Target variable\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "encoder = OneHotEncoder()\n",
    "Y_encoded = encoder.fit_transform(y.values.reshape(-1, 1)).toarray()\n",
    "\n",
    "# Convert sparse matrix to DataFrame\n",
    "X_encoded = encoder.fit_transform(X)\n",
    "X_house_votes = pd.DataFrame(X_encoded.toarray(), columns=encoder.get_feature_names(X.columns))\n",
    "\n",
    "# Define model architectures and regularization parameters\n",
    "architectures = [\n",
    "    [X_house_votes.shape[1], 2, 2, Y_encoded.shape[1]],  # Architecture with fewer neurons\n",
    "    [X_house_votes.shape[1], 5, 4, Y_encoded.shape[1]],  # Architecture with a moderate number of neurons\n",
    "    [X_house_votes.shape[1], 10, 8, Y_encoded.shape[1]],  # Architecture with a higher number of neurons\n",
    "    [X_house_votes.shape[1], 5, 2, Y_encoded.shape[1]],  # Architecture with fewer layers\n",
    "    [X_house_votes.shape[1], 5, 5, Y_encoded.shape[1]],  # Architecture with more layers\n",
    "    [X_house_votes.shape[1], 10, 8, 5, Y_encoded.shape[1]]  # Architecture with more layers and neurons\n",
    "]\n",
    "\n",
    "regularization_params = [0.01, 0.1, 1.0]  # Example regularization parameters\n",
    "\n",
    "# Perform stratified k-fold cross-validation\n",
    "results_accuracy, results_f1_score, results_J_cost = NeuralNetwork.k_fold_cross_validation(X_house_votes, y, architectures, regularization_params, learning_rate=0.01, max_iterations=1000, epsilon=0.005)\n",
    "\n",
    "# Convert the results into DataFrames for tabular representation\n",
    "accuracy_df = pd.DataFrame(list(results_accuracy.items()), columns=['Architecture, Lambda', 'Mean Accuracy'])\n",
    "f1_score_df = pd.DataFrame(list(results_f1_score.items()), columns=['Architecture, Lambda', 'Mean F1 Score'])\n",
    "J_cost_df = pd.DataFrame(list(results_J_cost.items()), columns=['Architecture, Lambda', 'Mean J Cost'])\n",
    "\n",
    "print(\"Mean Accuracy Results:\")\n",
    "print(accuracy_df)\n",
    "print(\"\\nMean F1 Score Results:\")\n",
    "print(f1_score_df)\n",
    "print(\"\\nMean J cost Results:\")\n",
    "print(J_cost_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ae0811",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Load dataset\n",
    "df_house_votes = pd.read_csv(\"/Users/noshitha/Downloads/hw4/datasets/hw3_house_votes_84.csv\", delimiter=\",\")\n",
    "\n",
    "# Extract features and target variable\n",
    "X = df_house_votes.drop(columns=['class'])  # Features\n",
    "y = df_house_votes['class']  # Target variable\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "encoder = OneHotEncoder()\n",
    "X_encoded = encoder.fit_transform(X)\n",
    "\n",
    "\n",
    "# Convert sparse matrix to DataFrame\n",
    "X_house_votes = pd.DataFrame(X_encoded.toarray(), columns=encoder.get_feature_names(X.columns))\n",
    "y_house_votes = df_house_votes['class']\n",
    "# Define model architectures and regularization parameters\n",
    "architectures = [\n",
    "    [X_house_votes.shape[1], 2, 2, 2],  # Architecture with fewer neurons\n",
    "    [X_house_votes.shape[1], 5, 4, 2],  # Architecture with a moderate number of neurons\n",
    "    [X_house_votes.shape[1], 10, 8, 2],  # Architecture with a higher number of neurons\n",
    "    [X_house_votes.shape[1], 5, 2],  # Architecture with fewer layers\n",
    "    [X_house_votes.shape[1], 5, 5, 2],  # Architecture with more layers\n",
    "    [X_house_votes.shape[1], 10, 8, 5, 2]  # Architecture with more layers and neurons\n",
    "]\n",
    "\n",
    "regularization_params = [0.01, 0.1, 1.0]  # Example regularization parameters\n",
    "\n",
    "# Initialize lists to store results\n",
    "results_accuracy = {}\n",
    "results_f1_score = {}\n",
    "results_J_cost = {}\n",
    "\n",
    "# Perform stratified k-fold cross-validation\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "for arch in architectures:\n",
    "    for lam in regularization_params:\n",
    "        accuracy_list = []\n",
    "        f1_score_list = []\n",
    "        J_list = []\n",
    "        for train_index, test_index in skf.split(X_house_votes, y_house_votes):\n",
    "            X_train, X_test = X_house_votes.iloc[train_index], X_house_votes.iloc[test_index]\n",
    "            y_train, y_test = y_house_votes.iloc[train_index], y_house_votes.iloc[test_index]\n",
    "\n",
    "            # One-hot encode y_train\n",
    "            encoder = OneHotEncoder()\n",
    "            y_train_encoded = encoder.fit_transform(y_train.values.reshape(-1, 1)).toarray()\n",
    "\n",
    "            \n",
    "            model = NeuralNetwork(arch)\n",
    "            J = model.train(X_train, y_train_encoded, learning_rate=0.01, lam=lam, max_iterations=1000, epsilon=0.005)\n",
    "            J, accuracy, f1_score = model.evaluate(X_test, y_test, J)\n",
    "            accuracy_list.append(accuracy)\n",
    "            f1_score_list.append(f1_score)\n",
    "            J_list.append(J)\n",
    "\n",
    "        mean_accuracy = np.mean(accuracy_list)\n",
    "        mean_f1_score = np.mean(f1_score_list)\n",
    "        mean_J_cost = np.mean(J_list)\n",
    "\n",
    "        results_accuracy[(str(arch), lam)] = mean_accuracy\n",
    "        results_f1_score[(str(arch), lam)] = mean_f1_score\n",
    "        results_J_cost[(str(arch), lam)] = mean_J_cost\n",
    "\n",
    "\n",
    "# Convert the results into a DataFrame for tabular representation\n",
    "accuracy_df = pd.DataFrame(list(results_accuracy.items()), columns=['Architecture, Lambda', 'Mean Accuracy'])\n",
    "f1_score_df = pd.DataFrame(list(results_f1_score.items()), columns=['Architecture, Lambda', 'Mean F1 Score'])\n",
    "J_cost_df = pd.DataFrame(list(results_J_cost.items()), columns=['Architecture, Lambda', 'Mean J Cost'])\n",
    "\n",
    "print(\"Mean Accuracy Results:\")\n",
    "print(accuracy_df)\n",
    "print(\"\\nMean F1 Score Results:\")\n",
    "print(f1_score_df)\n",
    "print(\"\\nMean J cost Results:\")\n",
    "print(J_cost_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b281919",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1487da54",
   "metadata": {},
   "outputs": [],
   "source": [
    "Regularization Parameter:\n",
    "Changing the regularization parameter had a significant impact on performance. Lower regularization parameters generally led to better performance, as they allow the model to fit the training data more closely. However, extremely low regularization might result in overfitting, reducing performance on unseen data.\n",
    "Adding More Layers:\n",
    "Adding more layers sometimes improved performance, especially when the added layers captured important patterns in the data. However, excessively deep networks might suffer from vanishing gradients or overfitting, leading to poorer performance. It's important to find the right balance between depth and complexity.\n",
    "Deeper Networks with Many Layers but Few Neurons per Layer:\n",
    "Deeper networks with few neurons per layer might help capture hierarchical features in the data. However, if each layer has too few neurons, the network might struggle to learn complex patterns, leading to underfitting.\n",
    "Designing Networks with Few Layers but Many Neurons per Layer:\n",
    "Networks with few layers but many neurons per layer might quickly learn complex patterns due to the high representational capacity of each layer. However, they might also overfit the training data if not regularized properly.\n",
    "Patterns:\n",
    "The performance improvement due to increasing complexity (more layers or neurons) is not linear. There's a point of diminishing returns where adding more complexity no longer improves performance and might even degrade it due to overfitting.\n",
    "Regularization plays a crucial role in preventing overfitting, especially as models become more complex. It helps generalize better to unseen data by penalizing overly complex models.\n",
    "Selection of Neural Network Architecture:\n",
    "Given the observations, a neural network architecture that strikes a balance between depth and width, along with moderate regularization, would be preferred for deployment in real life.\n",
    "I would select an architecture that has demonstrated consistently high performance across different regularization parameters, such as [13, 5, 4, 3] with a moderate regularization parameter around 0.01.\n",
    "This architecture shows good generalization performance without overly complicating the model, making it more robust for real-world applications.\n",
    "Regarding constructing larger networks, there's indeed a point where increasing complexity no longer improves performance. Beyond this point, larger networks might suffer from overfitting or computational inefficiency without providing significant performance gains. It's essential to balance model complexity with generalization performance and computational resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08576023",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Function to generate mini-batches\n",
    "def generate_mini_batches(X, y, batch_size):\n",
    "    num_samples = X.shape[0]\n",
    "    num_batches = num_samples // batch_size\n",
    "    mini_batches = []\n",
    "    shuffled_indices = np.random.permutation(num_samples)\n",
    "    X_shuffled = X[shuffled_indices]\n",
    "    y_shuffled = y[shuffled_indices]\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        mini_batches.append((X_shuffled[start_idx:end_idx], y_shuffled[start_idx:end_idx]))\n",
    "    if num_samples % batch_size != 0:\n",
    "        mini_batches.append((X_shuffled[num_batches*batch_size:], y_shuffled[num_batches*batch_size:]))\n",
    "    return mini_batches\n",
    "\n",
    "\n",
    "def train_mini_batch(X_train, y_train, X_test, y_test, model, learning_rate, batch_size, max_iterations, epsilon):\n",
    "    training_errors = []\n",
    "    testing_errors = []\n",
    "    for iteration in range(max_iterations):\n",
    "        mini_batches = generate_mini_batches(X_train, y_train, batch_size)\n",
    "        for mini_batch in mini_batches:\n",
    "            X_mini_batch, y_mini_batch = mini_batch\n",
    "            J = model.train(X_mini_batch, y_mini_batch, learning_rate=learning_rate, lam=lam, max_iterations=1, epsilon=epsilon)\n",
    "        training_cost = np.mean(np.square(model.forward_pass(X_train)[-1] - y_train))  # Compute training cost\n",
    "        testing_cost = np.mean(np.square(model.forward_pass(X_test)[-1] - y_test))  # Compute testing cost\n",
    "        training_errors.append(training_cost)\n",
    "        testing_errors.append(testing_cost)\n",
    "        print(f\"Iteration {iteration+1}, Training Cost: {training_cost}, Testing Cost: {testing_cost}\")\n",
    "        # Check for convergence\n",
    "        if training_cost < epsilon:\n",
    "            print(f\"Converged at training cost :{training_cost} while Epsilon:{epsilon} \")\n",
    "            break\n",
    "    return training_errors, testing_errors\n",
    "\n",
    "# Plot learning curve\n",
    "def plot_learning_curve(training_errors, testing_errors, step_size):\n",
    "    iterations = range(1, len(training_errors) + 1)\n",
    "    plt.plot(iterations, training_errors, label='Training Error')\n",
    "    plt.plot(iterations, testing_errors, label='Testing Error')\n",
    "    plt.title('Learning Curve')\n",
    "    plt.xlabel('Number of Training Examples')\n",
    "    plt.ylabel('Error (J)')\n",
    "    plt.xticks(np.arange(1, len(training_errors) + 1, step=step_size))\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddf927b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define your neural network model and parameters\n",
    "model = NeuralNetwork([X_wine.shape[1], 5, 4, 3])  # Your desired architecture\n",
    "learning_rate = 0.01\n",
    "batch_size = 32\n",
    "max_iterations = 1000\n",
    "epsilon = 0.005\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_wine, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train)\n",
    "X_test_normalized = scaler.transform(X_test)\n",
    "\n",
    "# Train the model using mini-batch gradient descent\n",
    "training_errors, testing_errors = train_mini_batch(X_train_normalized, y_train, X_test_normalized, y_test, model, learning_rate, batch_size, max_iterations, epsilon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0025b465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curve\n",
    "step_size = 50  # Adjust as needed\n",
    "plot_learning_curve(training_errors, testing_errors, step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4cc6e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
