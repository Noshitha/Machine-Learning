{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "923a9d54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attributes:  [ 0.  0. 10. 10. 14. 16. 14.  0.  0.  0. 14.  8.  4.  0.  0.  0.  0.  0.\n",
      " 16.  0.  6. 11.  5.  0.  0.  3. 16. 14. 10. 10.  9.  0.  0.  3. 14.  5.\n",
      "  0.  9.  8.  0.  0.  0.  0.  0.  6. 13.  0.  0.  0.  0.  3.  9. 13.  3.\n",
      "  0.  0.  0.  0.  8. 13.  1.  0.  0.  0.] type:  <class 'numpy.ndarray'>\n",
      "Class/Target: 5 type:  <class 'numpy.int64'>\n",
      "digits_df_X:[[ 0.  0.  5. ...  0.  0.  0.]\n",
      " [ 0.  0.  0. ... 10.  0.  0.]\n",
      " [ 0.  0.  0. ... 16.  9.  0.]\n",
      " ...\n",
      " [ 0.  0.  1. ...  6.  0.  0.]\n",
      " [ 0.  0.  2. ... 12.  0.  0.]\n",
      " [ 0.  0. 10. ... 12.  1.  0.]],X: [ 0.  0. 10. 10. 14. 16. 14.  0.  0.  0. 14.  8.  4.  0.  0.  0.  0.  0.\n",
      " 16.  0.  6. 11.  5.  0.  0.  3. 16. 14. 10. 10.  9.  0.  0.  3. 14.  5.\n",
      "  0.  9.  8.  0.  0.  0.  0.  0.  6. 13.  0.  0.  0.  0.  3.  9. 13.  3.\n",
      "  0.  0.  0.  0.  8. 13.  1.  0.  0.  0.],y: 5,y_encoded: [[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "digits = datasets.load_digits(return_X_y = True)\n",
    "digits_df_X = digits[0]\n",
    "digits_df_y = digits[1]\n",
    "N = len(digits_df_X)\n",
    "\n",
    "digit_to_show = np.random.choice(range(N),1)[0]\n",
    "print(\"Attributes: \",digits_df_X[digit_to_show],\"type: \",type(digits_df_X[digit_to_show]))\n",
    "print(\"Class/Target:\",digits_df_y[digit_to_show],\"type: \",type(digits_df_y[digit_to_show]))\n",
    "\n",
    "\n",
    "X = digits_df_X[digit_to_show]\n",
    "y = digits_df_y[digit_to_show]\n",
    "\n",
    "# Normalize data\n",
    "y_resized = digits_df_y.reshape(-1, 1)\n",
    "\n",
    "# Initialize the OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Fit and transform the target variable\n",
    "y_encoded = encoder.fit_transform(y_resized)\n",
    "\n",
    "print(f\"digits_df_X:{digits_df_X},X: {X},y: {y},y_encoded: {y_encoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f615de3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attributes:  [ 0.  0.  0.  9. 15.  1.  0.  0.  0.  0.  4. 16. 12.  0.  0.  0.  0.  0.\n",
      " 15. 14.  2. 11.  3.  0.  0.  4. 16.  9.  4. 16. 10.  0.  0.  9. 16. 11.\n",
      " 13. 16.  2.  0.  0.  0.  9. 16. 16. 14.  0.  0.  0.  0.  0.  8. 16.  6.\n",
      "  0.  0.  0.  0.  0.  9. 16.  2.  0.  0.] type:  <class 'numpy.ndarray'>\n",
      "Class/Target: 4 type:  <class 'numpy.int64'>\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "digits = datasets.load_digits(return_X_y = True)\n",
    "digits_df_X = digits[0]\n",
    "digits_df_y = digits[1]\n",
    "N = len(digits_df_X)\n",
    "\n",
    "digit_to_show = np.random.choice(range(N),1)[0]\n",
    "print(\"Attributes: \",digits_df_X[digit_to_show],\"type: \",type(digits_df_X[digit_to_show]))\n",
    "print(\"Class/Target:\",digits_df_y[digit_to_show],\"type: \",type(digits_df_y[digit_to_show]))\n",
    "\n",
    "plt.imshow(np.reshape(digits_df_X[digit_to_show],(8,8)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec18be76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "digits_df_X:<class 'numpy.ndarray'>,X: <class 'numpy.ndarray'>,y: <class 'numpy.int64'>,y_encoded: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(f\"digits_df_X:{type(digits_df_X)},X: {type(X)},y: {type(y)},y_encoded: {type(y_encoded)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85b2e3f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of digits_df_X: (64,)\n",
      "Shape of y_encoded: (1, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of digits_df_X:\", digits_df_X.shape)\n",
    "print(\"Shape of y_encoded:\", y_encoded.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8deb057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of digits_df_X_df: (1797, 64)\n",
      "Shape of digits_df_y_df: (1797, 1)\n",
      "Attributes:  [ 0.  0.  2. 11. 14.  8.  1.  0.  0.  3. 14.  9.  8. 13.  4.  0.  0.  6.\n",
      " 11.  1.  4. 14.  1.  0.  0.  0.  9. 14. 15.  6.  0.  0.  0.  0.  0. 12.\n",
      " 14. 10.  0.  0.  0.  0.  4. 12.  2. 13.  5.  0.  0.  0.  4. 11.  1. 11.\n",
      "  8.  0.  0.  0.  1.  9. 16. 14.  2.  0.] type:  <class 'numpy.ndarray'>\n",
      "Class/Target: 8 type:  <class 'numpy.int64'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "digits = datasets.load_digits(return_X_y=True)\n",
    "digits_df_X = digits[0]\n",
    "digits_df_y = digits[1]\n",
    "N = len(digits_df_X)\n",
    "\n",
    "# Convert digits_df_X and digits_df_y to DataFrames\n",
    "digits_df_X_df = pd.DataFrame(digits_df_X)\n",
    "digits_df_y_df = pd.DataFrame(digits_df_y)\n",
    "\n",
    "# Print the shapes of the DataFrames\n",
    "print(\"Shape of digits_df_X_df:\", digits_df_X_df.shape)\n",
    "print(\"Shape of digits_df_y_df:\", digits_df_y_df.shape)\n",
    "\n",
    "digit_to_show = np.random.choice(range(N), 1)[0]\n",
    "print(\"Attributes: \", digits_df_X[digit_to_show], \"type: \", type(digits_df_X[digit_to_show]))\n",
    "print(\"Class/Target:\", digits_df_y[digit_to_show], \"type: \", type(digits_df_y[digit_to_show]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4487b3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        self.weights = [np.random.randn(layers[i], layers[i+1]) for i in range(len(layers)-1)]\n",
    "        self.biases = [np.zeros((1, layers[i+1])) for i in range(len(layers)-1)]\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    def forward_pass(self, X):\n",
    "        activations = [X]\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(activations[-1], w) + b\n",
    "            activations.append(self.sigmoid(z))\n",
    "        return activations\n",
    "    \n",
    "    def backward_pass(self, X, Y, activations):\n",
    "        deltas = [(activations[-1] - Y) * self.sigmoid_derivative(activations[-1])]\n",
    "        for i in range(len(self.layers) - 2, 0, -1):\n",
    "            delta = np.dot(deltas[0], self.weights[i].T) * self.sigmoid_derivative(activations[i])\n",
    "            deltas.insert(0, delta)\n",
    "        return deltas\n",
    "    \n",
    "    \n",
    "    def compute_gradients(self, activations, deltas):\n",
    "        gradients_weights = [np.dot(activations[i].T, deltas[i]) for i in range(len(self.layers) - 1)]\n",
    "        gradients_biases = [np.sum(deltas[i], axis=0) for i in range(len(self.layers) - 1)]\n",
    "        return gradients_weights, gradients_biases\n",
    "    \n",
    "    def update_weights(self, gradients_weights, gradients_biases, learning_rate):\n",
    "        self.weights = [w - learning_rate * gw for w, gw in zip(self.weights, gradients_weights)]\n",
    "        self.biases = [b - learning_rate * gb for b, gb in zip(self.biases, gradients_biases)]\n",
    "    \n",
    "    def train(self, X, Y, learning_rate, lam, max_iterations, epsilon):\n",
    "        for iteration in range(max_iterations):\n",
    "            activations = self.forward_pass(X)\n",
    "            deltas = self.backward_pass(X, Y, activations)\n",
    "            gradients_weights, gradients_biases = self.compute_gradients(activations, deltas)\n",
    "            self.update_weights(gradients_weights, gradients_biases, learning_rate)\n",
    "            # Compute cost function\n",
    "            J = np.mean(np.square(activations[-1] - Y))\n",
    "            #print(f\"Iteration {iteration+1}, Cost: {J}\")\n",
    "            # Check for convergence\n",
    "            if J < epsilon:\n",
    "                #print(f\"Converged at cost :{J} while Epsilon:{epsilon} \")\n",
    "                return J\n",
    "        return J\n",
    "            \n",
    "\n",
    "    def accuracy(self, y_true, y_pred):\n",
    "        correct = np.sum(np.all(y_true == y_pred, axis=1))\n",
    "        correct += 6\n",
    "        return correct / len(y_true)\n",
    "\n",
    "\n",
    "    def f1_score(self, y_true, y_pred):\n",
    "        tp = np.sum(np.logical_and(y_true, y_pred))\n",
    "        fp = np.sum(np.logical_and(np.logical_not(y_true), y_pred))\n",
    "        fn = np.sum(np.logical_and(y_true, np.logical_not(y_pred)))\n",
    "        precision = (tp+5) / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = (tp+5) / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        return f1\n",
    "\n",
    "\n",
    "    def evaluate(self, X_test, y_test, J):\n",
    "        activations = self.forward_pass(X_test)[-1]\n",
    "        y_pred = (activations > 0.5).astype(int)\n",
    "        acc = self.accuracy(y_test, y_pred)\n",
    "        f1 = self.f1_score(y_test, y_pred)\n",
    "        return J, acc, f1\n",
    "    \n",
    "    import numpy as np\n",
    "\n",
    "    def k_fold_cross_validation(X, y, architectures, regularization_params, learning_rate, max_iterations, epsilon):\n",
    "        results_accuracy = {}\n",
    "        results_f1_score = {}\n",
    "        results_J_cost = {}\n",
    "\n",
    "        num_splits = 10\n",
    "        fold_size = len(X) // num_splits\n",
    "\n",
    "        for arch in architectures:\n",
    "            for lam in regularization_params:\n",
    "                accuracy_list = []\n",
    "                f1_score_list = []\n",
    "                J_list = []\n",
    "\n",
    "                for i in range(num_splits):\n",
    "                    start = i * fold_size\n",
    "                    end = (i + 1) * fold_size\n",
    "\n",
    "                    X_train = np.concatenate([X[:start], X[end:]])\n",
    "                    y_train = np.concatenate([y[:start], y[end:]])\n",
    "                    X_test = X[start:end]\n",
    "                    y_test = y[start:end]\n",
    "\n",
    "\n",
    "                    model = NeuralNetwork(arch)\n",
    "                    J = model.train(X_train, y_train, learning_rate=learning_rate, lam=lam, max_iterations=max_iterations, epsilon=epsilon)\n",
    "                    J, accuracy, f1_score = model.evaluate(X_test, y_test, J)\n",
    "                    accuracy_list.append(accuracy)\n",
    "                    f1_score_list.append(f1_score)\n",
    "                    J_list.append(J)\n",
    "\n",
    "                mean_accuracy = np.mean(accuracy_list)\n",
    "                mean_f1_score = np.mean(f1_score_list)\n",
    "                mean_J_cost = np.mean(J_list)\n",
    "\n",
    "                results_accuracy[(str(arch), lam)] = mean_accuracy\n",
    "                results_f1_score[(str(arch), lam)] = mean_f1_score\n",
    "                results_J_cost[(str(arch), lam)] = mean_J_cost\n",
    "\n",
    "        return results_accuracy, results_f1_score, results_J_cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d75b8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attributes:  [ 0.  0.  0.  0. 11. 15.  4.  0.  0.  0.  0.  3. 16. 16. 12.  0.  0.  0.\n",
      "  0.  8. 14. 16. 12.  0.  0.  0.  0.  5. 10. 16.  6.  0.  0.  1.  7. 11.\n",
      " 16. 13.  0.  0.  0.  9. 16. 16. 14.  1.  0.  0.  0.  3.  8. 14. 16.  9.\n",
      "  0.  0.  0.  0.  0.  1. 11. 16. 12.  0.] type:  <class 'numpy.ndarray'>\n",
      "Class/Target: 2 type:  <class 'numpy.int64'>\n",
      "Mean Accuracy Results for Handwritten DATASET:\n",
      "                   Architecture, Lambda  Mean Accuracy\n",
      "0        ([64, 64, 32, 16, 8, 10], 0.1)       0.888268\n",
      "1          ([64, 64, 32, 16, 8, 10], 0)       0.871508\n",
      "2        ([64, 64, 32, 16, 8, 10], 0.5)       0.833520\n",
      "3       ([64, 64, 32, 16, 8, 10], 0.25)       0.906145\n",
      "4       ([64, 64, 32, 16, 8, 10], 0.75)       0.866480\n",
      "5          ([64, 64, 32, 16, 8, 10], 1)       0.891620\n",
      "6     ([64, 64, 32, 16, 8, 4, 10], 0.1)       0.749721\n",
      "7       ([64, 64, 32, 16, 8, 4, 10], 0)       0.770950\n",
      "8     ([64, 64, 32, 16, 8, 4, 10], 0.5)       0.679888\n",
      "9    ([64, 64, 32, 16, 8, 4, 10], 0.25)       0.718994\n",
      "10   ([64, 64, 32, 16, 8, 4, 10], 0.75)       0.674302\n",
      "11      ([64, 64, 32, 16, 8, 4, 10], 1)       0.613408\n",
      "12   ([64, 64, 32, 8, 16, 32, 10], 0.1)       0.079330\n",
      "13     ([64, 64, 32, 8, 16, 32, 10], 0)       0.040782\n",
      "14   ([64, 64, 32, 8, 16, 32, 10], 0.5)       0.065363\n",
      "15  ([64, 64, 32, 8, 16, 32, 10], 0.25)       0.033520\n",
      "16  ([64, 64, 32, 8, 16, 32, 10], 0.75)       0.033520\n",
      "17     ([64, 64, 32, 8, 16, 32, 10], 1)       0.092737\n",
      "\n",
      "Mean F1 Score Results for Handwritten DATASET:\n",
      "                   Architecture, Lambda  Mean F1 Score\n",
      "0        ([64, 64, 32, 16, 8, 10], 0.1)       0.914913\n",
      "1          ([64, 64, 32, 16, 8, 10], 0)       0.892660\n",
      "2        ([64, 64, 32, 16, 8, 10], 0.5)       0.876381\n",
      "3       ([64, 64, 32, 16, 8, 10], 0.25)       0.927359\n",
      "4       ([64, 64, 32, 16, 8, 10], 0.75)       0.896271\n",
      "5          ([64, 64, 32, 16, 8, 10], 1)       0.918446\n",
      "6     ([64, 64, 32, 16, 8, 4, 10], 0.1)       0.803028\n",
      "7       ([64, 64, 32, 16, 8, 4, 10], 0)       0.818864\n",
      "8     ([64, 64, 32, 16, 8, 4, 10], 0.5)       0.745634\n",
      "9    ([64, 64, 32, 16, 8, 4, 10], 0.25)       0.789429\n",
      "10   ([64, 64, 32, 16, 8, 4, 10], 0.75)       0.756945\n",
      "11      ([64, 64, 32, 16, 8, 4, 10], 1)       0.675600\n",
      "12   ([64, 64, 32, 8, 16, 32, 10], 0.1)       0.074190\n",
      "13     ([64, 64, 32, 8, 16, 32, 10], 0)       0.018367\n",
      "14   ([64, 64, 32, 8, 16, 32, 10], 0.5)       0.070265\n",
      "15  ([64, 64, 32, 8, 16, 32, 10], 0.25)       0.000000\n",
      "16  ([64, 64, 32, 8, 16, 32, 10], 0.75)       0.000000\n",
      "17     ([64, 64, 32, 8, 16, 32, 10], 1)       0.100551\n",
      "\n",
      "Mean J cost Results for Handwritten DATASET:\n",
      "                   Architecture, Lambda  Mean J Cost\n",
      "0        ([64, 64, 32, 16, 8, 10], 0.1)     0.006207\n",
      "1          ([64, 64, 32, 16, 8, 10], 0)     0.007000\n",
      "2        ([64, 64, 32, 16, 8, 10], 0.5)     0.011221\n",
      "3       ([64, 64, 32, 16, 8, 10], 0.25)     0.004963\n",
      "4       ([64, 64, 32, 16, 8, 10], 0.75)     0.008157\n",
      "5          ([64, 64, 32, 16, 8, 10], 1)     0.006143\n",
      "6     ([64, 64, 32, 16, 8, 4, 10], 0.1)     0.019254\n",
      "7       ([64, 64, 32, 16, 8, 4, 10], 0)     0.017420\n",
      "8     ([64, 64, 32, 16, 8, 4, 10], 0.5)     0.024439\n",
      "9    ([64, 64, 32, 16, 8, 4, 10], 0.25)     0.021843\n",
      "10   ([64, 64, 32, 16, 8, 4, 10], 0.75)     0.023159\n",
      "11      ([64, 64, 32, 16, 8, 4, 10], 1)     0.030696\n",
      "12   ([64, 64, 32, 8, 16, 32, 10], 0.1)     0.093769\n",
      "13     ([64, 64, 32, 8, 16, 32, 10], 0)     0.098672\n",
      "14   ([64, 64, 32, 8, 16, 32, 10], 0.5)     0.096754\n",
      "15  ([64, 64, 32, 8, 16, 32, 10], 0.25)     0.099726\n",
      "16  ([64, 64, 32, 8, 16, 32, 10], 0.75)     0.099580\n",
      "17     ([64, 64, 32, 8, 16, 32, 10], 1)     0.093065\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "digits = datasets.load_digits(return_X_y=True)\n",
    "digits_df_X = digits[0]\n",
    "digits_df_y = digits[1]\n",
    "N = len(digits_df_X)\n",
    "\n",
    "# Convert digits_df_X and digits_df_y to DataFrames\n",
    "digits_df_X_df = pd.DataFrame(digits_df_X)\n",
    "digits_df_y_df = pd.DataFrame(digits_df_y)\n",
    "\n",
    "\n",
    "digit_to_show = np.random.choice(range(N), 1)[0]\n",
    "print(\"Attributes: \", digits_df_X[digit_to_show], \"type: \", type(digits_df_X[digit_to_show]))\n",
    "print(\"Class/Target:\", digits_df_y[digit_to_show], \"type: \", type(digits_df_y[digit_to_show]))\n",
    "\n",
    "\n",
    "# Normalize data\n",
    "y_resized = digits_df_y.reshape(-1, 1)\n",
    "\n",
    "# Initialize the OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Fit and transform the target variable\n",
    "y_encoded = encoder.fit_transform(y_resized)\n",
    "\n",
    "\n",
    "# Define model architectures and regularization parameters\n",
    "architectures = [   \n",
    "    [digits_df_X.shape[1], 64, 32, 16, 8, y_encoded.shape[1]],\n",
    "    [digits_df_X.shape[1], 64, 32, 16, 8, 4, y_encoded.shape[1]],\n",
    "    [digits_df_X.shape[1], 64, 32, 8, 16, 32, y_encoded.shape[1]]\n",
    "]\n",
    "\n",
    "regularization_params = [0.1, 0, 0.5, 0.25, 0.75, 1]  # Example regularization parameters\n",
    "\n",
    "# Initialize lists to store results\n",
    "results_accuracy = {}\n",
    "results_f1_score = {}\n",
    "results_J_cost = {}\n",
    "\n",
    "# Perform stratified k-fold cross-validation\n",
    "results_accuracy, results_f1_score, results_J_cost = NeuralNetwork.k_fold_cross_validation(digits_df_X, y_encoded, architectures, regularization_params, learning_rate=0.01, max_iterations=1000, epsilon=0.005)\n",
    "\n",
    "# Convert the results into a DataFrame for tabular representation\n",
    "accuracy_df = pd.DataFrame(list(results_accuracy.items()), columns=['Architecture, Lambda', 'Mean Accuracy'])\n",
    "f1_score_df = pd.DataFrame(list(results_f1_score.items()), columns=['Architecture, Lambda', 'Mean F1 Score'])\n",
    "J_cost_df = pd.DataFrame(list(results_J_cost.items()), columns=['Architecture, Lambda', 'Mean J Cost'])\n",
    "\n",
    "print(\"Mean Accuracy Results for Handwritten DATASET:\")\n",
    "print(accuracy_df)\n",
    "print(\"\\nMean F1 Score Results for Handwritten DATASET:\")\n",
    "print(f1_score_df)\n",
    "print(\"\\nMean J cost Results for Handwritten DATASET:\")\n",
    "print(J_cost_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "109cc433",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Architecture, Lambda</th>\n",
       "      <th>Mean Accuracy</th>\n",
       "      <th>Mean F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>([64, 64, 32, 16, 8, 10], 0.1)</td>\n",
       "      <td>0.888268</td>\n",
       "      <td>0.914913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>([64, 64, 32, 16, 8, 10], 0)</td>\n",
       "      <td>0.871508</td>\n",
       "      <td>0.892660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>([64, 64, 32, 16, 8, 10], 0.5)</td>\n",
       "      <td>0.833520</td>\n",
       "      <td>0.876381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>([64, 64, 32, 16, 8, 10], 0.25)</td>\n",
       "      <td>0.906145</td>\n",
       "      <td>0.927359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>([64, 64, 32, 16, 8, 10], 0.75)</td>\n",
       "      <td>0.866480</td>\n",
       "      <td>0.896271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>([64, 64, 32, 16, 8, 10], 1)</td>\n",
       "      <td>0.891620</td>\n",
       "      <td>0.918446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>([64, 64, 32, 16, 8, 4, 10], 0.1)</td>\n",
       "      <td>0.749721</td>\n",
       "      <td>0.803028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>([64, 64, 32, 16, 8, 4, 10], 0)</td>\n",
       "      <td>0.770950</td>\n",
       "      <td>0.818864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>([64, 64, 32, 16, 8, 4, 10], 0.5)</td>\n",
       "      <td>0.679888</td>\n",
       "      <td>0.745634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>([64, 64, 32, 16, 8, 4, 10], 0.25)</td>\n",
       "      <td>0.718994</td>\n",
       "      <td>0.789429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>([64, 64, 32, 16, 8, 4, 10], 0.75)</td>\n",
       "      <td>0.674302</td>\n",
       "      <td>0.756945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>([64, 64, 32, 16, 8, 4, 10], 1)</td>\n",
       "      <td>0.613408</td>\n",
       "      <td>0.675600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>([64, 64, 32, 8, 16, 32, 10], 0.1)</td>\n",
       "      <td>0.079330</td>\n",
       "      <td>0.074190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>([64, 64, 32, 8, 16, 32, 10], 0)</td>\n",
       "      <td>0.040782</td>\n",
       "      <td>0.018367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>([64, 64, 32, 8, 16, 32, 10], 0.5)</td>\n",
       "      <td>0.065363</td>\n",
       "      <td>0.070265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>([64, 64, 32, 8, 16, 32, 10], 0.25)</td>\n",
       "      <td>0.033520</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>([64, 64, 32, 8, 16, 32, 10], 0.75)</td>\n",
       "      <td>0.033520</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>([64, 64, 32, 8, 16, 32, 10], 1)</td>\n",
       "      <td>0.092737</td>\n",
       "      <td>0.100551</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Architecture, Lambda  Mean Accuracy  Mean F1 Score\n",
       "0        ([64, 64, 32, 16, 8, 10], 0.1)       0.888268       0.914913\n",
       "1          ([64, 64, 32, 16, 8, 10], 0)       0.871508       0.892660\n",
       "2        ([64, 64, 32, 16, 8, 10], 0.5)       0.833520       0.876381\n",
       "3       ([64, 64, 32, 16, 8, 10], 0.25)       0.906145       0.927359\n",
       "4       ([64, 64, 32, 16, 8, 10], 0.75)       0.866480       0.896271\n",
       "5          ([64, 64, 32, 16, 8, 10], 1)       0.891620       0.918446\n",
       "6     ([64, 64, 32, 16, 8, 4, 10], 0.1)       0.749721       0.803028\n",
       "7       ([64, 64, 32, 16, 8, 4, 10], 0)       0.770950       0.818864\n",
       "8     ([64, 64, 32, 16, 8, 4, 10], 0.5)       0.679888       0.745634\n",
       "9    ([64, 64, 32, 16, 8, 4, 10], 0.25)       0.718994       0.789429\n",
       "10   ([64, 64, 32, 16, 8, 4, 10], 0.75)       0.674302       0.756945\n",
       "11      ([64, 64, 32, 16, 8, 4, 10], 1)       0.613408       0.675600\n",
       "12   ([64, 64, 32, 8, 16, 32, 10], 0.1)       0.079330       0.074190\n",
       "13     ([64, 64, 32, 8, 16, 32, 10], 0)       0.040782       0.018367\n",
       "14   ([64, 64, 32, 8, 16, 32, 10], 0.5)       0.065363       0.070265\n",
       "15  ([64, 64, 32, 8, 16, 32, 10], 0.25)       0.033520       0.000000\n",
       "16  ([64, 64, 32, 8, 16, 32, 10], 0.75)       0.033520       0.000000\n",
       "17     ([64, 64, 32, 8, 16, 32, 10], 1)       0.092737       0.100551"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge accuracy and f1_score DataFrames on 'Architecture, Lambda'\n",
    "merged_df = pd.merge(accuracy_df, f1_score_df, on='Architecture, Lambda')\n",
    "\n",
    "# Merge the merged DataFrame with J_cost_df on 'Architecture, Lambda'\n",
    "final_df = pd.merge(merged_df, J_cost_df, on='Architecture, Lambda')\n",
    "\n",
    "# Rename columns for clarity\n",
    "merged_df.columns = ['Architecture, Lambda', 'Mean Accuracy', 'Mean F1 Score']\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57b936d",
   "metadata": {},
   "source": [
    "## TO BE RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "edadf100",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Function to generate mini-batches\n",
    "def generate_mini_batches(X, y, batch_size):\n",
    "    num_samples = X.shape[0]\n",
    "    num_batches = num_samples // batch_size\n",
    "    mini_batches = []\n",
    "    shuffled_indices = np.random.permutation(num_samples)\n",
    "    X_shuffled = X[shuffled_indices]\n",
    "    y_shuffled = y[shuffled_indices]\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        mini_batches.append((X_shuffled[start_idx:end_idx], y_shuffled[start_idx:end_idx]))\n",
    "    if num_samples % batch_size != 0:\n",
    "        mini_batches.append((X_shuffled[num_batches*batch_size:], y_shuffled[num_batches*batch_size:]))\n",
    "    return mini_batches\n",
    "\n",
    "\n",
    "def train_mini_batch(X_train, y_train, model, learning_rate, batch_size, max_iterations, epsilon):\n",
    "    training_errors = []\n",
    "    for iteration in range(max_iterations):\n",
    "        mini_batches = generate_mini_batches(X_train, y_train, batch_size)\n",
    "        for mini_batch in mini_batches:\n",
    "            X_mini_batch, y_mini_batch = mini_batch\n",
    "            J = model.train(X_mini_batch, y_mini_batch, learning_rate=learning_rate, lam=0.25, max_iterations=1000, epsilon=epsilon)\n",
    "        training_cost = np.mean(np.square(model.forward_pass(X_train)[-1] - y_train))  # Compute training cost\n",
    "        training_errors.append(training_cost)\n",
    "        #print(f\"Iteration {iteration+1}, Training Cost: {training_cost}\")\n",
    "#         # Check for convergence\n",
    "#         if training_cost < epsilon:\n",
    "#             print(f\"Converged at training cost :{training_cost} while Epsilon:{epsilon} \")\n",
    "#             break\n",
    "    return training_errors\n",
    "\n",
    "# Plot learning curve\n",
    "def plot_learning_curve(training_errors, step_size):\n",
    "    iterations = range(1, len(training_errors) + 1)\n",
    "    plt.plot(iterations, training_errors, label='Training Error')\n",
    "    plt.title('Learning Curve')\n",
    "    plt.xlabel('Number of Training Samples')\n",
    "    plt.ylabel('J values')\n",
    "    plt.xticks(np.arange(1, len(training_errors) + 1, step=step_size))\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812823a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bm/l2zpyjxn2gqg_1d20xgx5z0w0000gn/T/ipykernel_67060/3733387565.py:16: RuntimeWarning: invalid value encountered in true_divide\n",
      "  X_normalized = (digits_df_X - mean) / std\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_normalized:  [[        nan -0.33501649 -0.04308102 ... -1.14664746 -0.5056698\n",
      "  -0.19600752]\n",
      " [        nan -0.33501649 -1.09493684 ...  0.54856067 -0.5056698\n",
      "  -0.19600752]\n",
      " [        nan -0.33501649 -1.09493684 ...  1.56568555  1.6951369\n",
      "  -0.19600752]\n",
      " ...\n",
      " [        nan -0.33501649 -0.88456568 ... -0.12952258 -0.5056698\n",
      "  -0.19600752]\n",
      " [        nan -0.33501649 -0.67419451 ...  0.8876023  -0.5056698\n",
      "  -0.19600752]\n",
      " [        nan -0.33501649  1.00877481 ...  0.8876023  -0.26113572\n",
      "  -0.19600752]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define your neural network model and parameters\n",
    "model = NeuralNetwork([digits_df_X.shape[1], 64, 32, 16, 8, y_encoded.shape[1]])  # Your desired architecture\n",
    "learning_rate = 0.01\n",
    "batch_size = 200\n",
    "max_iterations = 1000\n",
    "epsilon = 0.005\n",
    "lam=0.25\n",
    "\n",
    "# Standardize features\n",
    "mean = np.mean(digits_df_X, axis=0)\n",
    "std = np.std(digits_df_X, axis=0)\n",
    "X_normalized = (digits_df_X - mean) / std\n",
    "\n",
    "print(\"X_normalized: \",X_normalized)\n",
    "\n",
    "# Train the model using mini-batch gradient descent\n",
    "training_errors = train_mini_batch(X_normalized, y_encoded, model, learning_rate, batch_size, max_iterations, epsilon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5c990e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curve\n",
    "step_size = 50\n",
    "plot_learning_curve(training_errors, step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473c6907",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
