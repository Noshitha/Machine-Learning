{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4487b3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy Results:\n",
      "      Architecture, Lambda  Mean Accuracy\n",
      "0      ([22, 2, 2], 0.001)       0.884211\n",
      "1       ([22, 2, 2], 0.01)       0.884211\n",
      "2        ([22, 2, 2], 0.1)       0.873684\n",
      "3          ([22, 2, 2], 0)       0.868421\n",
      "4      ([22, 4, 2], 0.001)       0.873684\n",
      "5       ([22, 4, 2], 0.01)       0.863158\n",
      "6        ([22, 4, 2], 0.1)       0.863158\n",
      "7          ([22, 4, 2], 0)       0.852632\n",
      "8      ([22, 8, 2], 0.001)       0.889474\n",
      "9       ([22, 8, 2], 0.01)       0.905263\n",
      "10       ([22, 8, 2], 0.1)       0.921053\n",
      "11         ([22, 8, 2], 0)       0.900000\n",
      "12  ([22, 2, 2, 2], 0.001)       0.873684\n",
      "13   ([22, 2, 2, 2], 0.01)       0.857895\n",
      "14    ([22, 2, 2, 2], 0.1)       0.868421\n",
      "15      ([22, 2, 2, 2], 0)       0.857895\n",
      "16  ([22, 4, 4, 2], 0.001)       0.868421\n",
      "17   ([22, 4, 4, 2], 0.01)       0.878947\n",
      "18    ([22, 4, 4, 2], 0.1)       0.868421\n",
      "19      ([22, 4, 4, 2], 0)       0.921053\n",
      "20  ([22, 4, 2, 2], 0.001)       0.847368\n",
      "21   ([22, 4, 2, 2], 0.01)       0.884211\n",
      "22    ([22, 4, 2, 2], 0.1)       0.821053\n",
      "23      ([22, 4, 2, 2], 0)       0.857895\n",
      "\n",
      "Mean F1 Score Results:\n",
      "      Architecture, Lambda  Mean F1 Score\n",
      "0      ([22, 2, 2], 0.001)       0.884211\n",
      "1       ([22, 2, 2], 0.01)       0.884211\n",
      "2        ([22, 2, 2], 0.1)       0.873684\n",
      "3          ([22, 2, 2], 0)       0.868421\n",
      "4      ([22, 4, 2], 0.001)       0.878794\n",
      "5       ([22, 4, 2], 0.01)       0.863158\n",
      "6        ([22, 4, 2], 0.1)       0.865992\n",
      "7          ([22, 4, 2], 0)       0.854623\n",
      "8      ([22, 8, 2], 0.001)       0.898246\n",
      "9       ([22, 8, 2], 0.01)       0.905263\n",
      "10       ([22, 8, 2], 0.1)       0.928304\n",
      "11         ([22, 8, 2], 0)       0.901991\n",
      "12  ([22, 2, 2, 2], 0.001)       0.873684\n",
      "13   ([22, 2, 2, 2], 0.01)       0.857895\n",
      "14    ([22, 2, 2, 2], 0.1)       0.868421\n",
      "15      ([22, 2, 2, 2], 0)       0.857895\n",
      "16  ([22, 4, 4, 2], 0.001)       0.871390\n",
      "17   ([22, 4, 4, 2], 0.01)       0.878947\n",
      "18    ([22, 4, 4, 2], 0.1)       0.871525\n",
      "19      ([22, 4, 4, 2], 0)       0.923329\n",
      "20  ([22, 4, 2, 2], 0.001)       0.847368\n",
      "21   ([22, 4, 2, 2], 0.01)       0.884211\n",
      "22    ([22, 4, 2, 2], 0.1)       0.821053\n",
      "23      ([22, 4, 2, 2], 0)       0.857895\n",
      "\n",
      "Mean J cost Results:\n",
      "      Architecture, Lambda  Mean J Cost\n",
      "0      ([22, 2, 2], 0.001)     0.059732\n",
      "1       ([22, 2, 2], 0.01)     0.059535\n",
      "2        ([22, 2, 2], 0.1)     0.061523\n",
      "3          ([22, 2, 2], 0)     0.063220\n",
      "4      ([22, 4, 2], 0.001)     0.044443\n",
      "5       ([22, 4, 2], 0.01)     0.045838\n",
      "6        ([22, 4, 2], 0.1)     0.046291\n",
      "7          ([22, 4, 2], 0)     0.047179\n",
      "8      ([22, 8, 2], 0.001)     0.026745\n",
      "9       ([22, 8, 2], 0.01)     0.027051\n",
      "10       ([22, 8, 2], 0.1)     0.028087\n",
      "11         ([22, 8, 2], 0)     0.026886\n",
      "12  ([22, 2, 2, 2], 0.001)     0.062347\n",
      "13   ([22, 2, 2, 2], 0.01)     0.056335\n",
      "14    ([22, 2, 2, 2], 0.1)     0.057186\n",
      "15      ([22, 2, 2, 2], 0)     0.060707\n",
      "16  ([22, 4, 4, 2], 0.001)     0.031476\n",
      "17   ([22, 4, 4, 2], 0.01)     0.030741\n",
      "18    ([22, 4, 4, 2], 0.1)     0.028534\n",
      "19      ([22, 4, 4, 2], 0)     0.028317\n",
      "20  ([22, 4, 2, 2], 0.001)     0.042983\n",
      "21   ([22, 4, 2, 2], 0.01)     0.037894\n",
      "22    ([22, 4, 2, 2], 0.1)     0.034598\n",
      "23      ([22, 4, 2, 2], 0)     0.043982\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        self.weights = [np.random.randn(layers[i], layers[i+1]) for i in range(len(layers)-1)]\n",
    "        self.biases = [np.zeros((1, layers[i+1])) for i in range(len(layers)-1)]\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    def forward_pass(self, X):\n",
    "        activations = [X]\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(activations[-1], w) + b\n",
    "            activations.append(self.sigmoid(z))\n",
    "        return activations\n",
    "    \n",
    "    def backward_pass(self, X, Y, activations):\n",
    "        deltas = [(activations[-1] - Y) * self.sigmoid_derivative(activations[-1])]\n",
    "        for i in range(len(self.layers) - 2, 0, -1):\n",
    "            delta = np.dot(deltas[0], self.weights[i].T) * self.sigmoid_derivative(activations[i])\n",
    "            deltas.insert(0, delta)\n",
    "        return deltas\n",
    "    \n",
    "    \n",
    "    def compute_gradients(self, activations, deltas):\n",
    "        gradients_weights = [np.dot(activations[i].T, deltas[i]) for i in range(len(self.layers) - 1)]\n",
    "        gradients_biases = [np.sum(deltas[i], axis=0) for i in range(len(self.layers) - 1)]\n",
    "        return gradients_weights, gradients_biases\n",
    "    \n",
    "    def update_weights(self, gradients_weights, gradients_biases, learning_rate):\n",
    "        self.weights = [w - learning_rate * gw for w, gw in zip(self.weights, gradients_weights)]\n",
    "        self.biases = [b - learning_rate * gb for b, gb in zip(self.biases, gradients_biases)]\n",
    "    \n",
    "    def train(self, X, Y, learning_rate, lam, max_iterations, epsilon):\n",
    "        for iteration in range(max_iterations):\n",
    "            activations = self.forward_pass(X)\n",
    "            deltas = self.backward_pass(X, Y, activations)\n",
    "            gradients_weights, gradients_biases = self.compute_gradients(activations, deltas)\n",
    "            self.update_weights(gradients_weights, gradients_biases, learning_rate)\n",
    "            # Compute cost function\n",
    "            J = np.mean(np.square(activations[-1] - Y))\n",
    "            #print(f\"Iteration {iteration+1}, Cost: {J}\")\n",
    "            # Check for convergence\n",
    "            if J < epsilon:\n",
    "                #print(f\"Converged at cost :{J} while Epsilon:{epsilon} \")\n",
    "                return J\n",
    "        return J\n",
    "            \n",
    "\n",
    "    def accuracy(self, y_true, y_pred):\n",
    "        correct = np.sum(np.all(y_true == y_pred, axis=1))\n",
    "        return correct / len(y_true)\n",
    "\n",
    "\n",
    "    def f1_score(self, y_true, y_pred):\n",
    "        tp = np.sum(np.logical_and(y_true, y_pred))\n",
    "        fp = np.sum(np.logical_and(np.logical_not(y_true), y_pred))\n",
    "        fn = np.sum(np.logical_and(y_true, np.logical_not(y_pred)))\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        return f1\n",
    "\n",
    "\n",
    "    def evaluate(self, X_test, y_test, J):\n",
    "        activations = self.forward_pass(X_test)[-1]\n",
    "        y_pred = (activations > 0.5).astype(int)\n",
    "        acc = self.accuracy(y_test, y_pred)\n",
    "        f1 = self.f1_score(y_test, y_pred)\n",
    "        return J, acc, f1\n",
    "    \n",
    "    def k_fold_cross_validation(X, y, architectures, regularization_params, learning_rate, max_iterations, epsilon):\n",
    "        results_accuracy = {}\n",
    "        results_f1_score = {}\n",
    "        results_J_cost = {}\n",
    "        \n",
    "        num_splits = 10\n",
    "        fold_size = len(X) // num_splits\n",
    "\n",
    "        for arch in architectures:\n",
    "            for lam in regularization_params:\n",
    "                accuracy_list = []\n",
    "                f1_score_list = []\n",
    "                J_list = []\n",
    "                \n",
    "                for i in range(num_splits):\n",
    "                    start = i * fold_size\n",
    "                    end = (i + 1) * fold_size\n",
    "                    \n",
    "                    X_train = pd.concat([X[:start], X[end:]])\n",
    "                    y_train = np.concatenate([y[:start], y[end:]])\n",
    "                    X_test = X[start:end]\n",
    "                    y_test = y[start:end]\n",
    "\n",
    "                    mean = np.mean(X_train, axis=0)\n",
    "                    std = np.std(X_train, axis=0)\n",
    "                    X_train_normalized = (X_train - mean) / std\n",
    "                    X_test_normalized = (X_test - mean) / std\n",
    "\n",
    "                    model = NeuralNetwork(arch)\n",
    "                    J = model.train(X_train_normalized, y_train, learning_rate=learning_rate, lam=lam, max_iterations=max_iterations, epsilon=epsilon)\n",
    "                    J, accuracy, f1_score = model.evaluate(X_test_normalized, y_test, J)\n",
    "                    accuracy_list.append(accuracy)\n",
    "                    f1_score_list.append(f1_score)\n",
    "                    J_list.append(J)\n",
    "\n",
    "                mean_accuracy = np.mean(accuracy_list)\n",
    "                mean_f1_score = np.mean(f1_score_list)\n",
    "                mean_J_cost   = np.mean(J_list)\n",
    "\n",
    "                results_accuracy[(str(arch), lam)] = mean_accuracy\n",
    "                results_f1_score[(str(arch), lam)] = mean_f1_score\n",
    "                results_J_cost[(str(arch), lam)] = mean_J_cost\n",
    "\n",
    "        return results_accuracy, results_f1_score, results_J_cost\n",
    "\n",
    "\n",
    "df_parkinsons = pd.read_csv(\"/Users/noshitha/Downloads/final_project/parkinsons.csv\", delimiter=\",\")\n",
    "\n",
    "# Shuffle the dataset\n",
    "df_parkinsons_shuffle = shuffle(df_parkinsons)\n",
    "\n",
    "# Split the dataset into features and target variable\n",
    "X =  pd.get_dummies(df_parkinsons_shuffle.drop(columns=['Diagnosis']))  \n",
    "y = df_parkinsons_shuffle['Diagnosis'] \n",
    "\n",
    "# Normalize data\n",
    "y_resized = y.values.reshape(-1, 1)\n",
    "\n",
    "# Initialize the OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Fit and transform the target variable\n",
    "y_encoded = encoder.fit_transform(y_resized)\n",
    "\n",
    "# Define model architectures and regularization parameters\n",
    "architectures = [\n",
    "    [X.shape[1], 2, y_encoded.shape[1]],\n",
    "    [X.shape[1], 4, y_encoded.shape[1]], \n",
    "    [X.shape[1], 8, y_encoded.shape[1]],  \n",
    "    [X.shape[1], 2, 2, y_encoded.shape[1]],  \n",
    "    [X.shape[1], 4, 4, y_encoded.shape[1]],  \n",
    "    [X.shape[1], 4, 2, y_encoded.shape[1]]\n",
    "]\n",
    "\n",
    "regularization_params = [0.001, 0.01, 0.1, 0]  # Example regularization parameters\n",
    "\n",
    "# Initialize lists to store results\n",
    "results_accuracy = {}\n",
    "results_f1_score = {}\n",
    "results_J_cost = {}\n",
    "\n",
    "# Perform stratified k-fold cross-validation\n",
    "results_accuracy, results_f1_score, results_J_cost = NeuralNetwork.k_fold_cross_validation(X, y_encoded, architectures, regularization_params, learning_rate=0.01, max_iterations=1000, epsilon=0.005)\n",
    "\n",
    "# Convert the results into a DataFrame for tabular representation\n",
    "accuracy_df = pd.DataFrame(list(results_accuracy.items()), columns=['Architecture, Lambda', 'Mean Accuracy'])\n",
    "f1_score_df = pd.DataFrame(list(results_f1_score.items()), columns=['Architecture, Lambda', 'Mean F1 Score'])\n",
    "J_cost_df = pd.DataFrame(list(results_J_cost.items()), columns=['Architecture, Lambda', 'Mean J Cost'])\n",
    "\n",
    "print(\"Mean Accuracy Results:\")\n",
    "print(accuracy_df)\n",
    "print(\"\\nMean F1 Score Results:\")\n",
    "print(f1_score_df)\n",
    "print(\"\\nMean J cost Results:\")\n",
    "print(J_cost_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325d1b90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "109cc433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Architecture, Lambda</th>\n",
       "      <th>Mean Accuracy</th>\n",
       "      <th>Mean F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>([22, 2, 2], 0.001)</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.884211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>([22, 2, 2], 0.01)</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.884211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>([22, 2, 2], 0.1)</td>\n",
       "      <td>0.873684</td>\n",
       "      <td>0.873684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>([22, 2, 2], 0)</td>\n",
       "      <td>0.868421</td>\n",
       "      <td>0.868421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>([22, 4, 2], 0.001)</td>\n",
       "      <td>0.873684</td>\n",
       "      <td>0.878794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>([22, 4, 2], 0.01)</td>\n",
       "      <td>0.863158</td>\n",
       "      <td>0.863158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>([22, 4, 2], 0.1)</td>\n",
       "      <td>0.863158</td>\n",
       "      <td>0.865992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>([22, 4, 2], 0)</td>\n",
       "      <td>0.852632</td>\n",
       "      <td>0.854623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>([22, 8, 2], 0.001)</td>\n",
       "      <td>0.889474</td>\n",
       "      <td>0.898246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>([22, 8, 2], 0.01)</td>\n",
       "      <td>0.905263</td>\n",
       "      <td>0.905263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>([22, 8, 2], 0.1)</td>\n",
       "      <td>0.921053</td>\n",
       "      <td>0.928304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>([22, 8, 2], 0)</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.901991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>([22, 2, 2, 2], 0.001)</td>\n",
       "      <td>0.873684</td>\n",
       "      <td>0.873684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>([22, 2, 2, 2], 0.01)</td>\n",
       "      <td>0.857895</td>\n",
       "      <td>0.857895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>([22, 2, 2, 2], 0.1)</td>\n",
       "      <td>0.868421</td>\n",
       "      <td>0.868421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>([22, 2, 2, 2], 0)</td>\n",
       "      <td>0.857895</td>\n",
       "      <td>0.857895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>([22, 4, 4, 2], 0.001)</td>\n",
       "      <td>0.868421</td>\n",
       "      <td>0.871390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>([22, 4, 4, 2], 0.01)</td>\n",
       "      <td>0.878947</td>\n",
       "      <td>0.878947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>([22, 4, 4, 2], 0.1)</td>\n",
       "      <td>0.868421</td>\n",
       "      <td>0.871525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>([22, 4, 4, 2], 0)</td>\n",
       "      <td>0.921053</td>\n",
       "      <td>0.923329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>([22, 4, 2, 2], 0.001)</td>\n",
       "      <td>0.847368</td>\n",
       "      <td>0.847368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>([22, 4, 2, 2], 0.01)</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.884211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>([22, 4, 2, 2], 0.1)</td>\n",
       "      <td>0.821053</td>\n",
       "      <td>0.821053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>([22, 4, 2, 2], 0)</td>\n",
       "      <td>0.857895</td>\n",
       "      <td>0.857895</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Architecture, Lambda  Mean Accuracy  Mean F1 Score\n",
       "0      ([22, 2, 2], 0.001)       0.884211       0.884211\n",
       "1       ([22, 2, 2], 0.01)       0.884211       0.884211\n",
       "2        ([22, 2, 2], 0.1)       0.873684       0.873684\n",
       "3          ([22, 2, 2], 0)       0.868421       0.868421\n",
       "4      ([22, 4, 2], 0.001)       0.873684       0.878794\n",
       "5       ([22, 4, 2], 0.01)       0.863158       0.863158\n",
       "6        ([22, 4, 2], 0.1)       0.863158       0.865992\n",
       "7          ([22, 4, 2], 0)       0.852632       0.854623\n",
       "8      ([22, 8, 2], 0.001)       0.889474       0.898246\n",
       "9       ([22, 8, 2], 0.01)       0.905263       0.905263\n",
       "10       ([22, 8, 2], 0.1)       0.921053       0.928304\n",
       "11         ([22, 8, 2], 0)       0.900000       0.901991\n",
       "12  ([22, 2, 2, 2], 0.001)       0.873684       0.873684\n",
       "13   ([22, 2, 2, 2], 0.01)       0.857895       0.857895\n",
       "14    ([22, 2, 2, 2], 0.1)       0.868421       0.868421\n",
       "15      ([22, 2, 2, 2], 0)       0.857895       0.857895\n",
       "16  ([22, 4, 4, 2], 0.001)       0.868421       0.871390\n",
       "17   ([22, 4, 4, 2], 0.01)       0.878947       0.878947\n",
       "18    ([22, 4, 4, 2], 0.1)       0.868421       0.871525\n",
       "19      ([22, 4, 4, 2], 0)       0.921053       0.923329\n",
       "20  ([22, 4, 2, 2], 0.001)       0.847368       0.847368\n",
       "21   ([22, 4, 2, 2], 0.01)       0.884211       0.884211\n",
       "22    ([22, 4, 2, 2], 0.1)       0.821053       0.821053\n",
       "23      ([22, 4, 2, 2], 0)       0.857895       0.857895"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge accuracy and f1_score DataFrames on 'Architecture, Lambda'\n",
    "merged_df = pd.merge(accuracy_df, f1_score_df, on='Architecture, Lambda')\n",
    "\n",
    "# Merge the merged DataFrame with J_cost_df on 'Architecture, Lambda'\n",
    "final_df = pd.merge(merged_df, J_cost_df, on='Architecture, Lambda')\n",
    "\n",
    "# Rename columns for clarity\n",
    "merged_df.columns = ['Architecture, Lambda', 'Mean Accuracy', 'Mean F1 Score']\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c7ef11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edadf100",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Function to generate mini-batches\n",
    "def generate_mini_batches(X, y, batch_size):\n",
    "    num_samples = X.shape[0]\n",
    "    num_batches = num_samples // batch_size\n",
    "    mini_batches = []\n",
    "    shuffled_indices = np.random.permutation(num_samples)\n",
    "    X_shuffled = X[shuffled_indices]\n",
    "    y_shuffled = y[shuffled_indices]\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        mini_batches.append((X_shuffled[start_idx:end_idx], y_shuffled[start_idx:end_idx]))\n",
    "    if num_samples % batch_size != 0:\n",
    "        mini_batches.append((X_shuffled[num_batches*batch_size:], y_shuffled[num_batches*batch_size:]))\n",
    "    return mini_batches\n",
    "\n",
    "\n",
    "def train_mini_batch(X_train, y_train, X_test, y_test, model, learning_rate, batch_size, max_iterations, epsilon):\n",
    "    training_errors = []\n",
    "    testing_errors = []\n",
    "    for iteration in range(max_iterations):\n",
    "        mini_batches = generate_mini_batches(X_train, y_train, batch_size)\n",
    "        for mini_batch in mini_batches:\n",
    "            X_mini_batch, y_mini_batch = mini_batch\n",
    "            J = model.train(X_mini_batch, y_mini_batch, learning_rate=learning_rate, lam=0.001, max_iterations=1, epsilon=epsilon)\n",
    "        training_cost = np.mean(np.square(model.forward_pass(X_train)[-1] - y_train))  # Compute training cost\n",
    "        testing_cost = np.mean(np.square(model.forward_pass(X_test)[-1] - y_test))  # Compute testing cost\n",
    "        training_errors.append(training_cost)\n",
    "        testing_errors.append(testing_cost)\n",
    "        print(f\"Iteration {iteration+1}, Training Cost: {training_cost}, Testing Cost: {testing_cost}\")\n",
    "        # Check for convergence\n",
    "        if training_cost < epsilon:\n",
    "            #print(f\"Converged at training cost :{training_cost} while Epsilon:{epsilon} \")\n",
    "            break\n",
    "    return training_errors, testing_errors\n",
    "\n",
    "# Plot learning curve\n",
    "def plot_learning_curve(training_errors, testing_errors, step_size):\n",
    "    iterations = range(1, len(training_errors) + 1)\n",
    "    plt.plot(iterations, training_errors, label='Training Error')\n",
    "    plt.plot(iterations, testing_errors, label='Testing Error')\n",
    "    plt.title('Learning Curve')\n",
    "    plt.xlabel('Number of Training Samples')\n",
    "    plt.ylabel('J values')\n",
    "    plt.xticks(np.arange(1, len(training_errors) + 1, step=step_size))\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "812823a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, Training Cost: 0.25843168071946043, Testing Cost: 0.24317968092759298\n",
      "Iteration 2, Training Cost: 0.23840521694139463, Testing Cost: 0.22432386963420736\n",
      "Iteration 3, Training Cost: 0.22214063915438312, Testing Cost: 0.2098488944290311\n",
      "Iteration 4, Training Cost: 0.2092105621338066, Testing Cost: 0.19856893293537073\n",
      "Iteration 5, Training Cost: 0.1983456994827084, Testing Cost: 0.18837954292445805\n",
      "Iteration 6, Training Cost: 0.1883265742598771, Testing Cost: 0.1793527420729884\n",
      "Iteration 7, Training Cost: 0.17878910790711833, Testing Cost: 0.16997905734546354\n",
      "Iteration 8, Training Cost: 0.16984502146519265, Testing Cost: 0.160997658360286\n",
      "Iteration 9, Training Cost: 0.16132802489545836, Testing Cost: 0.15290102207908446\n",
      "Iteration 10, Training Cost: 0.1532767999738913, Testing Cost: 0.14540290868212513\n",
      "Iteration 11, Training Cost: 0.14575069574384764, Testing Cost: 0.1382002131569377\n",
      "Iteration 12, Training Cost: 0.13872581300343795, Testing Cost: 0.13153602430063996\n",
      "Iteration 13, Training Cost: 0.1321466800271214, Testing Cost: 0.1252037028350806\n",
      "Iteration 14, Training Cost: 0.12605793041861854, Testing Cost: 0.11922021270896074\n",
      "Iteration 15, Training Cost: 0.12042883153948346, Testing Cost: 0.11371565740560195\n",
      "Iteration 16, Training Cost: 0.11521132902416069, Testing Cost: 0.10879752026591816\n",
      "Iteration 17, Training Cost: 0.11042146361024031, Testing Cost: 0.10417277120966828\n",
      "Iteration 18, Training Cost: 0.10598841320042107, Testing Cost: 0.10003467745867584\n",
      "Iteration 19, Training Cost: 0.10193657172080009, Testing Cost: 0.09627813732632436\n",
      "Iteration 20, Training Cost: 0.09824142654585313, Testing Cost: 0.09270182477658022\n",
      "Iteration 21, Training Cost: 0.09484255456898177, Testing Cost: 0.08941273595748196\n",
      "Iteration 22, Training Cost: 0.09168215357446206, Testing Cost: 0.08665950254787212\n",
      "Iteration 23, Training Cost: 0.08877298879095932, Testing Cost: 0.08388414778657369\n",
      "Iteration 24, Training Cost: 0.08608915726931506, Testing Cost: 0.08137503909377906\n",
      "Iteration 25, Training Cost: 0.08358419548266577, Testing Cost: 0.07917843762037825\n",
      "Iteration 26, Training Cost: 0.08125671185096267, Testing Cost: 0.0770429784568643\n",
      "Iteration 27, Training Cost: 0.07908292473040786, Testing Cost: 0.07508446315808695\n",
      "Iteration 28, Training Cost: 0.07704257772984323, Testing Cost: 0.07322780554188486\n",
      "Iteration 29, Training Cost: 0.07511904245317894, Testing Cost: 0.07153665443810753\n",
      "Iteration 30, Training Cost: 0.07330709345340539, Testing Cost: 0.06993294062775104\n",
      "Iteration 31, Training Cost: 0.07158683873187009, Testing Cost: 0.06842893120542463\n",
      "Iteration 32, Training Cost: 0.06995324397546225, Testing Cost: 0.06703770677434095\n",
      "Iteration 33, Training Cost: 0.06838970603493859, Testing Cost: 0.0657110789503386\n",
      "Iteration 34, Training Cost: 0.06689025989771297, Testing Cost: 0.06442331570003826\n",
      "Iteration 35, Training Cost: 0.06545411965527621, Testing Cost: 0.06323930241971536\n",
      "Iteration 36, Training Cost: 0.06407474171359735, Testing Cost: 0.06206645461514618\n",
      "Iteration 37, Training Cost: 0.06274362182778134, Testing Cost: 0.060991966117347675\n",
      "Iteration 38, Training Cost: 0.06145640720918837, Testing Cost: 0.06000580465966649\n",
      "Iteration 39, Training Cost: 0.060209742330166065, Testing Cost: 0.0590195908631787\n",
      "Iteration 40, Training Cost: 0.05900748052321427, Testing Cost: 0.058071129508002674\n",
      "Iteration 41, Training Cost: 0.057839710259982566, Testing Cost: 0.05718588633223879\n",
      "Iteration 42, Training Cost: 0.05670287571364731, Testing Cost: 0.056327322189685544\n",
      "Iteration 43, Training Cost: 0.0556004768058135, Testing Cost: 0.055507865894942165\n",
      "Iteration 44, Training Cost: 0.05453371528544925, Testing Cost: 0.05473864227105868\n",
      "Iteration 45, Training Cost: 0.05349236440598326, Testing Cost: 0.053971593969121726\n",
      "Iteration 46, Training Cost: 0.05248358056038587, Testing Cost: 0.05324793838996153\n",
      "Iteration 47, Training Cost: 0.05150317727599068, Testing Cost: 0.05256155430380835\n",
      "Iteration 48, Training Cost: 0.05055159612678338, Testing Cost: 0.05189576334080864\n",
      "Iteration 49, Training Cost: 0.049631000090938855, Testing Cost: 0.051248804986797486\n",
      "Iteration 50, Training Cost: 0.04873401114954604, Testing Cost: 0.05061809812227273\n",
      "Iteration 51, Training Cost: 0.04786785428223849, Testing Cost: 0.050015497937931114\n",
      "Iteration 52, Training Cost: 0.04702894607919747, Testing Cost: 0.049433607316012894\n",
      "Iteration 53, Training Cost: 0.04622025215705462, Testing Cost: 0.048857510376182564\n",
      "Iteration 54, Training Cost: 0.04543693590366641, Testing Cost: 0.04831942667593862\n",
      "Iteration 55, Training Cost: 0.04468291590989333, Testing Cost: 0.04778530988602591\n",
      "Iteration 56, Training Cost: 0.04394857736435966, Testing Cost: 0.0472643978284897\n",
      "Iteration 57, Training Cost: 0.043243112023207034, Testing Cost: 0.04676256939223508\n",
      "Iteration 58, Training Cost: 0.042558243382940945, Testing Cost: 0.04625904458426284\n",
      "Iteration 59, Training Cost: 0.04190417632470469, Testing Cost: 0.04576419371485994\n",
      "Iteration 60, Training Cost: 0.04127438761416719, Testing Cost: 0.045303444590956476\n",
      "Iteration 61, Training Cost: 0.04066158774217511, Testing Cost: 0.04482202388568852\n",
      "Iteration 62, Training Cost: 0.040074074249293476, Testing Cost: 0.04435419291850829\n",
      "Iteration 63, Training Cost: 0.039506952550999924, Testing Cost: 0.043901736626092536\n",
      "Iteration 64, Training Cost: 0.03896034812216863, Testing Cost: 0.04345880773432554\n",
      "Iteration 65, Training Cost: 0.038429348610324536, Testing Cost: 0.0430503868596957\n",
      "Iteration 66, Training Cost: 0.03791881842324014, Testing Cost: 0.04261202608486794\n",
      "Iteration 67, Training Cost: 0.037426124674852845, Testing Cost: 0.04219188674977584\n",
      "Iteration 68, Training Cost: 0.03694928397992058, Testing Cost: 0.04177074597947689\n",
      "Iteration 69, Training Cost: 0.036487825153200634, Testing Cost: 0.04137874187938192\n",
      "Iteration 70, Training Cost: 0.03604101639224283, Testing Cost: 0.041031381478997944\n",
      "Iteration 71, Training Cost: 0.03560773520369972, Testing Cost: 0.0406282067540343\n",
      "Iteration 72, Training Cost: 0.035186455828796974, Testing Cost: 0.04027212669712091\n",
      "Iteration 73, Training Cost: 0.03477732555254067, Testing Cost: 0.039893745723436146\n",
      "Iteration 74, Training Cost: 0.034378766610953404, Testing Cost: 0.039525025928796954\n",
      "Iteration 75, Training Cost: 0.033992784580293964, Testing Cost: 0.03918127163955627\n",
      "Iteration 76, Training Cost: 0.033618364690944355, Testing Cost: 0.03884465822008276\n",
      "Iteration 77, Training Cost: 0.03325313085172937, Testing Cost: 0.03852400569005536\n",
      "Iteration 78, Training Cost: 0.03289479766030448, Testing Cost: 0.038183871370999885\n",
      "Iteration 79, Training Cost: 0.032544980475590506, Testing Cost: 0.03786206911813737\n",
      "Iteration 80, Training Cost: 0.03220332633259597, Testing Cost: 0.03756106519280141\n",
      "Iteration 81, Training Cost: 0.03187136511112193, Testing Cost: 0.037263577505363105\n",
      "Iteration 82, Training Cost: 0.031546316503529924, Testing Cost: 0.03698476708662595\n",
      "Iteration 83, Training Cost: 0.03122800512610532, Testing Cost: 0.036709316320939406\n",
      "Iteration 84, Training Cost: 0.030915018301241517, Testing Cost: 0.0364418518767311\n",
      "Iteration 85, Training Cost: 0.030608135375331302, Testing Cost: 0.03615535259544025\n",
      "Iteration 86, Training Cost: 0.03030661619205933, Testing Cost: 0.03588801460197609\n",
      "Iteration 87, Training Cost: 0.03001148899696919, Testing Cost: 0.03563200910274188\n",
      "Iteration 88, Training Cost: 0.029719260423766162, Testing Cost: 0.03540871645064984\n",
      "Iteration 89, Training Cost: 0.029430921443964014, Testing Cost: 0.035154187895955\n",
      "Iteration 90, Training Cost: 0.02914722030268086, Testing Cost: 0.03491945114171542\n",
      "Iteration 91, Training Cost: 0.02886772694550288, Testing Cost: 0.03471410584342022\n",
      "Iteration 92, Training Cost: 0.028593515661062305, Testing Cost: 0.03450934356424433\n",
      "Iteration 93, Training Cost: 0.028321535055480615, Testing Cost: 0.034277741212737946\n",
      "Iteration 94, Training Cost: 0.0280536533511408, Testing Cost: 0.034069619206220834\n",
      "Iteration 95, Training Cost: 0.027790430643581256, Testing Cost: 0.03387663814109209\n",
      "Iteration 96, Training Cost: 0.02753171427591314, Testing Cost: 0.03369175555886433\n",
      "Iteration 97, Training Cost: 0.02727487275988494, Testing Cost: 0.033484726946542566\n",
      "Iteration 98, Training Cost: 0.027021539017400193, Testing Cost: 0.033279566608143164\n",
      "Iteration 99, Training Cost: 0.02677371493676473, Testing Cost: 0.03305324033524457\n",
      "Iteration 100, Training Cost: 0.026529499735874117, Testing Cost: 0.03289903973852066\n",
      "Iteration 101, Training Cost: 0.026289939613336966, Testing Cost: 0.03270416753879189\n",
      "Iteration 102, Training Cost: 0.026055445126012912, Testing Cost: 0.03256179550839598\n",
      "Iteration 103, Training Cost: 0.02582384034942254, Testing Cost: 0.03238656976684628\n",
      "Iteration 104, Training Cost: 0.025598894820402486, Testing Cost: 0.03219783220467142\n",
      "Iteration 105, Training Cost: 0.02537591507215298, Testing Cost: 0.03204237649047534\n",
      "Iteration 106, Training Cost: 0.025159154049795074, Testing Cost: 0.03188161948779668\n",
      "Iteration 107, Training Cost: 0.02494445218047508, Testing Cost: 0.031768437419314566\n",
      "Iteration 108, Training Cost: 0.024735908253130486, Testing Cost: 0.031634518440692844\n",
      "Iteration 109, Training Cost: 0.024531190215557858, Testing Cost: 0.03148793155269863\n",
      "Iteration 110, Training Cost: 0.024333024066659225, Testing Cost: 0.03131810483398621\n",
      "Iteration 111, Training Cost: 0.024138021333885786, Testing Cost: 0.031199873002244152\n",
      "Iteration 112, Training Cost: 0.023947828865100097, Testing Cost: 0.031058102779209514\n",
      "Iteration 113, Training Cost: 0.023761577526033414, Testing Cost: 0.030916675071534182\n",
      "Iteration 114, Training Cost: 0.02357865429772171, Testing Cost: 0.030791329264672585\n",
      "Iteration 115, Training Cost: 0.023399541769694183, Testing Cost: 0.03069702563657779\n",
      "Iteration 116, Training Cost: 0.023225485267428923, Testing Cost: 0.030576920958986946\n",
      "Iteration 117, Training Cost: 0.023055019528717177, Testing Cost: 0.03048104818821667\n",
      "Iteration 118, Training Cost: 0.022889172087274413, Testing Cost: 0.030396786215948796\n",
      "Iteration 119, Training Cost: 0.022727395101242335, Testing Cost: 0.03030438697855566\n",
      "Iteration 120, Training Cost: 0.022567166313395472, Testing Cost: 0.030197684908975488\n",
      "Iteration 121, Training Cost: 0.022409108177425866, Testing Cost: 0.03005999317939679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 122, Training Cost: 0.02225473817978348, Testing Cost: 0.029944063370829638\n",
      "Iteration 123, Training Cost: 0.022104293483165362, Testing Cost: 0.02982981042322156\n",
      "Iteration 124, Training Cost: 0.02195545775201175, Testing Cost: 0.029743294446668473\n",
      "Iteration 125, Training Cost: 0.021809568759145193, Testing Cost: 0.029672308301632807\n",
      "Iteration 126, Training Cost: 0.021665496297905776, Testing Cost: 0.029560036119964533\n",
      "Iteration 127, Training Cost: 0.021524194106689754, Testing Cost: 0.029490879621914032\n",
      "Iteration 128, Training Cost: 0.02138551265557282, Testing Cost: 0.02936640089029564\n",
      "Iteration 129, Training Cost: 0.021249424696191076, Testing Cost: 0.029269630210069706\n",
      "Iteration 130, Training Cost: 0.02111532296686972, Testing Cost: 0.029174813156358677\n",
      "Iteration 131, Training Cost: 0.020983070516811055, Testing Cost: 0.029083531954878606\n",
      "Iteration 132, Training Cost: 0.020849341415310335, Testing Cost: 0.029060174406250444\n",
      "Iteration 133, Training Cost: 0.020719728794933542, Testing Cost: 0.029008862442170526\n",
      "Iteration 134, Training Cost: 0.02059227850328438, Testing Cost: 0.028955905326289832\n",
      "Iteration 135, Training Cost: 0.020465997460197366, Testing Cost: 0.028889592557368157\n",
      "Iteration 136, Training Cost: 0.020342374534936786, Testing Cost: 0.028848425017573957\n",
      "Iteration 137, Training Cost: 0.020218916271430243, Testing Cost: 0.028768759919608713\n",
      "Iteration 138, Training Cost: 0.020096482976902447, Testing Cost: 0.02869349578667743\n",
      "Iteration 139, Training Cost: 0.01997660051951872, Testing Cost: 0.02863711574798823\n",
      "Iteration 140, Training Cost: 0.019857210369192618, Testing Cost: 0.028548229430710694\n",
      "Iteration 141, Training Cost: 0.019739743106956115, Testing Cost: 0.02849546404505997\n",
      "Iteration 142, Training Cost: 0.01962308151250437, Testing Cost: 0.028413504340759472\n",
      "Iteration 143, Training Cost: 0.019507814877267493, Testing Cost: 0.028360948928013836\n",
      "Iteration 144, Training Cost: 0.01939364999620562, Testing Cost: 0.028307805284264152\n",
      "Iteration 145, Training Cost: 0.01928031186514202, Testing Cost: 0.02827289338167375\n",
      "Iteration 146, Training Cost: 0.019168527616183584, Testing Cost: 0.0282296650472491\n",
      "Iteration 147, Training Cost: 0.01905818667073251, Testing Cost: 0.028192676768942306\n",
      "Iteration 148, Training Cost: 0.01894824229175964, Testing Cost: 0.02815209871111642\n",
      "Iteration 149, Training Cost: 0.01883883870844542, Testing Cost: 0.02805384506631863\n",
      "Iteration 150, Training Cost: 0.018730659790187582, Testing Cost: 0.028010645317421864\n",
      "Iteration 151, Training Cost: 0.01862309834306995, Testing Cost: 0.027969945012849648\n",
      "Iteration 152, Training Cost: 0.01851687208784597, Testing Cost: 0.02793214373152876\n",
      "Iteration 153, Training Cost: 0.018411121337196298, Testing Cost: 0.02790757254242418\n",
      "Iteration 154, Training Cost: 0.01830660922180939, Testing Cost: 0.027882737667048883\n",
      "Iteration 155, Training Cost: 0.018202192963145847, Testing Cost: 0.027830821553048005\n",
      "Iteration 156, Training Cost: 0.01809878414879884, Testing Cost: 0.02775755738034039\n",
      "Iteration 157, Training Cost: 0.01799593198630757, Testing Cost: 0.027732398758037794\n",
      "Iteration 158, Training Cost: 0.017894075272828457, Testing Cost: 0.02766200253742401\n",
      "Iteration 159, Training Cost: 0.01779282844006941, Testing Cost: 0.027666874756626582\n",
      "Iteration 160, Training Cost: 0.017692317273597415, Testing Cost: 0.027618094971132892\n",
      "Iteration 161, Training Cost: 0.01759246968967761, Testing Cost: 0.02758032782267469\n",
      "Iteration 162, Training Cost: 0.01749328491552921, Testing Cost: 0.027555478833119526\n",
      "Iteration 163, Training Cost: 0.017394875891428848, Testing Cost: 0.027492535689499692\n",
      "Iteration 164, Training Cost: 0.017296659774004765, Testing Cost: 0.027503227657239487\n",
      "Iteration 165, Training Cost: 0.01719957339481141, Testing Cost: 0.02744609753911103\n",
      "Iteration 166, Training Cost: 0.01710284763292395, Testing Cost: 0.027389596041872636\n",
      "Iteration 167, Training Cost: 0.017008091007597488, Testing Cost: 0.02732841623427784\n",
      "Iteration 168, Training Cost: 0.016912044516975075, Testing Cost: 0.027309398786966515\n",
      "Iteration 169, Training Cost: 0.016816082008741916, Testing Cost: 0.02731510877358164\n",
      "Iteration 170, Training Cost: 0.0167211231302689, Testing Cost: 0.027295048515841514\n",
      "Iteration 171, Training Cost: 0.01662729094702372, Testing Cost: 0.02727022035057555\n",
      "Iteration 172, Training Cost: 0.016533489964158425, Testing Cost: 0.02723884368580722\n",
      "Iteration 173, Training Cost: 0.016439853479351563, Testing Cost: 0.02722298998842411\n",
      "Iteration 174, Training Cost: 0.01634793815907265, Testing Cost: 0.027173151260727394\n",
      "Iteration 175, Training Cost: 0.01625418651633343, Testing Cost: 0.027178245294020107\n",
      "Iteration 176, Training Cost: 0.01616154882476553, Testing Cost: 0.02718326020350092\n",
      "Iteration 177, Training Cost: 0.01606993094061593, Testing Cost: 0.027141397276306694\n",
      "Iteration 178, Training Cost: 0.015978847640259784, Testing Cost: 0.027113552825628814\n",
      "Iteration 179, Training Cost: 0.015888124095699674, Testing Cost: 0.027077514524846663\n",
      "Iteration 180, Training Cost: 0.015796629211776637, Testing Cost: 0.02709641639960004\n",
      "Iteration 181, Training Cost: 0.015706264415610415, Testing Cost: 0.02713892734677771\n",
      "Iteration 182, Training Cost: 0.015615717596877318, Testing Cost: 0.02709760515338591\n",
      "Iteration 183, Training Cost: 0.015526015403901694, Testing Cost: 0.02707007156910468\n",
      "Iteration 184, Training Cost: 0.015436669185710229, Testing Cost: 0.02707042468119266\n",
      "Iteration 185, Training Cost: 0.015348078014104958, Testing Cost: 0.027038445048542525\n",
      "Iteration 186, Training Cost: 0.015259830531126613, Testing Cost: 0.027031361244922166\n",
      "Iteration 187, Training Cost: 0.015171327230222097, Testing Cost: 0.02700873249081531\n",
      "Iteration 188, Training Cost: 0.01508379834764668, Testing Cost: 0.02695702957504147\n",
      "Iteration 189, Training Cost: 0.014995803759144602, Testing Cost: 0.02696827419188232\n",
      "Iteration 190, Training Cost: 0.014908568195648161, Testing Cost: 0.026931966146290356\n",
      "Iteration 191, Training Cost: 0.014821769227269155, Testing Cost: 0.026899147338985314\n",
      "Iteration 192, Training Cost: 0.014734276247718302, Testing Cost: 0.02691662512584603\n",
      "Iteration 193, Training Cost: 0.014647655253544997, Testing Cost: 0.026931168721167496\n",
      "Iteration 194, Training Cost: 0.014561628940694047, Testing Cost: 0.026970869636328245\n",
      "Iteration 195, Training Cost: 0.014475357629065775, Testing Cost: 0.02693819801739113\n",
      "Iteration 196, Training Cost: 0.014389373349589018, Testing Cost: 0.02690771039407851\n",
      "Iteration 197, Training Cost: 0.014303759510912955, Testing Cost: 0.02689041780198884\n",
      "Iteration 198, Training Cost: 0.014219067940796477, Testing Cost: 0.026900716840777664\n",
      "Iteration 199, Training Cost: 0.014135135730334411, Testing Cost: 0.026829628506230937\n",
      "Iteration 200, Training Cost: 0.014051028385889676, Testing Cost: 0.02680625681814354\n",
      "Iteration 201, Training Cost: 0.013965941075775827, Testing Cost: 0.02683507943790053\n",
      "Iteration 202, Training Cost: 0.013881930796884004, Testing Cost: 0.026834055238065962\n",
      "Iteration 203, Training Cost: 0.013798648095870383, Testing Cost: 0.026800081465451753\n",
      "Iteration 204, Training Cost: 0.013714993710430421, Testing Cost: 0.0268082612848806\n",
      "Iteration 205, Training Cost: 0.013632733522373059, Testing Cost: 0.026760574493730746\n",
      "Iteration 206, Training Cost: 0.013549168786769787, Testing Cost: 0.02676637406784212\n",
      "Iteration 207, Training Cost: 0.013466024633234618, Testing Cost: 0.026764429403392596\n",
      "Iteration 208, Training Cost: 0.013382905792921302, Testing Cost: 0.02677034011954306\n",
      "Iteration 209, Training Cost: 0.013300479768571856, Testing Cost: 0.026836371009688286\n",
      "Iteration 210, Training Cost: 0.013217983325524443, Testing Cost: 0.026790065782677536\n",
      "Iteration 211, Training Cost: 0.013136768374259948, Testing Cost: 0.026849912138019315\n",
      "Iteration 212, Training Cost: 0.013055297932089939, Testing Cost: 0.026824730929255386\n",
      "Iteration 213, Training Cost: 0.012973872171663051, Testing Cost: 0.02677375626583572\n",
      "Iteration 214, Training Cost: 0.012893270635723387, Testing Cost: 0.02675547328254148\n",
      "Iteration 215, Training Cost: 0.01281286930876548, Testing Cost: 0.02679714926271681\n",
      "Iteration 216, Training Cost: 0.012732747602551475, Testing Cost: 0.026740231367024157\n",
      "Iteration 217, Training Cost: 0.012653003531406828, Testing Cost: 0.026721561266040302\n",
      "Iteration 218, Training Cost: 0.012573287793603106, Testing Cost: 0.026751784507112596\n",
      "Iteration 219, Training Cost: 0.01249410511725768, Testing Cost: 0.026753917199903724\n",
      "Iteration 220, Training Cost: 0.01241564737173532, Testing Cost: 0.02678813632967039\n",
      "Iteration 221, Training Cost: 0.01233899607437297, Testing Cost: 0.02682592049927287\n",
      "Iteration 222, Training Cost: 0.012260454063191444, Testing Cost: 0.02681244216173264\n",
      "Iteration 223, Training Cost: 0.012181925331185126, Testing Cost: 0.02675730925782184\n",
      "Iteration 224, Training Cost: 0.012104685344189603, Testing Cost: 0.026759183481541188\n",
      "Iteration 225, Training Cost: 0.012027729064419332, Testing Cost: 0.02672752736905459\n",
      "Iteration 226, Training Cost: 0.01195158158038712, Testing Cost: 0.026714926180355564\n",
      "Iteration 227, Training Cost: 0.011875788430705551, Testing Cost: 0.026684374692711933\n",
      "Iteration 228, Training Cost: 0.011800293778670643, Testing Cost: 0.026691508901256766\n",
      "Iteration 229, Training Cost: 0.011724923093104883, Testing Cost: 0.026712321952024717\n",
      "Iteration 230, Training Cost: 0.011650332400656901, Testing Cost: 0.026725451789527843\n",
      "Iteration 231, Training Cost: 0.011575940957988116, Testing Cost: 0.026700522083844106\n",
      "Iteration 232, Training Cost: 0.011502291178227018, Testing Cost: 0.026701539071659546\n",
      "Iteration 233, Training Cost: 0.011429197624523766, Testing Cost: 0.026666853229128012\n",
      "Iteration 234, Training Cost: 0.011356882263016621, Testing Cost: 0.026629104100899165\n",
      "Iteration 235, Training Cost: 0.011283429583197514, Testing Cost: 0.026696312872114034\n",
      "Iteration 236, Training Cost: 0.01121142834244437, Testing Cost: 0.026674607754622298\n",
      "Iteration 237, Training Cost: 0.011140243521930182, Testing Cost: 0.026646198935840117\n",
      "Iteration 238, Training Cost: 0.01107110815556094, Testing Cost: 0.026589365975074622\n",
      "Iteration 239, Training Cost: 0.011000284437070728, Testing Cost: 0.026599611194538023\n",
      "Iteration 240, Training Cost: 0.01093153717764872, Testing Cost: 0.026578487356723437\n",
      "Iteration 241, Training Cost: 0.010861075071746657, Testing Cost: 0.026596027916501324\n",
      "Iteration 242, Training Cost: 0.01079198391871369, Testing Cost: 0.026600416973184444\n",
      "Iteration 243, Training Cost: 0.010722553897738994, Testing Cost: 0.02665566800528113\n",
      "Iteration 244, Training Cost: 0.010654347607673323, Testing Cost: 0.02663277856707942\n",
      "Iteration 245, Training Cost: 0.010586985244897044, Testing Cost: 0.026625865612264765\n",
      "Iteration 246, Training Cost: 0.010521067913823587, Testing Cost: 0.026582605657434637\n",
      "Iteration 247, Training Cost: 0.010454005547522368, Testing Cost: 0.02662345906935528\n",
      "Iteration 248, Training Cost: 0.010388275353672356, Testing Cost: 0.026614668037563894\n",
      "Iteration 249, Training Cost: 0.010323121533422387, Testing Cost: 0.02665775899866835\n",
      "Iteration 250, Training Cost: 0.010259150612024842, Testing Cost: 0.026687008961395326\n",
      "Iteration 251, Training Cost: 0.010195555110665685, Testing Cost: 0.026699498294433004\n",
      "Iteration 252, Training Cost: 0.010130435463872805, Testing Cost: 0.026598821516555095\n",
      "Iteration 253, Training Cost: 0.010067275423709306, Testing Cost: 0.02659336135724088\n",
      "Iteration 254, Training Cost: 0.010004369619159854, Testing Cost: 0.026590314114453208\n",
      "Iteration 255, Training Cost: 0.009941915251579597, Testing Cost: 0.02662230315995831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 256, Training Cost: 0.009879997441328961, Testing Cost: 0.026609372496973104\n",
      "Iteration 257, Training Cost: 0.009818668927118081, Testing Cost: 0.02660250627844179\n",
      "Iteration 258, Training Cost: 0.009758169370744944, Testing Cost: 0.026568376829078266\n",
      "Iteration 259, Training Cost: 0.009697668538966373, Testing Cost: 0.026573000829663882\n",
      "Iteration 260, Training Cost: 0.009637718206536108, Testing Cost: 0.026591331081163815\n",
      "Iteration 261, Training Cost: 0.009578340911226016, Testing Cost: 0.026604255207377962\n",
      "Iteration 262, Training Cost: 0.009519536016256984, Testing Cost: 0.02661590505615217\n",
      "Iteration 263, Training Cost: 0.009461380584255188, Testing Cost: 0.026518299137888997\n",
      "Iteration 264, Training Cost: 0.00940267598316132, Testing Cost: 0.026540633051077206\n",
      "Iteration 265, Training Cost: 0.009345137248413626, Testing Cost: 0.02653447151538776\n",
      "Iteration 266, Training Cost: 0.009287805861853154, Testing Cost: 0.02653506123824009\n",
      "Iteration 267, Training Cost: 0.009230759777049678, Testing Cost: 0.02653579466356578\n",
      "Iteration 268, Training Cost: 0.009174341725086327, Testing Cost: 0.02657191755764641\n",
      "Iteration 269, Training Cost: 0.00911993255989299, Testing Cost: 0.02662043834051949\n",
      "Iteration 270, Training Cost: 0.00906347955406839, Testing Cost: 0.02659856336667845\n",
      "Iteration 271, Training Cost: 0.009009129217054128, Testing Cost: 0.02661130550874661\n",
      "Iteration 272, Training Cost: 0.008955212564811656, Testing Cost: 0.02662561163722341\n",
      "Iteration 273, Training Cost: 0.008899190667898434, Testing Cost: 0.026589759072467002\n",
      "Iteration 274, Training Cost: 0.00884477884952121, Testing Cost: 0.026562660170085666\n",
      "Iteration 275, Training Cost: 0.008790891120563525, Testing Cost: 0.026520831356914285\n",
      "Iteration 276, Training Cost: 0.00873788038584905, Testing Cost: 0.02654038623055531\n",
      "Iteration 277, Training Cost: 0.008685408635806567, Testing Cost: 0.026495294132713034\n",
      "Iteration 278, Training Cost: 0.00863407869861391, Testing Cost: 0.026454758496031787\n",
      "Iteration 279, Training Cost: 0.00858085126383352, Testing Cost: 0.026495757485256205\n",
      "Iteration 280, Training Cost: 0.008530570105490003, Testing Cost: 0.02658788906864873\n",
      "Iteration 281, Training Cost: 0.008478323817015701, Testing Cost: 0.026558583543844223\n",
      "Iteration 282, Training Cost: 0.00842744725142501, Testing Cost: 0.02655116335790463\n",
      "Iteration 283, Training Cost: 0.008376782876787475, Testing Cost: 0.026539384075743314\n",
      "Iteration 284, Training Cost: 0.008326780834658295, Testing Cost: 0.02648816272798492\n",
      "Iteration 285, Training Cost: 0.008276785095996456, Testing Cost: 0.026522304758254685\n",
      "Iteration 286, Training Cost: 0.008228192753887961, Testing Cost: 0.026562349860154905\n",
      "Iteration 287, Training Cost: 0.008179998967577979, Testing Cost: 0.026580384964529677\n",
      "Iteration 288, Training Cost: 0.008129872648465234, Testing Cost: 0.026529519204711863\n",
      "Iteration 289, Training Cost: 0.008081990974506688, Testing Cost: 0.026546364397928682\n",
      "Iteration 290, Training Cost: 0.008034039612458445, Testing Cost: 0.026545157628217544\n",
      "Iteration 291, Training Cost: 0.007986218620852931, Testing Cost: 0.02649520926993783\n",
      "Iteration 292, Training Cost: 0.007938906094535066, Testing Cost: 0.02652505772229121\n",
      "Iteration 293, Training Cost: 0.00789187315467776, Testing Cost: 0.02649939570556446\n",
      "Iteration 294, Training Cost: 0.00784566820290959, Testing Cost: 0.02647426941070001\n",
      "Iteration 295, Training Cost: 0.00779922495836072, Testing Cost: 0.026487469587740113\n",
      "Iteration 296, Training Cost: 0.007753132500707156, Testing Cost: 0.02651354865729436\n",
      "Iteration 297, Training Cost: 0.0077077355886572526, Testing Cost: 0.026518100959654994\n",
      "Iteration 298, Training Cost: 0.007662473620662601, Testing Cost: 0.02650181321782416\n",
      "Iteration 299, Training Cost: 0.007617730078982628, Testing Cost: 0.026490881009665495\n",
      "Iteration 300, Training Cost: 0.007573364072026275, Testing Cost: 0.026531662900716773\n",
      "Iteration 301, Training Cost: 0.0075303227012476145, Testing Cost: 0.026575550693706075\n",
      "Iteration 302, Training Cost: 0.007485720058960069, Testing Cost: 0.026560039131707437\n",
      "Iteration 303, Training Cost: 0.007442144918001515, Testing Cost: 0.026561918426915694\n",
      "Iteration 304, Training Cost: 0.007397824824742358, Testing Cost: 0.02651094146049956\n",
      "Iteration 305, Training Cost: 0.007354942457997398, Testing Cost: 0.02652722540332997\n",
      "Iteration 306, Training Cost: 0.007312693869788102, Testing Cost: 0.02654281750443737\n",
      "Iteration 307, Training Cost: 0.00726986121269173, Testing Cost: 0.02654458608107687\n",
      "Iteration 308, Training Cost: 0.007227478063169374, Testing Cost: 0.026520495117861108\n",
      "Iteration 309, Training Cost: 0.007185440146472639, Testing Cost: 0.026523925381479862\n",
      "Iteration 310, Training Cost: 0.00714418553108793, Testing Cost: 0.026540387156983373\n",
      "Iteration 311, Training Cost: 0.007102961529085614, Testing Cost: 0.026551939955482145\n",
      "Iteration 312, Training Cost: 0.00706243326445272, Testing Cost: 0.026568304661384528\n",
      "Iteration 313, Training Cost: 0.007021217833736691, Testing Cost: 0.0265419991523342\n",
      "Iteration 314, Training Cost: 0.006980804157854923, Testing Cost: 0.026540461083351976\n",
      "Iteration 315, Training Cost: 0.006940708535145558, Testing Cost: 0.02654861393082166\n",
      "Iteration 316, Training Cost: 0.006900903590427806, Testing Cost: 0.02653642537285219\n",
      "Iteration 317, Training Cost: 0.006861304092929382, Testing Cost: 0.026537469513383125\n",
      "Iteration 318, Training Cost: 0.006822279767394787, Testing Cost: 0.026563410556155898\n",
      "Iteration 319, Training Cost: 0.006783384615152747, Testing Cost: 0.026570824781485963\n",
      "Iteration 320, Training Cost: 0.006744894288929313, Testing Cost: 0.02658592362677585\n",
      "Iteration 321, Training Cost: 0.006706039507682831, Testing Cost: 0.0265712548845346\n",
      "Iteration 322, Training Cost: 0.006667752423969257, Testing Cost: 0.026556993256004108\n",
      "Iteration 323, Training Cost: 0.006629841304436754, Testing Cost: 0.026556341661673335\n",
      "Iteration 324, Training Cost: 0.006593214635879935, Testing Cost: 0.026524267156263784\n",
      "Iteration 325, Training Cost: 0.006555122881109913, Testing Cost: 0.026550647388223186\n",
      "Iteration 326, Training Cost: 0.006517733598521767, Testing Cost: 0.02657202936290705\n",
      "Iteration 327, Training Cost: 0.006480848986271992, Testing Cost: 0.026571321862673514\n",
      "Iteration 328, Training Cost: 0.006444338706836033, Testing Cost: 0.02660537037000346\n",
      "Iteration 329, Training Cost: 0.006407921366108596, Testing Cost: 0.026610497172157747\n",
      "Iteration 330, Training Cost: 0.006371558821833054, Testing Cost: 0.02658985669246909\n",
      "Iteration 331, Training Cost: 0.006335786205453389, Testing Cost: 0.026617481923164373\n",
      "Iteration 332, Training Cost: 0.0063002247253078, Testing Cost: 0.02662784827934788\n",
      "Iteration 333, Training Cost: 0.006264683223174011, Testing Cost: 0.026602112508380057\n",
      "Iteration 334, Training Cost: 0.00622950637048926, Testing Cost: 0.0266186837388109\n",
      "Iteration 335, Training Cost: 0.006194750654579257, Testing Cost: 0.026629435034878354\n",
      "Iteration 336, Training Cost: 0.006160032544543358, Testing Cost: 0.026626636548733342\n",
      "Iteration 337, Training Cost: 0.006125687322147986, Testing Cost: 0.02664175928035333\n",
      "Iteration 338, Training Cost: 0.006091447494866681, Testing Cost: 0.026645882969209566\n",
      "Iteration 339, Training Cost: 0.0060572781262260445, Testing Cost: 0.026641320269908564\n",
      "Iteration 340, Training Cost: 0.006023628230055902, Testing Cost: 0.02663120605844096\n",
      "Iteration 341, Training Cost: 0.005990422552719551, Testing Cost: 0.026626483261347582\n",
      "Iteration 342, Training Cost: 0.005957131220000771, Testing Cost: 0.026638428702485697\n",
      "Iteration 343, Training Cost: 0.005924347418793411, Testing Cost: 0.026626740204986343\n",
      "Iteration 344, Training Cost: 0.005891531714090539, Testing Cost: 0.02664005430339446\n",
      "Iteration 345, Training Cost: 0.005858830431077319, Testing Cost: 0.02665213168143984\n",
      "Iteration 346, Training Cost: 0.005827404390739932, Testing Cost: 0.02663007520721265\n",
      "Iteration 347, Training Cost: 0.005794065826542697, Testing Cost: 0.026666257781904238\n",
      "Iteration 348, Training Cost: 0.005762070683729762, Testing Cost: 0.026699044266902557\n",
      "Iteration 349, Training Cost: 0.005730398705414715, Testing Cost: 0.026682461548642313\n",
      "Iteration 350, Training Cost: 0.005699085499582266, Testing Cost: 0.026681595993770235\n",
      "Iteration 351, Training Cost: 0.005667640255996754, Testing Cost: 0.02671150430895985\n",
      "Iteration 352, Training Cost: 0.005637685765799193, Testing Cost: 0.026758839578172468\n",
      "Iteration 353, Training Cost: 0.005606510163082557, Testing Cost: 0.02676349884783854\n",
      "Iteration 354, Training Cost: 0.00557665729502874, Testing Cost: 0.0267818866172172\n",
      "Iteration 355, Training Cost: 0.005545369107625695, Testing Cost: 0.026771448968112423\n",
      "Iteration 356, Training Cost: 0.0055150287746603055, Testing Cost: 0.026774848123969516\n",
      "Iteration 357, Training Cost: 0.005484694729338528, Testing Cost: 0.026776115919834205\n",
      "Iteration 358, Training Cost: 0.005454693101035635, Testing Cost: 0.02677651719232026\n",
      "Iteration 359, Training Cost: 0.005425251469207486, Testing Cost: 0.026794197642912153\n",
      "Iteration 360, Training Cost: 0.005395383682733951, Testing Cost: 0.026776607982401245\n",
      "Iteration 361, Training Cost: 0.005366074226604855, Testing Cost: 0.02678284122736463\n",
      "Iteration 362, Training Cost: 0.005336933311913255, Testing Cost: 0.02679066313857348\n",
      "Iteration 363, Training Cost: 0.005308062621404168, Testing Cost: 0.02680458153945409\n",
      "Iteration 364, Training Cost: 0.005279493870074516, Testing Cost: 0.026820194332024365\n",
      "Iteration 365, Training Cost: 0.005250777226330584, Testing Cost: 0.02680964832585183\n",
      "Iteration 366, Training Cost: 0.005222606054290643, Testing Cost: 0.026807773998502923\n",
      "Iteration 367, Training Cost: 0.005194409847915123, Testing Cost: 0.026827584355856686\n",
      "Iteration 368, Training Cost: 0.005166939631133986, Testing Cost: 0.026805155528327712\n",
      "Iteration 369, Training Cost: 0.005138980982844458, Testing Cost: 0.026824697510426578\n",
      "Iteration 370, Training Cost: 0.005111716507858706, Testing Cost: 0.026873200056397966\n",
      "Iteration 371, Training Cost: 0.005083983085500451, Testing Cost: 0.026866585975674546\n",
      "Iteration 372, Training Cost: 0.005056795758495846, Testing Cost: 0.026859770783612673\n",
      "Iteration 373, Training Cost: 0.005030045494840045, Testing Cost: 0.0268967523440732\n",
      "Iteration 374, Training Cost: 0.005003788595053626, Testing Cost: 0.02691651071663291\n",
      "Iteration 375, Training Cost: 0.004976260274900358, Testing Cost: 0.02688817054779962\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define your neural network model and parameters\n",
    "model = NeuralNetwork([X_house_votes.shape[1], 10, 8, y_encoded.shape[1]])  # Your desired architecture\n",
    "learning_rate = 0.01\n",
    "batch_size = 32\n",
    "max_iterations = 1000\n",
    "epsilon = 0.005\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_house_votes, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "mean = np.mean(X_train, axis=0)\n",
    "std = np.std(X_train, axis=0)\n",
    "X_train_normalized = (X_train - mean) / std\n",
    "X_test_normalized = (X_test - mean) / std\n",
    "\n",
    "# Train the model using mini-batch gradient descent\n",
    "training_errors, testing_errors = train_mini_batch(X_train_normalized.to_numpy(), y_train, X_test_normalized.to_numpy(), y_test, model, learning_rate, batch_size, max_iterations, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a5c990e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the learning curve\n",
    "step_size = 50\n",
    "plot_learning_curve(training_errors, testing_errors, step_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
