{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abf87141",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine, fetch_openml\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Wine Dataset\n",
    "wine = pd.read_csv('/Users/noshitha/Downloads/hw4/datasets/hw3_wine.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ae820d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wine = pd.read_csv(\"/Users/noshitha/Downloads/hw3/datasets/hw3_wine.csv\", delimiter=\"\\t\")\n",
    "\n",
    "X_wine = df_wine.iloc[:, 1:]\n",
    "y_wine = df_wine.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d463802",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wine = pd.read_csv(\"/Users/noshitha/Downloads/hw3/datasets/hw3_wine.csv\", delimiter=\"\\t\")\n",
    "\n",
    "X_wine = df_wine.iloc[:, 1:]\n",
    "y_wine = df_wine.iloc[:, 0]\n",
    "# Convert categorical attributes to numerical inputs using one-hot encoding\n",
    "X_wine = pd.get_dummies(df_wine.iloc[:, 1:])  # Exclude the target column\n",
    "y_wine = df_wine.iloc[:, 0]\n",
    "X_wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5de8a690",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       1     2     3     4    5     6     7     8     9    10    11    12  \\\n",
       "0  14.23  1.71  2.43  15.6  127  2.80  3.06  0.28  2.29  5.64  1.04  3.92   \n",
       "1  13.20  1.78  2.14  11.2  100  2.65  2.76  0.26  1.28  4.38  1.05  3.40   \n",
       "2  13.16  2.36  2.67  18.6  101  2.80  3.24  0.30  2.81  5.68  1.03  3.17   \n",
       "3  14.37  1.95  2.50  16.8  113  3.85  3.49  0.24  2.18  7.80  0.86  3.45   \n",
       "4  13.24  2.59  2.87  21.0  118  2.80  2.69  0.39  1.82  4.32  1.04  2.93   \n",
       "\n",
       "     13  \n",
       "0  1065  \n",
       "1  1050  \n",
       "2  1185  \n",
       "3  1480  \n",
       "4   735  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f8ac5fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    1\n",
       "3    1\n",
       "4    1\n",
       "Name: # class, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f1a46561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X):\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        X = X.values\n",
    "    mean = np.mean(X, axis=0)\n",
    "    std = np.std(X, axis=0)\n",
    "    X_normalized = (X - mean) / std\n",
    "    return X_normalized, mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b16aea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.56685714]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        self.weights = [np.random.rand(layers[i], layers[i+1]) for i in range(len(layers)-1)]\n",
    "        self.biases = [np.zeros((1, layers[i+1])) for i in range(len(layers)-1)]\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Forward pass\n",
    "        activations = X\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(activations, w) + b\n",
    "            activations = self.sigmoid(z)\n",
    "        return activations\n",
    "    \n",
    "    def train(self, X, y, learning_rate, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            activations = X\n",
    "            zs = []\n",
    "            for w, b in zip(self.weights, self.biases):\n",
    "                z = np.dot(activations, w) + b\n",
    "                zs.append(z)\n",
    "                activations = self.sigmoid(z)\n",
    "\n",
    "            # Backward pass\n",
    "            error = y - activations\n",
    "            deltas = [error * self.sigmoid_derivative(activations)]\n",
    "            for i in range(len(self.weights) - 1, 0, -1):\n",
    "                delta = np.dot(deltas[-1], self.weights[i].T) * self.sigmoid_derivative(zs[i-1])\n",
    "                deltas.append(delta)\n",
    "            deltas.reverse()\n",
    "\n",
    "            # Weight update\n",
    "            for i in range(len(self.weights)):\n",
    "                activations = np.atleast_2d(activations)\n",
    "                self.weights[i] += learning_rate * np.dot(activations.T, deltas[i])\n",
    "                self.biases[i] += learning_rate * deltas[i].sum(axis=0, keepdims=True)\n",
    "\n",
    "# Example usage\n",
    "nn = NeuralNetwork([2, 3, 1])  # Network with 2 input, 3 hidden, and 1 output neuron\n",
    "X = np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])  # Sample input data\n",
    "y = np.array([[0.7], [0.9], [0.1]])  # Desired output\n",
    "nn.train(X, y, learning_rate=0.1, epochs=1000)  # Train the network\n",
    "\n",
    "# Make prediction on new data\n",
    "prediction = nn.predict(np.array([[0.8, 0.9]]))\n",
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76aa56ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([0.13, 0.42])\n",
    "y_train = np.array([0.9, 0.23])\n",
    "nn.train(X_train, y_train, learning_rate=0.1, epochs=1000)  # Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526cce80",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_x = nn.predict(np.array([[0.8, 0.9]]))\n",
    "print(prediction_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bb2732",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28b2700",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5284da7c",
   "metadata": {},
   "source": [
    "## VALIDATION -  1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "380138a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid activation function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Forward propagation\n",
    "def forward_propagation(x, theta1, theta2):\n",
    "    a1 = np.concatenate((np.ones((x.shape[0], 1)), x), axis=1)  # Add bias to input layer\n",
    "    z2 = np.dot(a1, theta1.T)\n",
    "    a2 = np.concatenate((np.ones((z2.shape[0], 1)), sigmoid(z2)), axis=1)  # Add bias to hidden layer\n",
    "    z3 = np.dot(a2, theta2.T)\n",
    "    a3 = sigmoid(z3)\n",
    "    return a1, z2, a2, z3, a3\n",
    "\n",
    "# Cost function\n",
    "def compute_cost(y, a3):\n",
    "    m = y.shape[0]\n",
    "    J = -1/m * np.sum(y * np.log(a3) + (1 - y) * np.log(1 - a3))\n",
    "    return J\n",
    "\n",
    "# Backpropagation\n",
    "def backpropagation(x, y, theta1, theta2, a1, z2, a2, z3, a3, lam):\n",
    "    m = y.shape[0]\n",
    "    \n",
    "    # Compute deltas\n",
    "    delta3 = a3 - y\n",
    "    delta2 = np.dot(delta3, theta2) * (a2 * (1 - a2))\n",
    "    delta2 = delta2[:, 1:]  # Remove delta for bias neuron\n",
    "    \n",
    "    # Compute gradients\n",
    "    grad1 = np.dot(delta2.T, a1)\n",
    "    grad2 = np.dot(delta3.T, a2)\n",
    "    \n",
    "    # Regularization (excluding bias terms)\n",
    "    grad1[:, 1:] += (lam / m) * theta1[:, 1:]\n",
    "    grad2[:, 1:] += (lam / m) * theta2[:, 1:]\n",
    "    \n",
    "    # Average gradients\n",
    "    grad1 /= m\n",
    "    grad2 /= m\n",
    "    \n",
    "    return grad1, grad2\n",
    "\n",
    "# Update weights\n",
    "def update_weights(theta1, theta2, grad1, grad2, alpha):\n",
    "    theta1 -= alpha * grad1\n",
    "    theta2 -= alpha * grad2\n",
    "    return theta1, theta2\n",
    "\n",
    "# Function to print required outputs\n",
    "def print_outputs(theta1, theta2, grad1, grad2, a3, y):\n",
    "    print(\"Theta1:\")\n",
    "    print(theta1)\n",
    "    print(\"\\nTheta2:\")\n",
    "    print(theta2)\n",
    "    print(\"\\nRegularized Gradients of Theta1:\")\n",
    "    print(grad1)\n",
    "    print(\"\\nRegularized Gradients of Theta2:\")\n",
    "    print(grad2)\n",
    "    print(\"\\nFinal Prediction of Our Network:\")\n",
    "    print(a3)\n",
    "    print(\"\\nExpected Prediction:\")\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "28146266",
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_1():\n",
    "    # Initialize network architecture\n",
    "    layers = [1, 2, 1]\n",
    "    theta1 = np.array([[0.4, 0.1], [0.3, 0.2]])\n",
    "    theta2 = np.array([[0.7, 0.5, 0.6]])\n",
    "\n",
    "    # Training instances\n",
    "    x = np.array([[0.13], [0.42]])\n",
    "    y = np.array([[0.9], [0.23]])\n",
    "\n",
    "    # Regularization parameter\n",
    "    lam = 0.000\n",
    "\n",
    "    # Hyperparameters\n",
    "    alpha = 0.01  # Learning rate\n",
    "\n",
    "    # Forward propagation\n",
    "    a1, z2, a2, z3, a3 = forward_propagation(x, theta1, theta2)\n",
    "    \n",
    "    print(\"a1:\",a1)\n",
    "    print(\"z2:\",z2)\n",
    "    print(\"a2:\",a2)\n",
    "    print(\"z3:\",z3)\n",
    "    print(\"a3:\",a3)\n",
    "    \n",
    "    # Calculate cost\n",
    "    J = compute_cost(y, a3)\n",
    "    \n",
    "    print(\"Regularization cost: \",J)\n",
    "    \n",
    "    # Backpropagation\n",
    "    grad1, grad2 = backpropagation(x, y, theta1, theta2, a1, z2, a2, z3, a3, lam)\n",
    "\n",
    "    # Update weights\n",
    "    theta1, theta2 = update_weights(theta1, theta2, grad1, grad2, alpha)\n",
    "\n",
    "    # Print outputs\n",
    "    print_outputs(theta1, theta2, grad1, grad2, a3, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a904af58",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def example_2():\n",
    "    # Initialize network architecture\n",
    "    layers = [2, 4, 3, 2]\n",
    "    theta1 = np.array([[0.42, 0.15, 0.4],\n",
    "                       [0.72, 0.1, 0.54],\n",
    "                       [0.01, 0.19, 0.42],\n",
    "                       [0.3, 0.35, 0.68]])\n",
    "    theta2 = np.array([[0.21, 0.67, 0.14, 0.96, 0.87],\n",
    "                       [0.87, 0.42, 0.2, 0.32, 0.89],\n",
    "                       [0.03, 0.56, 0.8, 0.69, 0.09]])\n",
    "    theta3 = np.array([[0.04, 0.87, 0.42, 0.53],\n",
    "                       [0.17, 0.1, 0.95, 0.69]])\n",
    "\n",
    "    # Training instances\n",
    "    x = np.array([[0.32, 0.68], [0.83, 0.02]])\n",
    "    y = np.array([[0.75, 0.98], [0.75, 0.28]])\n",
    "\n",
    "    # Regularization parameter\n",
    "    lam = 0.25\n",
    "\n",
    "    # Hyperparameters\n",
    "    alpha = 0.01  # Learning rate\n",
    "\n",
    "    # Forward propagation\n",
    "    a1, z2, a2, z3, a3, z4, a4 = forward_propagation(x, theta1, theta2, theta3)\n",
    "    \n",
    "    # Calculate cost\n",
    "    J = compute_cost(y, a4)\n",
    "    \n",
    "    # Backpropagation\n",
    "    grad1, grad2, grad3 = backpropagation(x, y, theta1, theta2, theta3, a1, z2, a2, z3, a3, z4, a4, lam)\n",
    "\n",
    "    # Update weights\n",
    "    theta1, theta2, theta3 = update_weights(theta1, theta2, theta3, grad1, grad2, grad3, alpha)\n",
    "\n",
    "    # Print outputs\n",
    "    print_outputs(theta1, theta2, theta3, grad1, grad2, grad3, a4, y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "01bc5a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXAMPLE WITH GIVEN WEIGHTS :\n",
      "a1: [[1.   0.13]\n",
      " [1.   0.42]]\n",
      "z2: [[0.413 0.326]\n",
      " [0.442 0.384]]\n",
      "a2: [[1.         0.601807   0.5807858 ]\n",
      " [1.         0.60873549 0.59483749]]\n",
      "z3: [[1.34937498]\n",
      " [1.36127024]]\n",
      "a3: [[0.79402743]\n",
      " [0.79596607]]\n",
      "Regularization cost:  0.8209757904998143\n",
      "Theta1:\n",
      "[[0.39972649 0.09986671]\n",
      " [0.2996682  0.1998382 ]]\n",
      "\n",
      "Theta2:\n",
      "[[0.69770003 0.49859626 0.59862445]]\n",
      "\n",
      "Regularized Gradients of Theta1:\n",
      "[[0.02735127 0.01332866]\n",
      " [0.03317988 0.01618028]]\n",
      "\n",
      "Regularized Gradients of Theta2:\n",
      "[[0.22999675 0.1403743  0.13755523]]\n",
      "\n",
      "Final Prediction of Our Network:\n",
      "[[0.79402743]\n",
      " [0.79596607]]\n",
      "\n",
      "Expected Prediction:\n",
      "[[0.9 ]\n",
      " [0.23]]\n"
     ]
    }
   ],
   "source": [
    "print(\"EXAMPLE WITH GIVEN WEIGHTS :\")\n",
    "example_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "db714a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXAMPLE WITH RANDOM WEIGHTS :\n",
      "a1: [[1.   0.13]\n",
      " [1.   0.42]]\n",
      "z2: [[0.86333966 0.1051517 ]\n",
      " [1.04952884 0.15202402]]\n",
      "a2: [[1.         0.70335793 0.52626373]\n",
      " [1.         0.74068441 0.53793298]]\n",
      "z3: [[1.32993095]\n",
      " [1.35267414]]\n",
      "a3: [[0.79082921]\n",
      " [0.79456647]]\n",
      "Regularization cost:  0.819592029062117\n",
      "Theta1:\n",
      "[[0.77961582 0.64190253]\n",
      " [0.08413476 0.16162617]]\n",
      "\n",
      "Theta2:\n",
      "[[0.89627721 0.60472216 0.00796582]]\n",
      "\n",
      "Regularized Gradients of Theta1:\n",
      "[[0.02597301 0.01291158]\n",
      " [0.00052015 0.00025476]]\n",
      "\n",
      "Regularized Gradients of Theta2:\n",
      "[[0.22769784 0.17068972 0.12312315]]\n",
      "\n",
      "Final Prediction of Our Network:\n",
      "[[0.79082921]\n",
      " [0.79456647]]\n",
      "\n",
      "Expected Prediction:\n",
      "[[0.9 ]\n",
      " [0.23]]\n"
     ]
    }
   ],
   "source": [
    "def example_1_random():\n",
    "    # Initialize network architecture\n",
    "    layers = [1, 2, 1]\n",
    "    # Randomly initialize weights\n",
    "    theta1 = np.random.rand(layers[1], layers[0] + 1)  # +1 for bias\n",
    "    theta2 = np.random.rand(layers[2], layers[1] + 1)  # +1 for bias\n",
    "\n",
    "    # Training instances\n",
    "    x = np.array([[0.13], [0.42]])\n",
    "    y = np.array([[0.9], [0.23]])\n",
    "\n",
    "    # Regularization parameter\n",
    "    lam = 0.000\n",
    "\n",
    "    # Hyperparameters\n",
    "    alpha = 0.01  # Learning rate\n",
    "\n",
    "    # Forward propagation\n",
    "    a1, z2, a2, z3, a3 = forward_propagation(x, theta1, theta2)\n",
    "    \n",
    "    print(\"a1:\", a1)\n",
    "    print(\"z2:\", z2)\n",
    "    print(\"a2:\", a2)\n",
    "    print(\"z3:\", z3)\n",
    "    print(\"a3:\", a3)\n",
    "    \n",
    "    # Calculate cost\n",
    "    J = compute_cost(y, a3)\n",
    "    \n",
    "    print(\"Regularization cost: \", J)\n",
    "    \n",
    "    # Backpropagation\n",
    "    grad1, grad2 = backpropagation(x, y, theta1, theta2, a1, z2, a2, z3, a3, lam)\n",
    "\n",
    "    # Update weights\n",
    "    theta1, theta2 = update_weights(theta1, theta2, grad1, grad2, alpha)\n",
    "\n",
    "    # Print outputs\n",
    "    print_outputs(theta1, theta2, grad1, grad2, a3, y)\n",
    "\n",
    "print(\"EXAMPLE WITH RANDOM WEIGHTS :\")\n",
    "example_1_random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11038ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"EXAMPLE WIHT GIVEN WEIGHTS : \")\n",
    "example_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b45872e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def example_2_RANDOM():\n",
    "    # Initialize network architecture\n",
    "    layers = [2, 4, 3, 2]\n",
    "    # Randomly initialize weights\n",
    "    theta1 = np.random.rand(layers[1], layers[0] + 1)  # +1 for bias\n",
    "    theta2 = np.random.rand(layers[2], layers[1] + 1)  # +1 for bias\n",
    "    theta3 = np.random.rand(layers[3], layers[2] + 1)  # +1 for bias\n",
    "    \n",
    "#     theta1 = np.array([[0.42, 0.15, 0.4],\n",
    "#                        [0.72, 0.1, 0.54],\n",
    "#                        [0.01, 0.19, 0.42],\n",
    "#                        [0.3, 0.35, 0.68]])\n",
    "#     theta2 = np.array([[0.21, 0.67, 0.14, 0.96, 0.87],\n",
    "#                        [0.87, 0.42, 0.2, 0.32, 0.89],\n",
    "#                        [0.03, 0.56, 0.8, 0.69, 0.09]])\n",
    "#     theta3 = np.array([[0.04, 0.87, 0.42, 0.53],\n",
    "#                        [0.17, 0.1, 0.95, 0.69]])\n",
    "\n",
    "    # Training instances\n",
    "    x = np.array([[0.32, 0.68], [0.83, 0.02]])\n",
    "    y = np.array([[0.75, 0.98], [0.75, 0.28]])\n",
    "\n",
    "    # Regularization parameter\n",
    "    lam = 0.25\n",
    "\n",
    "    # Hyperparameters\n",
    "    alpha = 0.01  # Learning rate\n",
    "\n",
    "    # Forward propagation\n",
    "    a1, z2, a2, z3, a3, z4, a4 = forward_propagation(x, theta1, theta2, theta3)\n",
    "    \n",
    "    # Calculate cost\n",
    "    J = compute_cost(y, a4)\n",
    "    \n",
    "    # Backpropagation\n",
    "    grad1, grad2, grad3 = backpropagation(x, y, theta1, theta2, theta3, a1, z2, a2, z3, a3, z4, a4, lam)\n",
    "\n",
    "    # Update weights\n",
    "    theta1, theta2, theta3 = update_weights(theta1, theta2, theta3, grad1, grad2, grad3, alpha)\n",
    "\n",
    "    # Print outputs\n",
    "    print_outputs(theta1, theta2, theta3, grad1, grad2, grad3, a4, y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062a6ff1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34870230",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "459748c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1:\n",
      "Training instance 1\n",
      "    x: [0.13]\n",
      "    y: [0.9]\n",
      "\n",
      "Forward propagation:\n",
      "    a1: [[1.   0.13]]\n",
      "    z2: [[0.413 0.326]]\n",
      "    a2: [[1.        0.601807  0.5807858]]\n",
      "    z3: [[1.34937498]]\n",
      "    a3: [[0.79402743]]\n",
      "\n",
      "Predicted output: [[0.79402743]]\n",
      "Expected output: [0.9]\n",
      "Cost, J: 0.36557477431084995\n",
      "\n",
      "Training instance 2\n",
      "    x: [0.42]\n",
      "    y: [0.23]\n",
      "\n",
      "Forward propagation:\n",
      "    a1: [[1.   0.42]]\n",
      "    z2: [[0.442 0.384]]\n",
      "    a2: [[1.         0.60873549 0.59483749]]\n",
      "    z3: [[1.36127024]]\n",
      "    a3: [[0.79596607]]\n",
      "\n",
      "Predicted output: [[0.79596607]]\n",
      "Expected output: [0.23]\n",
      "Cost, J: 1.2763768066887786\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, theta1=None, theta2=None):\n",
    "        self.layers = layers\n",
    "        if theta1 is None:\n",
    "            self.theta1 = np.array([[0.4, 0.1], [0.3, 0.2]])  # Initial Theta1\n",
    "        else:\n",
    "            self.theta1 = theta1\n",
    "        if theta2 is None:\n",
    "            self.theta2 = np.array([[0.7, 0.5, 0.6]])  # Initial Theta2\n",
    "        else:\n",
    "            self.theta2 = theta2\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def forward_propagation(self, x):\n",
    "        a1 = np.concatenate((np.ones((x.shape[0], 1)), x), axis=1)  # Add bias to input layer\n",
    "        z2 = np.dot(a1, self.theta1.T)\n",
    "        a2 = np.concatenate((np.ones((z2.shape[0], 1)), self.sigmoid(z2)), axis=1)  # Add bias to hidden layer\n",
    "        z3 = np.dot(a2, self.theta2.T)\n",
    "        a3 = self.sigmoid(z3)\n",
    "        return a1, z2, a2, z3, a3\n",
    "\n",
    "    def compute_cost(self, y, a3):\n",
    "        m = y.shape[0]\n",
    "        J = -1/m * np.sum(y * np.log(a3) + (1 - y) * np.log(1 - a3))\n",
    "        return J\n",
    "\n",
    "    def train(self, X, Y):\n",
    "        for i, (x, y) in enumerate(zip(X, Y), 1):\n",
    "            print(f\"Training instance {i}\")\n",
    "            print(f\"    x: {x}\")\n",
    "            print(f\"    y: {y}\")\n",
    "\n",
    "            # Forward propagation\n",
    "            a1, z2, a2, z3, a3 = self.forward_propagation(x.reshape(1, -1))\n",
    "\n",
    "            # Print forward propagation results\n",
    "            print(\"\\nForward propagation:\")\n",
    "            print(f\"    a1: {a1}\")\n",
    "            print(f\"    z2: {z2}\")\n",
    "            print(f\"    a2: {a2}\")\n",
    "            print(f\"    z3: {z3}\")\n",
    "            print(f\"    a3: {a3}\")\n",
    "\n",
    "            # Compute cost\n",
    "            J = self.compute_cost(y.reshape(1, -1), a3)\n",
    "            print(f\"\\nPredicted output: {a3}\")\n",
    "            print(f\"Expected output: {y}\")\n",
    "            print(f\"Cost, J: {J}\\n\")\n",
    "\n",
    "# Example usage\n",
    "nn_example1 = NeuralNetwork([1, 2, 1])  # Example 1 network with 1 input, 2 hidden, and 1 output neuron\n",
    "nn_example2 = NeuralNetwork([2, 4, 3, 2])  # Example 2 network with 2 input, 4 hidden, 3 hidden, and 2 output neurons\n",
    "\n",
    "X_example1 = np.array([[0.13], [0.42]])  # Example 1 input data\n",
    "Y_example1 = np.array([[0.9], [0.23]])  # Example 1 desired output\n",
    "\n",
    "print(\"Example 1:\")\n",
    "nn_example1.train(X_example1, Y_example1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "53e96940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1:\n",
      "Training instance 1\n",
      "    x: [0.13]\n",
      "    y: [0.9]\n",
      "\n",
      "Forward propagation:\n",
      "    a1: [0.13]\n",
      "    z2: [[0.00344647 0.07615083]]\n",
      "    a2: [0.68343712]\n",
      "\n",
      "Predicted output: [[0.68343712]]\n",
      "Expected output: [0.9]\n",
      "Cost, J, associated with instance 2: 0.45758190338572863\n",
      "Training instance 2\n",
      "    x: [0.42]\n",
      "    y: [0.23]\n",
      "\n",
      "Forward propagation:\n",
      "    a1: [0.42]\n",
      "    z2: [[0.01113475 0.24602574]]\n",
      "    a2: [0.68905122]\n",
      "\n",
      "Predicted output: [[0.68905122]]\n",
      "Expected output: [0.23]\n",
      "Cost, J, associated with instance 2: 0.985118967292997\n",
      "FINAL REG COST : 0.0\n",
      "\n",
      "Example 2:\n",
      "Training instance 1\n",
      "    x: [0.32 0.68]\n",
      "    y: [0.75 0.98]\n",
      "\n",
      "Forward propagation:\n",
      "    a1: [0.32 0.68]\n",
      "    z2: [[0.76441045 0.46840202 0.80025051 0.79023581]]\n",
      "    a2: [0.74723525 0.83655055]\n",
      "\n",
      "Predicted output: [[0.74723525 0.83655055]]\n",
      "Expected output: [0.75 0.98]\n",
      "Cost, J, associated with instance 2: 0.7734794243084668\n",
      "Training instance 2\n",
      "    x: [0.83 0.02]\n",
      "    y: [0.75 0.28]\n",
      "\n",
      "Forward propagation:\n",
      "    a1: [0.83 0.02]\n",
      "    z2: [[0.34101033 0.54165127 0.39957371 0.47096895]]\n",
      "    a2: [0.74238115 0.83164585]\n",
      "\n",
      "Predicted output: [[0.74238115 0.83164585]]\n",
      "Expected output: [0.75 0.28]\n",
      "Cost, J, associated with instance 2: 1.896919043143436\n",
      "FINAL REG COST : 0.8454828202262639\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        self.weights = [np.random.rand(layers[i], layers[i+1]) for i in range(len(layers)-1)]\n",
    "        self.biases = [np.zeros((1, layers[i+1])) for i in range(len(layers)-1)]\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Forward pass\n",
    "        activations = X\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(activations, w) + b\n",
    "            activations = self.sigmoid(z)\n",
    "        return activations\n",
    "    \n",
    "    def train(self, X, Y, lam):\n",
    "        total_cost = 0.0\n",
    "        total_reg_cost = 0.0  # Variable to store the total regularization cost\n",
    "        m = len(X)\n",
    "        for i, (x, y) in enumerate(zip(X, Y), 1):\n",
    "            print(f\"Training instance {i}\")\n",
    "            print(f\"    x: {x}\")\n",
    "            print(f\"    y: {y}\")\n",
    "\n",
    "            # Forward propagation\n",
    "            activations = x\n",
    "            zs = []\n",
    "            for w, b in zip(self.weights, self.biases):\n",
    "                z = np.dot(activations, w) + b\n",
    "                zs.append(z)\n",
    "                activations = self.sigmoid(z)\n",
    "\n",
    "            # Compute cost\n",
    "            prediction = activations\n",
    "            cost = -y * np.log(prediction) - (1 - y) * np.log(1 - prediction)\n",
    "            instance_cost = np.sum(cost)\n",
    "\n",
    "            # Regularization term\n",
    "            reg_term = sum(np.sum(w[:, 1:] ** 2) for w in self.weights)\n",
    "            total_reg_cost += reg_term  # Accumulate regularization cost for this instance\n",
    "\n",
    "            # Update total cost with regularization\n",
    "            total_cost += instance_cost\n",
    "\n",
    "            # Print forward propagation results\n",
    "            print(\"\\nForward propagation:\")\n",
    "            print(f\"    a1: {x}\")\n",
    "            for i, (z, a) in enumerate(zip(zs, activations), 2):\n",
    "                print(f\"    z{i}: {z}\")\n",
    "                print(f\"    a{i}: {a}\")\n",
    "\n",
    "            # Predicted output\n",
    "            prediction = activations\n",
    "            print(f\"\\nPredicted output: {prediction}\")\n",
    "            print(f\"Expected output: {y}\")\n",
    "\n",
    "            # Print cost associated with instance\n",
    "            print(f\"Cost, J, associated with instance {i}: {instance_cost}\")\n",
    "\n",
    "        # Average cost over all training instances\n",
    "        avg_cost = total_cost / m\n",
    "\n",
    "        # Calculate final regularization cost\n",
    "        final_reg_cost = (lam / (2 * m)) * total_reg_cost\n",
    "\n",
    "        print(\"FINAL REG COST :\", final_reg_cost)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "nn_example1 = NeuralNetwork([1, 2, 1])  # Example 1 network with 1 input, 2 hidden, and 1 output neuron\n",
    "nn_example2 = NeuralNetwork([2, 4, 3, 2])  # Example 2 network with 2 input, 4 hidden, 3 hidden, and 2 output neurons\n",
    "\n",
    "X_example1 = np.array([[0.13], [0.42]])  # Example 1 input data\n",
    "Y_example1 = np.array([[0.9], [0.23]])  # Example 1 desired output\n",
    "\n",
    "X_example2 = np.array([[0.32, 0.68], [0.83, 0.02]])  # Example 2 input data\n",
    "Y_example2 = np.array([[0.75, 0.98], [0.75, 0.28]])  # Example 2 desired output\n",
    "\n",
    "print(\"Example 1:\")\n",
    "nn_example1.train(X_example1, Y_example1,lam=0.00)\n",
    "\n",
    "print(\"\\nExample 2:\")\n",
    "nn_example2.train(X_example2, Y_example2,lam=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa74121",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "834e2cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1:\n",
      "Training instance 1\n",
      "    x: [0.13]\n",
      "    y: [0.9]\n",
      "\n",
      "Forward propagation:\n",
      "    a1: [0.13]\n",
      "    z2: [[0.12818014 0.09793916]]\n",
      "    a2: [0.56069023]\n",
      "\n",
      "Predicted output: [[0.56069023]]\n",
      "Expected output: [0.9]\n",
      "Cost, J, associated with instance 2: 0.6029830799846142\n",
      "Training instance 2\n",
      "    x: [0.42]\n",
      "    y: [0.23]\n",
      "\n",
      "Forward propagation:\n",
      "    a1: [0.42]\n",
      "    z2: [[0.41412045 0.31641884]]\n",
      "    a2: [0.56828018]\n",
      "\n",
      "Predicted output: [[0.56828018]]\n",
      "Expected output: [0.23]\n",
      "Cost, J, associated with instance 2: 0.7767657823403614\n",
      "FINAL COST : 1.4506961985779827\n",
      "\n",
      "Example 2:\n",
      "Training instance 1\n",
      "    x: [0.32 0.68]\n",
      "    y: [0.75 0.98]\n",
      "\n",
      "Forward propagation:\n",
      "    a1: [0.32 0.68]\n",
      "    z2: [[0.3243013  0.51367443 0.14347152 0.60872264]]\n",
      "    a2: [0.60992229 0.89938657]\n",
      "\n",
      "Predicted output: [[0.60992229 0.89938657]]\n",
      "Expected output: [0.75 0.98]\n",
      "Cost, J, associated with instance 2: 0.7560209995566727\n",
      "Training instance 2\n",
      "    x: [0.83 0.02]\n",
      "    y: [0.75 0.28]\n",
      "\n",
      "Forward propagation:\n",
      "    a1: [0.83 0.02]\n",
      "    z2: [[0.64725589 0.47332803 0.35233134 0.76164735]]\n",
      "    a2: [0.61112932 0.90161485]\n",
      "\n",
      "Predicted output: [[0.61112932 0.90161485]]\n",
      "Expected output: [0.75 0.28]\n",
      "Cost, J, associated with instance 2: 2.3040441856410006\n",
      "FINAL COST : 3.8877243678902373\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        self.weights = [np.random.rand(layers[i], layers[i+1]) for i in range(len(layers)-1)]\n",
    "        self.biases = [np.zeros((1, layers[i+1])) for i in range(len(layers)-1)]\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Forward pass\n",
    "        activations = X\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(activations, w) + b\n",
    "            activations = self.sigmoid(z)\n",
    "        return activations\n",
    "    \n",
    "    def train(self, X, Y, lam):\n",
    "        total_cost = 0.0\n",
    "        total_reg_cost = 0.0  # Variable to store the total regularization cost\n",
    "        m = len(X)\n",
    "        for i, (x, y) in enumerate(zip(X, Y), 1):\n",
    "            print(f\"Training instance {i}\")\n",
    "            print(f\"    x: {x}\")\n",
    "            print(f\"    y: {y}\")\n",
    "\n",
    "            # Forward propagation\n",
    "            activations = x\n",
    "            zs = []\n",
    "            for w, b in zip(self.weights, self.biases):\n",
    "                z = np.dot(activations, w) + b\n",
    "                zs.append(z)\n",
    "                activations = self.sigmoid(z)\n",
    "\n",
    "            # Compute cost\n",
    "            prediction = activations\n",
    "            cost = -y * np.log(prediction) - (1 - y) * np.log(1 - prediction)\n",
    "            instance_cost = np.sum(cost)\n",
    "\n",
    "            # Regularization term\n",
    "            reg_term = sum(np.sum(w[:, 1:] ** 2) for w in self.weights)\n",
    "            total_reg_cost += reg_term  # Accumulate regularization cost for this instance\n",
    "\n",
    "            # Update total cost with regularization\n",
    "            total_cost += instance_cost\n",
    "\n",
    "            # Print forward propagation results\n",
    "            print(\"\\nForward propagation:\")\n",
    "            print(f\"    a1: {x}\")\n",
    "            for i, (z, a) in enumerate(zip(zs, activations), 2):\n",
    "                print(f\"    z{i}: {z}\")\n",
    "                print(f\"    a{i}: {a}\")\n",
    "\n",
    "            # Predicted output\n",
    "            prediction = activations\n",
    "            print(f\"\\nPredicted output: {prediction}\")\n",
    "            print(f\"Expected output: {y}\")\n",
    "\n",
    "            # Print cost associated with instance\n",
    "            print(f\"Cost, J, associated with instance {i}: {instance_cost}\")\n",
    "\n",
    "        # Average cost over all training instances\n",
    "        avg_cost = total_cost / m\n",
    "\n",
    "        # Calculate final regularization cost\n",
    "        final_reg_cost = (lam / (2 * m)) * total_reg_cost\n",
    "\n",
    "        # Add regularization term to the total cost\n",
    "        total_cost += final_reg_cost\n",
    "\n",
    "        print(\"FINAL COST :\", total_cost)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "nn_example1 = NeuralNetwork([1, 2, 1])  # Example 1 network with 1 input, 2 hidden, and 1 output neuron\n",
    "nn_example2 = NeuralNetwork([2, 4, 3, 2])  # Example 2 network with 2 input, 4 hidden, 3 hidden, and 2 output neurons\n",
    "\n",
    "X_example1 = np.array([[0.13], [0.42]])  # Example 1 input data\n",
    "Y_example1 = np.array([[0.9], [0.23]])  # Example 1 desired output\n",
    "\n",
    "X_example2 = np.array([[0.32, 0.68], [0.83, 0.02]])  # Example 2 input data\n",
    "Y_example2 = np.array([[0.75, 0.98], [0.75, 0.28]])  # Example 2 desired output\n",
    "\n",
    "print(\"Example 1:\")\n",
    "nn_example1.train(X_example1, Y_example1, lam=0.25)  # Using regularization parameter lambda=0.25\n",
    "\n",
    "print(\"\\nExample 2:\")\n",
    "nn_example2.train(X_example2, Y_example2, lam=0.25)  # Using regularization parameter lambda=0.25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03465512",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "20bbc196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1:\n",
      "Iteration 1\n",
      "Total cost: 0.7199456326315257\n",
      "Iteration 2\n",
      "Total cost: 0.7199456326315257\n",
      "Stopping criteria reached: Improvement in cost function less than epsilon.\n",
      "\n",
      "Example 2:\n",
      "Iteration 1\n",
      "Total cost: 2.0859261907417728\n",
      "Iteration 2\n",
      "Total cost: 2.0859261907417728\n",
      "Stopping criteria reached: Improvement in cost function less than epsilon.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        self.weights = [np.random.rand(layers[i], layers[i+1]) for i in range(len(layers)-1)]\n",
    "        self.biases = [np.zeros((1, layers[i+1])) for i in range(len(layers)-1)]\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Forward pass\n",
    "        activations = X\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(activations, w) + b\n",
    "            activations = self.sigmoid(z)\n",
    "        return activations\n",
    "    \n",
    "    def train(self, X, Y, lam, epsilon, max_iterations):\n",
    "        total_cost = 0.0\n",
    "        total_reg_cost = 0.0  # Variable to store the total regularization cost\n",
    "        m = len(X)\n",
    "        prev_cost = float('inf')\n",
    "        iteration = 0\n",
    "        \n",
    "        while True:\n",
    "            iteration += 1\n",
    "            print(f\"Iteration {iteration}\")\n",
    "            for i, (x, y) in enumerate(zip(X, Y), 1):\n",
    "                # Forward propagation\n",
    "                activations = x\n",
    "                zs = []\n",
    "                for w, b in zip(self.weights, self.biases):\n",
    "                    z = np.dot(activations, w) + b\n",
    "                    zs.append(z)\n",
    "                    activations = self.sigmoid(z)\n",
    "\n",
    "                # Compute cost\n",
    "                prediction = activations\n",
    "                cost = -y * np.log(prediction) - (1 - y) * np.log(1 - prediction)\n",
    "                instance_cost = np.sum(cost)\n",
    "\n",
    "                # Regularization term\n",
    "                reg_term = sum(np.sum(w[:, 1:] ** 2) for w in self.weights)\n",
    "                total_reg_cost += reg_term  # Accumulate regularization cost for this instance\n",
    "\n",
    "                # Update total cost with regularization\n",
    "                total_cost += instance_cost\n",
    "            \n",
    "            # Average cost over all training instances\n",
    "            avg_cost = total_cost / m\n",
    "\n",
    "            # Calculate final regularization cost\n",
    "            final_reg_cost = (lam / (2 * m)) * total_reg_cost\n",
    "\n",
    "            # Print total cost\n",
    "            print(f\"Total cost: {avg_cost + final_reg_cost}\")\n",
    "\n",
    "            # Check for improvement in cost function\n",
    "            if abs(prev_cost - (avg_cost + final_reg_cost)) < epsilon:\n",
    "                print(\"Stopping criteria reached: Improvement in cost function less than epsilon.\")\n",
    "                break\n",
    "            \n",
    "            # Check for maximum number of iterations\n",
    "            if iteration >= max_iterations:\n",
    "                print(\"Stopping criteria reached: Maximum number of iterations reached.\")\n",
    "                break\n",
    "            \n",
    "            prev_cost = avg_cost + final_reg_cost\n",
    "            \n",
    "            # Reset total cost and regularization cost for next iteration\n",
    "            total_cost = 0.0\n",
    "            total_reg_cost = 0.0\n",
    "            \n",
    "            # Shuffle training data for next iteration\n",
    "            combined = list(zip(X, Y))\n",
    "            np.random.shuffle(combined)\n",
    "            X, Y = zip(*combined)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "nn_example1 = NeuralNetwork([1, 2, 1])  # Example 1 network with 1 input, 2 hidden, and 1 output neuron\n",
    "nn_example2 = NeuralNetwork([2, 4, 3, 2])  # Example 2 network with 2 input, 4 hidden, 3 hidden, and 2 output neurons\n",
    "\n",
    "X_example1 = np.array([[0.13], [0.42]])  # Example 1 input data\n",
    "Y_example1 = np.array([[0.9], [0.23]])  # Example 1 desired output\n",
    "\n",
    "X_example2 = np.array([[0.32, 0.68], [0.83, 0.02]])  # Example 2 input data\n",
    "Y_example2 = np.array([[0.75, 0.98], [0.75, 0.28]])  # Example 2 desired output\n",
    "\n",
    "print(\"Example 1:\")\n",
    "nn_example1.train(X_example1, Y_example1, lam=0.00, epsilon=0.0001, max_iterations=100)\n",
    "\n",
    "print(\"\\nExample 2:\")\n",
    "nn_example2.train(X_example2, Y_example2, lam=0.25, epsilon=0.0001, max_iterations=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a0f6c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1379929c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "74af5e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_encoded:       1  2  3\n",
      "0    1  0  0\n",
      "1    1  0  0\n",
      "2    1  0  0\n",
      "3    1  0  0\n",
      "4    1  0  0\n",
      "..  .. .. ..\n",
      "173  0  0  1\n",
      "174  0  0  1\n",
      "175  0  0  1\n",
      "176  0  0  1\n",
      "177  0  0  1\n",
      "\n",
      "[178 rows x 3 columns]\n",
      "Iteration 1\n",
      "Total cost: -0.04882722492668656\n",
      "Iteration 2\n",
      "Total cost: -0.04882722492668656\n",
      "Stopping criteria reached: Improvement in cost function less than epsilon.\n",
      "Accuracy: 0.08333333333333333\n",
      "F1 Score: 1.0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791c29be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b200dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071f2c19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087f4a9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a03494b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (1,5) (1,3) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/bm/l2zpyjxn2gqg_1d20xgx5z0w0000gn/T/ipykernel_72260/2621096704.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;31m# Evaluate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/bm/l2zpyjxn2gqg_1d20xgx5z0w0000gn/T/ipykernel_72260/2621096704.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, Y, learning_rate, lam)\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m                 \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid_derivative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlam\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbiases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (1,5) (1,3) "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define function for normalization\n",
    "def normalize(X):\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        X = X.values\n",
    "    mean = np.mean(X, axis=0)\n",
    "    std = np.std(X, axis=0)\n",
    "    X_normalized = (X - mean) / std\n",
    "    return X_normalized, mean, std\n",
    "\n",
    "# Define function to calculate accuracy\n",
    "def accuracy_score(y_true, y_pred):\n",
    "    correct = sum(1 for yt, yp in zip(y_true, y_pred) if yt == yp)\n",
    "    return correct / len(y_true)\n",
    "\n",
    "# Define function to calculate F1 score\n",
    "def f1_score(y_true, y_pred):\n",
    "    tp = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 1 and yp == 1)\n",
    "    fp = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 0 and yp == 1)\n",
    "    fn = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 1 and yp == 0)\n",
    "    precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "    recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "    return 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "\n",
    "# Define function to evaluate model performance\n",
    "def evaluate_model_performance(model, X_train, y_train, X_test, y_test):\n",
    "    model.train(X_train, y_train, learning_rate=0.01, lam=0.01)  # Example: Training the model with lambda=0.01\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    return accuracy, f1\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        self.weights = [np.random.rand(layers[i], layers[i+1]) for i in range(len(layers)-1)]\n",
    "        self.biases = [np.zeros((1, layers[i+1])) for i in range(len(layers)-1)]\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Forward pass\n",
    "        activations = X\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(activations, w) + b\n",
    "            activations = self.sigmoid(z)\n",
    "        return activations\n",
    "    \n",
    "    def train(self, X, Y, learning_rate, lam):\n",
    "        total_cost = 0.0\n",
    "        total_reg_cost = 0.0\n",
    "        m = len(X)\n",
    "        for i, (x, y) in enumerate(zip(X, Y), 1):\n",
    "            activations = x\n",
    "            zs = []\n",
    "            activation_layers = [activations]  # Store activations for all layers\n",
    "            # Forward pass\n",
    "            for w, b in zip(self.weights, self.biases):\n",
    "                z = np.dot(activations, w) + b\n",
    "                zs.append(z)\n",
    "                activations = self.sigmoid(z)\n",
    "                activation_layers.append(activations)\n",
    "\n",
    "            prediction = activations\n",
    "            cost = -y * np.log(prediction) - (1 - y) * np.log(1 - prediction)\n",
    "            instance_cost = np.sum(cost)\n",
    "\n",
    "            reg_term = sum(np.sum(w[:, 1:] ** 2) for w in self.weights)\n",
    "            total_reg_cost += reg_term\n",
    "\n",
    "            total_cost += instance_cost\n",
    "\n",
    "            delta = prediction - y\n",
    "            for i in range(len(self.weights)-1, -1, -1):\n",
    "                delta = np.dot(delta, self.weights[i].T) * self.sigmoid_derivative(activation_layers[i+1])\n",
    "                self.weights[i] -= (learning_rate * np.dot(activation_layers[i].T, delta) + (lam / m) * self.weights[i])\n",
    "                self.biases[i] -= learning_rate * np.sum(delta, axis=0)\n",
    "\n",
    "        avg_cost = total_cost / m\n",
    "        final_reg_cost = (lam / (2 * m)) * total_reg_cost\n",
    "        print(\"FINAL REG COST :\", final_reg_cost)\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "df_wine = pd.read_csv(\"/Users/noshitha/Downloads/hw3/datasets/hw3_wine.csv\", delimiter=\"\\t\")\n",
    "\n",
    "# Preprocess data\n",
    "X_wine = pd.get_dummies(df_wine.iloc[:, 1:])  # Exclude the target column\n",
    "y_wine = df_wine.iloc[:, 0]\n",
    "\n",
    "# Normalize data\n",
    "X_wine_normalized, mean, std = normalize(X_wine)\n",
    "\n",
    "# Reshape y_wine to a 2D array\n",
    "y_wine = y_wine.values.reshape(-1, 1)\n",
    "\n",
    "# Initialize the OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Fit and transform the target variable\n",
    "y_encoded = encoder.fit_transform(y_wine)\n",
    "\n",
    "# Convert y_encoded to a DataFrame\n",
    "y_encoded_df = pd.DataFrame(y_encoded, columns=['y1', 'y2', 'y3'])\n",
    "\n",
    "# Define model architecture\n",
    "architectures = [\n",
    "    [X_wine_normalized.shape[1], 5, 3, 3],  # Example architecture\n",
    "]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_wine_normalized, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Instantiate the NeuralNetwork class with the desired architecture\n",
    "model = NeuralNetwork(architectures[0])\n",
    "\n",
    "# Train the model\n",
    "model.train(X_train, y_train, learning_rate=0.01, lam=0.01)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3683d084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df_wine = pd.read_csv(\"/Users/noshitha/Downloads/hw3/datasets/hw3_wine.csv\", delimiter=\"\\t\")\n",
    "\n",
    "# Preprocess data\n",
    "X_wine = pd.get_dummies(df_wine.iloc[:, 1:])  # Exclude the target column\n",
    "y_wine = df_wine.iloc[:, 0]\n",
    "\n",
    "# Normalize data\n",
    "X_wine_normalized, mean, std = normalize(X_wine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "692dcf88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_wine.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "899d6e3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># class</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   # class      1     2     3     4    5     6     7     8     9    10    11  \\\n",
       "0        1  14.23  1.71  2.43  15.6  127  2.80  3.06  0.28  2.29  5.64  1.04   \n",
       "1        1  13.20  1.78  2.14  11.2  100  2.65  2.76  0.26  1.28  4.38  1.05   \n",
       "2        1  13.16  2.36  2.67  18.6  101  2.80  3.24  0.30  2.81  5.68  1.03   \n",
       "3        1  14.37  1.95  2.50  16.8  113  3.85  3.49  0.24  2.18  7.80  0.86   \n",
       "4        1  13.24  2.59  2.87  21.0  118  2.80  2.69  0.39  1.82  4.32  1.04   \n",
       "\n",
       "     12    13  \n",
       "0  3.92  1065  \n",
       "1  3.40  1050  \n",
       "2  3.17  1185  \n",
       "3  3.45  1480  \n",
       "4  2.93   735  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f4a01027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    1\n",
       "3    1\n",
       "4    1\n",
       "Name: # class, dtype: int64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d6c5c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1:\n",
      "Regularization parameter lambda=0.000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (1,2) and (1,2) not aligned: 2 (dim 1) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/bm/l2zpyjxn2gqg_1d20xgx5z0w0000gn/T/ipykernel_84064/4172211924.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;31m# Train the neural network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m \u001b[0mnn_example_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_example_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train_example_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.005\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/bm/l2zpyjxn2gqg_1d20xgx5z0w0000gn/T/ipykernel_84064/4172211924.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, Y, learning_rate, lam, max_iterations, epsilon)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mactivations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0mdeltas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mgradients_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradients_biases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeltas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/bm/l2zpyjxn2gqg_1d20xgx5z0w0000gn/T/ipykernel_84064/4172211924.py\u001b[0m in \u001b[0;36mforward_pass\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mactivations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbiases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (1,2) and (1,2) not aligned: 2 (dim 1) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        self.weights = [np.random.randn(layers[i], layers[i+1]) for i in range(len(layers)-1)]\n",
    "        self.biases = [np.zeros((1, layers[i+1])) for i in range(len(layers)-1)]\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    def forward_pass(self, X):\n",
    "        activations = [X]\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(activations[-1], w) + b\n",
    "            activations.append(self.sigmoid(z))\n",
    "        return activations\n",
    "    \n",
    "    def backward_pass(self, X, Y, activations):\n",
    "        deltas = [(activations[-1] - Y) * self.sigmoid_derivative(activations[-1])]\n",
    "        for i in range(len(self.layers) - 2, 0, -1):\n",
    "            delta = np.dot(deltas[0], self.weights[i].T) * self.sigmoid_derivative(activations[i])\n",
    "            deltas.insert(0, delta)\n",
    "        return deltas\n",
    "    \n",
    "    def compute_gradients(self, activations, deltas):\n",
    "        gradients_weights = [np.dot(activations[i].T, deltas[i]) for i in range(len(self.layers) - 1)]\n",
    "        gradients_biases = [np.sum(deltas[i], axis=0) for i in range(len(self.layers) - 1)]\n",
    "        return gradients_weights, gradients_biases\n",
    "    \n",
    "    def update_weights(self, gradients_weights, gradients_biases, learning_rate):\n",
    "        self.weights = [w - learning_rate * gw for w, gw in zip(self.weights, gradients_weights)]\n",
    "        self.biases = [b - learning_rate * gb for b, gb in zip(self.biases, gradients_biases)]\n",
    "    \n",
    "    def train(self, X, Y, learning_rate, lam, max_iterations, epsilon):\n",
    "        for iteration in range(max_iterations):\n",
    "            activations = self.forward_pass(X)\n",
    "            deltas = self.backward_pass(X, Y, activations)\n",
    "            gradients_weights, gradients_biases = self.compute_gradients(activations, deltas)\n",
    "            self.update_weights(gradients_weights, gradients_biases, learning_rate)\n",
    "            # Compute cost function\n",
    "            J = np.mean(np.square(activations[-1] - Y))\n",
    "            print(f\"Iteration {iteration+1}, Cost: {J}\")\n",
    "            # Check for convergence\n",
    "            if J < epsilon:\n",
    "                print(\"Converged!\")\n",
    "                break\n",
    "\n",
    "# Example 1\n",
    "print(\"Example 1:\")\n",
    "print(\"Regularization parameter lambda=0.000\")\n",
    "\n",
    "# Define the structure of the neural network\n",
    "nn_example_1 = NeuralNetwork([1, 2, 1])\n",
    "\n",
    "# Initialize Theta1 and Theta2 with provided values\n",
    "nn_example_1.weights[0] = np.array([[0.4, 0.1], [0.3, 0.2]])\n",
    "nn_example_1.weights[1] = np.array([[0.7, 0.5]])\n",
    "\n",
    "# Define the training data\n",
    "X_train_example_1 = np.array([[0.13, 0.9]])\n",
    "Y_train_example_1 = np.array([[0.5]])  # Placeholder target output\n",
    "\n",
    "# Train the neural network\n",
    "nn_example_1.train(X_train_example_1, Y_train_example_1, learning_rate=0.1, lam=0.0, max_iterations=1000, epsilon=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "505eecde",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tuple' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/bm/l2zpyjxn2gqg_1d20xgx5z0w0000gn/T/ipykernel_72260/3793863026.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_encoded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'tuple' object is not callable"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34f3f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        self.weights = [np.random.randn(layers[i], layers[i+1]) for i in range(len(layers)-1)]\n",
    "        self.biases = [np.zeros((1, layers[i+1])) for i in range(len(layers)-1)]\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    def forward_pass(self, X):\n",
    "        activations = [X]\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(activations[-1], w) + b\n",
    "            activations.append(self.sigmoid(z))\n",
    "        return activations\n",
    "    \n",
    "    def backward_pass(self, X, Y, activations):\n",
    "        deltas = [(activations[-1] - Y) * self.sigmoid_derivative(activations[-1])]\n",
    "        for i in range(len(self.layers) - 2, 0, -1):\n",
    "            delta = np.dot(deltas[0], self.weights[i].T) * self.sigmoid_derivative(activations[i])\n",
    "            deltas.insert(0, delta)\n",
    "        return deltas\n",
    "    \n",
    "    def compute_gradients(self, activations, deltas):\n",
    "        gradients_weights = [np.dot(activations[i].T, deltas[i]) for i in range(len(self.layers) - 1)]\n",
    "        gradients_biases = [np.sum(deltas[i], axis=0) for i in range(len(self.layers) - 1)]\n",
    "        print(\"gradient_weights: \",gradients_weights)\n",
    "        print(\"gradients_biases: \",gradients_biases)\n",
    "        return gradients_weights, gradients_biases\n",
    "    \n",
    "    def update_weights(self, gradients_weights, gradients_biases, learning_rate):\n",
    "        self.weights = [w - learning_rate * gw for w, gw in zip(self.weights, gradients_weights)]\n",
    "        self.biases = [b - learning_rate * gb for b, gb in zip(self.biases, gradients_biases)]\n",
    "    \n",
    "    def train(self, X, Y, learning_rate, lam, max_iterations, epsilon):\n",
    "        for iteration in range(max_iterations):\n",
    "            activations = self.forward_pass(X)\n",
    "            deltas = self.backward_pass(X, Y, activations)\n",
    "            gradients_weights, gradients_biases = self.compute_gradients(activations, deltas)\n",
    "            self.update_weights(gradients_weights, gradients_biases, learning_rate)\n",
    "            # Compute cost function\n",
    "            J = np.mean(np.square(activations[-1] - Y))\n",
    "            print(f\"Iteration {iteration+1}, Cost: {J}\")\n",
    "            # Check for convergence\n",
    "            if J < epsilon:\n",
    "                print(\"Converged!\")\n",
    "                break\n",
    "\n",
    "    def accuracy(self, y_true, y_pred):\n",
    "        correct = np.sum(y_true == y_pred)\n",
    "        return correct / len(y_true)\n",
    "\n",
    "    def f1_score(self, y_true, y_pred):\n",
    "        tp = np.sum(y_true & y_pred)\n",
    "        fp = np.sum((~y_true) & y_pred)\n",
    "        fn = np.sum(y_true & (~y_pred))\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        return f1\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        activations = self.forward_pass(X_test)[-1]\n",
    "        y_pred = (activations > 0.5).astype(int)\n",
    "        acc = self.accuracy(y_test, y_pred)\n",
    "        f1 = self.f1_score(y_test, y_pred)\n",
    "        return acc, f1\n",
    "\n",
    "# Example usage\n",
    "# Create a neural network with 2 input neurons, 3 hidden neurons, and 1 output neuron\n",
    "nn = NeuralNetwork([2, 3, 1])\n",
    "\n",
    "# Generate some dummy data for training\n",
    "X_train = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "Y_train = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# Train the neural network\n",
    "nn.train(X_train, Y_train, learning_rate=0.1, lam=0.0, max_iterations=1000, epsilon=0.005)\n",
    "\n",
    "# Generate some dummy data for testing\n",
    "X_test = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "Y_test = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# Evaluate the trained model\n",
    "accuracy, f1_score = nn.evaluate(X_test, Y_test)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1 Score:\", f1_score)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
