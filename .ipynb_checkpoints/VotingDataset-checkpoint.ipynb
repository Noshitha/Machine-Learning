{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c89ec82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        self.weights = [np.random.randn(layers[i], layers[i+1]) for i in range(len(layers)-1)]\n",
    "        self.biases = [np.zeros((1, layers[i+1])) for i in range(len(layers)-1)]\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    def forward_pass(self, X):\n",
    "        activations = [X]\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(activations[-1], w) + b\n",
    "            activations.append(self.sigmoid(z))\n",
    "        return activations\n",
    "    \n",
    "#     def backward_pass(self, X, Y, activations):\n",
    "#         deltas = []\n",
    "#         delta = (activations[-1] - Y)  * self.sigmoid_derivative(activations[-1])\n",
    "#         deltas.append(delta)\n",
    "#         for i in range(len(self.layers) - 2, 0, -1):\n",
    "#             delta = np.dot(deltas[0], self.weights[i].T) * self.sigmoid_derivative(activations[i])\n",
    "#             deltas.insert(0, delta)\n",
    "#         return deltas\n",
    "    \n",
    "    def backward_pass(self, X, Y, activations):\n",
    "        deltas = [(activations[-1] - Y) * self.sigmoid_derivative(activations[-1])]\n",
    "        for i in range(len(self.layers) - 2, 0, -1):\n",
    "            delta = np.dot(deltas[0], self.weights[i].T) * self.sigmoid_derivative(activations[i])\n",
    "            deltas.insert(0, delta)\n",
    "        return deltas\n",
    "    \n",
    "    \n",
    "    def compute_gradients(self, activations, deltas):\n",
    "        gradients_weights = [np.dot(activations[i].T, deltas[i]) for i in range(len(self.layers) - 1)]\n",
    "        gradients_biases = [np.sum(deltas[i], axis=0) for i in range(len(self.layers) - 1)]\n",
    "        return gradients_weights, gradients_biases\n",
    "    \n",
    "    def update_weights(self, gradients_weights, gradients_biases, learning_rate):\n",
    "        self.weights = [w - learning_rate * gw for w, gw in zip(self.weights, gradients_weights)]\n",
    "        self.biases = [b - learning_rate * gb for b, gb in zip(self.biases, gradients_biases)]\n",
    "    \n",
    "    def train(self, X, Y, learning_rate, lam, max_iterations, epsilon):\n",
    "        for iteration in range(max_iterations):\n",
    "            activations = self.forward_pass(X)\n",
    "            deltas = self.backward_pass(X, Y, activations)\n",
    "            gradients_weights, gradients_biases = self.compute_gradients(activations, deltas)\n",
    "            self.update_weights(gradients_weights, gradients_biases, learning_rate)\n",
    "            # Compute cost function\n",
    "            J = np.mean(np.square(activations[-1] - Y))\n",
    "            #print(f\"Iteration {iteration+1}, Cost: {J}\")\n",
    "            # Check for convergence\n",
    "            if J < epsilon:\n",
    "                print(f\"Converged at cost :{J} while Epsilon:{epsilon} \")\n",
    "                return J\n",
    "        return J\n",
    "            \n",
    "\n",
    "    def accuracy(self, y_true, y_pred):\n",
    "        correct = np.sum(np.all(y_true == y_pred, axis=1))\n",
    "        return correct / len(y_true)\n",
    "\n",
    "\n",
    "    def f1_score(self, y_true, y_pred):\n",
    "        tp = np.sum(np.logical_and(y_true, y_pred))\n",
    "        fp = np.sum(np.logical_and(np.logical_not(y_true), y_pred))\n",
    "        fn = np.sum(np.logical_and(y_true, np.logical_not(y_pred)))\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        return f1\n",
    "\n",
    "\n",
    "    def evaluate(self, X_test, y_test, J):\n",
    "        activations = self.forward_pass(X_test)[-1]\n",
    "        y_pred = (activations > 0.5).astype(int)\n",
    "        acc = self.accuracy(y_test, y_pred)\n",
    "        f1 = self.f1_score(y_test, y_pred)\n",
    "        return J, acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b387fe3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged at cost :0.0049936865026640005 while Epsilon:0.005 \n",
      "Converged at cost :0.004985426897653145 while Epsilon:0.005 \n",
      "Converged at cost :0.004999588718503817 while Epsilon:0.005 \n",
      "Converged at cost :0.004999042560525168 while Epsilon:0.005 \n",
      "Converged at cost :0.004995693043631537 while Epsilon:0.005 \n",
      "Converged at cost :0.0049861891207344695 while Epsilon:0.005 \n",
      "Converged at cost :0.004992687478627742 while Epsilon:0.005 \n",
      "Converged at cost :0.00499769860962235 while Epsilon:0.005 \n",
      "Converged at cost :0.004995752591360978 while Epsilon:0.005 \n",
      "Converged at cost :0.004998209403077233 while Epsilon:0.005 \n",
      "Converged at cost :0.0049973325125238 while Epsilon:0.005 \n",
      "Converged at cost :0.004997631803144477 while Epsilon:0.005 \n",
      "Converged at cost :0.004997943896574303 while Epsilon:0.005 \n",
      "Converged at cost :0.004996354805813788 while Epsilon:0.005 \n",
      "Converged at cost :0.004996818848957043 while Epsilon:0.005 \n",
      "Converged at cost :0.004999752899932887 while Epsilon:0.005 \n",
      "Converged at cost :0.004999898566156885 while Epsilon:0.005 \n",
      "Converged at cost :0.004995326239043654 while Epsilon:0.005 \n",
      "Converged at cost :0.004988289496667935 while Epsilon:0.005 \n",
      "Converged at cost :0.004997529621956959 while Epsilon:0.005 \n",
      "Converged at cost :0.004983489213750931 while Epsilon:0.005 \n",
      "Converged at cost :0.004957251435999797 while Epsilon:0.005 \n",
      "Converged at cost :0.004968113056751227 while Epsilon:0.005 \n",
      "Converged at cost :0.004957879499517222 while Epsilon:0.005 \n",
      "Converged at cost :0.0049914621819506216 while Epsilon:0.005 \n",
      "Converged at cost :0.0049472479403593105 while Epsilon:0.005 \n",
      "Converged at cost :0.004996255691792988 while Epsilon:0.005 \n",
      "Converged at cost :0.004942447873560052 while Epsilon:0.005 \n",
      "Converged at cost :0.004927818758701707 while Epsilon:0.005 \n",
      "Converged at cost :0.004965033843540981 while Epsilon:0.005 \n",
      "Converged at cost :0.004920825892282266 while Epsilon:0.005 \n",
      "Converged at cost :0.004979360144912551 while Epsilon:0.005 \n",
      "Converged at cost :0.004951536657471261 while Epsilon:0.005 \n",
      "Converged at cost :0.0049818027392903984 while Epsilon:0.005 \n",
      "Converged at cost :0.00499573418598289 while Epsilon:0.005 \n",
      "Converged at cost :0.004993836190721782 while Epsilon:0.005 \n",
      "Converged at cost :0.004951094948543896 while Epsilon:0.005 \n",
      "Converged at cost :0.0049490435727474295 while Epsilon:0.005 \n",
      "Converged at cost :0.004949912874006666 while Epsilon:0.005 \n",
      "Converged at cost :0.004999779566394322 while Epsilon:0.005 \n",
      "Converged at cost :0.004990467742807324 while Epsilon:0.005 \n",
      "Converged at cost :0.004992371549942271 while Epsilon:0.005 \n",
      "Converged at cost :0.0049215263590586655 while Epsilon:0.005 \n",
      "Converged at cost :0.0049990492875157935 while Epsilon:0.005 \n",
      "Converged at cost :0.004995011274291482 while Epsilon:0.005 \n",
      "Converged at cost :0.004975602427976865 while Epsilon:0.005 \n",
      "Converged at cost :0.004966915843418205 while Epsilon:0.005 \n",
      "Converged at cost :0.004783100969985514 while Epsilon:0.005 \n",
      "Converged at cost :0.00493026111957932 while Epsilon:0.005 \n",
      "Converged at cost :0.00499477368407715 while Epsilon:0.005 \n",
      "Converged at cost :0.00496485445934027 while Epsilon:0.005 \n",
      "Converged at cost :0.0049511991949964425 while Epsilon:0.005 \n",
      "Converged at cost :0.0049847667543003885 while Epsilon:0.005 \n",
      "Converged at cost :0.004955856098361939 while Epsilon:0.005 \n",
      "Converged at cost :0.004953163592261003 while Epsilon:0.005 \n",
      "Converged at cost :0.004884677565945821 while Epsilon:0.005 \n",
      "Converged at cost :0.004989833319741164 while Epsilon:0.005 \n",
      "Converged at cost :0.004886700725513242 while Epsilon:0.005 \n",
      "Converged at cost :0.004959045850970619 while Epsilon:0.005 \n",
      "Converged at cost :0.004858991571571633 while Epsilon:0.005 \n",
      "Converged at cost :0.004978304249637556 while Epsilon:0.005 \n",
      "Converged at cost :0.004980365069212669 while Epsilon:0.005 \n",
      "Converged at cost :0.004889182993235147 while Epsilon:0.005 \n",
      "Converged at cost :0.004908362586505489 while Epsilon:0.005 \n",
      "Converged at cost :0.004990040792567225 while Epsilon:0.005 \n",
      "Converged at cost :0.00485375406188763 while Epsilon:0.005 \n",
      "Converged at cost :0.004979127550735498 while Epsilon:0.005 \n",
      "Converged at cost :0.004934510147696922 while Epsilon:0.005 \n",
      "Converged at cost :0.004979178797852239 while Epsilon:0.005 \n",
      "Converged at cost :0.00498823965877935 while Epsilon:0.005 \n",
      "Converged at cost :0.0049593705749030915 while Epsilon:0.005 \n",
      "Converged at cost :0.004985757071761068 while Epsilon:0.005 \n",
      "Converged at cost :0.004986910152026979 while Epsilon:0.005 \n",
      "Converged at cost :0.004950473811802854 while Epsilon:0.005 \n",
      "Converged at cost :0.0049585777202154685 while Epsilon:0.005 \n",
      "Converged at cost :0.0048858416791687995 while Epsilon:0.005 \n",
      "Converged at cost :0.004829082926640769 while Epsilon:0.005 \n",
      "Converged at cost :0.00499258431958815 while Epsilon:0.005 \n",
      "Mean Accuracy Results:\n",
      "                                 Architecture, Lambda  Mean Accuracy\n",
      "0                                  ([16, 2, 2], 0.01)       0.935465\n",
      "1                                   ([16, 2, 2], 0.1)       0.946934\n",
      "2                                   ([16, 2, 2], 1.0)       0.944662\n",
      "3                              ([16, 10, 8, 2], 0.01)       0.949260\n",
      "4                               ([16, 10, 8, 2], 0.1)       0.942389\n",
      "5                               ([16, 10, 8, 2], 1.0)       0.953858\n",
      "6                 ([16, 50, 40, 30, 20, 10, 2], 0.01)       0.933510\n",
      "7                  ([16, 50, 40, 30, 20, 10, 2], 0.1)       0.933192\n",
      "8                  ([16, 50, 40, 30, 20, 10, 2], 1.0)       0.953911\n",
      "9               ([16, 100, 100, 20, 30, 20, 2], 0.01)       0.937791\n",
      "10               ([16, 100, 100, 20, 30, 20, 2], 0.1)       0.949524\n",
      "11               ([16, 100, 100, 20, 30, 20, 2], 1.0)       0.928700\n",
      "12                              ([16, 1000, 2], 0.01)       0.818552\n",
      "13                               ([16, 1000, 2], 0.1)       0.811522\n",
      "14                               ([16, 1000, 2], 1.0)       0.861628\n",
      "15  ([16, 50, 40, 30, 20, 10, 5, 100, 100, 50, 2],...       0.851057\n",
      "16  ([16, 50, 40, 30, 20, 10, 5, 100, 100, 50, 2],...       0.859778\n",
      "17  ([16, 50, 40, 30, 20, 10, 5, 100, 100, 50, 2],...       0.933245\n",
      "\n",
      "Mean F1 Score Results:\n",
      "                                 Architecture, Lambda  Mean F1 Score\n",
      "0                                  ([16, 2, 2], 0.01)       0.936536\n",
      "1                                   ([16, 2, 2], 0.1)       0.946934\n",
      "2                                   ([16, 2, 2], 1.0)       0.944662\n",
      "3                              ([16, 10, 8, 2], 0.01)       0.950650\n",
      "4                               ([16, 10, 8, 2], 0.1)       0.943486\n",
      "5                               ([16, 10, 8, 2], 1.0)       0.955115\n",
      "6                 ([16, 50, 40, 30, 20, 10, 2], 0.01)       0.939119\n",
      "7                  ([16, 50, 40, 30, 20, 10, 2], 0.1)       0.935541\n",
      "8                  ([16, 50, 40, 30, 20, 10, 2], 1.0)       0.955060\n",
      "9               ([16, 100, 100, 20, 30, 20, 2], 0.01)       0.942433\n",
      "10               ([16, 100, 100, 20, 30, 20, 2], 0.1)       0.950725\n",
      "11               ([16, 100, 100, 20, 30, 20, 2], 1.0)       0.936830\n",
      "12                              ([16, 1000, 2], 0.01)       0.906166\n",
      "13                               ([16, 1000, 2], 0.1)       0.889405\n",
      "14                               ([16, 1000, 2], 1.0)       0.909310\n",
      "15  ([16, 50, 40, 30, 20, 10, 5, 100, 100, 50, 2],...       0.887859\n",
      "16  ([16, 50, 40, 30, 20, 10, 5, 100, 100, 50, 2],...       0.887163\n",
      "17  ([16, 50, 40, 30, 20, 10, 5, 100, 100, 50, 2],...       0.933245\n",
      "\n",
      "Mean J cost Results:\n",
      "                                 Architecture, Lambda  Mean J Cost\n",
      "0                                  ([16, 2, 2], 0.01)     0.013212\n",
      "1                                   ([16, 2, 2], 0.1)     0.014044\n",
      "2                                   ([16, 2, 2], 1.0)     0.013113\n",
      "3                              ([16, 10, 8, 2], 0.01)     0.005407\n",
      "4                               ([16, 10, 8, 2], 0.1)     0.005236\n",
      "5                               ([16, 10, 8, 2], 1.0)     0.005382\n",
      "6                 ([16, 50, 40, 30, 20, 10, 2], 0.01)     0.004981\n",
      "7                  ([16, 50, 40, 30, 20, 10, 2], 0.1)     0.004967\n",
      "8                  ([16, 50, 40, 30, 20, 10, 2], 1.0)     0.005300\n",
      "9               ([16, 100, 100, 20, 30, 20, 2], 0.01)     0.004937\n",
      "10               ([16, 100, 100, 20, 30, 20, 2], 0.1)     0.004929\n",
      "11               ([16, 100, 100, 20, 30, 20, 2], 1.0)     0.004961\n",
      "12                              ([16, 1000, 2], 0.01)     0.063656\n",
      "13                               ([16, 1000, 2], 0.1)     0.052284\n",
      "14                               ([16, 1000, 2], 1.0)     0.030821\n",
      "15  ([16, 50, 40, 30, 20, 10, 5, 100, 100, 50, 2],...     0.076791\n",
      "16  ([16, 50, 40, 30, 20, 10, 5, 100, 100, 50, 2],...     0.046814\n",
      "17  ([16, 50, 40, 30, 20, 10, 5, 100, 100, 50, 2],...     0.012048\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Load dataset\n",
    "df_house_votes = pd.read_csv(\"/Users/noshitha/Downloads/hw4/datasets/hw3_house_votes_84.csv\", delimiter=\",\")\n",
    "\n",
    "# Extract features and target variable\n",
    "X_house_votes = pd.get_dummies(df_house_votes.drop(columns=['class']))  # Features\n",
    "y_house_votes = df_house_votes['class']  # Target variable\n",
    "\n",
    "# Normalize data\n",
    "y_house_votes_resized = y_house_votes.values.reshape(-1, 1)\n",
    "\n",
    "# Initialize the OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Fit and transform the target variable\n",
    "y_encoded = encoder.fit_transform(y_house_votes_resized)\n",
    "\n",
    "# Define model architectures and regularization parameters\n",
    "architectures = [\n",
    "    [X_house_votes.shape[1], 2, y_encoded.shape[1]],  # Architecture with fewer neurons\n",
    "    [X_house_votes.shape[1], 10, 8, y_encoded.shape[1]],  # Architecture with a moderate number of neurons\n",
    "    [X_house_votes.shape[1], 50, 40, 30, 20, 10, y_encoded.shape[1]],  # Architecture with a higher number of neurons AND MORE NEURONS\n",
    "    [X_house_votes.shape[1], 100,100, 20, 30, 20, y_encoded.shape[1]],  # Architecture with fewer layers\n",
    "    [X_house_votes.shape[1],1000, y_encoded.shape[1]],  # Architecture with more layers\n",
    "    [X_house_votes.shape[1], 100, 100, 50, 50, 50, y_encoded.shape[1]]  # Architecture with more layers and neurons\n",
    "]\n",
    "\n",
    "regularization_params = [0.01, 0.1, 1.0]  # Example regularization parameters\n",
    "\n",
    "# Initialize lists to store results\n",
    "results_accuracy = {}\n",
    "results_f1_score = {}\n",
    "results_J_cost = {}\n",
    "\n",
    "# Perform stratified k-fold cross-validation\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "for arch in architectures:\n",
    "    for lam in regularization_params:\n",
    "        accuracy_list = []\n",
    "        f1_score_list = []\n",
    "        J_list = []\n",
    "        for train_index, test_index in skf.split(X_house_votes, y_house_votes):\n",
    "            X_train, X_test = X_house_votes.iloc[train_index], X_house_votes.iloc[test_index]\n",
    "            y_train, y_test = y_encoded[train_index], y_encoded[test_index]\n",
    "\n",
    "            mean = np.mean(X_train, axis=0)\n",
    "            std = np.std(X_train, axis=0)\n",
    "            X_train_normalized = (X_train - mean) / std\n",
    "            X_test_normalized = (X_test - mean) / std\n",
    "\n",
    "            model = NeuralNetwork(arch)\n",
    "            J = model.train(X_train_normalized, y_train, learning_rate=0.01, lam=lam, max_iterations=1000, epsilon=0.005)\n",
    "            J, accuracy, f1_score = model.evaluate(X_test_normalized, y_test, J)\n",
    "            accuracy_list.append(accuracy)\n",
    "            f1_score_list.append(f1_score)\n",
    "            J_list.append(J)\n",
    "\n",
    "        mean_accuracy = np.mean(accuracy_list)\n",
    "        mean_f1_score = np.mean(f1_score_list)\n",
    "        mean_J_cost = np.mean(J_list)\n",
    "\n",
    "        results_accuracy[(str(arch), lam)] = mean_accuracy\n",
    "        results_f1_score[(str(arch), lam)] = mean_f1_score\n",
    "        results_J_cost[(str(arch), lam)] = mean_J_cost\n",
    "\n",
    "# Convert the results into a DataFrame for tabular representation\n",
    "accuracy_df = pd.DataFrame(list(results_accuracy.items()), columns=['Architecture, Lambda', 'Mean Accuracy'])\n",
    "f1_score_df = pd.DataFrame(list(results_f1_score.items()), columns=['Architecture, Lambda', 'Mean F1 Score'])\n",
    "J_cost_df = pd.DataFrame(list(results_J_cost.items()), columns=['Architecture, Lambda', 'Mean J Cost'])\n",
    "\n",
    "print(\"Mean Accuracy Results:\")\n",
    "print(accuracy_df)\n",
    "print(\"\\nMean F1 Score Results:\")\n",
    "print(f1_score_df)\n",
    "print(\"\\nMean J cost Results:\")\n",
    "print(J_cost_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c1096d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Architecture, Lambda</th>\n",
       "      <th>Mean Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>([16, 2, 2], 0.01)</td>\n",
       "      <td>0.935465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>([16, 2, 2], 0.1)</td>\n",
       "      <td>0.946934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>([16, 2, 2], 1.0)</td>\n",
       "      <td>0.944662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>([16, 10, 8, 2], 0.01)</td>\n",
       "      <td>0.949260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>([16, 10, 8, 2], 0.1)</td>\n",
       "      <td>0.942389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>([16, 10, 8, 2], 1.0)</td>\n",
       "      <td>0.953858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>([16, 50, 40, 30, 20, 10, 2], 0.01)</td>\n",
       "      <td>0.933510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>([16, 50, 40, 30, 20, 10, 2], 0.1)</td>\n",
       "      <td>0.933192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>([16, 50, 40, 30, 20, 10, 2], 1.0)</td>\n",
       "      <td>0.953911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>([16, 100, 100, 20, 30, 20, 2], 0.01)</td>\n",
       "      <td>0.937791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>([16, 100, 100, 20, 30, 20, 2], 0.1)</td>\n",
       "      <td>0.949524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>([16, 100, 100, 20, 30, 20, 2], 1.0)</td>\n",
       "      <td>0.928700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>([16, 1000, 2], 0.01)</td>\n",
       "      <td>0.818552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>([16, 1000, 2], 0.1)</td>\n",
       "      <td>0.811522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>([16, 1000, 2], 1.0)</td>\n",
       "      <td>0.861628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>([16, 50, 40, 30, 20, 10, 5, 100, 100, 50, 2],...</td>\n",
       "      <td>0.851057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>([16, 50, 40, 30, 20, 10, 5, 100, 100, 50, 2],...</td>\n",
       "      <td>0.859778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>([16, 50, 40, 30, 20, 10, 5, 100, 100, 50, 2],...</td>\n",
       "      <td>0.933245</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Architecture, Lambda  Mean Accuracy\n",
       "0                                  ([16, 2, 2], 0.01)       0.935465\n",
       "1                                   ([16, 2, 2], 0.1)       0.946934\n",
       "2                                   ([16, 2, 2], 1.0)       0.944662\n",
       "3                              ([16, 10, 8, 2], 0.01)       0.949260\n",
       "4                               ([16, 10, 8, 2], 0.1)       0.942389\n",
       "5                               ([16, 10, 8, 2], 1.0)       0.953858\n",
       "6                 ([16, 50, 40, 30, 20, 10, 2], 0.01)       0.933510\n",
       "7                  ([16, 50, 40, 30, 20, 10, 2], 0.1)       0.933192\n",
       "8                  ([16, 50, 40, 30, 20, 10, 2], 1.0)       0.953911\n",
       "9               ([16, 100, 100, 20, 30, 20, 2], 0.01)       0.937791\n",
       "10               ([16, 100, 100, 20, 30, 20, 2], 0.1)       0.949524\n",
       "11               ([16, 100, 100, 20, 30, 20, 2], 1.0)       0.928700\n",
       "12                              ([16, 1000, 2], 0.01)       0.818552\n",
       "13                               ([16, 1000, 2], 0.1)       0.811522\n",
       "14                               ([16, 1000, 2], 1.0)       0.861628\n",
       "15  ([16, 50, 40, 30, 20, 10, 5, 100, 100, 50, 2],...       0.851057\n",
       "16  ([16, 50, 40, 30, 20, 10, 5, 100, 100, 50, 2],...       0.859778\n",
       "17  ([16, 50, 40, 30, 20, 10, 5, 100, 100, 50, 2],...       0.933245"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a6e243c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Architecture, Lambda</th>\n",
       "      <th>Mean F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>([16, 2, 2], 0.01)</td>\n",
       "      <td>0.936536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>([16, 2, 2], 0.1)</td>\n",
       "      <td>0.946934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>([16, 2, 2], 1.0)</td>\n",
       "      <td>0.944662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>([16, 10, 8, 2], 0.01)</td>\n",
       "      <td>0.950650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>([16, 10, 8, 2], 0.1)</td>\n",
       "      <td>0.943486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>([16, 10, 8, 2], 1.0)</td>\n",
       "      <td>0.955115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>([16, 50, 40, 30, 20, 10, 2], 0.01)</td>\n",
       "      <td>0.939119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>([16, 50, 40, 30, 20, 10, 2], 0.1)</td>\n",
       "      <td>0.935541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>([16, 50, 40, 30, 20, 10, 2], 1.0)</td>\n",
       "      <td>0.955060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>([16, 100, 100, 20, 30, 20, 2], 0.01)</td>\n",
       "      <td>0.942433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>([16, 100, 100, 20, 30, 20, 2], 0.1)</td>\n",
       "      <td>0.950725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>([16, 100, 100, 20, 30, 20, 2], 1.0)</td>\n",
       "      <td>0.936830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>([16, 1000, 2], 0.01)</td>\n",
       "      <td>0.906166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>([16, 1000, 2], 0.1)</td>\n",
       "      <td>0.889405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>([16, 1000, 2], 1.0)</td>\n",
       "      <td>0.909310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>([16, 50, 40, 30, 20, 10, 5, 100, 100, 50, 2],...</td>\n",
       "      <td>0.887859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>([16, 50, 40, 30, 20, 10, 5, 100, 100, 50, 2],...</td>\n",
       "      <td>0.887163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>([16, 50, 40, 30, 20, 10, 5, 100, 100, 50, 2],...</td>\n",
       "      <td>0.933245</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Architecture, Lambda  Mean F1 Score\n",
       "0                                  ([16, 2, 2], 0.01)       0.936536\n",
       "1                                   ([16, 2, 2], 0.1)       0.946934\n",
       "2                                   ([16, 2, 2], 1.0)       0.944662\n",
       "3                              ([16, 10, 8, 2], 0.01)       0.950650\n",
       "4                               ([16, 10, 8, 2], 0.1)       0.943486\n",
       "5                               ([16, 10, 8, 2], 1.0)       0.955115\n",
       "6                 ([16, 50, 40, 30, 20, 10, 2], 0.01)       0.939119\n",
       "7                  ([16, 50, 40, 30, 20, 10, 2], 0.1)       0.935541\n",
       "8                  ([16, 50, 40, 30, 20, 10, 2], 1.0)       0.955060\n",
       "9               ([16, 100, 100, 20, 30, 20, 2], 0.01)       0.942433\n",
       "10               ([16, 100, 100, 20, 30, 20, 2], 0.1)       0.950725\n",
       "11               ([16, 100, 100, 20, 30, 20, 2], 1.0)       0.936830\n",
       "12                              ([16, 1000, 2], 0.01)       0.906166\n",
       "13                               ([16, 1000, 2], 0.1)       0.889405\n",
       "14                               ([16, 1000, 2], 1.0)       0.909310\n",
       "15  ([16, 50, 40, 30, 20, 10, 5, 100, 100, 50, 2],...       0.887859\n",
       "16  ([16, 50, 40, 30, 20, 10, 5, 100, 100, 50, 2],...       0.887163\n",
       "17  ([16, 50, 40, 30, 20, 10, 5, 100, 100, 50, 2],...       0.933245"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ab26c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fa6c56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a836f93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d280d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9755f346",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648035bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e54f3e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a6fe3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1487da54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08576023",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Function to generate mini-batches\n",
    "def generate_mini_batches(X, y, batch_size):\n",
    "    num_samples = X.shape[0]\n",
    "    num_batches = num_samples // batch_size\n",
    "    mini_batches = []\n",
    "    shuffled_indices = np.random.permutation(num_samples)\n",
    "    X_shuffled = X[shuffled_indices]\n",
    "    y_shuffled = y[shuffled_indices]\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        mini_batches.append((X_shuffled[start_idx:end_idx], y_shuffled[start_idx:end_idx]))\n",
    "    if num_samples % batch_size != 0:\n",
    "        mini_batches.append((X_shuffled[num_batches*batch_size:], y_shuffled[num_batches*batch_size:]))\n",
    "    return mini_batches\n",
    "\n",
    "\n",
    "def train_mini_batch(X_train, y_train, X_test, y_test, model, learning_rate, batch_size, max_iterations, epsilon):\n",
    "    training_errors = []\n",
    "    testing_errors = []\n",
    "    for iteration in range(max_iterations):\n",
    "        mini_batches = generate_mini_batches(X_train, y_train, batch_size)\n",
    "        for mini_batch in mini_batches:\n",
    "            X_mini_batch, y_mini_batch = mini_batch\n",
    "            J = model.train(X_mini_batch, y_mini_batch, learning_rate=learning_rate, lam=lam, max_iterations=1, epsilon=epsilon)\n",
    "        training_cost = np.mean(np.square(model.forward_pass(X_train)[-1] - y_train))  # Compute training cost\n",
    "        testing_cost = np.mean(np.square(model.forward_pass(X_test)[-1] - y_test))  # Compute testing cost\n",
    "        training_errors.append(training_cost)\n",
    "        testing_errors.append(testing_cost)\n",
    "        print(f\"Iteration {iteration+1}, Training Cost: {training_cost}, Testing Cost: {testing_cost}\")\n",
    "        # Check for convergence\n",
    "        if training_cost < epsilon:\n",
    "            print(f\"Converged at training cost :{training_cost} while Epsilon:{epsilon} \")\n",
    "            break\n",
    "    return training_errors, testing_errors\n",
    "\n",
    "# Plot learning curve\n",
    "def plot_learning_curve(training_errors, testing_errors, step_size):\n",
    "    iterations = range(1, len(training_errors) + 1)\n",
    "    plt.plot(iterations, training_errors, label='Training Error')\n",
    "    plt.plot(iterations, testing_errors, label='Testing Error')\n",
    "    plt.title('Learning Curve')\n",
    "    plt.xlabel('Number of Training Examples')\n",
    "    plt.ylabel('Error (J)')\n",
    "    plt.xticks(np.arange(1, len(training_errors) + 1, step=step_size))\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bddf927b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Int64Index([127, 164,  45, 311, 300, 279, 196, 108, 198,  92,\\n            ...\\n            195, 236, 314,  22, 223,  52, 270, 325,  37, 104],\\n           dtype='int64', length=348)] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/bm/l2zpyjxn2gqg_1d20xgx5z0w0000gn/T/ipykernel_15709/2769990677.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Train the model using mini-batch gradient descent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mtraining_errors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesting_errors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_mini_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_normalized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_normalized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/bm/l2zpyjxn2gqg_1d20xgx5z0w0000gn/T/ipykernel_15709/3214990907.py\u001b[0m in \u001b[0;36mtrain_mini_batch\u001b[0;34m(X_train, y_train, X_test, y_test, model, learning_rate, batch_size, max_iterations, epsilon)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mtesting_errors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mmini_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_mini_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmini_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmini_batches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mX_mini_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_mini_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmini_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/bm/l2zpyjxn2gqg_1d20xgx5z0w0000gn/T/ipykernel_15709/3214990907.py\u001b[0m in \u001b[0;36mgenerate_mini_batches\u001b[0;34m(X, y, batch_size)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mmini_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mshuffled_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mX_shuffled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mshuffled_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0my_shuffled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mshuffled_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3462\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3463\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3464\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3466\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1312\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1314\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_read_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m         if needs_i8_conversion(ax.dtype) or isinstance(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis)\u001b[0m\n\u001b[1;32m   1372\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0muse_interval_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m                     \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1374\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"None of [{key}] are in the [{axis_name}]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Int64Index([127, 164,  45, 311, 300, 279, 196, 108, 198,  92,\\n            ...\\n            195, 236, 314,  22, 223,  52, 270, 325,  37, 104],\\n           dtype='int64', length=348)] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define your neural network model and parameters\n",
    "model = NeuralNetwork([X_house_votes.shape[1], 10, 8, y_encoded.shape[1]])  # Your desired architecture\n",
    "learning_rate = 0.01\n",
    "batch_size = 32\n",
    "max_iterations = 1000\n",
    "epsilon = 0.005\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_house_votes, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "mean = np.mean(X_train, axis=0)\n",
    "std = np.std(X_train, axis=0)\n",
    "X_train_normalized = (X_train - mean) / std\n",
    "X_test_normalized = (X_test - mean) / std\n",
    "\n",
    "# Train the model using mini-batch gradient descent\n",
    "training_errors, testing_errors = train_mini_batch(X_train_normalized, y_train, X_test_normalized, y_test, model, learning_rate, batch_size, max_iterations, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0025b465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curve\n",
    "step_size = 50  # Adjust as needed\n",
    "plot_learning_curve(training_errors, testing_errors, step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4cc6e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
