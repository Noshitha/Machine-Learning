{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "869938fa",
   "metadata": {},
   "source": [
    "### 38% ACCURACY "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59434136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted output: [[0.74351271 0.87511972 0.75547537]]\n",
      "Expected output: [0. 0. 1.]\n",
      "Cost, J, associated with instance 1: 3.721484011945635\n",
      "\n",
      "Predicted output: [[0.73056237 0.86184172 0.74292703]]\n",
      "Expected output: [0. 0. 1.]\n",
      "Cost, J, associated with instance 2: 3.587931110225161\n",
      "\n",
      "Predicted output: [[0.70389984 0.83051304 0.71745446]]\n",
      "Expected output: [0. 1. 0.]\n",
      "Cost, J, associated with instance 3: 2.666684692812588\n",
      "\n",
      "Predicted output: [[0.73643044 0.86749435 0.7484343 ]]\n",
      "Expected output: [0. 0. 1.]\n",
      "Cost, J, associated with instance 4: 3.6443397854858137\n",
      "\n",
      "Predicted output: [[0.68511526 0.80536255 0.69536486]]\n",
      "Expected output: [1. 0. 0.]\n",
      "Cost, J, associated with instance 5: 3.2034253576643454\n",
      "\n",
      "Predicted output: [[0.65390065 0.76260214 0.66327455]]\n",
      "Expected output: [0. 1. 0.]\n",
      "Cost, J, associated with instance 6: 2.4205356043098876\n",
      "\n",
      "Predicted output: [[0.67060624 0.78569728 0.68032545]]\n",
      "Expected output: [0. 1. 0.]\n",
      "Cost, J, associated with instance 7: 2.4921369325218805\n",
      "\n",
      "Predicted output: [[0.67865997 0.79647549 0.68802743]]\n",
      "Expected output: [0. 1. 0.]\n",
      "Cost, J, associated with instance 8: 2.527654382022026\n",
      "\n",
      "Predicted output: [[0.70321855 0.82896014 0.71353631]]\n",
      "Expected output: [0. 0. 1.]\n",
      "Cost, J, associated with instance 9: 3.318139876947203\n",
      "\n",
      "Predicted output: [[0.74365348 0.87526335 0.75558523]]\n",
      "Expected output: [1. 0. 0.]\n",
      "Cost, J, associated with instance 10: 3.7866192747982934\n",
      "\n",
      "Predicted output: [[0.65239037 0.76090736 0.66210046]]\n",
      "Expected output: [0. 1. 0.]\n",
      "Cost, J, associated with instance 11: 2.414925483833271\n",
      "\n",
      "Predicted output: [[0.7401126  0.87158521 0.75211588]]\n",
      "Expected output: [0. 1. 0.]\n",
      "Cost, J, associated with instance 12: 2.8797423854230377\n",
      "\n",
      "Predicted output: [[0.64952239 0.75625303 0.65831203]]\n",
      "Expected output: [0. 0. 1.]\n",
      "Cost, J, associated with instance 13: 2.8781592905079485\n",
      "\n",
      "Predicted output: [[0.74210655 0.87348789 0.75388223]]\n",
      "Expected output: [1. 0. 0.]\n",
      "Cost, J, associated with instance 14: 3.7676247593086316\n",
      "\n",
      "Predicted output: [[0.64079353 0.74363116 0.64960826]]\n",
      "Expected output: [0. 1. 0.]\n",
      "Cost, J, associated with instance 15: 2.36877156081923\n",
      "\n",
      "Predicted output: [[0.74173233 0.8730635  0.75345462]]\n",
      "Expected output: [1. 0. 0.]\n",
      "Cost, J, associated with instance 16: 3.763044315482772\n",
      "\n",
      "Predicted output: [[0.73721806 0.86840965 0.74914573]]\n",
      "Expected output: [1. 0. 0.]\n",
      "Cost, J, associated with instance 17: 3.7158162478233274\n",
      "\n",
      "Predicted output: [[0.71177956 0.83820225 0.72097139]]\n",
      "Expected output: [0. 0. 1.]\n",
      "Cost, J, associated with instance 18: 3.3925936809169044\n",
      "\n",
      "Predicted output: [[0.6656213  0.77893305 0.67445445]]\n",
      "Expected output: [0. 0. 1.]\n",
      "Cost, J, associated with instance 19: 2.9986219104869987\n",
      "\n",
      "Predicted output: [[0.6656788  0.78057374 0.67719985]]\n",
      "Expected output: [0. 1. 0.]\n",
      "Cost, J, associated with instance 20: 2.474101004927048\n",
      "\n",
      "Predicted output: [[0.69096614 0.81064982 0.69841462]]\n",
      "Expected output: [0. 1. 0.]\n",
      "Cost, J, associated with instance 21: 2.582925666594145\n",
      "\n",
      "Predicted output: [[0.73828236 0.8690974  0.74953342]]\n",
      "Expected output: [1. 0. 0.]\n",
      "Cost, J, associated with instance 22: 3.721160425170977\n",
      "\n",
      "Predicted output: [[0.67675219 0.79487771 0.68777524]]\n",
      "Expected output: [0. 1. 0.]\n",
      "Cost, J, associated with instance 23: 2.5229350143481044\n",
      "\n",
      "Predicted output: [[0.73794661 0.86897536 0.74965035]]\n",
      "Expected output: [1. 0. 0.]\n",
      "Cost, J, associated with instance 24: 3.7211504110938893\n",
      "\n",
      "Predicted output: [[0.64304964 0.74616332 0.65058918]]\n",
      "Expected output: [0. 0. 1.]\n",
      "Cost, J, associated with instance 25: 2.8310996649453712\n",
      "\n",
      "Predicted output: [[0.65382204 0.7627058  0.66349831]]\n",
      "Expected output: [0. 1. 0.]\n",
      "Cost, J, associated with instance 26: 2.420837298670061\n",
      "\n",
      "Predicted output: [[0.68768154 0.80640867 0.6948592 ]]\n",
      "Expected output: [0. 1. 0.]\n",
      "Cost, J, associated with instance 27: 2.565878486899173\n",
      "\n",
      "Predicted output: [[0.67037845 0.78507036 0.67819255]]\n",
      "Expected output: [0. 0. 1.]\n",
      "Cost, J, associated with instance 28: 3.035578668508506\n",
      "\n",
      "Predicted output: [[0.72513482 0.85392416 0.73557739]]\n",
      "Expected output: [1. 0. 0.]\n",
      "Cost, J, associated with instance 29: 3.5752337139002712\n",
      "\n",
      "Predicted output: [[0.73944216 0.87040356 0.75083141]]\n",
      "Expected output: [1. 0. 0.]\n",
      "Cost, J, associated with instance 30: 3.7348147433620364\n",
      "\n",
      "Predicted output: [[0.6952706 0.8188172 0.7059461]]\n",
      "Expected output: [1. 0. 0.]\n",
      "Cost, J, associated with instance 31: 3.2956951321436163\n",
      "\n",
      "Predicted output: [[0.68217254 0.80054294 0.68909125]]\n",
      "Expected output: [0. 0. 1.]\n",
      "Cost, J, associated with instance 32: 3.130784505507341\n",
      "\n",
      "Predicted output: [[0.74350348 0.87509747 0.75541185]]\n",
      "Expected output: [1. 0. 0.]\n",
      "Cost, J, associated with instance 33: 3.7847829204795014\n",
      "\n",
      "Predicted output: [[0.73708784 0.86754721 0.74818847]]\n",
      "Expected output: [1. 0. 0.]\n",
      "Cost, J, associated with instance 34: 3.705651617118547\n",
      "\n",
      "Predicted output: [[0.74159035 0.87301923 0.7534319 ]]\n",
      "Expected output: [0. 1. 0.]\n",
      "Cost, J, associated with instance 35: 2.889123880303586\n",
      "\n",
      "Predicted output: [[0.6434924  0.74695392 0.65121851]]\n",
      "Expected output: [0. 0. 1.]\n",
      "Cost, J, associated with instance 36: 2.8344934392102967\n",
      "\n",
      "Predicted output: [[0.64048045 0.7430533  0.64907427]]\n",
      "Expected output: [0. 1. 0.]\n",
      "Cost, J, associated with instance 37: 2.367154898704328\n",
      "\n",
      "Predicted output: [[0.74363649 0.87523698 0.75556387]]\n",
      "Expected output: [1. 0. 0.]\n",
      "Cost, J, associated with instance 38: 3.7863433775650197\n",
      "\n",
      "Predicted output: [[0.69336984 0.81715183 0.70355114]]\n",
      "Expected output: [0. 0. 1.]\n",
      "Cost, J, associated with instance 39: 3.232826827239589\n",
      "\n",
      "Predicted output: [[0.7217132  0.85253167 0.73541965]]\n",
      "Expected output: [0. 1. 0.]\n",
      "Cost, J, associated with instance 40: 2.768258264954233\n",
      "\n",
      "Predicted output: [[0.74350428 0.87505118 0.75534901]]\n",
      "Expected output: [1. 0. 0.]\n",
      "Cost, J, associated with instance 41: 3.7841544054036094\n",
      "\n",
      "Predicted output: [[0.70467633 0.83076307 0.71519055]]\n",
      "Expected output: [0. 0. 1.]\n",
      "Cost, J, associated with instance 42: 3.3313451802609073\n",
      "\n",
      "Predicted output: [[0.69100807 0.81477328 0.70452302]]\n",
      "Expected output: [0. 1. 0.]\n",
      "Cost, J, associated with instance 43: 2.598449873330668\n",
      "\n",
      "Predicted output: [[0.68892958 0.80965363 0.69855217]]\n",
      "Expected output: [0. 1. 0.]\n",
      "Cost, J, associated with instance 44: 2.578043015664312\n",
      "\n",
      "Predicted output: [[0.72832767 0.85845743 0.74024701]]\n",
      "Expected output: [1. 0. 0.]\n",
      "Cost, J, associated with instance 45: 3.6201831035614505\n",
      "\n",
      "Predicted output: [[0.70008964 0.82446347 0.70938378]]\n",
      "Expected output: [0. 1. 0.]\n",
      "Cost, J, associated with instance 46: 2.6330458119355624\n",
      "\n",
      "Predicted output: [[0.74380937 0.8754147  0.75572699]]\n",
      "Expected output: [1. 0. 0.]\n",
      "Cost, J, associated with instance 47: 3.7882039446754083\n",
      "\n",
      "Predicted output: [[0.74169939 0.87302412 0.75344602]]\n",
      "Expected output: [1. 0. 0.]\n",
      "Cost, J, associated with instance 48: 3.7627436867497996\n",
      "\n",
      "Predicted output: [[0.73694038 0.86853722 0.74978697]]\n",
      "Expected output: [0. 1. 0.]\n",
      "Cost, J, associated with instance 49: 2.861762015359723\n",
      "\n",
      "Predicted output: [[0.74430677 0.87596333 0.75624398]]\n",
      "Expected output: [1. 0. 0.]\n",
      "Cost, J, associated with instance 50: 3.7940674760022928\n",
      "\n",
      "Predicted output: [[0.72781493 0.85811598 0.73981099]]\n",
      "Expected output: [1. 0. 0.]\n",
      "Cost, J, associated with instance 51: 3.616800728621638\n",
      "\n",
      "Predicted output: [[0.64685929 0.75232099 0.65527917]]\n",
      "Expected output: [0. 0. 1.]\n",
      "Cost, J, associated with instance 52: 2.8592043202724104\n",
      "\n",
      "Predicted output: [[0.6883435  0.81041348 0.70014979]]\n",
      "Expected output: [0. 1. 0.]\n",
      "Cost, J, associated with instance 53: 2.5805365806738605\n",
      "\n",
      "Predicted output: [[0.74091634 0.8724502  0.7530678 ]]\n",
      "Expected output: [0. 1. 0.]\n",
      "Cost, J, associated with instance 54: 2.885695428295688\n",
      "\n",
      "Predicted output: [[0.66977468 0.78494253 0.67982899]]\n",
      "Expected output: [0. 1. 0.]\n",
      "Cost, J, associated with instance 55: 2.4890248537921624\n",
      "\n",
      "Predicted output: [[0.74169629 0.87305972 0.75336053]]\n",
      "Expected output: [1. 0. 0.]\n",
      "Cost, J, associated with instance 56: 3.762681644678974\n",
      "\n",
      "Predicted output: [[0.6396977  0.74195879 0.64835896]]\n",
      "Expected output: [0. 1. 0.]\n",
      "Cost, J, associated with instance 57: 2.3644178446606303\n",
      "\n",
      "Predicted output: [[0.68118722 0.80087165 0.69254487]]\n",
      "Expected output: [0. 1. 0.]\n",
      "Cost, J, associated with instance 58: 2.544631947015885\n",
      "\n",
      "Predicted output: [[0.71789566 0.84819154 0.73186683]]\n",
      "Expected output: [0. 1. 0.]\n",
      "Cost, J, associated with instance 59: 2.746398574603636\n",
      "\n",
      "Predicted output: [[0.72167168 0.8516433  0.73320189]]\n",
      "Expected output: [0. 0. 1.]\n",
      "Cost, J, associated with instance 60: 3.497423837557382\n",
      "\n",
      "Predicted output: [[0.72049478 0.84961432 0.73146743]]\n",
      "Expected output: [0. 0. 1.]\n",
      "Cost, J, associated with instance 61: 3.4819889684143415\n",
      "\n",
      "Predicted output: [[0.73332233 0.86375442 0.74492879]]\n",
      "Expected output: [1. 0. 0.]\n",
      "Cost, J, associated with instance 62: 3.669678714645439\n",
      "\n",
      "Predicted output: [[0.67178175 0.78894878 0.68361606]]\n",
      "Expected output: [0. 1. 0.]\n",
      "Cost, J, associated with instance 63: 2.5019291910001034\n",
      "\n",
      "Predicted output: [[0.66178575 0.77363733 0.67067082]]\n",
      "Expected output: [0. 0. 1.]\n",
      "Cost, J, associated with instance 64: 2.9691693508368022\n",
      "\n",
      "Predicted output: [[0.73738941 0.86877801 0.74930753]]\n",
      "Expected output: [0. 0. 1.]\n",
      "Cost, J, associated with instance 65: 3.656553576340375\n",
      "\n",
      "Predicted output: [[0.64708258 0.75300802 0.65626218]]\n",
      "Expected output: [0. 1. 0.]\n",
      "Cost, J, associated with instance 66: 2.393076662073125\n",
      "\n",
      "Predicted output: [[0.74425203 0.87590564 0.75619289]]\n",
      "Expected output: [0. 1. 0.]\n",
      "Cost, J, associated with instance 67: 2.9074376203995707\n",
      "\n",
      "Predicted output: [[0.74421806 0.87586943 0.75615281]]\n",
      "Expected output: [1. 0. 0.]\n",
      "Cost, J, associated with instance 68: 3.7930559958618484\n",
      "\n",
      "Predicted output: [[0.675729   0.79518354 0.68894774]]\n",
      "Expected output: [0. 1. 0.]\n",
      "Cost, J, associated with instance 69: 2.5231523759908425\n",
      "\n",
      "Predicted output: [[0.65781383 0.76767691 0.66566257]]\n",
      "Expected output: [0. 0. 1.]\n",
      "Cost, J, associated with instance 70: 2.9389989567832613\n",
      "\n",
      "Predicted output: [[0.72334982 0.85380098 0.73599446]]\n",
      "Expected output: [0. 0. 1.]\n",
      "Cost, J, associated with instance 71: 3.514320608107553\n",
      "\n",
      "Predicted output: [[0.65351893 0.7625853  0.66345455]]\n",
      "Expected output: [0. 1. 0.]\n",
      "Cost, J, associated with instance 72: 2.4199900501304437\n",
      "\n",
      "Predicted output: [[0.72059758 0.84898125 0.73032205]]\n",
      "Expected output: [0. 0. 1.]\n",
      "Cost, J, associated with instance 73: 3.4797231193628146\n",
      "\n",
      "Predicted output: [[0.73510608 0.86613356 0.7468114 ]]\n",
      "Expected output: [0. 1. 0.]\n",
      "Cost, J, associated with instance 74: 2.845762609557697\n",
      "\n",
      "Predicted output: [[0.70991466 0.83666792 0.72185338]]\n",
      "Expected output: [0. 1. 0.]\n",
      "Cost, J, associated with instance 75: 2.695515036711792\n",
      "\n",
      "Predicted output: [[0.69520004 0.81942936 0.70714111]]\n",
      "Expected output: [0. 1. 0.]\n",
      "Cost, J, associated with instance 76: 2.6153110823983976\n",
      "\n",
      "Predicted output: [[0.7438914  0.87552154 0.75581874]]\n",
      "Expected output: [1. 0. 0.]\n",
      "Cost, J, associated with instance 77: 3.7893273013000215\n",
      "\n",
      "Predicted output: [[0.74367144 0.87525116 0.75555487]]\n",
      "Expected output: [1. 0. 0.]\n",
      "Cost, J, associated with instance 78: 3.786373168822039\n",
      "\n",
      "Predicted output: [[0.68012787 0.80027469 0.69094034]]\n",
      "Expected output: [0. 0. 1.]\n",
      "Cost, J, associated with instance 79: 3.1203480618881003\n",
      "\n",
      "Predicted output: [[0.74409669 0.8757457  0.75603824]]\n",
      "Expected output: [1. 0. 0.]\n",
      "Cost, J, associated with instance 80: 3.7917530951987093\n",
      "\n",
      "Predicted output: [[0.68363723 0.80370672 0.69322872]]\n",
      "Expected output: [0. 0. 1.]\n",
      "Cost, J, associated with instance 81: 3.145406397174156\n",
      "\n",
      "Predicted output: [[0.743528   0.87510417 0.75543019]]\n",
      "Expected output: [1. 0. 0.]\n",
      "Cost, J, associated with instance 82: 3.7848785801325526\n",
      "\n",
      "Predicted output: [[0.73951836 0.87088871 0.75158204]]\n",
      "Expected output: [1. 0. 0.]\n",
      "Cost, J, associated with instance 83: 3.74147932494295\n",
      "\n",
      "Predicted output: [[0.65635152 0.76634997 0.66624504]]\n",
      "Expected output: [0. 1. 0.]\n",
      "Cost, J, associated with instance 84: 2.431600544911064\n",
      "\n",
      "Predicted output: [[0.66014663 0.77088664 0.6681666 ]]\n",
      "Expected output: [0. 1. 0.]\n",
      "Cost, J, associated with instance 85: 2.442577192472421\n",
      "\n",
      "Predicted output: [[0.73979265 0.87104752 0.75154586]]\n",
      "Expected output: [1. 0. 0.]\n",
      "Cost, J, associated with instance 86: 3.742193698482458\n",
      "\n",
      "Predicted output: [[0.74288913 0.87443605 0.75479127]]\n",
      "Expected output: [1. 0. 0.]\n",
      "Cost, J, associated with instance 87: 3.7777940043155893\n",
      "\n",
      "Predicted output: [[0.74315336 0.87475422 0.75507886]]\n",
      "Expected output: [1. 0. 0.]\n",
      "Cost, J, associated with instance 88: 3.7811490299147748\n",
      "\n",
      "Predicted output: [[0.72627188 0.85738632 0.73988821]]\n",
      "Expected output: [0. 1. 0.]\n",
      "Cost, J, associated with instance 89: 2.796130361067273\n",
      "\n",
      "Predicted output: [[0.72952351 0.8601101  0.7414045 ]]\n",
      "Expected output: [1. 0. 0.]\n",
      "Cost, J, associated with instance 90: 3.6347535280265326\n",
      "\n",
      "Predicted output: [[0.64890064 0.75542812 0.65774707]]\n",
      "Expected output: [0. 1. 0.]\n",
      "Cost, J, associated with instance 91: 2.399361922952577\n",
      "\n",
      "Predicted output: [[0.67407627 0.79011204 0.68177223]]\n",
      "Expected output: [0. 0. 1.]\n",
      "Cost, J, associated with instance 92: 3.0653329186515745\n",
      "\n",
      "Predicted output: [[0.71331617 0.8426469  0.72740292]]\n",
      "Expected output: [0. 1. 0.]\n",
      "Cost, J, associated with instance 93: 2.7203430495685184\n",
      "\n",
      "Predicted output: [[0.67001299 0.78553542 0.68030684]]\n",
      "Expected output: [0. 1. 0.]\n",
      "Cost, J, associated with instance 94: 2.490485326174092\n",
      "\n",
      "Predicted output: [[0.70710636 0.83424944 0.71909928]]\n",
      "Expected output: [0. 1. 0.]\n",
      "Cost, J, associated with instance 95: 2.6789225662263965\n",
      "\n",
      "Predicted output: [[0.69562323 0.81846344 0.7044022 ]]\n",
      "Expected output: [0. 0. 1.]\n",
      "Cost, J, associated with instance 96: 3.2461929954575717\n",
      "\n",
      "Predicted output: [[0.71587081 0.84331848 0.72539632]]\n",
      "Expected output: [0. 0. 1.]\n",
      "Cost, J, associated with instance 97: 3.4329034277899395\n",
      "\n",
      "Predicted output: [[0.66824982 0.78472454 0.68081139]]\n",
      "Expected output: [0. 1. 0.]\n",
      "Cost, J, associated with instance 98: 2.4877686798969387\n",
      "\n",
      "Predicted output: [[0.72891712 0.8574299  0.73867448]]\n",
      "Expected output: [1. 0. 0.]\n",
      "Cost, J, associated with instance 99: 3.606105116578469\n",
      "\n",
      "Predicted output: [[0.74068749 0.8711154  0.75136942]]\n",
      "Expected output: [1. 0. 0.]\n",
      "Cost, J, associated with instance 100: 3.7408014571425143\n",
      "\n",
      "Predicted output: [[0.63553984 0.73565092 0.64388149]]\n",
      "Expected output: [0. 1. 0.]\n",
      "Cost, J, associated with instance 101: 2.3488292959053583\n",
      "\n",
      "Predicted output: [[0.71712279 0.84615936 0.72926352]]\n",
      "Expected output: [0. 0. 1.]\n",
      "Cost, J, associated with instance 102: 3.450300553014342\n",
      "\n",
      "Predicted output: [[0.64932614 0.75506784 0.65659352]]\n",
      "Expected output: [0. 0. 1.]\n",
      "Cost, J, associated with instance 103: 2.875362829996746\n",
      "\n",
      "Predicted output: [[0.74387249 0.87548514 0.75579284]]\n",
      "Expected output: [1. 0. 0.]\n",
      "Cost, J, associated with instance 104: 3.7889542370147606\n",
      "\n",
      "Predicted output: [[0.66917006 0.78527406 0.68086746]]\n",
      "Expected output: [0. 1. 0.]\n",
      "Cost, J, associated with instance 105: 2.49002210135841\n",
      "\n",
      "Predicted output: [[0.72193467 0.85198995 0.73377079]]\n",
      "Expected output: [0. 0. 1.]\n",
      "Cost, J, associated with instance 106: 3.499932857974312\n",
      "\n",
      "Predicted output: [[0.64887486 0.75584565 0.65842154]]\n",
      "Expected output: [0. 0. 1.]\n",
      "Cost, J, associated with instance 107: 2.8744772089988304\n",
      "\n",
      "Predicted output: [[0.68707637 0.80847948 0.69696232]]\n",
      "Expected output: [0. 0. 1.]\n",
      "Cost, J, associated with instance 108: 3.1755803713505153\n",
      "\n",
      "Predicted output: [[0.66363624 0.77452771 0.66970903]]\n",
      "Expected output: [0. 0. 1.]\n",
      "Cost, J, associated with instance 109: 2.980032038939073\n",
      "\n",
      "Predicted output: [[0.69812463 0.82295124 0.7111113 ]]\n",
      "Expected output: [0. 1. 0.]\n",
      "Cost, J, associated with instance 110: 2.6343131398769204\n",
      "\n",
      "Predicted output: [[0.7344049  0.86495706 0.74572672]]\n",
      "Expected output: [1. 0. 0.]\n",
      "Cost, J, associated with instance 111: 3.6802029487480845\n",
      "\n",
      "Predicted output: [[0.66199778 0.774673   0.67231861]]\n",
      "Expected output: [0. 1. 0.]\n",
      "Cost, J, associated with instance 112: 2.455730583909011\n",
      "\n",
      "Predicted output: [[0.72006592 0.8477043  0.73027235]]\n",
      "Expected output: [1. 0. 0.]\n",
      "Cost, J, associated with instance 113: 3.520686322798947\n",
      "\n",
      "Predicted output: [[0.72143884 0.85080524 0.73209639]]\n",
      "Expected output: [0. 0. 1.]\n",
      "Cost, J, associated with instance 114: 3.4924634765348803\n",
      "\n",
      "Predicted output: [[0.74203063 0.87354727 0.75396774]]\n",
      "Expected output: [1. 0. 0.]\n",
      "Cost, J, associated with instance 115: 3.7685440805318358\n",
      "\n",
      "Predicted output: [[0.74215051 0.87353284 0.75394804]]\n",
      "Expected output: [1. 0. 0.]\n",
      "Cost, J, associated with instance 116: 3.7681884073700465\n",
      "\n",
      "Predicted output: [[0.68032507 0.80104535 0.69276599]]\n",
      "Expected output: [0. 1. 0.]\n",
      "Cost, J, associated with instance 117: 2.542433950099025\n",
      "\n",
      "Predicted output: [[0.73994083 0.87132558 0.75178415]]\n",
      "Expected output: [1. 0. 0.]\n",
      "Cost, J, associated with instance 118: 3.74511151309813\n",
      "\n",
      "Predicted output: [[0.74250952 0.87407358 0.7544593 ]]\n",
      "Expected output: [1. 0. 0.]\n",
      "Cost, J, associated with instance 119: 3.7740696179119815\n",
      "\n",
      "Predicted output: [[0.69131411 0.81250713 0.69957644]]\n",
      "Expected output: [0. 0. 1.]\n",
      "Cost, J, associated with instance 120: 3.2067257502928945\n",
      "\n",
      "Predicted output: [[0.68634399 0.81015094 0.69996566]]\n",
      "Expected output: [0. 1. 0.]\n",
      "Cost, J, associated with instance 121: 2.5738514549912974\n",
      "\n",
      "Predicted output: [[0.72596957 0.85558649 0.73731941]]\n",
      "Expected output: [1. 0. 0.]\n",
      "Cost, J, associated with instance 122: 3.5921381515132955\n",
      "\n",
      "Predicted output: [[0.73291258 0.8645132  0.74564422]]\n",
      "Expected output: [0. 0. 1.]\n",
      "Cost, J, associated with instance 123: 3.612567022294328\n",
      "\n",
      "Predicted output: [[0.73131423 0.86210766 0.74313431]]\n",
      "Expected output: [0. 0. 1.]\n",
      "Cost, J, associated with instance 124: 3.592373252224909\n",
      "\n",
      "Predicted output: [[0.69197916 0.81292614 0.70004384]]\n",
      "Expected output: [1. 0. 0.]\n",
      "Cost, J, associated with instance 125: 3.2485701697688905\n",
      "\n",
      "Predicted output: [[0.74351232 0.87509158 0.7554114 ]]\n",
      "Expected output: [1. 0. 0.]\n",
      "Cost, J, associated with instance 126: 3.7847220165594404\n",
      "\n",
      "Predicted output: [[0.69813377 0.82203571 0.70696542]]\n",
      "Expected output: [0. 0. 1.]\n",
      "Cost, J, associated with instance 127: 3.270717175422149\n",
      "\n",
      "Predicted output: [[0.64301349 0.74615059 0.65072321]]\n",
      "Expected output: [0. 0. 1.]\n",
      "Cost, J, associated with instance 128: 2.830742254414949\n",
      "\n",
      "Predicted output: [[0.68623732 0.80684286 0.69502109]]\n",
      "Expected output: [0. 0. 1.]\n",
      "Cost, J, associated with instance 129: 3.1671826733865025\n",
      "\n",
      "Predicted output: [[0.64186697 0.74534301 0.65088193]]\n",
      "Expected output: [0. 1. 0.]\n",
      "Cost, J, associated with instance 130: 2.3731066137895596\n",
      "\n",
      "Predicted output: [[0.72895715 0.85888331 0.74106761]]\n",
      "Expected output: [0. 1. 0.]\n",
      "Cost, J, associated with instance 131: 2.808788875859133\n",
      "\n",
      "Predicted output: [[0.64443988 0.74927281 0.65372424]]\n",
      "Expected output: [0. 1. 0.]\n",
      "Cost, J, associated with instance 132: 2.3832328758833112\n",
      "\n",
      "Predicted output: [[0.72091009 0.85156192 0.73516304]]\n",
      "Expected output: [0. 1. 0.]\n",
      "Cost, J, associated with instance 133: 2.7655452371330185\n",
      "\n",
      "Predicted output: [[0.70612188 0.83477744 0.72112474]]\n",
      "Expected output: [0. 1. 0.]\n",
      "Cost, J, associated with instance 134: 2.682170976217808\n",
      "\n",
      "Predicted output: [[0.7443087  0.87596054 0.75623782]]\n",
      "Expected output: [0. 1. 0.]\n",
      "Cost, J, associated with instance 135: 2.907780881593132\n",
      "\n",
      "Predicted output: [[0.73879807 0.87031793 0.75088615]]\n",
      "Expected output: [0. 0. 1.]\n",
      "Cost, J, associated with instance 136: 3.6716321776076195\n",
      "\n",
      "Predicted output: [[0.74178577 0.87308062 0.75346003]]\n",
      "Expected output: [1. 0. 0.]\n",
      "Cost, J, associated with instance 137: 3.763129151965929\n",
      "\n",
      "Predicted output: [[0.73665433 0.8679533  0.74931399]]\n",
      "Expected output: [0. 1. 0.]\n",
      "Cost, J, associated with instance 138: 2.859459212643231\n",
      "\n",
      "Predicted output: [[0.65122413 0.75907512 0.66070936]]\n",
      "Expected output: [0. 1. 0.]\n",
      "Cost, J, associated with instance 139: 2.409878496785214\n",
      "\n",
      "Predicted output: [[0.74380797 0.87533597 0.75561725]]\n",
      "Expected output: [1. 0. 0.]\n",
      "Cost, J, associated with instance 140: 3.787124929227417\n",
      "\n",
      "Predicted output: [[0.65978365 0.77164212 0.67016939]]\n",
      "Expected output: [0. 1. 0.]\n",
      "Cost, J, associated with instance 141: 2.446584015703694\n",
      "\n",
      "Predicted output: [[0.70598969 0.83400751 0.71964425]]\n",
      "Expected output: [0. 1. 0.]\n",
      "Cost, J, associated with instance 142: 2.677349284710714\n",
      "FINAL REG COST : 1.0863325470548264\n",
      "Accuracy: 0.3888888888888889\n",
      "F1 Score: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        self.weights = [np.random.rand(layers[i], layers[i+1]) for i in range(len(layers)-1)]\n",
    "        self.biases = [np.zeros((1, layers[i+1])) for i in range(len(layers)-1)]\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def predict(self, X):\n",
    "        activations = X\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(activations, w) + b\n",
    "            activations = self.sigmoid(z)\n",
    "        return activations\n",
    "    \n",
    "    def train(self, X, Y, lam):\n",
    "        total_cost = 0.0\n",
    "        total_reg_cost = 0.0\n",
    "        m = len(X)\n",
    "        for i, (x, y) in enumerate(zip(X, Y), 1):\n",
    "            activations = x\n",
    "            zs = []\n",
    "            for w, b in zip(self.weights, self.biases):\n",
    "                z = np.dot(activations, w) + b\n",
    "                zs.append(z)\n",
    "                activations = self.sigmoid(z)\n",
    "            prediction = activations\n",
    "            cost = -y * np.log(prediction) - (1 - y) * np.log(1 - prediction)\n",
    "            instance_cost = np.sum(cost)\n",
    "            reg_term = sum(np.sum(w[:, 1:] ** 2) for w in self.weights)\n",
    "            total_reg_cost += reg_term\n",
    "            total_cost += instance_cost\n",
    "            print(f\"\\nPredicted output: {prediction}\")\n",
    "            print(f\"Expected output: {y}\")\n",
    "            print(f\"Cost, J, associated with instance {i}: {instance_cost}\")\n",
    "        avg_cost = total_cost / m\n",
    "        final_reg_cost = (lam / (2 * m)) * total_reg_cost\n",
    "        print(\"FINAL REG COST :\", final_reg_cost)\n",
    "\n",
    "def normalize(X):\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        X = X.values\n",
    "    mean = np.mean(X, axis=0)\n",
    "    std = np.std(X, axis=0)\n",
    "    X_normalized = (X - mean) / std\n",
    "    return X_normalized, mean, std\n",
    "\n",
    "def min_max_normalize(y):\n",
    "    y_min = np.min(y)\n",
    "    y_max = np.max(y)\n",
    "    y_scaled = (y - y_min) / (y_max - y_min)\n",
    "    return y_scaled\n",
    "\n",
    "def accuracy_score(y_true, y_pred):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for true_row, pred_row in zip(y_true, y_pred):\n",
    "        true_label = true_row.argmax()  # Get index of true label\n",
    "        pred_label = pred_row.argmax()  # Get index of predicted label\n",
    "        if true_label == pred_label:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    return correct / total\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    for true_row, pred_row in zip(y_true, y_pred):\n",
    "        true_label = true_row.argmax()  # Get index of true label\n",
    "        pred_label = pred_row.argmax()  # Get index of predicted label\n",
    "        if true_label == 1 and pred_label == 1:\n",
    "            true_positives += 1\n",
    "        elif true_label == 0 and pred_label == 1:\n",
    "            false_positives += 1\n",
    "        elif true_label == 1 and pred_label == 0:\n",
    "            false_negatives += 1\n",
    "    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n",
    "    return 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
    "\n",
    "def evaluate_model_performance(model, X_train, y_train, X_test, y_test):\n",
    "    model.train(X_train, y_train, lam=0.1)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    return accuracy, f1\n",
    "\n",
    "# Load dataset\n",
    "df_wine = pd.read_csv(\"/Users/noshitha/Downloads/hw3/datasets/hw3_wine.csv\", delimiter=\"\\t\")\n",
    "\n",
    "# Preprocess data\n",
    "X_wine = pd.get_dummies(df_wine.iloc[:, 1:])\n",
    "y_wine = df_wine.iloc[:, 0]\n",
    "\n",
    "# Normalize data\n",
    "X_wine_normalized, mean, std = normalize(X_wine)\n",
    "y_wine_resized = y_wine.values.reshape(-1, 1)\n",
    "\n",
    "# Initialize the OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Fit and transform the target variable\n",
    "y_encoded = encoder.fit_transform(y_wine_resized)\n",
    "\n",
    "# Define model architectures\n",
    "architectures = [\n",
    "    [X_wine_normalized.shape[1], 5, 3, 3],  # Example architecture\n",
    "]\n",
    "\n",
    "# Instantiate the NeuralNetwork class with the desired architecture\n",
    "model = NeuralNetwork(architectures[0])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_wine_normalized, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train and evaluate the model\n",
    "accuracy, f1 = evaluate_model_performance(model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8273b8b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c231f4fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efd1d2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fc4d4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient_weights:  [array([[ 0.00123032, -0.00061742, -0.00158181],\n",
      "       [ 0.00148644, -0.00108035,  0.00023128]]), array([[0.00948701],\n",
      "       [0.01227936],\n",
      "       [0.01831538]])]\n",
      "gradients_biases:  [array([-1.17683697e-04,  9.54263274e-05,  9.91072654e-04]), array([0.02983926])]\n",
      "Iteration 1, Cost: 0.2511552269672239\n",
      "gradient_weights:  [array([[ 0.00125196, -0.00061949, -0.00160434],\n",
      "       [ 0.00151172, -0.00107877,  0.00019835]]), array([[0.00845177],\n",
      "       [0.0112689 ],\n",
      "       [0.01739451]])]\n",
      "gradients_biases:  [array([-8.47550809e-05,  6.90907268e-05,  9.14129009e-04]), array([0.02843103])]\n",
      "Iteration 2, Cost: 0.25108348152586213\n",
      "gradient_weights:  [array([[ 0.001272  , -0.00062137, -0.00162487],\n",
      "       [ 0.00153513, -0.00107732,  0.00016779]]), array([[0.00747873],\n",
      "       [0.01031873],\n",
      "       [0.01652843]])]\n",
      "gradients_biases:  [array([-5.33350034e-05,  4.49324969e-05,  8.42691176e-04]), array([0.0271067])]\n",
      "Iteration 3, Cost: 0.2510193632328699\n",
      "gradient_weights:  [array([[ 0.00129051, -0.0006231 , -0.00164357],\n",
      "       [ 0.00155675, -0.00107601,  0.0001394 ]]), array([[0.00656436],\n",
      "       [0.00942544],\n",
      "       [0.01571404]])]\n",
      "gradients_biases:  [array([-2.34163793e-05,  2.27371427e-05,  7.76339298e-04]), array([0.02586152])]\n",
      "Iteration 4, Cost: 0.2509619803724282\n",
      "gradient_weights:  [array([[ 0.00130754, -0.00062474, -0.0016606 ],\n",
      "       [ 0.00157664, -0.00107488,  0.00011304]]), array([[0.00570533],\n",
      "       [0.0085858 ],\n",
      "       [0.01494841]])]\n",
      "gradients_biases:  [array([5.01827671e-06, 2.31256582e-06, 7.14687145e-04]), array([0.02469097])]\n",
      "Iteration 5, Cost: 0.25091054392845846\n",
      "gradient_weights:  [array([[ 1.32316392e-03, -6.26304950e-04, -1.67608553e-03],\n",
      "       [ 1.59488715e-03, -1.07395587e-03,  8.85494907e-05]]), array([[0.00489844],\n",
      "       [0.00779672],\n",
      "       [0.01422874]])]\n",
      "gradients_biases:  [array([ 3.19949013e-05, -1.65132758e-05,  6.57379430e-04]), array([0.02359074])]\n",
      "Iteration 6, Cost: 0.250864356124191\n",
      "gradient_weights:  [array([[ 1.33744308e-03, -6.27827235e-04, -1.69016333e-03],\n",
      "       [ 1.61156692e-03, -1.07324437e-03,  6.57872800e-05]]), array([[0.00414069],\n",
      "       [0.00705528],\n",
      "       [0.01355237]])]\n",
      "gradients_biases:  [array([ 5.75462022e-05, -3.38944811e-05,  6.04089297e-04]), array([0.02255675])]\n",
      "Iteration 7, Cost: 0.2508228001601336\n",
      "gradient_weights:  [array([[ 1.35044160e-03, -6.29329242e-04, -1.70294450e-03],\n",
      "       [ 1.62675289e-03, -1.07276093e-03,  4.46273202e-05]]), array([[0.0034292 ],\n",
      "       [0.00635869],\n",
      "       [0.01291678]])]\n",
      "gradients_biases:  [array([ 8.17102149e-05, -4.99691088e-05,  5.54516005e-04]), array([0.02158513])]\n",
      "Iteration 8, Cost: 0.25078533104316425\n",
      "gradient_weights:  [array([[ 1.36222176e-03, -6.30829898e-04, -1.71453415e-03],\n",
      "       [ 1.64051812e-03, -1.07251364e-03,  2.49514192e-05]]), array([[0.00276125],\n",
      "       [0.00570434],\n",
      "       [0.01231957]])]\n",
      "gradients_biases:  [array([ 1.04529068e-04, -6.48608865e-05,  5.08382771e-04]), array([0.02067221])]\n",
      "Iteration 9, Cost: 0.2507514674048077\n",
      "gradient_weights:  [array([[ 1.37284431e-03, -6.32344814e-04, -1.72502837e-03],\n",
      "       [ 1.65293381e-03, -1.07250741e-03,  6.65058001e-06]]), array([[0.00213427],\n",
      "       [0.00508971],\n",
      "       [0.01175848]])]\n",
      "gradients_biases:  [array([ 1.26047931e-04, -7.86807449e-05,  4.65434790e-04]), array([0.01981452])]\n",
      "Iteration 10, Cost: 0.2507207842129846\n",
      "gradient_weights:  [array([[ 1.38236832e-03, -6.33886738e-04, -1.73451491e-03],\n",
      "       [ 1.66406909e-03, -1.07274445e-03, -1.03757329e-05]]), array([[0.00154583],\n",
      "       [0.00451247],\n",
      "       [0.01123138]])]\n",
      "gradients_biases:  [array([ 1.46314123e-04, -9.15281905e-05,  4.25437413e-04]), array([0.01900878])]\n",
      "Iteration 11, Cost: 0.2506929062882428\n",
      "gradient_weights:  [array([[ 1.39085101e-03, -6.35465951e-04, -1.74307405e-03],\n",
      "       [ 1.67399083e-03, -1.07322474e-03, -2.62202714e-05]]), array([[0.00099364],\n",
      "       [0.00397037],\n",
      "       [0.01073623]])]\n",
      "gradients_biases:  [array([ 0.00016538, -0.00010349,  0.00038817]), array([0.0182519])]\n",
      "Iteration 12, Cost: 0.25066750254237125\n",
      "gradient_weights:  [array([[ 1.39834761e-03, -6.37090615e-04, -1.75077919e-03],\n",
      "       [ 1.68276350e-03, -1.07394639e-03, -4.09686306e-05]]), array([[0.00047552],\n",
      "       [0.00346134],\n",
      "       [0.01027114]])]\n",
      "gradients_biases:  [array([ 0.00018328, -0.00011465,  0.00035345]), array([0.01754095])]\n",
      "Iteration 13, Cost: 0.2506442808641516\n",
      "gradient_weights:  [array([[ 1.40491128e-03, -6.38767079e-04, -1.75769749e-03],\n",
      "       [ 1.69044915e-03, -1.07490603e-03, -5.46998258e-05]]), array([[-1.05662744e-05],\n",
      "       [ 2.98337293e-03],\n",
      "       [ 9.83430714e-03]])]\n",
      "gradients_biases:  [array([ 0.00020009, -0.00012508,  0.00032107]), array([0.01687317])]\n",
      "Iteration 14, Cost: 0.250622983583653\n",
      "gradient_weights:  [array([[ 1.41059308e-03, -6.40500147e-04, -1.76389045e-03],\n",
      "       [ 1.69710730e-03, -1.07609902e-03, -6.74868240e-05]]), array([[-0.00046655],\n",
      "       [ 0.00253461],\n",
      "       [ 0.00942403]])]\n",
      "gradients_biases:  [array([ 0.00021583, -0.00013485,  0.00029088]), array([0.01624596])]\n",
      "Iteration 15, Cost: 0.250603383452829\n",
      "gradient_weights:  [array([[ 1.41544194e-03, -6.42293310e-04, -1.76941439e-03],\n",
      "       [ 1.70279494e-03, -1.07751977e-03, -7.93970322e-05]]), array([[-0.00089425],\n",
      "       [ 0.0021133 ],\n",
      "       [ 0.00903871]])]\n",
      "gradients_biases:  [array([ 0.00023058, -0.00014401,  0.00026271]), array([0.01565688])]\n",
      "Iteration 16, Cost: 0.2505852800861487\n",
      "gradient_weights:  [array([[ 1.41950466e-03, -6.44148951e-04, -1.77432095e-03],\n",
      "       [ 1.70756656e-03, -1.07916189e-03, -9.04927455e-05]]), array([[-0.00129538],\n",
      "       [ 0.00171776],\n",
      "       [ 0.00867684]])]\n",
      "gradients_biases:  [array([ 0.00024436, -0.00015261,  0.00023642]), array([0.01510362])]\n",
      "Iteration 17, Cost: 0.25056849681057636\n",
      "gradient_weights:  [array([[ 0.00142283, -0.00064607, -0.00177866],\n",
      "       [ 0.00171147, -0.00108102, -0.00010083]]), array([[-0.00167155],\n",
      "       [ 0.00134646],\n",
      "       [ 0.008337  ]])]\n",
      "gradients_biases:  [array([ 0.00025723, -0.0001607 ,  0.00021188]), array([0.01458401])]\n",
      "Iteration 18, Cost: 0.25055287787935654\n",
      "gradient_weights:  [array([[ 0.00142545, -0.00064805, -0.00178247],\n",
      "       [ 0.00171457, -0.00108308, -0.00011047]]), array([[-0.00202426],\n",
      "       [ 0.0009979 ],\n",
      "       [ 0.00801785]])]\n",
      "gradients_biases:  [array([ 0.00026924, -0.00016833,  0.00018897]), array([0.014096])]\n",
      "Iteration 19, Cost: 0.2505382860087944\n",
      "gradient_weights:  [array([[ 0.00142741, -0.0006501 , -0.00178579],\n",
      "       [ 0.00171689, -0.00108535, -0.00011945]]), array([[-0.00235496],\n",
      "       [ 0.00067072],\n",
      "       [ 0.00771815]])]\n",
      "gradients_biases:  [array([ 0.00028043, -0.00017554,  0.00016756]), array([0.01363768])]\n",
      "Iteration 20, Cost: 0.250524600201531\n",
      "gradient_weights:  [array([[ 0.00142876, -0.00065221, -0.00178866],\n",
      "       [ 0.0017185 , -0.0010878 , -0.00012782]]), array([[-0.00266499],\n",
      "       [ 0.00036361],\n",
      "       [ 0.00743671]])]\n",
      "gradients_biases:  [array([ 0.00029084, -0.00018235,  0.00014756]), array([0.01320725])]\n",
      "Iteration 21, Cost: 0.2505117138237327\n",
      "gradient_weights:  [array([[ 0.00142952, -0.00065439, -0.00179112],\n",
      "       [ 0.00171942, -0.00109044, -0.00013563]]), array([[-2.95559444e-03],\n",
      "       [ 7.53526897e-05],\n",
      "       [ 7.17241597e-03]])]\n",
      "gradients_biases:  [array([ 0.00030051, -0.0001888 ,  0.00012887]), array([0.01280298])]\n",
      "Iteration 22, Cost: 0.2504995329071582\n",
      "gradient_weights:  [array([[ 0.00142973, -0.00065663, -0.00179319],\n",
      "       [ 0.0017197 , -0.00109325, -0.0001429 ]]), array([[-0.00322797],\n",
      "       [-0.00019521],\n",
      "       [ 0.00692422]])]\n",
      "gradients_biases:  [array([ 0.00030948, -0.00019493,  0.00011139]), array([0.01242331])]\n",
      "Iteration 23, Cost: 0.2504879746502587\n",
      "gradient_weights:  [array([[ 0.00142943, -0.00065893, -0.00179491],\n",
      "       [ 0.00171938, -0.00109623, -0.00014969]]), array([[-0.00348324],\n",
      "       [-0.00044916],\n",
      "       [ 0.00669115]])]\n",
      "gradients_biases:  [array([ 3.17801457e-04, -2.00744836e-04,  9.50509323e-05]), array([0.01206671])]\n",
      "Iteration 24, Cost: 0.2504769660953391\n",
      "gradient_weights:  [array([[ 0.00142864, -0.00066128, -0.00179629],\n",
      "       [ 0.0017185 , -0.00109937, -0.00015602]]), array([[-0.00372244],\n",
      "       [-0.0006875 ],\n",
      "       [ 0.00647228]])]\n",
      "gradients_biases:  [array([ 3.25497049e-04, -2.06282758e-04,  7.97637657e-05]), array([0.01173178])]\n",
      "Iteration 25, Cost: 0.25046644296138393\n",
      "gradient_weights:  [array([[ 0.00142739, -0.0006637 , -0.00179737],\n",
      "       [ 0.00171708, -0.00110267, -0.00016193]]), array([[-0.00394655],\n",
      "       [-0.0009112 ],\n",
      "       [ 0.00626673]])]\n",
      "gradients_biases:  [array([ 3.32605572e-04, -2.11561595e-04,  6.54598728e-05]), array([0.01141719])]\n",
      "Iteration 26, Cost: 0.250456348614449\n",
      "gradient_weights:  [array([[ 0.00142572, -0.00066616, -0.00179816],\n",
      "       [ 0.00171517, -0.0011061 , -0.00016743]]), array([[-0.00415652],\n",
      "       [-0.00112114],\n",
      "       [ 0.0060737 ]])]\n",
      "gradients_biases:  [array([ 3.39160268e-04, -2.16601026e-04,  5.20723921e-05]), array([0.01112171])]\n",
      "Iteration 27, Cost: 0.2504466331595828\n",
      "gradient_weights:  [array([[ 0.00142365, -0.00066869, -0.00179869],\n",
      "       [ 0.00171278, -0.00110968, -0.00017257]]), array([[-0.00435319],\n",
      "       [-0.00131818],\n",
      "       [ 0.00589242]])]\n",
      "gradients_biases:  [array([ 3.45192778e-04, -2.21419008e-04,  3.95392570e-05]), array([0.01084415])]\n",
      "Iteration 28, Cost: 0.2504372526400679\n",
      "gradient_weights:  [array([[ 0.0014212 , -0.00067126, -0.00179898],\n",
      "       [ 0.00170996, -0.00111338, -0.00017736]]), array([[-0.0045374 ],\n",
      "       [-0.00150311],\n",
      "       [ 0.00572217]])]\n",
      "gradients_biases:  [array([ 3.50733188e-04, -2.26031942e-04,  2.78028274e-05]), array([0.01058342])]\n",
      "Iteration 29, Cost: 0.2504281683314087\n",
      "gradient_weights:  [array([[ 0.0014184 , -0.00067387, -0.00179904],\n",
      "       [ 0.00170672, -0.00111721, -0.00018183]]), array([[-0.0047099 ],\n",
      "       [-0.00167666],\n",
      "       [ 0.00556228]])]\n",
      "gradients_biases:  [array([ 3.55810063e-04, -2.30454828e-04,  1.68095520e-05]), array([0.01033849])]\n",
      "Iteration 30, Cost: 0.2504193461189395\n",
      "gradient_weights:  [array([[ 0.00141527, -0.00067654, -0.00179888],\n",
      "       [ 0.0017031 , -0.00112116, -0.000186  ]]), array([[-0.00487141],\n",
      "       [-0.00183954],\n",
      "       [ 0.0054121 ]])]\n",
      "gradients_biases:  [array([ 3.60450494e-04, -2.34701405e-04,  6.50965808e-06]), array([0.01010839])]\n",
      "Iteration 31, Cost: 0.2504107559492179\n",
      "gradient_weights:  [array([[ 0.00141183, -0.00067925, -0.00179854],\n",
      "       [ 0.00169911, -0.00112522, -0.00018989]]), array([[-0.00502262],\n",
      "       [-0.00199239],\n",
      "       [ 0.00527105]])]\n",
      "gradients_biases:  [array([ 3.64680150e-04, -2.38784270e-04, -3.14313257e-06]), array([0.00989221])]\n",
      "Iteration 32, Cost: 0.2504023713465068\n",
      "gradient_weights:  [array([[ 0.0014081 , -0.000682  , -0.00179801],\n",
      "       [ 0.00169479, -0.00112938, -0.00019351]]), array([[-0.00516415],\n",
      "       [-0.00213585],\n",
      "       [ 0.00513856]])]\n",
      "gradients_biases:  [array([ 3.68523328e-04, -2.42714987e-04, -1.21918654e-05]), array([0.00968909])]\n",
      "Iteration 33, Cost: 0.2503941689866672\n",
      "gradient_weights:  [array([[ 0.00140409, -0.00068478, -0.00179731],\n",
      "       [ 0.00169014, -0.00113364, -0.0001969 ]]), array([[-0.00529661],\n",
      "       [-0.00227048],\n",
      "       [ 0.00501411]])]\n",
      "gradients_biases:  [array([ 3.72003005e-04, -2.46504186e-04, -2.06765929e-05]), array([0.00949824])]\n",
      "Iteration 34, Cost: 0.25038612832167706\n",
      "gradient_weights:  [array([[ 0.00139984, -0.00068761, -0.00179645],\n",
      "       [ 0.0016852 , -0.00113799, -0.00020005]]), array([[-0.00542055],\n",
      "       [-0.00239682],\n",
      "       [ 0.0048972 ]])]\n",
      "gradients_biases:  [array([ 3.75140893e-04, -2.50161651e-04, -2.86345959e-05]), array([0.00931889])]\n",
      "Iteration 35, Cost: 0.25037823124878744\n",
      "gradient_weights:  [array([[ 0.00139534, -0.00069047, -0.00179545],\n",
      "       [ 0.00167997, -0.00114243, -0.00020299]]), array([[-0.00553649],\n",
      "       [-0.0025154 ],\n",
      "       [ 0.00478738]])]\n",
      "gradients_biases:  [array([ 3.77957495e-04, -2.53696394e-04, -3.61005862e-05]), array([0.00915034])]\n",
      "Iteration 36, Cost: 0.25037046181903083\n",
      "gradient_weights:  [array([[ 0.00139062, -0.00069337, -0.00179431],\n",
      "       [ 0.00167449, -0.00114695, -0.00020574]]), array([[-0.00564494],\n",
      "       [-0.00262668],\n",
      "       [ 0.00468421]])]\n",
      "gradients_biases:  [array([ 3.80472157e-04, -2.57116731e-04, -4.31068945e-05]), array([0.00899194])]\n",
      "Iteration 37, Cost: 0.2503628059804172\n",
      "gradient_weights:  [array([[ 0.00138569, -0.00069629, -0.00179305],\n",
      "       [ 0.00166875, -0.00115156, -0.00020829]]), array([[-0.00574634],\n",
      "       [-0.00273111],\n",
      "       [ 0.00458727]])]\n",
      "gradients_biases:  [array([ 3.82703123e-04, -2.60430342e-04, -4.96836422e-05]), array([0.00884305])]\n",
      "Iteration 38, Cost: 0.2503552513517022\n",
      "gradient_weights:  [array([[ 0.00138057, -0.00069925, -0.00179167],\n",
      "       [ 0.00166279, -0.00115623, -0.00021068]]), array([[-0.00584115],\n",
      "       [-0.00282912],\n",
      "       [ 0.00449619]])]\n",
      "gradients_biases:  [array([ 3.84667589e-04, -2.63644327e-04, -5.58589008e-05]), array([0.00870309])]\n",
      "Iteration 39, Cost: 0.2503477870230998\n",
      "gradient_weights:  [array([[ 0.00137526, -0.00070223, -0.00179018],\n",
      "       [ 0.00165662, -0.00116098, -0.0002129 ]]), array([[-0.00592976],\n",
      "       [-0.0029211 ],\n",
      "       [ 0.00441061]])]\n",
      "gradients_biases:  [array([ 3.86381750e-04, -2.66765256e-04, -6.16588388e-05]), array([0.00857151])]\n",
      "Iteration 40, Cost: 0.2503404033807364\n",
      "gradient_weights:  [array([[ 0.00136978, -0.00070524, -0.00178859],\n",
      "       [ 0.00165024, -0.00116579, -0.00021497]]), array([[-0.00601255],\n",
      "       [-0.00300742],\n",
      "       [ 0.00433019]])]\n",
      "gradients_biases:  [array([ 3.87860856e-04, -2.69799219e-04, -6.71078564e-05]), array([0.0084478])]\n",
      "Iteration 41, Cost: 0.2503330919520263\n",
      "gradient_weights:  [array([[ 0.00136415, -0.00070828, -0.0017869 ],\n",
      "       [ 0.00164367, -0.00117066, -0.00021689]]), array([[-0.00608989],\n",
      "       [-0.00308843],\n",
      "       [ 0.00425461]])]\n",
      "gradients_biases:  [array([ 3.89119259e-04, -2.72751863e-04, -7.22287116e-05]), array([0.00833147])]\n",
      "Iteration 42, Cost: 0.2503258452694797\n",
      "gradient_weights:  [array([[ 0.00135837, -0.00071134, -0.00178512],\n",
      "       [ 0.00163693, -0.00117559, -0.00021869]]), array([[-0.00616212],\n",
      "       [-0.00316446],\n",
      "       [ 0.00418358]])]\n",
      "gradients_biases:  [array([ 3.90170457e-04, -2.75628431e-04, -7.70426351e-05]), array([0.00822207])]\n",
      "Iteration 43, Cost: 0.25031865675074916\n",
      "gradient_weights:  [array([[ 0.00135244, -0.00071442, -0.00178327],\n",
      "       [ 0.00163003, -0.00118057, -0.00022036]]), array([[-0.00622954],\n",
      "       [-0.00323582],\n",
      "       [ 0.00411681]])]\n",
      "gradients_biases:  [array([ 3.91027144e-04, -2.78433794e-04, -8.15694376e-05]), array([0.00811917])]\n",
      "Iteration 44, Cost: 0.2503115205929829\n",
      "gradient_weights:  [array([[ 0.00134639, -0.00071752, -0.00178133],\n",
      "       [ 0.00162297, -0.00118561, -0.00022191]]), array([[-0.00629247],\n",
      "       [-0.00330279],\n",
      "       [ 0.00405404]])]\n",
      "gradients_biases:  [array([ 3.91701250e-04, -2.81172484e-04, -8.58276089e-05]), array([0.00802237])]\n",
      "Iteration 45, Cost: 0.2503044316797801\n",
      "gradient_weights:  [array([[ 0.00134022, -0.00072064, -0.00177933],\n",
      "       [ 0.00161577, -0.00119069, -0.00022335]]), array([[-0.00635116],\n",
      "       [-0.00336564],\n",
      "       [ 0.00399504]])]\n",
      "gradients_biases:  [array([ 3.92203987e-04, -2.83848720e-04, -8.98344090e-05]), array([0.00793131])]\n",
      "Iteration 46, Cost: 0.2502973854992478\n",
      "gradient_weights:  [array([[ 0.00133394, -0.00072378, -0.00177726],\n",
      "       [ 0.00160843, -0.00119582, -0.0002247 ]]), array([[-0.0064059 ],\n",
      "       [-0.00342463],\n",
      "       [ 0.00393956]])]\n",
      "gradients_biases:  [array([ 3.92545885e-04, -2.86466432e-04, -9.36059536e-05]), array([0.00784562])]\n",
      "Iteration 47, Cost: 0.2502903780718355\n",
      "gradient_weights:  [array([[ 0.00132755, -0.00072693, -0.00177513],\n",
      "       [ 0.00160097, -0.00120099, -0.00022594]]), array([[-0.00645691],\n",
      "       [-0.00348   ],\n",
      "       [ 0.00388739]])]\n",
      "gradients_biases:  [array([ 3.92736832e-04, -2.89029285e-04, -9.71572925e-05]), array([0.00776497])]\n",
      "Iteration 48, Cost: 0.250283405886783\n",
      "gradient_weights:  [array([[ 0.00132107, -0.0007301 , -0.00177294],\n",
      "       [ 0.0015934 , -0.0012062 , -0.0002271 ]]), array([[-0.00650444],\n",
      "       [-0.00353197],\n",
      "       [ 0.00383833]])]\n",
      "gradients_biases:  [array([ 0.00039279, -0.00029154, -0.0001005 ]), array([0.00768907])]\n",
      "Iteration 49, Cost: 0.25027646584615393\n",
      "gradient_weights:  [array([[ 0.00131449, -0.00073329, -0.00177069],\n",
      "       [ 0.00158571, -0.00121145, -0.00022818]]), array([[-0.00654869],\n",
      "       [-0.00358074],\n",
      "       [ 0.00379218]])]\n",
      "gradients_biases:  [array([ 0.0003927 , -0.000294  , -0.00010365]), array([0.0076176])]\n",
      "Iteration 50, Cost: 0.25026955521554956\n",
      "gradient_weights:  [array([[ 0.00130783, -0.00073649, -0.0017684 ],\n",
      "       [ 0.00157793, -0.00121673, -0.00022918]]), array([[-0.00658988],\n",
      "       [-0.00362652],\n",
      "       [ 0.00374877]])]\n",
      "gradients_biases:  [array([ 0.00039249, -0.00029642, -0.00010663]), array([0.00755031])]\n",
      "Iteration 51, Cost: 0.2502626715807079\n",
      "gradient_weights:  [array([[ 0.00130109, -0.0007397 , -0.00176606],\n",
      "       [ 0.00157005, -0.00122205, -0.0002301 ]]), array([[-0.00662819],\n",
      "       [-0.0036695 ],\n",
      "       [ 0.00370794]])]\n",
      "gradients_biases:  [array([ 0.00039217, -0.0002988 , -0.00010943]), array([0.00748693])]\n",
      "Iteration 52, Cost: 0.2502558128092846\n",
      "gradient_weights:  [array([[ 0.00129428, -0.00074293, -0.00176368],\n",
      "       [ 0.00156208, -0.0012274 , -0.00023095]]), array([[-0.0066638 ],\n",
      "       [-0.00370983],\n",
      "       [ 0.00366951]])]\n",
      "gradients_biases:  [array([ 0.00039173, -0.00030113, -0.00011207]), array([0.00742722])]\n",
      "Iteration 53, Cost: 0.25024897701719684\n",
      "gradient_weights:  [array([[ 0.0012874 , -0.00074617, -0.00176125],\n",
      "       [ 0.00155403, -0.00123277, -0.00023175]]), array([[-0.00669687],\n",
      "       [-0.0037477 ],\n",
      "       [ 0.00363335]])]\n",
      "gradients_biases:  [array([ 0.00039119, -0.00030343, -0.00011457]), array([0.00737097])]\n",
      "Iteration 54, Cost: 0.25024216253898723\n",
      "gradient_weights:  [array([[ 0.00128046, -0.00074941, -0.00175879],\n",
      "       [ 0.0015459 , -0.00123818, -0.00023248]]), array([[-0.00672757],\n",
      "       [-0.00378324],\n",
      "       [ 0.00359931]])]\n",
      "gradients_biases:  [array([ 0.00039056, -0.00030569, -0.00011692]), array([0.00731795])]\n",
      "Iteration 55, Cost: 0.2502353679017265\n",
      "gradient_weights:  [array([[ 0.00127346, -0.00075267, -0.0017563 ],\n",
      "       [ 0.00153771, -0.00124361, -0.00023315]]), array([[-0.00675604],\n",
      "       [-0.00381661],\n",
      "       [ 0.00356727]])]\n",
      "gradients_biases:  [array([ 0.00038983, -0.00030792, -0.00011915]), array([0.00726797])]\n",
      "Iteration 56, Cost: 0.2502285918020335\n",
      "gradient_weights:  [array([[ 0.00126641, -0.00075594, -0.00175377],\n",
      "       [ 0.00152944, -0.00124906, -0.00023377]]), array([[-0.00678242],\n",
      "       [-0.00384793],\n",
      "       [ 0.0035371 ]])]\n",
      "gradients_biases:  [array([ 0.00038902, -0.00031012, -0.00012125]), array([0.00722084])]\n",
      "Iteration 57, Cost: 0.25022183308583895\n",
      "gradient_weights:  [array([[ 0.00125931, -0.00075921, -0.00175121],\n",
      "       [ 0.00152112, -0.00125454, -0.00023435]]), array([[-0.00680684],\n",
      "       [-0.00387734],\n",
      "       [ 0.00350868]])]\n",
      "gradients_biases:  [array([ 0.00038813, -0.00031229, -0.00012324]), array([0.00717639])]\n",
      "Iteration 58, Cost: 0.25021509073056747\n",
      "gradient_weights:  [array([[ 0.00125216, -0.0007625 , -0.00174862],\n",
      "       [ 0.00151274, -0.00126004, -0.00023487]]), array([[-0.00682942],\n",
      "       [-0.00390495],\n",
      "       [ 0.00348192]])]\n",
      "gradients_biases:  [array([ 0.00038716, -0.00031444, -0.00012512]), array([0.00713445])]\n",
      "Iteration 59, Cost: 0.2502083638294467\n",
      "gradient_weights:  [array([[ 0.00124497, -0.00076579, -0.00174601],\n",
      "       [ 0.00150431, -0.00126555, -0.00023536]]), array([[-0.00685028],\n",
      "       [-0.00393087],\n",
      "       [ 0.00345671]])]\n",
      "gradients_biases:  [array([ 0.00038612, -0.00031655, -0.0001269 ]), array([0.00709487])]\n",
      "Iteration 60, Cost: 0.25020165157769164\n",
      "gradient_weights:  [array([[ 0.00123774, -0.00076909, -0.00174337],\n",
      "       [ 0.00149583, -0.00127109, -0.0002358 ]]), array([[-0.00686952],\n",
      "       [-0.00395521],\n",
      "       [ 0.00343295]])]\n",
      "gradients_biases:  [array([ 0.00038502, -0.00031865, -0.00012859]), array([0.0070575])]\n",
      "Iteration 61, Cost: 0.2501949532603383\n",
      "gradient_weights:  [array([[ 0.00123048, -0.00077239, -0.00174072],\n",
      "       [ 0.0014873 , -0.00127665, -0.0002362 ]]), array([[-0.00688725],\n",
      "       [-0.00397806],\n",
      "       [ 0.00341055]])]\n",
      "gradients_biases:  [array([ 0.00038385, -0.00032072, -0.00013018]), array([0.00702221])]\n",
      "Iteration 62, Cost: 0.25018826824153145\n",
      "gradient_weights:  [array([[ 0.00122318, -0.00077571, -0.00173803],\n",
      "       [ 0.00147873, -0.00128222, -0.00023657]]), array([[-0.00690355],\n",
      "       [-0.00399953],\n",
      "       [ 0.00338944]])]\n",
      "gradients_biases:  [array([ 0.00038263, -0.00032277, -0.0001317 ]), array([0.00698887])]\n",
      "Iteration 63, Cost: 0.2501815959550915\n",
      "gradient_weights:  [array([[ 0.00121585, -0.00077902, -0.00173533],\n",
      "       [ 0.00147013, -0.00128781, -0.0002369 ]]), array([[-0.00691852],\n",
      "       [-0.00401968],\n",
      "       [ 0.00336953]])]\n",
      "gradients_biases:  [array([ 0.00038135, -0.00032479, -0.00013314]), array([0.00695736])]\n",
      "Iteration 64, Cost: 0.2501749358962083\n",
      "gradient_weights:  [array([[ 0.00120849, -0.00078235, -0.00173262],\n",
      "       [ 0.00146149, -0.00129341, -0.0002372 ]]), array([[-0.00693224],\n",
      "       [-0.00403861],\n",
      "       [ 0.00335075]])]\n",
      "gradients_biases:  [array([ 0.00038001, -0.00032681, -0.0001345 ]), array([0.00692757])]\n",
      "Iteration 65, Cost: 0.25016828761412696\n",
      "gradient_weights:  [array([[ 0.00120111, -0.00078567, -0.00172988],\n",
      "       [ 0.00145281, -0.00129902, -0.00023748]]), array([[-0.00694479],\n",
      "       [-0.00405639],\n",
      "       [ 0.00333303]])]\n",
      "gradients_biases:  [array([ 0.00037863, -0.0003288 , -0.00013579]), array([0.0068994])]\n",
      "Iteration 66, Cost: 0.2501616507057078\n",
      "gradient_weights:  [array([[ 0.00119371, -0.00078901, -0.00172713],\n",
      "       [ 0.00144411, -0.00130465, -0.00023772]]), array([[-0.00695624],\n",
      "       [-0.00407309],\n",
      "       [ 0.00331631]])]\n",
      "gradients_biases:  [array([ 0.00037721, -0.00033077, -0.00013702]), array([0.00687274])]\n",
      "Iteration 67, Cost: 0.2501550248097547\n",
      "gradient_weights:  [array([[ 0.00118628, -0.00079234, -0.00172436],\n",
      "       [ 0.00143537, -0.0013103 , -0.00023794]]), array([[-0.00696666],\n",
      "       [-0.00408878],\n",
      "       [ 0.00330052]])]\n",
      "gradients_biases:  [array([ 0.00037574, -0.00033274, -0.00013819]), array([0.0068475])]\n",
      "Iteration 68, Cost: 0.2501484096020218\n",
      "gradient_weights:  [array([[ 0.00117884, -0.00079568, -0.00172158],\n",
      "       [ 0.00142662, -0.00131595, -0.00023813]]), array([[-0.00697612],\n",
      "       [-0.00410352],\n",
      "       [ 0.00328561]])]\n",
      "gradients_biases:  [array([ 0.00037423, -0.00033468, -0.0001393 ]), array([0.0068236])]\n",
      "Iteration 69, Cost: 0.2501418047908158\n",
      "gradient_weights:  [array([[ 0.00117137, -0.00079903, -0.00171879],\n",
      "       [ 0.00141784, -0.00132161, -0.00023831]]), array([[-0.00698467],\n",
      "       [-0.00411737],\n",
      "       [ 0.00327153]])]\n",
      "gradients_biases:  [array([ 0.00037268, -0.00033661, -0.00014036]), array([0.00680095])]\n",
      "Iteration 70, Cost: 0.2501352101131242\n",
      "gradient_weights:  [array([[ 0.0011639 , -0.00080238, -0.00171599],\n",
      "       [ 0.00140903, -0.00132729, -0.00023845]]), array([[-0.00699237],\n",
      "       [-0.00413038],\n",
      "       [ 0.00325822]])]\n",
      "gradients_biases:  [array([ 0.00037109, -0.00033853, -0.00014137]), array([0.00677948])]\n",
      "Iteration 71, Cost: 0.25012862533120517\n",
      "gradient_weights:  [array([[ 0.00115641, -0.00080573, -0.00171317],\n",
      "       [ 0.00140021, -0.00133297, -0.00023858]]), array([[-0.00699928],\n",
      "       [-0.00414261],\n",
      "       [ 0.00324564]])]\n",
      "gradients_biases:  [array([ 0.00036947, -0.00034044, -0.00014233]), array([0.00675911])]\n",
      "Iteration 72, Cost: 0.25012205022958534\n",
      "gradient_weights:  [array([[ 0.0011489 , -0.00080909, -0.00171035],\n",
      "       [ 0.00139137, -0.00133867, -0.00023869]]), array([[-0.00700544],\n",
      "       [-0.0041541 ],\n",
      "       [ 0.00323374]])]\n",
      "gradients_biases:  [array([ 0.00036782, -0.00034234, -0.00014325]), array([0.00673978])]\n",
      "Iteration 73, Cost: 0.25011548461241573\n",
      "gradient_weights:  [array([[ 0.00114139, -0.00081244, -0.00170751],\n",
      "       [ 0.00138252, -0.00134437, -0.00023878]]), array([[-0.0070109 ],\n",
      "       [-0.00416491],\n",
      "       [ 0.00322248]])]\n",
      "gradients_biases:  [array([ 0.00036614, -0.00034423, -0.00014413]), array([0.00672143])]\n",
      "Iteration 74, Cost: 0.2501089283011434\n",
      "gradient_weights:  [array([[ 0.00113386, -0.0008158 , -0.00170467],\n",
      "       [ 0.00137364, -0.00135008, -0.00023886]]), array([[-0.00701571],\n",
      "       [-0.00417507],\n",
      "       [ 0.00321183]])]\n",
      "gradients_biases:  [array([ 0.00036443, -0.0003461 , -0.00014496]), array([0.00670399])]\n",
      "Iteration 75, Cost: 0.2501023811324613\n",
      "gradient_weights:  [array([[ 0.00112633, -0.00081917, -0.00170182],\n",
      "       [ 0.00136476, -0.0013558 , -0.00023892]]), array([[-0.00701991],\n",
      "       [-0.00418462],\n",
      "       [ 0.00320174]])]\n",
      "gradients_biases:  [array([ 0.00036269, -0.00034797, -0.00014576]), array([0.00668742])]\n",
      "Iteration 76, Cost: 0.25009584295650267\n",
      "gradient_weights:  [array([[ 0.00111879, -0.00082253, -0.00169896],\n",
      "       [ 0.00135587, -0.00136153, -0.00023896]]), array([[-0.00702353],\n",
      "       [-0.0041936 ],\n",
      "       [ 0.00319218]])]\n",
      "gradients_biases:  [array([ 0.00036093, -0.00034983, -0.00014653]), array([0.00667165])]\n",
      "Iteration 77, Cost: 0.250089313635251\n",
      "gradient_weights:  [array([[ 0.00111124, -0.0008259 , -0.0016961 ],\n",
      "       [ 0.00134696, -0.00136726, -0.00023899]]), array([[-0.00702661],\n",
      "       [-0.00420205],\n",
      "       [ 0.00318312]])]\n",
      "gradients_biases:  [array([ 0.00035914, -0.00035168, -0.00014726]), array([0.00665663])]\n",
      "Iteration 78, Cost: 0.2500827930411399\n",
      "gradient_weights:  [array([[ 0.00110369, -0.00082927, -0.00169323],\n",
      "       [ 0.00133804, -0.001373  , -0.00023901]]), array([[-0.00702919],\n",
      "       [-0.00421   ],\n",
      "       [ 0.00317454]])]\n",
      "gradients_biases:  [array([ 0.00035734, -0.00035352, -0.00014796]), array([0.00664233])]\n",
      "Iteration 79, Cost: 0.25007628105582014\n",
      "gradient_weights:  [array([[ 0.00109613, -0.00083265, -0.00169035],\n",
      "       [ 0.00132912, -0.00137875, -0.00023901]]), array([[-0.0070313 ],\n",
      "       [-0.00421748],\n",
      "       [ 0.00316639]])]\n",
      "gradients_biases:  [array([ 0.00035551, -0.00035536, -0.00014863]), array([0.0066287])]\n",
      "Iteration 80, Cost: 0.2500697775690733\n",
      "gradient_weights:  [array([[ 0.00108857, -0.00083602, -0.00168747],\n",
      "       [ 0.00132019, -0.0013845 , -0.000239  ]]), array([[-0.00703296],\n",
      "       [-0.00422452],\n",
      "       [ 0.00315866]])]\n",
      "gradients_biases:  [array([ 0.00035366, -0.00035719, -0.00014928]), array([0.0066157])]\n",
      "Iteration 81, Cost: 0.2500632824778559\n",
      "gradient_weights:  [array([[ 0.00108101, -0.0008394 , -0.00168458],\n",
      "       [ 0.00131125, -0.00139026, -0.00023898]]), array([[-0.00703422],\n",
      "       [-0.00423115],\n",
      "       [ 0.00315132]])]\n",
      "gradients_biases:  [array([ 0.00035179, -0.00035901, -0.0001499 ]), array([0.00660329])]\n",
      "Iteration 82, Cost: 0.25005679568545675\n",
      "gradient_weights:  [array([[ 0.00107344, -0.00084277, -0.00168169],\n",
      "       [ 0.00130231, -0.00139602, -0.00023895]]), array([[-0.00703508],\n",
      "       [-0.00423739],\n",
      "       [ 0.00314435]])]\n",
      "gradients_biases:  [array([ 0.0003499 , -0.00036083, -0.00015049]), array([0.00659144])]\n",
      "Iteration 83, Cost: 0.2500503171007552\n",
      "gradient_weights:  [array([[ 0.00106588, -0.00084615, -0.0016788 ],\n",
      "       [ 0.00129336, -0.00140179, -0.00023891]]), array([[-0.00703558],\n",
      "       [-0.00424327],\n",
      "       [ 0.00313772]])]\n",
      "gradients_biases:  [array([ 0.00034799, -0.00036264, -0.00015106]), array([0.0065801])]\n",
      "Iteration 84, Cost: 0.25004384663756807\n",
      "gradient_weights:  [array([[ 0.00105831, -0.00084954, -0.0016759 ],\n",
      "       [ 0.00128441, -0.00140756, -0.00023886]]), array([[-0.00703574],\n",
      "       [-0.0042488 ],\n",
      "       [ 0.00313141]])]\n",
      "gradients_biases:  [array([ 0.00034607, -0.00036445, -0.00015161]), array([0.00656926])]\n",
      "Iteration 85, Cost: 0.25003738421407373\n",
      "gradient_weights:  [array([[ 0.00105075, -0.00085292, -0.001673  ],\n",
      "       [ 0.00127546, -0.00141334, -0.0002388 ]]), array([[-0.00703559],\n",
      "       [-0.00425402],\n",
      "       [ 0.00312541]])]\n",
      "gradients_biases:  [array([ 0.00034413, -0.00036625, -0.00015214]), array([0.00655888])]\n",
      "Iteration 86, Cost: 0.2500309297523057\n",
      "gradient_weights:  [array([[ 0.00104318, -0.0008563 , -0.00167009],\n",
      "       [ 0.0012665 , -0.00141912, -0.00023874]]), array([[-0.00703513],\n",
      "       [-0.00425894],\n",
      "       [ 0.00311969]])]\n",
      "gradients_biases:  [array([ 0.00034218, -0.00036805, -0.00015265]), array([0.00654893])]\n",
      "Iteration 87, Cost: 0.25002448317770565\n",
      "gradient_weights:  [array([[ 0.00103562, -0.00085969, -0.00166719],\n",
      "       [ 0.00125754, -0.00142491, -0.00023866]]), array([[-0.0070344 ],\n",
      "       [-0.00426358],\n",
      "       [ 0.00311425]])]\n",
      "gradients_biases:  [array([ 0.00034021, -0.00036984, -0.00015315]), array([0.00653939])]\n",
      "Iteration 88, Cost: 0.25001804441873077\n",
      "gradient_weights:  [array([[ 0.00102805, -0.00086307, -0.00166427],\n",
      "       [ 0.00124858, -0.0014307 , -0.00023858]]), array([[-0.00703341],\n",
      "       [-0.00426796],\n",
      "       [ 0.00310906]])]\n",
      "gradients_biases:  [array([ 0.00033823, -0.00037163, -0.00015362]), array([0.00653023])]\n",
      "Iteration 89, Cost: 0.25001161340650707\n",
      "gradient_weights:  [array([[ 0.00102049, -0.00086646, -0.00166136],\n",
      "       [ 0.00123962, -0.00143649, -0.00023849]]), array([[-0.00703217],\n",
      "       [-0.00427209],\n",
      "       [ 0.0031041 ]])]\n",
      "gradients_biases:  [array([ 0.00033624, -0.00037342, -0.00015408]), array([0.00652143])]\n",
      "Iteration 90, Cost: 0.2500051900745238\n",
      "gradient_weights:  [array([[ 0.00101293, -0.00086985, -0.00165845],\n",
      "       [ 0.00123066, -0.00144229, -0.0002384 ]]), array([[-0.00703071],\n",
      "       [-0.00427599],\n",
      "       [ 0.00309937]])]\n",
      "gradients_biases:  [array([ 0.00033424, -0.0003752 , -0.00015452]), array([0.00651297])]\n",
      "Iteration 91, Cost: 0.24999877435836546\n",
      "gradient_weights:  [array([[ 0.00100538, -0.00087324, -0.00165553],\n",
      "       [ 0.0012217 , -0.0014481 , -0.00023829]]), array([[-0.00702903],\n",
      "       [-0.00427968],\n",
      "       [ 0.00309485]])]\n",
      "gradients_biases:  [array([ 0.00033222, -0.00037699, -0.00015495]), array([0.00650482])]\n",
      "Iteration 92, Cost: 0.24999236619547402\n",
      "gradient_weights:  [array([[ 0.00099783, -0.00087663, -0.00165261],\n",
      "       [ 0.00121274, -0.0014539 , -0.00023818]]), array([[-0.00702716],\n",
      "       [-0.00428316],\n",
      "       [ 0.00309054]])]\n",
      "gradients_biases:  [array([ 0.00033019, -0.00037876, -0.00015537]), array([0.00649698])]\n",
      "Iteration 93, Cost: 0.24998596552494096\n",
      "gradient_weights:  [array([[ 0.00099028, -0.00088002, -0.00164969],\n",
      "       [ 0.00120379, -0.00145971, -0.00023807]]), array([[-0.0070251 ],\n",
      "       [-0.00428645],\n",
      "       [ 0.0030864 ]])]\n",
      "gradients_biases:  [array([ 0.00032815, -0.00038054, -0.00015577]), array([0.00648942])]\n",
      "Iteration 94, Cost: 0.24997957228732293\n",
      "gradient_weights:  [array([[ 0.00098274, -0.00088342, -0.00164677],\n",
      "       [ 0.00119483, -0.00146552, -0.00023795]]), array([[-0.00702287],\n",
      "       [-0.00428956],\n",
      "       [ 0.00308245]])]\n",
      "gradients_biases:  [array([ 0.00032611, -0.00038231, -0.00015616]), array([0.00648212])]\n",
      "Iteration 95, Cost: 0.2499731864244798\n",
      "gradient_weights:  [array([[ 0.0009752 , -0.00088681, -0.00164385],\n",
      "       [ 0.00118588, -0.00147134, -0.00023782]]), array([[-0.00702047],\n",
      "       [-0.00429251],\n",
      "       [ 0.00307866]])]\n",
      "gradients_biases:  [array([ 0.00032405, -0.00038409, -0.00015654]), array([0.00647507])]\n",
      "Iteration 96, Cost: 0.2499668078794321\n",
      "gradient_weights:  [array([[ 0.00096766, -0.0008902 , -0.00164093],\n",
      "       [ 0.00117693, -0.00147716, -0.00023769]]), array([[-0.00701793],\n",
      "       [-0.0042953 ],\n",
      "       [ 0.00307502]])]\n",
      "gradients_biases:  [array([ 0.00032198, -0.00038586, -0.0001569 ]), array([0.00646826])]\n",
      "Iteration 97, Cost: 0.24996043659623474\n",
      "gradient_weights:  [array([[ 0.00096013, -0.0008936 , -0.001638  ],\n",
      "       [ 0.00116798, -0.00148298, -0.00023756]]), array([[-0.00701524],\n",
      "       [-0.00429795],\n",
      "       [ 0.00307154]])]\n",
      "gradients_biases:  [array([ 0.0003199 , -0.00038762, -0.00015726]), array([0.00646167])]\n",
      "Iteration 98, Cost: 0.24995407251986662\n",
      "gradient_weights:  [array([[ 0.00095261, -0.000897  , -0.00163508],\n",
      "       [ 0.00115903, -0.00148881, -0.00023742]]), array([[-0.00701242],\n",
      "       [-0.00430046],\n",
      "       [ 0.00306819]])]\n",
      "gradients_biases:  [array([ 0.00031782, -0.00038939, -0.0001576 ]), array([0.00645529])]\n",
      "Iteration 99, Cost: 0.2499477155961321\n",
      "gradient_weights:  [array([[ 0.00094509, -0.00090039, -0.00163215],\n",
      "       [ 0.00115009, -0.00149463, -0.00023728]]), array([[-0.00700948],\n",
      "       [-0.00430284],\n",
      "       [ 0.00306497]])]\n",
      "gradients_biases:  [array([ 0.00031573, -0.00039116, -0.00015794]), array([0.0064491])]\n",
      "Iteration 100, Cost: 0.24994136577157552\n",
      "gradient_weights:  [array([[ 0.00093758, -0.00090379, -0.00162923],\n",
      "       [ 0.00114115, -0.00150046, -0.00023713]]), array([[-0.00700643],\n",
      "       [-0.0043051 ],\n",
      "       [ 0.00306188]])]\n",
      "gradients_biases:  [array([ 0.00031362, -0.00039292, -0.00015826]), array([0.0064431])]\n",
      "Iteration 101, Cost: 0.24993502299340484\n",
      "gradient_weights:  [array([[ 0.00093007, -0.00090719, -0.0016263 ],\n",
      "       [ 0.00113221, -0.0015063 , -0.00023698]]), array([[-0.00700327],\n",
      "       [-0.00430725],\n",
      "       [ 0.00305891]])]\n",
      "gradients_biases:  [array([ 0.00031151, -0.00039468, -0.00015858]), array([0.00643727])]\n",
      "Iteration 102, Cost: 0.2499286872094245\n",
      "gradient_weights:  [array([[ 0.00092257, -0.00091059, -0.00162338],\n",
      "       [ 0.00112328, -0.00151213, -0.00023682]]), array([[-0.00700001],\n",
      "       [-0.00430929],\n",
      "       [ 0.00305604]])]\n",
      "gradients_biases:  [array([ 0.0003094 , -0.00039644, -0.00015889]), array([0.00643161])]\n",
      "Iteration 103, Cost: 0.24992235836797647\n",
      "gradient_weights:  [array([[ 0.00091507, -0.00091399, -0.00162045],\n",
      "       [ 0.00111436, -0.00151797, -0.00023666]]), array([[-0.00699666],\n",
      "       [-0.00431124],\n",
      "       [ 0.00305328]])]\n",
      "gradients_biases:  [array([ 0.00030727, -0.0003982 , -0.00015919]), array([0.0064261])]\n",
      "Iteration 104, Cost: 0.24991603641788804\n",
      "gradient_weights:  [array([[ 0.00090758, -0.00091739, -0.00161752],\n",
      "       [ 0.00110543, -0.00152381, -0.0002365 ]]), array([[-0.00699323],\n",
      "       [-0.00431309],\n",
      "       [ 0.00305061]])]\n",
      "gradients_biases:  [array([ 0.00030514, -0.00039996, -0.00015949]), array([0.00642074])]\n",
      "Iteration 105, Cost: 0.24990972130842581\n",
      "gradient_weights:  [array([[ 0.0009001 , -0.0009208 , -0.0016146 ],\n",
      "       [ 0.00109651, -0.00152965, -0.00023633]]), array([[-0.00698971],\n",
      "       [-0.00431486],\n",
      "       [ 0.00304804]])]\n",
      "gradients_biases:  [array([ 0.00030301, -0.00040172, -0.00015977]), array([0.00641551])]\n",
      "Iteration 106, Cost: 0.24990341298925495\n",
      "gradient_weights:  [array([[ 0.00089262, -0.0009242 , -0.00161167],\n",
      "       [ 0.0010876 , -0.0015355 , -0.00023616]]), array([[-0.00698612],\n",
      "       [-0.00431655],\n",
      "       [ 0.00304556]])]\n",
      "gradients_biases:  [array([ 0.00030086, -0.00040348, -0.00016005]), array([0.00641042])]\n",
      "Iteration 107, Cost: 0.24989711141040355\n",
      "gradient_weights:  [array([[ 0.00088515, -0.0009276 , -0.00160874],\n",
      "       [ 0.00107869, -0.00154135, -0.00023599]]), array([[-0.00698247],\n",
      "       [-0.00431816],\n",
      "       [ 0.00304316]])]\n",
      "gradients_biases:  [array([ 0.00029871, -0.00040524, -0.00016032]), array([0.00640544])]\n",
      "Iteration 108, Cost: 0.24989081652223072\n",
      "gradient_weights:  [array([[ 0.00087769, -0.00093101, -0.00160582],\n",
      "       [ 0.00106979, -0.0015472 , -0.00023582]]), array([[-0.00697875],\n",
      "       [-0.0043197 ],\n",
      "       [ 0.00304084]])]\n",
      "gradients_biases:  [array([ 0.00029656, -0.000407  , -0.00016059]), array([0.00640059])]\n",
      "Iteration 109, Cost: 0.24988452827539887\n",
      "gradient_weights:  [array([[ 0.00087024, -0.00093441, -0.00160289],\n",
      "       [ 0.00106089, -0.00155305, -0.00023564]]), array([[-0.00697497],\n",
      "       [-0.00432118],\n",
      "       [ 0.00303859]])]\n",
      "gradients_biases:  [array([ 0.0002944 , -0.00040875, -0.00016085]), array([0.00639584])]\n",
      "Iteration 110, Cost: 0.2498782466208487\n",
      "gradient_weights:  [array([[ 0.00086279, -0.00093782, -0.00159997],\n",
      "       [ 0.001052  , -0.0015589 , -0.00023546]]), array([[-0.00697113],\n",
      "       [-0.00432259],\n",
      "       [ 0.00303641]])]\n",
      "gradients_biases:  [array([ 0.00029223, -0.00041051, -0.0001611 ]), array([0.0063912])]\n",
      "Iteration 111, Cost: 0.24987197150977764\n",
      "gradient_weights:  [array([[ 0.00085535, -0.00094122, -0.00159704],\n",
      "       [ 0.00104311, -0.00156476, -0.00023527]]), array([[-0.00696725],\n",
      "       [-0.00432395],\n",
      "       [ 0.0030343 ]])]\n",
      "gradients_biases:  [array([ 0.00029006, -0.00041227, -0.00016135]), array([0.00638665])]\n",
      "Iteration 112, Cost: 0.24986570289362037\n",
      "gradient_weights:  [array([[ 0.00084791, -0.00094463, -0.00159412],\n",
      "       [ 0.00103423, -0.00157062, -0.00023509]]), array([[-0.00696332],\n",
      "       [-0.00432525],\n",
      "       [ 0.00303225]])]\n",
      "gradients_biases:  [array([ 0.00028788, -0.00041403, -0.00016159]), array([0.0063822])]\n",
      "Iteration 113, Cost: 0.2498594407240317\n",
      "gradient_weights:  [array([[ 0.00084048, -0.00094804, -0.00159119],\n",
      "       [ 0.00102535, -0.00157648, -0.0002349 ]]), array([[-0.00695935],\n",
      "       [-0.00432651],\n",
      "       [ 0.00303026]])]\n",
      "gradients_biases:  [array([ 0.00028569, -0.00041578, -0.00016183]), array([0.00637783])]\n",
      "Iteration 114, Cost: 0.24985318495287182\n",
      "gradient_weights:  [array([[ 0.00083306, -0.00095145, -0.00158827],\n",
      "       [ 0.00101648, -0.00158234, -0.00023471]]), array([[-0.00695534],\n",
      "       [-0.00432772],\n",
      "       [ 0.00302833]])]\n",
      "gradients_biases:  [array([ 0.00028351, -0.00041754, -0.00016206]), array([0.00637355])]\n",
      "Iteration 115, Cost: 0.24984693553219214\n",
      "gradient_weights:  [array([[ 0.00082565, -0.00095486, -0.00158535],\n",
      "       [ 0.00100761, -0.00158821, -0.00023452]]), array([[-0.00695129],\n",
      "       [-0.00432888],\n",
      "       [ 0.00302645]])]\n",
      "gradients_biases:  [array([ 0.00028131, -0.0004193 , -0.00016229]), array([0.00636934])]\n",
      "Iteration 116, Cost: 0.24984069241422435\n",
      "gradient_weights:  [array([[ 0.00081825, -0.00095827, -0.00158242],\n",
      "       [ 0.00099876, -0.00159408, -0.00023432]]), array([[-0.00694722],\n",
      "       [-0.00433001],\n",
      "       [ 0.00302463]])]\n",
      "gradients_biases:  [array([ 0.00027911, -0.00042105, -0.00016251]), array([0.00636521])]\n",
      "Iteration 117, Cost: 0.24983445555136902\n",
      "gradient_weights:  [array([[ 0.00081085, -0.00096168, -0.0015795 ],\n",
      "       [ 0.0009899 , -0.00159995, -0.00023412]]), array([[-0.00694311],\n",
      "       [-0.00433109],\n",
      "       [ 0.00302285]])]\n",
      "gradients_biases:  [array([ 0.00027691, -0.00042281, -0.00016272]), array([0.00636115])]\n",
      "Iteration 118, Cost: 0.24982822489618686\n",
      "gradient_weights:  [array([[ 0.00080346, -0.00096509, -0.00157658],\n",
      "       [ 0.00098105, -0.00160582, -0.00023392]]), array([[-0.00693897],\n",
      "       [-0.00433214],\n",
      "       [ 0.00302112]])]\n",
      "gradients_biases:  [array([ 0.0002747 , -0.00042457, -0.00016294]), array([0.00635716])]\n",
      "Iteration 119, Cost: 0.24982200040139024\n",
      "gradient_weights:  [array([[ 0.00079608, -0.0009685 , -0.00157366],\n",
      "       [ 0.00097221, -0.00161169, -0.00023372]]), array([[-0.00693481],\n",
      "       [-0.00433316],\n",
      "       [ 0.00301943]])]\n",
      "gradients_biases:  [array([ 0.00027249, -0.00042633, -0.00016314]), array([0.00635323])]\n",
      "Iteration 120, Cost: 0.24981578201983565\n",
      "gradient_weights:  [array([[ 0.00078871, -0.00097191, -0.00157074],\n",
      "       [ 0.00096338, -0.00161757, -0.00023352]]), array([[-0.00693063],\n",
      "       [-0.00433415],\n",
      "       [ 0.00301778]])]\n",
      "gradients_biases:  [array([ 0.00027027, -0.00042808, -0.00016335]), array([0.00634936])]\n",
      "Iteration 121, Cost: 0.24980956970451743\n",
      "gradient_weights:  [array([[ 0.00078134, -0.00097533, -0.00156782],\n",
      "       [ 0.00095455, -0.00162344, -0.00023331]]), array([[-0.00692642],\n",
      "       [-0.00433512],\n",
      "       [ 0.00301618]])]\n",
      "gradients_biases:  [array([ 0.00026805, -0.00042984, -0.00016355]), array([0.00634555])]\n",
      "Iteration 122, Cost: 0.24980336340856152\n",
      "gradient_weights:  [array([[ 0.00077398, -0.00097874, -0.0015649 ],\n",
      "       [ 0.00094572, -0.00162932, -0.00023311]]), array([[-0.0069222 ],\n",
      "       [-0.00433605],\n",
      "       [ 0.00301461]])]\n",
      "gradients_biases:  [array([ 0.00026583, -0.0004316 , -0.00016374]), array([0.00634179])]\n",
      "Iteration 123, Cost: 0.24979716308522065\n",
      "gradient_weights:  [array([[ 0.00076663, -0.00098215, -0.00156198],\n",
      "       [ 0.00093691, -0.0016352 , -0.0002329 ]]), array([[-0.00691796],\n",
      "       [-0.00433696],\n",
      "       [ 0.00301308]])]\n",
      "gradients_biases:  [array([ 0.0002636 , -0.00043336, -0.00016393]), array([0.00633809])]\n",
      "Iteration 124, Cost: 0.24979096868786932\n",
      "gradient_weights:  [array([[ 0.00075929, -0.00098557, -0.00155907],\n",
      "       [ 0.0009281 , -0.00164109, -0.00023269]]), array([[-0.00691371],\n",
      "       [-0.00433785],\n",
      "       [ 0.00301158]])]\n",
      "gradients_biases:  [array([ 0.00026136, -0.00043512, -0.00016412]), array([0.00633443])]\n",
      "Iteration 125, Cost: 0.24978478016999994\n",
      "gradient_weights:  [array([[ 0.00075195, -0.00098899, -0.00155615],\n",
      "       [ 0.00091929, -0.00164697, -0.00023247]]), array([[-0.00690944],\n",
      "       [-0.00433872],\n",
      "       [ 0.00301012]])]\n",
      "gradients_biases:  [array([ 0.00025912, -0.00043688, -0.00016431]), array([0.00633082])]\n",
      "Iteration 126, Cost: 0.24977859748521883\n",
      "gradient_weights:  [array([[ 0.00074462, -0.0009924 , -0.00155323],\n",
      "       [ 0.00091049, -0.00165286, -0.00023226]]), array([[-0.00690516],\n",
      "       [-0.00433957],\n",
      "       [ 0.00300869]])]\n",
      "gradients_biases:  [array([ 0.00025688, -0.00043864, -0.00016449]), array([0.00632726])]\n",
      "Iteration 127, Cost: 0.24977242058724286\n",
      "gradient_weights:  [array([[ 0.0007373 , -0.00099582, -0.00155032],\n",
      "       [ 0.0009017 , -0.00165875, -0.00023204]]), array([[-0.00690088],\n",
      "       [-0.0043404 ],\n",
      "       [ 0.00300728]])]\n",
      "gradients_biases:  [array([ 0.00025464, -0.0004404 , -0.00016466]), array([0.00632374])]\n",
      "Iteration 128, Cost: 0.24976624942989673\n",
      "gradient_weights:  [array([[ 0.00072999, -0.00099924, -0.00154741],\n",
      "       [ 0.00089291, -0.00166464, -0.00023182]]), array([[-0.00689658],\n",
      "       [-0.00434121],\n",
      "       [ 0.00300591]])]\n",
      "gradients_biases:  [array([ 0.00025239, -0.00044216, -0.00016484]), array([0.00632026])]\n",
      "Iteration 129, Cost: 0.24976008396710975\n",
      "gradient_weights:  [array([[ 0.00072269, -0.00100266, -0.00154449],\n",
      "       [ 0.00088413, -0.00167053, -0.0002316 ]]), array([[-0.00689228],\n",
      "       [-0.00434201],\n",
      "       [ 0.00300457]])]\n",
      "gradients_biases:  [array([ 0.00025013, -0.00044393, -0.00016501]), array([0.00631682])]\n",
      "Iteration 130, Cost: 0.24975392415291398\n",
      "gradient_weights:  [array([[ 0.00071539, -0.00100608, -0.00154158],\n",
      "       [ 0.00087536, -0.00167643, -0.00023138]]), array([[-0.00688797],\n",
      "       [-0.0043428 ],\n",
      "       [ 0.00300325]])]\n",
      "gradients_biases:  [array([ 0.00024788, -0.00044569, -0.00016517]), array([0.00631341])]\n",
      "Iteration 131, Cost: 0.24974776994144132\n",
      "gradient_weights:  [array([[ 0.0007081 , -0.0010095 , -0.00153867],\n",
      "       [ 0.00086659, -0.00168233, -0.00023116]]), array([[-0.00688366],\n",
      "       [-0.00434357],\n",
      "       [ 0.00300196]])]\n",
      "gradients_biases:  [array([ 0.00024561, -0.00044745, -0.00016534]), array([0.00631004])]\n",
      "Iteration 132, Cost: 0.2497416212869219\n",
      "gradient_weights:  [array([[ 0.00070082, -0.00101292, -0.00153576],\n",
      "       [ 0.00085783, -0.00168823, -0.00023094]]), array([[-0.00687934],\n",
      "       [-0.00434434],\n",
      "       [ 0.0030007 ]])]\n",
      "gradients_biases:  [array([ 0.00024335, -0.00044922, -0.0001655 ]), array([0.00630671])]\n",
      "Iteration 133, Cost: 0.24973547814368233\n",
      "gradient_weights:  [array([[ 0.00069355, -0.00101635, -0.00153285],\n",
      "       [ 0.00084908, -0.00169413, -0.00023071]]), array([[-0.00687502],\n",
      "       [-0.00434509],\n",
      "       [ 0.00299945]])]\n",
      "gradients_biases:  [array([ 0.00024108, -0.00045098, -0.00016565]), array([0.00630341])]\n",
      "Iteration 134, Cost: 0.24972934046614362\n",
      "gradient_weights:  [array([[ 0.00068628, -0.00101977, -0.00152994],\n",
      "       [ 0.00084033, -0.00170003, -0.00023048]]), array([[-0.0068707 ],\n",
      "       [-0.00434583],\n",
      "       [ 0.00299824]])]\n",
      "gradients_biases:  [array([ 0.00023881, -0.00045275, -0.00016581]), array([0.00630014])]\n",
      "Iteration 135, Cost: 0.2497232082088198\n",
      "gradient_weights:  [array([[ 0.00067903, -0.00102319, -0.00152703],\n",
      "       [ 0.00083159, -0.00170593, -0.00023025]]), array([[-0.00686637],\n",
      "       [-0.00434656],\n",
      "       [ 0.00299704]])]\n",
      "gradients_biases:  [array([ 0.00023654, -0.00045451, -0.00016596]), array([0.0062969])]\n",
      "Iteration 136, Cost: 0.24971708132631665\n",
      "gradient_weights:  [array([[ 0.00067178, -0.00102662, -0.00152412],\n",
      "       [ 0.00082286, -0.00171184, -0.00023002]]), array([[-0.00686205],\n",
      "       [-0.00434729],\n",
      "       [ 0.00299587]])]\n",
      "gradients_biases:  [array([ 0.00023426, -0.00045628, -0.0001661 ]), array([0.00629369])]\n",
      "Iteration 137, Cost: 0.2497109597733303\n",
      "gradient_weights:  [array([[ 0.00066454, -0.00103004, -0.00152122],\n",
      "       [ 0.00081413, -0.00171775, -0.00022979]]), array([[-0.00685773],\n",
      "       [-0.00434801],\n",
      "       [ 0.00299471]])]\n",
      "gradients_biases:  [array([ 0.00023197, -0.00045805, -0.00016625]), array([0.00629051])]\n",
      "Iteration 138, Cost: 0.24970484350464578\n",
      "gradient_weights:  [array([[ 0.0006573 , -0.00103347, -0.00151831],\n",
      "       [ 0.00080541, -0.00172366, -0.00022956]]), array([[-0.00685341],\n",
      "       [-0.00434872],\n",
      "       [ 0.00299358]])]\n",
      "gradients_biases:  [array([ 0.00022969, -0.00045981, -0.00016639]), array([0.00628736])]\n",
      "Iteration 139, Cost: 0.2496987324751361\n",
      "gradient_weights:  [array([[ 0.00065008, -0.0010369 , -0.00151541],\n",
      "       [ 0.00079669, -0.00172957, -0.00022932]]), array([[-0.00684909],\n",
      "       [-0.00434943],\n",
      "       [ 0.00299246]])]\n",
      "gradients_biases:  [array([ 0.0002274 , -0.00046158, -0.00016653]), array([0.00628423])]\n",
      "Iteration 140, Cost: 0.24969262663976138\n",
      "gradient_weights:  [array([[ 0.00064286, -0.00104033, -0.0015125 ],\n",
      "       [ 0.00078798, -0.00173549, -0.00022909]]), array([[-0.00684477],\n",
      "       [-0.00435013],\n",
      "       [ 0.00299137]])]\n",
      "gradients_biases:  [array([ 0.00022511, -0.00046335, -0.00016666]), array([0.00628112])]\n",
      "Iteration 141, Cost: 0.24968652595356733\n",
      "gradient_weights:  [array([[ 0.00063565, -0.00104376, -0.0015096 ],\n",
      "       [ 0.00077928, -0.0017414 , -0.00022885]]), array([[-0.00684046],\n",
      "       [-0.00435083],\n",
      "       [ 0.00299029]])]\n",
      "gradients_biases:  [array([ 0.00022281, -0.00046512, -0.0001668 ]), array([0.00627805])]\n",
      "Iteration 142, Cost: 0.24968043037168494\n",
      "gradient_weights:  [array([[ 0.00062845, -0.00104719, -0.0015067 ],\n",
      "       [ 0.00077058, -0.00174732, -0.00022861]]), array([[-0.00683615],\n",
      "       [-0.00435152],\n",
      "       [ 0.00298923]])]\n",
      "gradients_biases:  [array([ 0.00022052, -0.00046689, -0.00016693]), array([0.00627499])]\n",
      "Iteration 143, Cost: 0.24967433984932902\n",
      "gradient_weights:  [array([[ 0.00062125, -0.00105062, -0.0015038 ],\n",
      "       [ 0.00076189, -0.00175324, -0.00022837]]), array([[-0.00683185],\n",
      "       [-0.00435221],\n",
      "       [ 0.00298819]])]\n",
      "gradients_biases:  [array([ 0.00021821, -0.00046867, -0.00016705]), array([0.00627196])]\n",
      "Iteration 144, Cost: 0.24966825434179796\n",
      "gradient_weights:  [array([[ 0.00061406, -0.00105405, -0.0015009 ],\n",
      "       [ 0.00075321, -0.00175917, -0.00022813]]), array([[-0.00682755],\n",
      "       [-0.0043529 ],\n",
      "       [ 0.00298717]])]\n",
      "gradients_biases:  [array([ 0.00021591, -0.00047044, -0.00016718]), array([0.00626895])]\n",
      "Iteration 145, Cost: 0.24966217380447236\n",
      "gradient_weights:  [array([[ 0.00060689, -0.00105748, -0.001498  ],\n",
      "       [ 0.00074453, -0.00176509, -0.00022788]]), array([[-0.00682326],\n",
      "       [-0.00435358],\n",
      "       [ 0.00298616]])]\n",
      "gradients_biases:  [array([ 0.0002136 , -0.00047221, -0.0001673 ]), array([0.00626596])]\n",
      "Iteration 146, Cost: 0.2496560981928147\n",
      "gradient_weights:  [array([[ 0.00059971, -0.00106092, -0.0014951 ],\n",
      "       [ 0.00073586, -0.00177102, -0.00022764]]), array([[-0.00681897],\n",
      "       [-0.00435426],\n",
      "       [ 0.00298517]])]\n",
      "gradients_biases:  [array([ 0.00021129, -0.00047398, -0.00016742]), array([0.00626299])]\n",
      "Iteration 147, Cost: 0.24965002746236847\n",
      "gradient_weights:  [array([[ 0.00059255, -0.00106435, -0.0014922 ],\n",
      "       [ 0.00072719, -0.00177695, -0.00022739]]), array([[-0.00681469],\n",
      "       [-0.00435495],\n",
      "       [ 0.0029842 ]])]\n",
      "gradients_biases:  [array([ 0.00020898, -0.00047576, -0.00016754]), array([0.00626004])]\n",
      "Iteration 148, Cost: 0.24964396156875746\n",
      "gradient_weights:  [array([[ 0.00058539, -0.00106779, -0.0014893 ],\n",
      "       [ 0.00071853, -0.00178288, -0.00022715]]), array([[-0.00681041],\n",
      "       [-0.00435563],\n",
      "       [ 0.00298324]])]\n",
      "gradients_biases:  [array([ 0.00020666, -0.00047753, -0.00016765]), array([0.00625712])]\n",
      "Iteration 149, Cost: 0.24963790046768508\n",
      "gradient_weights:  [array([[ 0.00057825, -0.00107122, -0.00148641],\n",
      "       [ 0.00070988, -0.00178881, -0.0002269 ]]), array([[-0.00680615],\n",
      "       [-0.00435631],\n",
      "       [ 0.00298229]])]\n",
      "gradients_biases:  [array([ 0.00020434, -0.00047931, -0.00016776]), array([0.00625421])]\n",
      "Iteration 150, Cost: 0.24963184411493394\n",
      "gradient_weights:  [array([[ 0.0005711 , -0.00107466, -0.00148351],\n",
      "       [ 0.00070123, -0.00179474, -0.00022665]]), array([[-0.00680188],\n",
      "       [-0.00435698],\n",
      "       [ 0.00298136]])]\n",
      "gradients_biases:  [array([ 0.00020202, -0.00048109, -0.00016787]), array([0.00625132])]\n",
      "Iteration 151, Cost: 0.2496257924663649\n",
      "gradient_weights:  [array([[ 0.00056397, -0.0010781 , -0.00148062],\n",
      "       [ 0.00069259, -0.00180068, -0.0002264 ]]), array([[-0.00679763],\n",
      "       [-0.00435766],\n",
      "       [ 0.00298045]])]\n",
      "gradients_biases:  [array([ 0.00019969, -0.00048287, -0.00016798]), array([0.00624845])]\n",
      "Iteration 152, Cost: 0.24961974547791665\n",
      "gradient_weights:  [array([[ 0.00055685, -0.00108154, -0.00147772],\n",
      "       [ 0.00068396, -0.00180662, -0.00022615]]), array([[-0.00679339],\n",
      "       [-0.00435834],\n",
      "       [ 0.00297954]])]\n",
      "gradients_biases:  [array([ 0.00019736, -0.00048464, -0.00016808]), array([0.00624559])]\n",
      "Iteration 153, Cost: 0.24961370310560532\n",
      "gradient_weights:  [array([[ 0.00054973, -0.00108498, -0.00147483],\n",
      "       [ 0.00067533, -0.00181256, -0.00022589]]), array([[-0.00678915],\n",
      "       [-0.00435902],\n",
      "       [ 0.00297866]])]\n",
      "gradients_biases:  [array([ 0.00019503, -0.00048642, -0.00016818]), array([0.00624276])]\n",
      "Iteration 154, Cost: 0.24960766530552334\n",
      "gradient_weights:  [array([[ 0.00054262, -0.00108842, -0.00147194],\n",
      "       [ 0.00066671, -0.0018185 , -0.00022564]]), array([[-0.00678492],\n",
      "       [-0.0043597 ],\n",
      "       [ 0.00297778]])]\n",
      "gradients_biases:  [array([ 0.0001927 , -0.0004882 , -0.00016828]), array([0.00623994])]\n",
      "Iteration 155, Cost: 0.24960163203383964\n",
      "gradient_weights:  [array([[ 0.00053552, -0.00109186, -0.00146905],\n",
      "       [ 0.0006581 , -0.00182445, -0.00022538]]), array([[-0.0067807 ],\n",
      "       [-0.00436039],\n",
      "       [ 0.00297692]])]\n",
      "gradients_biases:  [array([ 0.00019036, -0.00048998, -0.00016838]), array([0.00623714])]\n",
      "Iteration 156, Cost: 0.24959560324679841\n",
      "gradient_weights:  [array([[ 0.00052842, -0.00109531, -0.00146616],\n",
      "       [ 0.00064949, -0.00183039, -0.00022513]]), array([[-0.00677649],\n",
      "       [-0.00436107],\n",
      "       [ 0.00297608]])]\n",
      "gradients_biases:  [array([ 0.00018802, -0.00049177, -0.00016847]), array([0.00623435])]\n",
      "Iteration 157, Cost: 0.24958957890071914\n",
      "gradient_weights:  [array([[ 0.00052133, -0.00109875, -0.00146327],\n",
      "       [ 0.00064089, -0.00183634, -0.00022487]]), array([[-0.00677229],\n",
      "       [-0.00436176],\n",
      "       [ 0.00297524]])]\n",
      "gradients_biases:  [array([ 0.00018568, -0.00049355, -0.00016856]), array([0.00623158])]\n",
      "Iteration 158, Cost: 0.2495835589519956\n",
      "gradient_weights:  [array([[ 0.00051425, -0.0011022 , -0.00146038],\n",
      "       [ 0.00063229, -0.00184229, -0.00022461]]), array([[-0.00676809],\n",
      "       [-0.00436244],\n",
      "       [ 0.00297442]])]\n",
      "gradients_biases:  [array([ 0.00018333, -0.00049533, -0.00016865]), array([0.00622883])]\n",
      "Iteration 159, Cost: 0.24957754335709575\n",
      "gradient_weights:  [array([[ 0.00050718, -0.00110565, -0.00145749],\n",
      "       [ 0.0006237 , -0.00184825, -0.00022435]]), array([[-0.00676391],\n",
      "       [-0.00436313],\n",
      "       [ 0.00297361]])]\n",
      "gradients_biases:  [array([ 0.00018099, -0.00049712, -0.00016874]), array([0.00622609])]\n",
      "Iteration 160, Cost: 0.24957153207256094\n",
      "gradient_weights:  [array([[ 0.00050011, -0.00110909, -0.0014546 ],\n",
      "       [ 0.00061511, -0.0018542 , -0.00022409]]), array([[-0.00675974],\n",
      "       [-0.00436382],\n",
      "       [ 0.00297282]])]\n",
      "gradients_biases:  [array([ 0.00017864, -0.0004989 , -0.00016882]), array([0.00622336])]\n",
      "Iteration 161, Cost: 0.24956552505500557\n",
      "gradient_weights:  [array([[ 0.00049306, -0.00111254, -0.00145172],\n",
      "       [ 0.00060654, -0.00186016, -0.00022382]]), array([[-0.00675557],\n",
      "       [-0.00436451],\n",
      "       [ 0.00297203]])]\n",
      "gradients_biases:  [array([ 0.00017628, -0.00050069, -0.00016891]), array([0.00622066])]\n",
      "Iteration 162, Cost: 0.2495595222611165\n",
      "gradient_weights:  [array([[ 0.00048601, -0.00111599, -0.00144883],\n",
      "       [ 0.00059796, -0.00186612, -0.00022356]]), array([[-0.00675142],\n",
      "       [-0.00436521],\n",
      "       [ 0.00297126]])]\n",
      "gradients_biases:  [array([ 0.00017393, -0.00050247, -0.00016898]), array([0.00621796])]\n",
      "Iteration 163, Cost: 0.24955352364765276\n",
      "gradient_weights:  [array([[ 0.00047896, -0.00111944, -0.00144595],\n",
      "       [ 0.0005894 , -0.00187208, -0.00022329]]), array([[-0.00674727],\n",
      "       [-0.00436591],\n",
      "       [ 0.0029705 ]])]\n",
      "gradients_biases:  [array([ 0.00017157, -0.00050426, -0.00016906]), array([0.00621528])]\n",
      "Iteration 164, Cost: 0.24954752917144474\n",
      "gradient_weights:  [array([[ 0.00047193, -0.00112289, -0.00144306],\n",
      "       [ 0.00058084, -0.00187804, -0.00022303]]), array([[-0.00674314],\n",
      "       [-0.00436661],\n",
      "       [ 0.00296975]])]\n",
      "gradients_biases:  [array([ 0.00016921, -0.00050605, -0.00016914]), array([0.00621262])]\n",
      "Iteration 165, Cost: 0.249541538789394\n",
      "gradient_weights:  [array([[ 0.0004649 , -0.00112635, -0.00144018],\n",
      "       [ 0.00057228, -0.00188401, -0.00022276]]), array([[-0.00673901],\n",
      "       [-0.00436731],\n",
      "       [ 0.00296902]])]\n",
      "gradients_biases:  [array([ 0.00016684, -0.00050784, -0.00016921]), array([0.00620997])]\n",
      "Iteration 166, Cost: 0.24953555245847273\n",
      "gradient_weights:  [array([[ 0.00045788, -0.0011298 , -0.0014373 ],\n",
      "       [ 0.00056374, -0.00188997, -0.00022249]]), array([[-0.0067349 ],\n",
      "       [-0.00436802],\n",
      "       [ 0.0029683 ]])]\n",
      "gradients_biases:  [array([ 0.00016448, -0.00050962, -0.00016928]), array([0.00620733])]\n",
      "Iteration 167, Cost: 0.24952957013572316\n",
      "gradient_weights:  [array([[ 0.00045086, -0.00113326, -0.00143441],\n",
      "       [ 0.00055519, -0.00189594, -0.00022222]]), array([[-0.0067308 ],\n",
      "       [-0.00436873],\n",
      "       [ 0.00296758]])]\n",
      "gradients_biases:  [array([ 0.00016211, -0.00051141, -0.00016935]), array([0.00620471])]\n",
      "Iteration 168, Cost: 0.24952359177825734\n",
      "gradient_weights:  [array([[ 0.00044386, -0.00113671, -0.00143153],\n",
      "       [ 0.00054666, -0.00190192, -0.00022195]]), array([[-0.00672671],\n",
      "       [-0.00436944],\n",
      "       [ 0.00296688]])]\n",
      "gradients_biases:  [array([ 0.00015974, -0.00051321, -0.00016941]), array([0.0062021])]\n",
      "Iteration 169, Cost: 0.24951761734325628\n",
      "gradient_weights:  [array([[ 0.00043686, -0.00114017, -0.00142865],\n",
      "       [ 0.00053813, -0.00190789, -0.00022168]]), array([[-0.00672262],\n",
      "       [-0.00437016],\n",
      "       [ 0.00296619]])]\n",
      "gradients_biases:  [array([ 0.00015736, -0.000515  , -0.00016948]), array([0.0061995])]\n",
      "Iteration 170, Cost: 0.24951164678797005\n",
      "gradient_weights:  [array([[ 0.00042987, -0.00114363, -0.00142577],\n",
      "       [ 0.0005296 , -0.00191387, -0.0002214 ]]), array([[-0.00671855],\n",
      "       [-0.00437088],\n",
      "       [ 0.00296552]])]\n",
      "gradients_biases:  [array([ 0.00015499, -0.00051679, -0.00016954]), array([0.00619692])]\n",
      "Iteration 171, Cost: 0.2495056800697169\n",
      "gradient_weights:  [array([[ 0.00042288, -0.00114709, -0.00142289],\n",
      "       [ 0.00052109, -0.00191984, -0.00022113]]), array([[-0.00671449],\n",
      "       [-0.0043716 ],\n",
      "       [ 0.00296485]])]\n",
      "gradients_biases:  [array([ 0.00015261, -0.00051858, -0.00016959]), array([0.00619435])]\n",
      "Iteration 172, Cost: 0.24949971714588287\n",
      "gradient_weights:  [array([[ 0.0004159 , -0.00115055, -0.00142002],\n",
      "       [ 0.00051257, -0.00192582, -0.00022085]]), array([[-0.00671044],\n",
      "       [-0.00437233],\n",
      "       [ 0.00296419]])]\n",
      "gradients_biases:  [array([ 0.00015023, -0.00052038, -0.00016965]), array([0.00619179])]\n",
      "Iteration 173, Cost: 0.24949375797392148\n",
      "gradient_weights:  [array([[ 0.00040893, -0.00115401, -0.00141714],\n",
      "       [ 0.00050407, -0.00193181, -0.00022057]]), array([[-0.0067064 ],\n",
      "       [-0.00437306],\n",
      "       [ 0.00296355]])]\n",
      "gradients_biases:  [array([ 0.00014784, -0.00052217, -0.00016971]), array([0.00618925])]\n",
      "Iteration 174, Cost: 0.24948780251135322\n",
      "gradient_weights:  [array([[ 0.00040197, -0.00115748, -0.00141426],\n",
      "       [ 0.00049557, -0.00193779, -0.0002203 ]]), array([[-0.00670238],\n",
      "       [-0.0043738 ],\n",
      "       [ 0.00296292]])]\n",
      "gradients_biases:  [array([ 0.00014546, -0.00052397, -0.00016976]), array([0.00618672])]\n",
      "Iteration 175, Cost: 0.24948185071576523\n",
      "gradient_weights:  [array([[ 0.00039501, -0.00116094, -0.00141139],\n",
      "       [ 0.00048707, -0.00194378, -0.00022002]]), array([[-0.00669836],\n",
      "       [-0.00437453],\n",
      "       [ 0.00296229]])]\n",
      "gradients_biases:  [array([ 0.00014307, -0.00052576, -0.00016981]), array([0.0061842])]\n",
      "Iteration 176, Cost: 0.2494759025448104\n",
      "gradient_weights:  [array([[ 0.00038807, -0.00116441, -0.00140851],\n",
      "       [ 0.00047858, -0.00194977, -0.00021974]]), array([[-0.00669435],\n",
      "       [-0.00437528],\n",
      "       [ 0.00296168]])]\n",
      "gradients_biases:  [array([ 0.00014068, -0.00052756, -0.00016985]), array([0.0061817])]\n",
      "Iteration 177, Cost: 0.24946995795620758\n",
      "gradient_weights:  [array([[ 0.00038112, -0.00116787, -0.00140563],\n",
      "       [ 0.0004701 , -0.00195576, -0.00021945]]), array([[-0.00669036],\n",
      "       [-0.00437602],\n",
      "       [ 0.00296108]])]\n",
      "gradients_biases:  [array([ 0.00013829, -0.00052936, -0.0001699 ]), array([0.0061792])]\n",
      "Iteration 178, Cost: 0.24946401690774084\n",
      "gradient_weights:  [array([[ 0.00037419, -0.00117134, -0.00140276],\n",
      "       [ 0.00046162, -0.00196175, -0.00021917]]), array([[-0.00668638],\n",
      "       [-0.00437677],\n",
      "       [ 0.00296049]])]\n",
      "gradients_biases:  [array([ 0.00013589, -0.00053115, -0.00016994]), array([0.00617672])]\n",
      "Iteration 179, Cost: 0.2494580793572589\n",
      "gradient_weights:  [array([[ 0.00036726, -0.00117481, -0.00139989],\n",
      "       [ 0.00045315, -0.00196775, -0.00021889]]), array([[-0.0066824 ],\n",
      "       [-0.00437753],\n",
      "       [ 0.00295991]])]\n",
      "gradients_biases:  [array([ 0.00013349, -0.00053295, -0.00016998]), array([0.00617426])]\n",
      "Iteration 180, Cost: 0.24945214526267478\n",
      "gradient_weights:  [array([[ 0.00036034, -0.00117828, -0.00139701],\n",
      "       [ 0.00044468, -0.00197375, -0.0002186 ]]), array([[-0.00667844],\n",
      "       [-0.00437829],\n",
      "       [ 0.00295934]])]\n",
      "gradients_biases:  [array([ 0.00013109, -0.00053475, -0.00017002]), array([0.0061718])]\n",
      "Iteration 181, Cost: 0.24944621458196564\n",
      "gradient_weights:  [array([[ 0.00035343, -0.00118175, -0.00139414],\n",
      "       [ 0.00043622, -0.00197975, -0.00021832]]), array([[-0.00667449],\n",
      "       [-0.00437905],\n",
      "       [ 0.00295878]])]\n",
      "gradients_biases:  [array([ 0.00012869, -0.00053655, -0.00017006]), array([0.00616936])]\n",
      "Iteration 182, Cost: 0.24944028727317197\n",
      "gradient_weights:  [array([[ 0.00034652, -0.00118523, -0.00139127],\n",
      "       [ 0.00042777, -0.00198575, -0.00021803]]), array([[-0.00667055],\n",
      "       [-0.00437982],\n",
      "       [ 0.00295824]])]\n",
      "gradients_biases:  [array([ 0.00012629, -0.00053835, -0.00017009]), array([0.00616693])]\n",
      "Iteration 183, Cost: 0.24943436329439744\n",
      "gradient_weights:  [array([[ 0.00033962, -0.0011887 , -0.0013884 ],\n",
      "       [ 0.00041932, -0.00199175, -0.00021774]]), array([[-0.00666663],\n",
      "       [-0.00438059],\n",
      "       [ 0.0029577 ]])]\n",
      "gradients_biases:  [array([ 0.00012388, -0.00054016, -0.00017012]), array([0.00616451])]\n",
      "Iteration 184, Cost: 0.24942844260380836\n",
      "gradient_weights:  [array([[ 0.00033272, -0.00119218, -0.00138553],\n",
      "       [ 0.00041088, -0.00199776, -0.00021745]]), array([[-0.00666271],\n",
      "       [-0.00438137],\n",
      "       [ 0.00295717]])]\n",
      "gradients_biases:  [array([ 0.00012147, -0.00054196, -0.00017015]), array([0.0061621])]\n",
      "Iteration 185, Cost: 0.24942252515963326\n",
      "gradient_weights:  [array([[ 0.00032584, -0.00119565, -0.00138266],\n",
      "       [ 0.00040244, -0.00200377, -0.00021716]]), array([[-0.00665881],\n",
      "       [-0.00438215],\n",
      "       [ 0.00295666]])]\n",
      "gradients_biases:  [array([ 0.00011906, -0.00054376, -0.00017018]), array([0.0061597])]\n",
      "Iteration 186, Cost: 0.24941661092016232\n",
      "gradient_weights:  [array([[ 0.00031896, -0.00119913, -0.00137979],\n",
      "       [ 0.00039401, -0.00200978, -0.00021687]]), array([[-0.00665492],\n",
      "       [-0.00438293],\n",
      "       [ 0.00295615]])]\n",
      "gradients_biases:  [array([ 0.00011665, -0.00054556, -0.00017021]), array([0.00615732])]\n",
      "Iteration 187, Cost: 0.24941069984374736\n",
      "gradient_weights:  [array([[ 0.00031209, -0.00120261, -0.00137692],\n",
      "       [ 0.00038558, -0.00201579, -0.00021658]]), array([[-0.00665103],\n",
      "       [-0.00438372],\n",
      "       [ 0.00295566]])]\n",
      "gradients_biases:  [array([ 0.00011423, -0.00054737, -0.00017023]), array([0.00615495])]\n",
      "Iteration 188, Cost: 0.2494047918888011\n",
      "gradient_weights:  [array([[ 0.00030522, -0.00120609, -0.00137405],\n",
      "       [ 0.00037716, -0.00202181, -0.00021628]]), array([[-0.00664716],\n",
      "       [-0.00438452],\n",
      "       [ 0.00295517]])]\n",
      "gradients_biases:  [array([ 0.00011182, -0.00054917, -0.00017025]), array([0.00615259])]\n",
      "Iteration 189, Cost: 0.2493988870137968\n",
      "gradient_weights:  [array([[ 0.00029836, -0.00120958, -0.00137119],\n",
      "       [ 0.00036874, -0.00202783, -0.00021599]]), array([[-0.00664331],\n",
      "       [-0.00438532],\n",
      "       [ 0.0029547 ]])]\n",
      "gradients_biases:  [array([ 0.0001094 , -0.00055098, -0.00017027]), array([0.00615024])]\n",
      "Iteration 190, Cost: 0.24939298517726777\n",
      "gradient_weights:  [array([[ 0.00029151, -0.00121306, -0.00136832],\n",
      "       [ 0.00036033, -0.00203385, -0.00021569]]), array([[-0.00663946],\n",
      "       [-0.00438612],\n",
      "       [ 0.00295423]])]\n",
      "gradients_biases:  [array([ 0.00010697, -0.00055278, -0.00017029]), array([0.0061479])]\n",
      "Iteration 191, Cost: 0.24938708633780726\n",
      "gradient_weights:  [array([[ 0.00028466, -0.00121655, -0.00136545],\n",
      "       [ 0.00035193, -0.00203987, -0.0002154 ]]), array([[-0.00663562],\n",
      "       [-0.00438693],\n",
      "       [ 0.00295378]])]\n",
      "gradients_biases:  [array([ 0.00010455, -0.00055459, -0.0001703 ]), array([0.00614558])]\n",
      "Iteration 192, Cost: 0.2493811904540677\n",
      "gradient_weights:  [array([[ 0.00027782, -0.00122003, -0.00136259],\n",
      "       [ 0.00034353, -0.0020459 , -0.0002151 ]]), array([[-0.0066318 ],\n",
      "       [-0.00438774],\n",
      "       [ 0.00295333]])]\n",
      "gradients_biases:  [array([ 0.00010212, -0.0005564 , -0.00017031]), array([0.00614326])]\n",
      "Iteration 193, Cost: 0.24937529748476045\n",
      "gradient_weights:  [array([[ 0.00027099, -0.00122352, -0.00135972],\n",
      "       [ 0.00033514, -0.00205192, -0.0002148 ]]), array([[-0.00662799],\n",
      "       [-0.00438856],\n",
      "       [ 0.0029529 ]])]\n",
      "gradients_biases:  [array([ 9.96968034e-05, -5.58207612e-04, -1.70324483e-04]), array([0.00614096])]\n",
      "Iteration 194, Cost: 0.24936940738865537\n",
      "gradient_weights:  [array([[ 0.00026417, -0.00122701, -0.00135686],\n",
      "       [ 0.00032675, -0.00205795, -0.0002145 ]]), array([[-0.00662418],\n",
      "       [-0.00438938],\n",
      "       [ 0.00295247]])]\n",
      "gradients_biases:  [array([ 9.72670098e-05, -5.60016437e-04, -1.70332857e-04]), array([0.00613867])]\n",
      "Iteration 195, Cost: 0.24936352012458043\n",
      "gradient_weights:  [array([[ 0.00025735, -0.0012305 , -0.001354  ],\n",
      "       [ 0.00031837, -0.00206398, -0.0002142 ]]), array([[-0.00662039],\n",
      "       [-0.00439021],\n",
      "       [ 0.00295206]])]\n",
      "gradients_biases:  [array([ 9.48352061e-05, -5.61825889e-04, -1.70339123e-04]), array([0.00613639])]\n",
      "Iteration 196, Cost: 0.24935763565142144\n",
      "gradient_weights:  [array([[ 0.00025053, -0.00123399, -0.00135113],\n",
      "       [ 0.00030999, -0.00207002, -0.00021389]]), array([[-0.00661662],\n",
      "       [-0.00439104],\n",
      "       [ 0.00295166]])]\n",
      "gradients_biases:  [array([ 9.24014092e-05, -5.63635961e-04, -1.70343285e-04]), array([0.00613412])]\n",
      "Iteration 197, Cost: 0.24935175392812117\n",
      "gradient_weights:  [array([[ 0.00024373, -0.00123749, -0.00134827],\n",
      "       [ 0.00030162, -0.00207606, -0.00021359]]), array([[-0.00661285],\n",
      "       [-0.00439188],\n",
      "       [ 0.00295126]])]\n",
      "gradients_biases:  [array([ 8.9965636e-05, -5.6544665e-04, -1.7034535e-04]), array([0.00613187])]\n",
      "Iteration 198, Cost: 0.2493458749136796\n",
      "gradient_weights:  [array([[ 0.00023693, -0.00124098, -0.00134541],\n",
      "       [ 0.00029325, -0.0020821 , -0.00021329]]), array([[-0.00660909],\n",
      "       [-0.00439272],\n",
      "       [ 0.00295088]])]\n",
      "gradients_biases:  [array([ 8.75279030e-05, -5.67257949e-04, -1.70345321e-04]), array([0.00612962])]\n",
      "Iteration 199, Cost: 0.2493399985671531\n",
      "gradient_weights:  [array([[ 0.00023014, -0.00124448, -0.00134255],\n",
      "       [ 0.00028489, -0.00208814, -0.00021298]]), array([[-0.00660535],\n",
      "       [-0.00439357],\n",
      "       [ 0.00295051]])]\n",
      "gradients_biases:  [array([ 8.50882268e-05, -5.69069852e-04, -1.70343206e-04]), array([0.00612739])]\n",
      "Iteration 200, Cost: 0.24933412484765397\n",
      "gradient_weights:  [array([[ 0.00022335, -0.00124797, -0.00133969],\n",
      "       [ 0.00027653, -0.00209418, -0.00021267]]), array([[-0.00660162],\n",
      "       [-0.00439442],\n",
      "       [ 0.00295015]])]\n",
      "gradients_biases:  [array([ 8.26466241e-05, -5.70882354e-04, -1.70339008e-04]), array([0.00612516])]\n",
      "Iteration 201, Cost: 0.24932825371435044\n",
      "gradient_weights:  [array([[ 0.00021657, -0.00125147, -0.00133682],\n",
      "       [ 0.00026818, -0.00210023, -0.00021236]]), array([[-0.0065979 ],\n",
      "       [-0.00439527],\n",
      "       [ 0.00294979]])]\n",
      "gradients_biases:  [array([ 8.02031113e-05, -5.72695450e-04, -1.70332732e-04]), array([0.00612295])]\n",
      "Iteration 202, Cost: 0.24932238512646582\n",
      "gradient_weights:  [array([[ 0.0002098 , -0.00125497, -0.00133396],\n",
      "       [ 0.00025983, -0.00210628, -0.00021206]]), array([[-0.00659419],\n",
      "       [-0.00439614],\n",
      "       [ 0.00294945]])]\n",
      "gradients_biases:  [array([ 7.77577046e-05, -5.74509133e-04, -1.70324385e-04]), array([0.00612075])]\n",
      "Iteration 203, Cost: 0.24931651904327842\n",
      "gradient_weights:  [array([[ 0.00020303, -0.00125848, -0.00133111],\n",
      "       [ 0.00025149, -0.00211233, -0.00021175]]), array([[-0.00659049],\n",
      "       [-0.004397  ],\n",
      "       [ 0.00294912]])]\n",
      "gradients_biases:  [array([ 7.53104206e-05, -5.76323398e-04, -1.70313970e-04]), array([0.00611856])]\n",
      "Iteration 204, Cost: 0.249310655424121\n",
      "gradient_weights:  [array([[ 0.00019627, -0.00126198, -0.00132825],\n",
      "       [ 0.00024316, -0.00211838, -0.00021144]]), array([[-0.0065868 ],\n",
      "       [-0.00439787],\n",
      "       [ 0.00294879]])]\n",
      "gradients_biases:  [array([ 7.28612753e-05, -5.78138239e-04, -1.70301493e-04]), array([0.00611638])]\n",
      "Iteration 205, Cost: 0.24930479422838053\n",
      "gradient_weights:  [array([[ 0.00018952, -0.00126549, -0.00132539],\n",
      "       [ 0.00023483, -0.00212444, -0.00021112]]), array([[-0.00658313],\n",
      "       [-0.00439875],\n",
      "       [ 0.00294848]])]\n",
      "gradients_biases:  [array([ 7.04102850e-05, -5.79953650e-04, -1.70286958e-04]), array([0.00611422])]\n",
      "Iteration 206, Cost: 0.24929893541549747\n",
      "gradient_weights:  [array([[ 0.00018277, -0.00126899, -0.00132253],\n",
      "       [ 0.0002265 , -0.00213049, -0.00021081]]), array([[-0.00657946],\n",
      "       [-0.00439963],\n",
      "       [ 0.00294818]])]\n",
      "gradients_biases:  [array([ 6.79574658e-05, -5.81769626e-04, -1.70270370e-04]), array([0.00611206])]\n",
      "Iteration 207, Cost: 0.24929307894496583\n",
      "gradient_weights:  [array([[ 0.00017603, -0.0012725 , -0.00131967],\n",
      "       [ 0.00021818, -0.00213656, -0.0002105 ]]), array([[-0.00657581],\n",
      "       [-0.00440052],\n",
      "       [ 0.00294789]])]\n",
      "gradients_biases:  [array([ 6.55028338e-05, -5.83586160e-04, -1.70251734e-04]), array([0.00610992])]\n",
      "Iteration 208, Cost: 0.24928722477633247\n",
      "gradient_weights:  [array([[ 0.00016929, -0.00127601, -0.00131681],\n",
      "       [ 0.00020987, -0.00214262, -0.00021018]]), array([[-0.00657217],\n",
      "       [-0.00440141],\n",
      "       [ 0.0029476 ]])]\n",
      "gradients_biases:  [array([ 6.30464048e-05, -5.85403247e-04, -1.70231055e-04]), array([0.00610778])]\n",
      "Iteration 209, Cost: 0.24928137286919666\n",
      "gradient_weights:  [array([[ 0.00016256, -0.00127952, -0.00131396],\n",
      "       [ 0.00020156, -0.00214868, -0.00020986]]), array([[-0.00656854],\n",
      "       [-0.0044023 ],\n",
      "       [ 0.00294733]])]\n",
      "gradients_biases:  [array([ 6.05881948e-05, -5.87220880e-04, -1.70208338e-04]), array([0.00610566])]\n",
      "Iteration 210, Cost: 0.24927552318321\n",
      "gradient_weights:  [array([[ 0.00015584, -0.00128303, -0.0013111 ],\n",
      "       [ 0.00019325, -0.00215475, -0.00020955]]), array([[-0.00656492],\n",
      "       [-0.0044032 ],\n",
      "       [ 0.00294707]])]\n",
      "gradients_biases:  [array([ 5.81282197e-05, -5.89039054e-04, -1.70183586e-04]), array([0.00610355])]\n",
      "Iteration 211, Cost: 0.24926967567807573\n",
      "gradient_weights:  [array([[ 0.00014912, -0.00128655, -0.00130825],\n",
      "       [ 0.00018495, -0.00216082, -0.00020923]]), array([[-0.00656132],\n",
      "       [-0.00440411],\n",
      "       [ 0.00294681]])]\n",
      "gradients_biases:  [array([ 5.56664952e-05, -5.90857763e-04, -1.70156805e-04]), array([0.00610145])]\n",
      "Iteration 212, Cost: 0.24926383031354857\n",
      "gradient_weights:  [array([[ 0.00014241, -0.00129006, -0.00130539],\n",
      "       [ 0.00017666, -0.0021669 , -0.00020891]]), array([[-0.00655772],\n",
      "       [-0.00440502],\n",
      "       [ 0.00294657]])]\n",
      "gradients_biases:  [array([ 5.32030372e-05, -5.92677001e-04, -1.70127999e-04]), array([0.00609936])]\n",
      "Iteration 213, Cost: 0.2492579870494342\n",
      "gradient_weights:  [array([[ 0.00013571, -0.00129358, -0.00130253],\n",
      "       [ 0.00016837, -0.00217297, -0.00020859]]), array([[-0.00655414],\n",
      "       [-0.00440594],\n",
      "       [ 0.00294634]])]\n",
      "gradients_biases:  [array([ 5.07378612e-05, -5.94496761e-04, -1.70097172e-04]), array([0.00609728])]\n",
      "Iteration 214, Cost: 0.2492521458455888\n",
      "gradient_weights:  [array([[ 0.00012901, -0.0012971 , -0.00129968],\n",
      "       [ 0.00016009, -0.00217905, -0.00020827]]), array([[-0.00655057],\n",
      "       [-0.00440686],\n",
      "       [ 0.00294611]])]\n",
      "gradients_biases:  [array([ 4.82709828e-05, -5.96317038e-04, -1.70064330e-04]), array([0.00609521])]\n",
      "Iteration 215, Cost: 0.24924630666191888\n",
      "gradient_weights:  [array([[ 0.00012232, -0.00130062, -0.00129683],\n",
      "       [ 0.00015181, -0.00218513, -0.00020794]]), array([[-0.00654701],\n",
      "       [-0.00440778],\n",
      "       [ 0.0029459 ]])]\n",
      "gradients_biases:  [array([ 4.58024177e-05, -5.98137826e-04, -1.70029476e-04]), array([0.00609315])]\n",
      "Iteration 216, Cost: 0.24924046945838085\n",
      "gradient_weights:  [array([[ 0.00011563, -0.00130414, -0.00129397],\n",
      "       [ 0.00014353, -0.00219121, -0.00020762]]), array([[-0.00654346],\n",
      "       [-0.00440872],\n",
      "       [ 0.0029457 ]])]\n",
      "gradients_biases:  [array([ 4.33321813e-05, -5.99959118e-04, -1.69992615e-04]), array([0.00609111])]\n",
      "Iteration 217, Cost: 0.24923463419498057\n",
      "gradient_weights:  [array([[ 0.00010895, -0.00130767, -0.00129112],\n",
      "       [ 0.00013526, -0.0021973 , -0.0002073 ]]), array([[-0.00653992],\n",
      "       [-0.00440965],\n",
      "       [ 0.0029455 ]])]\n",
      "gradients_biases:  [array([ 4.08602891e-05, -6.01780908e-04, -1.69953751e-04]), array([0.00608907])]\n",
      "Iteration 218, Cost: 0.2492288008317728\n",
      "gradient_weights:  [array([[ 0.00010228, -0.00131119, -0.00128827],\n",
      "       [ 0.000127  , -0.00220339, -0.00020697]]), array([[-0.00653639],\n",
      "       [-0.00441059],\n",
      "       [ 0.00294532]])]\n",
      "gradients_biases:  [array([ 3.83867564e-05, -6.03603191e-04, -1.69912889e-04]), array([0.00608705])]\n",
      "Iteration 219, Cost: 0.24922296932886118\n",
      "gradient_weights:  [array([[ 9.56134507e-05, -1.31471953e-03, -1.28541318e-03],\n",
      "       [ 1.18737640e-04, -2.20947703e-03, -2.06644262e-04]]), array([[-0.00653288],\n",
      "       [-0.00441154],\n",
      "       [ 0.00294514]])]\n",
      "gradients_biases:  [array([ 3.59115986e-05, -6.05425960e-04, -1.69870032e-04]), array([0.00608503])]\n",
      "Iteration 220, Cost: 0.2492171396463978\n",
      "gradient_weights:  [array([[ 8.89520963e-05, -1.31824827e-03, -1.28256096e-03],\n",
      "       [ 1.10482557e-04, -2.21557011e-03, -2.06316635e-04]]), array([[-0.00652937],\n",
      "       [-0.00441249],\n",
      "       [ 0.00294498]])]\n",
      "gradients_biases:  [array([ 3.34348310e-05, -6.07249209e-04, -1.69825187e-04]), array([0.00608303])]\n",
      "Iteration 221, Cost: 0.24921131174458233\n",
      "gradient_weights:  [array([[ 8.22967376e-05, -1.32177866e-03, -1.27970911e-03],\n",
      "       [ 1.02232306e-04, -2.22166573e-03, -2.05987991e-04]]), array([[-0.00652588],\n",
      "       [-0.00441345],\n",
      "       [ 0.00294483]])]\n",
      "gradients_biases:  [array([ 3.09564688e-05, -6.09072932e-04, -1.69778355e-04]), array([0.00608104])]\n",
      "Iteration 222, Cost: 0.24920548558366246\n",
      "gradient_weights:  [array([[ 7.56473505e-05, -1.32531071e-03, -1.27685763e-03],\n",
      "       [ 9.39868674e-05, -2.22776390e-03, -2.05658331e-04]]), array([[-0.0065224 ],\n",
      "       [-0.00441441],\n",
      "       [ 0.00294468]])]\n",
      "gradients_biases:  [array([ 2.84765271e-05, -6.10897122e-04, -1.69729542e-04]), array([0.00607905])]\n",
      "Iteration 223, Cost: 0.24919966112393283\n",
      "gradient_weights:  [array([[ 6.90039115e-05, -1.32884443e-03, -1.27400651e-03],\n",
      "       [ 8.57462191e-05, -2.23386462e-03, -2.05327657e-04]]), array([[-0.00651893],\n",
      "       [-0.00441538],\n",
      "       [ 0.00294455]])]\n",
      "gradients_biases:  [array([ 2.59950211e-05, -6.12721774e-04, -1.69678753e-04]), array([0.00607708])]\n",
      "Iteration 224, Cost: 0.24919383832573488\n",
      "gradient_weights:  [array([[ 6.23663968e-05, -1.33237983e-03, -1.27115574e-03],\n",
      "       [ 7.75103403e-05, -2.23996791e-03, -2.04995972e-04]]), array([[-0.00651547],\n",
      "       [-0.00441635],\n",
      "       [ 0.00294442]])]\n",
      "gradients_biases:  [array([ 2.35119659e-05, -6.14546880e-04, -1.69625990e-04]), array([0.00607512])]\n",
      "Iteration 225, Cost: 0.24918801714945682\n",
      "gradient_weights:  [array([[ 5.57347826e-05, -1.33591693e-03, -1.26830531e-03],\n",
      "       [ 6.92792097e-05, -2.24607376e-03, -2.04663277e-04]]), array([[-0.00651202],\n",
      "       [-0.00441733],\n",
      "       [ 0.00294431]])]\n",
      "gradients_biases:  [array([ 2.10273765e-05, -6.16372435e-04, -1.69571258e-04]), array([0.00607317])]\n",
      "Iteration 226, Cost: 0.2491821975555326\n",
      "gradient_weights:  [array([[ 4.91090454e-05, -1.33945572e-03, -1.26545523e-03],\n",
      "       [ 6.10528063e-05, -2.25218220e-03, -2.04329576e-04]]), array([[-0.00650859],\n",
      "       [-0.00441831],\n",
      "       [ 0.0029442 ]])]\n",
      "gradients_biases:  [array([ 1.85412678e-05, -6.18198433e-04, -1.69514562e-04]), array([0.00607124])]\n",
      "Iteration 227, Cost: 0.24917637950444216\n",
      "gradient_weights:  [array([[ 4.24891615e-05, -1.34299622e-03, -1.26260547e-03],\n",
      "       [ 5.28311091e-05, -2.25829322e-03, -2.03994869e-04]]), array([[-0.00650516],\n",
      "       [-0.0044193 ],\n",
      "       [ 0.00294411]])]\n",
      "gradients_biases:  [array([ 1.60536548e-05, -6.20024867e-04, -1.69455906e-04]), array([0.00606931])]\n",
      "Iteration 228, Cost: 0.24917056295671064\n",
      "gradient_weights:  [array([[ 3.58751073e-05, -1.34653844e-03, -1.25975604e-03],\n",
      "       [ 4.46140969e-05, -2.26440683e-03, -2.03659160e-04]]), array([[-0.00650175],\n",
      "       [-0.00442029],\n",
      "       [ 0.00294402]])]\n",
      "gradients_biases:  [array([ 1.35645523e-05, -6.21851731e-04, -1.69395293e-04]), array([0.00606739])]\n",
      "Iteration 229, Cost: 0.24916474787290827\n",
      "gradient_weights:  [array([[ 2.92668593e-05, -1.35008240e-03, -1.25690693e-03],\n",
      "       [ 3.64017489e-05, -2.27052304e-03, -2.03322451e-04]]), array([[-0.00649834],\n",
      "       [-0.00442129],\n",
      "       [ 0.00294395]])]\n",
      "gradients_biases:  [array([ 1.10739752e-05, -6.23679018e-04, -1.69332727e-04]), array([0.00606548])]\n",
      "Iteration 230, Cost: 0.24915893421364993\n",
      "gradient_weights:  [array([[ 2.26643939e-05, -1.35362809e-03, -1.25405813e-03],\n",
      "       [ 2.81940441e-05, -2.27664186e-03, -2.02984743e-04]]), array([[-0.00649495],\n",
      "       [-0.00442229],\n",
      "       [ 0.00294388]])]\n",
      "gradients_biases:  [array([ 8.58193831e-06, -6.25506722e-04, -1.69268213e-04]), array([0.00606359])]\n",
      "Iteration 231, Cost: 0.2491531219395947\n",
      "gradient_weights:  [array([[ 1.60676876e-05, -1.35717553e-03, -1.25120964e-03],\n",
      "       [ 1.99909614e-05, -2.28276330e-03, -2.02646038e-04]]), array([[-0.00649157],\n",
      "       [-0.0044233 ],\n",
      "       [ 0.00294382]])]\n",
      "gradients_biases:  [array([ 6.08845625e-06, -6.27334837e-04, -1.69201755e-04]), array([0.0060617])]\n",
      "Iteration 232, Cost: 0.24914731101144558\n",
      "gradient_weights:  [array([[ 9.47671702e-06, -1.36072472e-03, -1.24836144e-03],\n",
      "       [ 1.17924801e-05, -2.28888736e-03, -2.02306340e-04]]), array([[-0.0064882 ],\n",
      "       [-0.00442432],\n",
      "       [ 0.00294378]])]\n",
      "gradients_biases:  [array([ 3.59354375e-06, -6.29163356e-04, -1.69133357e-04]), array([0.00605983])]\n",
      "Iteration 233, Cost: 0.2491415013899492\n",
      "gradient_weights:  [array([[ 2.89145868e-06, -1.36427569e-03, -1.24551354e-03],\n",
      "       [ 3.59857924e-06, -2.29501405e-03, -2.01965649e-04]]), array([[-0.00648484],\n",
      "       [-0.00442534],\n",
      "       [ 0.00294374]])]\n",
      "gradients_biases:  [array([ 1.09721545e-06, -6.30992273e-04, -1.69063022e-04]), array([0.00605797])]\n",
      "Iteration 234, Cost: 0.24913569303589514\n",
      "gradient_weights:  [array([[-3.68811078e-06, -1.36782844e-03, -1.24266592e-03],\n",
      "       [-4.59076200e-06, -2.30114338e-03, -2.01623968e-04]]), array([[-0.0064815 ],\n",
      "       [-0.00442636],\n",
      "       [ 0.00294371]])]\n",
      "gradients_biases:  [array([-1.40051406e-06, -6.32821581e-04, -1.68990754e-04]), array([0.00605611])]\n",
      "Iteration 235, Cost: 0.24912988591011614\n",
      "gradient_weights:  [array([[-1.02620147e-05, -1.37138297e-03, -1.23981858e-03],\n",
      "       [-1.27755644e-05, -2.30727535e-03, -2.01281300e-04]]), array([[-0.00647816],\n",
      "       [-0.00442739],\n",
      "       [ 0.0029437 ]])]\n",
      "gradients_biases:  [array([-3.89963023e-06, -6.34651274e-04, -1.68916558e-04]), array([0.00605427])]\n",
      "Iteration 236, Cost: 0.24912407997348712\n",
      "gradient_weights:  [array([[-1.68302765e-05, -1.37493930e-03, -1.23697151e-03],\n",
      "       [-2.09558488e-05, -2.31340998e-03, -2.00937646e-04]]), array([[-0.00647484],\n",
      "       [-0.00442843],\n",
      "       [ 0.00294369]])]\n",
      "gradients_biases:  [array([-6.40011857e-06, -6.36481346e-04, -1.68840437e-04]), array([0.00605244])]\n",
      "Iteration 237, Cost: 0.24911827518692536\n",
      "gradient_weights:  [array([[-2.33929194e-05, -1.37849743e-03, -1.23412470e-03],\n",
      "       [-2.91316359e-05, -2.31954727e-03, -2.00593009e-04]]), array([[-0.00647152],\n",
      "       [-0.00442947],\n",
      "       [ 0.00294369]])]\n",
      "gradients_biases:  [array([-8.90196463e-06, -6.38311790e-04, -1.68762396e-04]), array([0.00605062])]\n",
      "Iteration 238, Cost: 0.2491124715113896\n",
      "gradient_weights:  [array([[-2.99499666e-05, -1.38205739e-03, -1.23127815e-03],\n",
      "       [-3.73029464e-05, -2.32568722e-03, -2.00247390e-04]]), array([[-0.00646822],\n",
      "       [-0.00443051],\n",
      "       [ 0.00294371]])]\n",
      "gradients_biases:  [array([-1.14051540e-05, -6.40142599e-04, -1.68682438e-04]), array([0.0060488])]\n",
      "Iteration 239, Cost: 0.24910666890788014\n",
      "gradient_weights:  [array([[-3.65014413e-05, -1.38561917e-03, -1.22843186e-03],\n",
      "       [-4.54698009e-05, -2.33182986e-03, -1.99900791e-04]]), array([[-0.00646493],\n",
      "       [-0.00443156],\n",
      "       [ 0.00294373]])]\n",
      "gradients_biases:  [array([-1.39096723e-05, -6.41973767e-04, -1.68600566e-04]), array([0.006047])]\n",
      "Iteration 240, Cost: 0.24910086733743836\n",
      "gradient_weights:  [array([[-4.30473668e-05, -1.38918278e-03, -1.22558581e-03],\n",
      "       [-5.36322202e-05, -2.33797518e-03, -1.99553216e-04]]), array([[-0.00646165],\n",
      "       [-0.00443262],\n",
      "       [ 0.00294376]])]\n",
      "gradients_biases:  [array([-1.64155051e-05, -6.43805288e-04, -1.68516786e-04]), array([0.00604521])]\n",
      "Iteration 241, Cost: 0.2490950667611463\n",
      "gradient_weights:  [array([[-4.95877662e-05, -1.39274824e-03, -1.22274000e-03],\n",
      "       [-6.17902248e-05, -2.34412318e-03, -1.99204666e-04]]), array([[-0.00645838],\n",
      "       [-0.00443368],\n",
      "       [ 0.0029438 ]])]\n",
      "gradients_biases:  [array([-1.89226384e-05, -6.45637154e-04, -1.68431100e-04]), array([0.00604343])]\n",
      "Iteration 242, Cost: 0.24908926714012622\n",
      "gradient_weights:  [array([[-5.61226626e-05, -1.39631556e-03, -1.21989442e-03],\n",
      "       [-6.99438353e-05, -2.35027389e-03, -1.98855142e-04]]), array([[-0.00645512],\n",
      "       [-0.00443475],\n",
      "       [ 0.00294385]])]\n",
      "gradients_biases:  [array([-2.14310577e-05, -6.47469359e-04, -1.68343512e-04]), array([0.00604167])]\n",
      "Iteration 243, Cost: 0.24908346843554052\n",
      "gradient_weights:  [array([[-6.26520791e-05, -1.39988474e-03, -1.21704906e-03],\n",
      "       [-7.80930722e-05, -2.35642730e-03, -1.98504648e-04]]), array([[-0.00645188],\n",
      "       [-0.00443582],\n",
      "       [ 0.00294391]])]\n",
      "gradients_biases:  [array([-2.39407488e-05, -6.49301897e-04, -1.68254026e-04]), array([0.00603991])]\n",
      "Iteration 244, Cost: 0.24907767060859115\n",
      "gradient_weights:  [array([[-6.91760386e-05, -1.40345580e-03, -1.21420392e-03],\n",
      "       [-8.62379561e-05, -2.36258343e-03, -1.98153185e-04]]), array([[-0.00644864],\n",
      "       [-0.0044369 ],\n",
      "       [ 0.00294399]])]\n",
      "gradients_biases:  [array([-2.64516978e-05, -6.51134761e-04, -1.68162647e-04]), array([0.00603816])]\n",
      "Iteration 245, Cost: 0.24907187362051936\n",
      "gradient_weights:  [array([[-7.56945643e-05, -1.40702874e-03, -1.21135900e-03],\n",
      "       [-9.43785074e-05, -2.36874227e-03, -1.97800755e-04]]), array([[-0.00644541],\n",
      "       [-0.00443798],\n",
      "       [ 0.00294407]])]\n",
      "gradients_biases:  [array([-2.89638903e-05, -6.52967945e-04, -1.68069377e-04]), array([0.00603642])]\n",
      "Iteration 246, Cost: 0.24906607743260534\n",
      "gradient_weights:  [array([[-8.22076791e-05, -1.41060358e-03, -1.20851428e-03],\n",
      "       [-1.02514746e-04, -2.37490384e-03, -1.97447361e-04]]), array([[-0.0064422 ],\n",
      "       [-0.00443907],\n",
      "       [ 0.00294416]])]\n",
      "gradients_biases:  [array([-3.14773124e-05, -6.54801441e-04, -1.67974220e-04]), array([0.00603469])]\n",
      "Iteration 247, Cost: 0.24906028200616787\n",
      "gradient_weights:  [array([[-8.87154059e-05, -1.41418032e-03, -1.20566976e-03],\n",
      "       [-1.10646694e-04, -2.38106815e-03, -1.97093004e-04]]), array([[-0.006439  ],\n",
      "       [-0.00444016],\n",
      "       [ 0.00294426]])]\n",
      "gradients_biases:  [array([-3.39919500e-05, -6.56635243e-04, -1.67877180e-04]), array([0.00603298])]\n",
      "Iteration 248, Cost: 0.249054487302564\n",
      "gradient_weights:  [array([[-9.52177676e-05, -1.41775897e-03, -1.20282543e-03],\n",
      "       [-1.18774370e-04, -2.38723520e-03, -1.96737687e-04]]), array([[-0.00643581],\n",
      "       [-0.00444126],\n",
      "       [ 0.00294437]])]\n",
      "gradients_biases:  [array([-3.65077891e-05, -6.58469344e-04, -1.67778261e-04]), array([0.00603127])]\n",
      "Iteration 249, Cost: 0.2490486932831886\n",
      "gradient_weights:  [array([[-0.00010171, -0.00142134, -0.00119998],\n",
      "       [-0.0001269 , -0.00239341, -0.00019638]]), array([[-0.00643262],\n",
      "       [-0.00444237],\n",
      "       [ 0.00294449]])]\n",
      "gradients_biases:  [array([-3.90248158e-05, -6.60303738e-04, -1.67677466e-04]), array([0.00602958])]\n",
      "Iteration 250, Cost: 0.2490428999094742\n",
      "gradient_weights:  [array([[-0.00010821, -0.00142492, -0.00119714],\n",
      "       [-0.00013502, -0.00239958, -0.00019602]]), array([[-0.00642945],\n",
      "       [-0.00444348],\n",
      "       [ 0.00294462]])]\n",
      "gradients_biases:  [array([-4.15430162e-05, -6.62138417e-04, -1.67574800e-04]), array([0.00602789])]\n",
      "Iteration 251, Cost: 0.24903710714289057\n",
      "gradient_weights:  [array([[-0.00011469, -0.00142851, -0.00119429],\n",
      "       [-0.00014313, -0.00240575, -0.00019567]]), array([[-0.00642629],\n",
      "       [-0.00444459],\n",
      "       [ 0.00294475]])]\n",
      "gradients_biases:  [array([-4.40623763e-05, -6.63973376e-04, -1.67470264e-04]), array([0.00602622])]\n",
      "Iteration 252, Cost: 0.24903131494494415\n",
      "gradient_weights:  [array([[-0.00012117, -0.00143209, -0.00119145],\n",
      "       [-0.00015124, -0.00241193, -0.00019531]]), array([[-0.00642315],\n",
      "       [-0.00444571],\n",
      "       [ 0.0029449 ]])]\n",
      "gradients_biases:  [array([-4.65828824e-05, -6.65808607e-04, -1.67363865e-04]), array([0.00602455])]\n",
      "Iteration 253, Cost: 0.24902552327717806\n",
      "gradient_weights:  [array([[-0.00012765, -0.00143568, -0.00118861],\n",
      "       [-0.00015935, -0.00241811, -0.00019495]]), array([[-0.00642001],\n",
      "       [-0.00444684],\n",
      "       [ 0.00294506]])]\n",
      "gradients_biases:  [array([-4.91045206e-05, -6.67644104e-04, -1.67255604e-04]), array([0.0060229])]\n",
      "Iteration 254, Cost: 0.24901973210117156\n",
      "gradient_weights:  [array([[-0.00013412, -0.00143927, -0.00118576],\n",
      "       [-0.00016745, -0.0024243 , -0.00019459]]), array([[-0.00641688],\n",
      "       [-0.00444797],\n",
      "       [ 0.00294523]])]\n",
      "gradients_biases:  [array([-5.16272772e-05, -6.69479859e-04, -1.67145485e-04]), array([0.00602125])]\n",
      "Iteration 255, Cost: 0.24901394137853988\n",
      "gradient_weights:  [array([[-0.00014059, -0.00144286, -0.00118292],\n",
      "       [-0.00017555, -0.00243048, -0.00019422]]), array([[-0.00641377],\n",
      "       [-0.0044491 ],\n",
      "       [ 0.00294541]])]\n",
      "gradients_biases:  [array([-5.41511385e-05, -6.71315867e-04, -1.67033512e-04]), array([0.00601962])]\n",
      "Iteration 256, Cost: 0.24900815107093371\n",
      "gradient_weights:  [array([[-0.00014705, -0.00144646, -0.00118008],\n",
      "       [-0.00018364, -0.00243667, -0.00019386]]), array([[-0.00641066],\n",
      "       [-0.00445025],\n",
      "       [ 0.0029456 ]])]\n",
      "gradients_biases:  [array([-5.66760908e-05, -6.73152119e-04, -1.66919689e-04]), array([0.006018])]\n",
      "Iteration 257, Cost: 0.2490023611400388\n",
      "gradient_weights:  [array([[-0.0001535 , -0.00145005, -0.00117723],\n",
      "       [-0.00019173, -0.00244286, -0.0001935 ]]), array([[-0.00640757],\n",
      "       [-0.00445139],\n",
      "       [ 0.00294579]])]\n",
      "gradients_biases:  [array([-5.92021205e-05, -6.74988610e-04, -1.66804019e-04]), array([0.00601639])]\n",
      "Iteration 258, Cost: 0.24899657154757576\n",
      "gradient_weights:  [array([[-0.00015995, -0.00145365, -0.00117439],\n",
      "       [-0.00019982, -0.00244906, -0.00019313]]), array([[-0.00640449],\n",
      "       [-0.00445255],\n",
      "       [ 0.002946  ]])]\n",
      "gradients_biases:  [array([-6.17292138e-05, -6.76825332e-04, -1.66686506e-04]), array([0.00601478])]\n",
      "Iteration 259, Cost: 0.2489907822552999\n",
      "gradient_weights:  [array([[-0.0001664 , -0.00145725, -0.00117155],\n",
      "       [-0.0002079 , -0.00245526, -0.00019277]]), array([[-0.00640141],\n",
      "       [-0.0044537 ],\n",
      "       [ 0.00294622]])]\n",
      "gradients_biases:  [array([-6.42573574e-05, -6.78662278e-04, -1.66567152e-04]), array([0.00601319])]\n",
      "Iteration 260, Cost: 0.24898499322500056\n",
      "gradient_weights:  [array([[-0.00017284, -0.00146086, -0.00116871],\n",
      "       [-0.00021598, -0.00246146, -0.0001924 ]]), array([[-0.00639835],\n",
      "       [-0.00445487],\n",
      "       [ 0.00294644]])]\n",
      "gradients_biases:  [array([-6.67865375e-05, -6.80499442e-04, -1.66445963e-04]), array([0.00601161])]\n",
      "Iteration 261, Cost: 0.2489792044185009\n",
      "gradient_weights:  [array([[-0.00017927, -0.00146446, -0.00116586],\n",
      "       [-0.00022405, -0.00246766, -0.00019203]]), array([[-0.0063953 ],\n",
      "       [-0.00445604],\n",
      "       [ 0.00294668]])]\n",
      "gradients_biases:  [array([-6.93167408e-05, -6.82336817e-04, -1.66322940e-04]), array([0.00601004])]\n",
      "Iteration 262, Cost: 0.24897341579765764\n",
      "gradient_weights:  [array([[-0.0001857 , -0.00146807, -0.00116302],\n",
      "       [-0.00023212, -0.00247387, -0.00019166]]), array([[-0.00639226],\n",
      "       [-0.00445721],\n",
      "       [ 0.00294693]])]\n",
      "gradients_biases:  [array([-7.18479538e-05, -6.84174395e-04, -1.66198088e-04]), array([0.00600848])]\n",
      "Iteration 263, Cost: 0.24896762732436073\n",
      "gradient_weights:  [array([[-0.00019212, -0.00147167, -0.00116018],\n",
      "       [-0.00024019, -0.00248008, -0.00019129]]), array([[-0.00638923],\n",
      "       [-0.00445839],\n",
      "       [ 0.00294718]])]\n",
      "gradients_biases:  [array([-7.43801630e-05, -6.86012170e-04, -1.66071409e-04]), array([0.00600693])]\n",
      "Iteration 264, Cost: 0.2489618389605328\n",
      "gradient_weights:  [array([[-0.00019854, -0.00147529, -0.00115734],\n",
      "       [-0.00024825, -0.00248629, -0.00019092]]), array([[-0.00638621],\n",
      "       [-0.00445958],\n",
      "       [ 0.00294745]])]\n",
      "gradients_biases:  [array([-7.69133550e-05, -6.87850135e-04, -1.65942908e-04]), array([0.00600539])]\n",
      "Iteration 265, Cost: 0.2489560506681292\n",
      "gradient_weights:  [array([[-0.00020496, -0.0014789 , -0.0011545 ],\n",
      "       [-0.00025631, -0.0024925 , -0.00019055]]), array([[-0.00638321],\n",
      "       [-0.00446077],\n",
      "       [ 0.00294772]])]\n",
      "gradients_biases:  [array([-7.94475166e-05, -6.89688283e-04, -1.65812588e-04]), array([0.00600386])]\n",
      "Iteration 266, Cost: 0.24895026240913734\n",
      "gradient_weights:  [array([[-0.00021137, -0.00148251, -0.00115165],\n",
      "       [-0.00026437, -0.00249872, -0.00019018]]), array([[-0.00638021],\n",
      "       [-0.00446196],\n",
      "       [ 0.00294801]])]\n",
      "gradients_biases:  [array([-8.19826342e-05, -6.91526606e-04, -1.65680452e-04]), array([0.00600234])]\n",
      "Iteration 267, Cost: 0.24894447414557658\n",
      "gradient_weights:  [array([[-0.00021777, -0.00148613, -0.00114881],\n",
      "       [-0.00027242, -0.00250494, -0.00018981]]), array([[-0.00637722],\n",
      "       [-0.00446316],\n",
      "       [ 0.0029483 ]])]\n",
      "gradients_biases:  [array([-8.45186948e-05, -6.93365097e-04, -1.65546503e-04]), array([0.00600083])]\n",
      "Iteration 268, Cost: 0.24893868583949783\n",
      "gradient_weights:  [array([[-0.00022417, -0.00148975, -0.00114597],\n",
      "       [-0.00028046, -0.00251116, -0.00018943]]), array([[-0.00637425],\n",
      "       [-0.00446437],\n",
      "       [ 0.00294861]])]\n",
      "gradients_biases:  [array([-8.70556849e-05, -6.95203751e-04, -1.65410746e-04]), array([0.00599933])]\n",
      "Iteration 269, Cost: 0.2489328974529832\n",
      "gradient_weights:  [array([[-0.00023057, -0.00149337, -0.00114313],\n",
      "       [-0.0002885 , -0.00251739, -0.00018906]]), array([[-0.00637128],\n",
      "       [-0.00446558],\n",
      "       [ 0.00294892]])]\n",
      "gradients_biases:  [array([-8.95935914e-05, -6.97042559e-04, -1.65273182e-04]), array([0.00599784])]\n",
      "Iteration 270, Cost: 0.24892710894814546\n",
      "gradient_weights:  [array([[-0.00023696, -0.00149699, -0.00114029],\n",
      "       [-0.00029654, -0.00252362, -0.00018868]]), array([[-0.00636833],\n",
      "       [-0.0044668 ],\n",
      "       [ 0.00294924]])]\n",
      "gradients_biases:  [array([-9.21324011e-05, -6.98881514e-04, -1.65133816e-04]), array([0.00599636])]\n",
      "Iteration 271, Cost: 0.24892132028712832\n",
      "gradient_weights:  [array([[-0.00024334, -0.00150062, -0.00113744],\n",
      "       [-0.00030458, -0.00252985, -0.00018831]]), array([[-0.00636538],\n",
      "       [-0.00446802],\n",
      "       [ 0.00294958]])]\n",
      "gradients_biases:  [array([-9.46721009e-05, -7.00720610e-04, -1.64992651e-04]), array([0.00599489])]\n",
      "Iteration 272, Cost: 0.2489155314321056\n",
      "gradient_weights:  [array([[-0.00024972, -0.00150425, -0.0011346 ],\n",
      "       [-0.00031261, -0.00253609, -0.00018793]]), array([[-0.00636245],\n",
      "       [-0.00446925],\n",
      "       [ 0.00294992]])]\n",
      "gradients_biases:  [array([-9.72126775e-05, -7.02559838e-04, -1.64849691e-04]), array([0.00599343])]\n",
      "Iteration 273, Cost: 0.24890974234528107\n",
      "gradient_weights:  [array([[-0.0002561 , -0.00150788, -0.00113176],\n",
      "       [-0.00032064, -0.00254232, -0.00018755]]), array([[-0.00635953],\n",
      "       [-0.00447049],\n",
      "       [ 0.00295027]])]\n",
      "gradients_biases:  [array([-9.97541180e-05, -7.04399193e-04, -1.64704937e-04]), array([0.00599198])]\n",
      "Iteration 274, Cost: 0.24890395298888787\n",
      "gradient_weights:  [array([[-0.00026247, -0.00151151, -0.00112892],\n",
      "       [-0.00032866, -0.00254857, -0.00018717]]), array([[-0.00635662],\n",
      "       [-0.00447173],\n",
      "       [ 0.00295063]])]\n",
      "gradients_biases:  [array([-0.0001023 , -0.00070624, -0.00016456]), array([0.00599054])]\n",
      "Iteration 275, Cost: 0.2488981633251889\n",
      "gradient_weights:  [array([[-0.00026884, -0.00151514, -0.00112608],\n",
      "       [-0.00033668, -0.00255481, -0.00018679]]), array([[-0.00635372],\n",
      "       [-0.00447297],\n",
      "       [ 0.00295101]])]\n",
      "gradients_biases:  [array([-0.00010484, -0.00070808, -0.00016441]), array([0.00598912])]\n",
      "Iteration 276, Cost: 0.24889237331647557\n",
      "gradient_weights:  [array([[-0.0002752 , -0.00151878, -0.00112324],\n",
      "       [-0.0003447 , -0.00256106, -0.00018641]]), array([[-0.00635083],\n",
      "       [-0.00447422],\n",
      "       [ 0.00295139]])]\n",
      "gradients_biases:  [array([-0.00010738, -0.00070992, -0.00016426]), array([0.0059877])]\n",
      "Iteration 277, Cost: 0.24888658292506827\n",
      "gradient_weights:  [array([[-0.00028155, -0.00152242, -0.00112039],\n",
      "       [-0.00035271, -0.00256731, -0.00018603]]), array([[-0.00634795],\n",
      "       [-0.00447548],\n",
      "       [ 0.00295178]])]\n",
      "gradients_biases:  [array([-0.00010993, -0.00071176, -0.00016411]), array([0.00598629])]\n",
      "Iteration 278, Cost: 0.24888079211331554\n",
      "gradient_weights:  [array([[-0.00028791, -0.00152606, -0.00111755],\n",
      "       [-0.00036072, -0.00257356, -0.00018564]]), array([[-0.00634508],\n",
      "       [-0.00447674],\n",
      "       [ 0.00295218]])]\n",
      "gradients_biases:  [array([-0.00011247, -0.0007136 , -0.00016395]), array([0.00598489])]\n",
      "Iteration 279, Cost: 0.24887500084359396\n",
      "gradient_weights:  [array([[-0.00029425, -0.0015297 , -0.00111471],\n",
      "       [-0.00036872, -0.00257981, -0.00018526]]), array([[-0.00634222],\n",
      "       [-0.00447801],\n",
      "       [ 0.00295259]])]\n",
      "gradients_biases:  [array([-0.00011502, -0.00071544, -0.0001638 ]), array([0.00598351])]\n",
      "Iteration 280, Cost: 0.24886920907830795\n",
      "gradient_weights:  [array([[-0.0003006 , -0.00153334, -0.00111187],\n",
      "       [-0.00037672, -0.00258607, -0.00018487]]), array([[-0.00633937],\n",
      "       [-0.00447928],\n",
      "       [ 0.00295301]])]\n",
      "gradients_biases:  [array([-0.00011757, -0.00071728, -0.00016364]), array([0.00598213])]\n",
      "Iteration 281, Cost: 0.24886341677988932\n",
      "gradient_weights:  [array([[-0.00030693, -0.00153699, -0.00110903],\n",
      "       [-0.00038472, -0.00259233, -0.00018449]]), array([[-0.00633653],\n",
      "       [-0.00448056],\n",
      "       [ 0.00295344]])]\n",
      "gradients_biases:  [array([-0.00012012, -0.00071912, -0.00016348]), array([0.00598076])]\n",
      "Iteration 282, Cost: 0.24885762391079685\n",
      "gradient_weights:  [array([[-0.00031327, -0.00154064, -0.00110619],\n",
      "       [-0.00039272, -0.0025986 , -0.0001841 ]]), array([[-0.00633371],\n",
      "       [-0.00448184],\n",
      "       [ 0.00295388]])]\n",
      "gradients_biases:  [array([-0.00012266, -0.00072096, -0.00016332]), array([0.0059794])]\n",
      "Iteration 283, Cost: 0.24885183043351608\n",
      "gradient_weights:  [array([[-0.0003196 , -0.00154429, -0.00110335],\n",
      "       [-0.00040071, -0.00260487, -0.00018371]]), array([[-0.00633089],\n",
      "       [-0.00448313],\n",
      "       [ 0.00295433]])]\n",
      "gradients_biases:  [array([-0.00012521, -0.0007228 , -0.00016316]), array([0.00597806])]\n",
      "Iteration 284, Cost: 0.24884603631055924\n",
      "gradient_weights:  [array([[-0.00032592, -0.00154795, -0.0011005 ],\n",
      "       [-0.00040869, -0.00261114, -0.00018332]]), array([[-0.00632809],\n",
      "       [-0.00448443],\n",
      "       [ 0.00295479]])]\n",
      "gradients_biases:  [array([-0.00012776, -0.00072464, -0.000163  ]), array([0.00597672])]\n",
      "Iteration 285, Cost: 0.24884024150446438\n",
      "gradient_weights:  [array([[-0.00033224, -0.0015516 , -0.00109766],\n",
      "       [-0.00041668, -0.00261741, -0.00018294]]), array([[-0.00632529],\n",
      "       [-0.00448573],\n",
      "       [ 0.00295526]])]\n",
      "gradients_biases:  [array([-0.00013031, -0.00072648, -0.00016283]), array([0.00597539])]\n",
      "Iteration 286, Cost: 0.24883444597779575\n",
      "gradient_weights:  [array([[-0.00033855, -0.00155526, -0.00109482],\n",
      "       [-0.00042466, -0.00262369, -0.00018254]]), array([[-0.0063225 ],\n",
      "       [-0.00448703],\n",
      "       [ 0.00295573]])]\n",
      "gradients_biases:  [array([-0.00013287, -0.00072832, -0.00016266]), array([0.00597408])]\n",
      "Iteration 287, Cost: 0.24882864969314292\n",
      "gradient_weights:  [array([[-0.00034487, -0.00155892, -0.00109198],\n",
      "       [-0.00043264, -0.00262997, -0.00018215]]), array([[-0.00631973],\n",
      "       [-0.00448835],\n",
      "       [ 0.00295622]])]\n",
      "gradients_biases:  [array([-0.00013542, -0.00073016, -0.00016249]), array([0.00597277])]\n",
      "Iteration 288, Cost: 0.24882285261312082\n",
      "gradient_weights:  [array([[-0.00035117, -0.00156258, -0.00108914],\n",
      "       [-0.00044061, -0.00263625, -0.00018176]]), array([[-0.00631697],\n",
      "       [-0.00448966],\n",
      "       [ 0.00295672]])]\n",
      "gradients_biases:  [array([-0.00013797, -0.000732  , -0.00016232]), array([0.00597147])]\n",
      "Iteration 289, Cost: 0.24881705470036916\n",
      "gradient_weights:  [array([[-0.00035747, -0.00156625, -0.0010863 ],\n",
      "       [-0.00044858, -0.00264253, -0.00018137]]), array([[-0.00631421],\n",
      "       [-0.00449099],\n",
      "       [ 0.00295723]])]\n",
      "gradients_biases:  [array([-0.00014052, -0.00073384, -0.00016215]), array([0.00597019])]\n",
      "Iteration 290, Cost: 0.24881125591755243\n",
      "gradient_weights:  [array([[-0.00036377, -0.00156992, -0.00108345],\n",
      "       [-0.00045654, -0.00264882, -0.00018098]]), array([[-0.00631147],\n",
      "       [-0.00449232],\n",
      "       [ 0.00295774]])]\n",
      "gradients_biases:  [array([-0.00014308, -0.00073568, -0.00016197]), array([0.00596891])]\n",
      "Iteration 291, Cost: 0.24880545622735936\n",
      "gradient_weights:  [array([[-0.00037006, -0.00157359, -0.00108061],\n",
      "       [-0.00046451, -0.00265512, -0.00018058]]), array([[-0.00630874],\n",
      "       [-0.00449365],\n",
      "       [ 0.00295827]])]\n",
      "gradients_biases:  [array([-0.00014563, -0.00073752, -0.0001618 ]), array([0.00596764])]\n",
      "Iteration 292, Cost: 0.24879965559250283\n",
      "gradient_weights:  [array([[-0.00037635, -0.00157726, -0.00107777],\n",
      "       [-0.00047247, -0.00266141, -0.00018018]]), array([[-0.00630601],\n",
      "       [-0.00449499],\n",
      "       [ 0.0029588 ]])]\n",
      "gradients_biases:  [array([-0.00014819, -0.00073936, -0.00016162]), array([0.00596639])]\n",
      "Iteration 293, Cost: 0.24879385397571924\n",
      "gradient_weights:  [array([[-0.00038264, -0.00158093, -0.00107493],\n",
      "       [-0.00048042, -0.00266771, -0.00017979]]), array([[-0.0063033 ],\n",
      "       [-0.00449633],\n",
      "       [ 0.00295935]])]\n",
      "gradients_biases:  [array([-0.00015074, -0.0007412 , -0.00016144]), array([0.00596514])]\n",
      "Iteration 294, Cost: 0.24878805133976856\n",
      "gradient_weights:  [array([[-0.00038892, -0.00158461, -0.00107209],\n",
      "       [-0.00048838, -0.00267401, -0.00017939]]), array([[-0.0063006 ],\n",
      "       [-0.00449768],\n",
      "       [ 0.00295991]])]\n",
      "gradients_biases:  [array([-0.0001533 , -0.00074304, -0.00016126]), array([0.0059639])]\n",
      "Iteration 295, Cost: 0.24878224764743387\n",
      "gradient_weights:  [array([[-0.00039519, -0.00158829, -0.00106924],\n",
      "       [-0.00049632, -0.00268031, -0.00017899]]), array([[-0.00629791],\n",
      "       [-0.00449904],\n",
      "       [ 0.00296047]])]\n",
      "gradients_biases:  [array([-0.00015586, -0.00074488, -0.00016107]), array([0.00596268])]\n",
      "Iteration 296, Cost: 0.24877644286152092\n",
      "gradient_weights:  [array([[-0.00040146, -0.00159197, -0.0010664 ],\n",
      "       [-0.00050427, -0.00268662, -0.00017859]]), array([[-0.00629523],\n",
      "       [-0.0045004 ],\n",
      "       [ 0.00296105]])]\n",
      "gradients_biases:  [array([-0.00015842, -0.00074672, -0.00016089]), array([0.00596146])]\n",
      "Iteration 297, Cost: 0.24877063694485818\n",
      "gradient_weights:  [array([[-0.00040773, -0.00159566, -0.00106356],\n",
      "       [-0.00051221, -0.00269293, -0.00017819]]), array([[-0.00629256],\n",
      "       [-0.00450177],\n",
      "       [ 0.00296163]])]\n",
      "gradients_biases:  [array([-0.00016097, -0.00074856, -0.0001607 ]), array([0.00596025])]\n",
      "Iteration 298, Cost: 0.24876482986029635\n",
      "gradient_weights:  [array([[-0.00041399, -0.00159934, -0.00106071],\n",
      "       [-0.00052015, -0.00269924, -0.00017779]]), array([[-0.0062899 ],\n",
      "       [-0.00450314],\n",
      "       [ 0.00296222]])]\n",
      "gradients_biases:  [array([-0.00016353, -0.0007504 , -0.00016051]), array([0.00595906])]\n",
      "Iteration 299, Cost: 0.248759021570708\n",
      "gradient_weights:  [array([[-0.00042025, -0.00160303, -0.00105787],\n",
      "       [-0.00052809, -0.00270556, -0.00017739]]), array([[-0.00628725],\n",
      "       [-0.00450452],\n",
      "       [ 0.00296283]])]\n",
      "gradients_biases:  [array([-0.00016609, -0.00075224, -0.00016032]), array([0.00595787])]\n",
      "Iteration 300, Cost: 0.24875321203898712\n",
      "gradient_weights:  [array([[-0.0004265 , -0.00160672, -0.00105503],\n",
      "       [-0.00053602, -0.00271188, -0.00017699]]), array([[-0.00628461],\n",
      "       [-0.00450591],\n",
      "       [ 0.00296344]])]\n",
      "gradients_biases:  [array([-0.00016865, -0.00075408, -0.00016013]), array([0.00595669])]\n",
      "Iteration 301, Cost: 0.24874740122804948\n",
      "gradient_weights:  [array([[-0.00043275, -0.00161042, -0.00105219],\n",
      "       [-0.00054395, -0.0027182 , -0.00017659]]), array([[-0.00628198],\n",
      "       [-0.0045073 ],\n",
      "       [ 0.00296406]])]\n",
      "gradients_biases:  [array([-0.00017121, -0.00075592, -0.00015994]), array([0.00595553])]\n",
      "Iteration 302, Cost: 0.24874158910083155\n",
      "gradient_weights:  [array([[-0.000439  , -0.00161411, -0.00104934],\n",
      "       [-0.00055188, -0.00272453, -0.00017618]]), array([[-0.00627936],\n",
      "       [-0.00450869],\n",
      "       [ 0.00296469]])]\n",
      "gradients_biases:  [array([-0.00017377, -0.00075776, -0.00015974]), array([0.00595437])]\n",
      "Iteration 303, Cost: 0.24873577562029078\n",
      "gradient_weights:  [array([[-0.00044524, -0.00161781, -0.0010465 ],\n",
      "       [-0.0005598 , -0.00273086, -0.00017578]]), array([[-0.00627675],\n",
      "       [-0.00451009],\n",
      "       [ 0.00296534]])]\n",
      "gradients_biases:  [array([-0.00017634, -0.0007596 , -0.00015954]), array([0.00595322])]\n",
      "Iteration 304, Cost: 0.24872996074940495\n",
      "gradient_weights:  [array([[-0.00045148, -0.00162151, -0.00104366],\n",
      "       [-0.00056772, -0.00273719, -0.00017537]]), array([[-0.00627415],\n",
      "       [-0.0045115 ],\n",
      "       [ 0.00296599]])]\n",
      "gradients_biases:  [array([-0.0001789 , -0.00076144, -0.00015935]), array([0.00595208])]\n",
      "Iteration 305, Cost: 0.24872414445117208\n",
      "gradient_weights:  [array([[-0.00045771, -0.00162522, -0.00104081],\n",
      "       [-0.00057564, -0.00274352, -0.00017497]]), array([[-0.00627156],\n",
      "       [-0.00451291],\n",
      "       [ 0.00296665]])]\n",
      "gradients_biases:  [array([-0.00018146, -0.00076328, -0.00015915]), array([0.00595096])]\n",
      "Iteration 306, Cost: 0.24871832668861005\n",
      "gradient_weights:  [array([[-0.00046394, -0.00162892, -0.00103797],\n",
      "       [-0.00058355, -0.00274986, -0.00017456]]), array([[-0.00626899],\n",
      "       [-0.00451433],\n",
      "       [ 0.00296732]])]\n",
      "gradients_biases:  [array([-0.00018402, -0.00076512, -0.00015894]), array([0.00594984])]\n",
      "Iteration 307, Cost: 0.24871250742475642\n",
      "gradient_weights:  [array([[-0.00047016, -0.00163263, -0.00103512],\n",
      "       [-0.00059146, -0.0027562 , -0.00017415]]), array([[-0.00626642],\n",
      "       [-0.00451576],\n",
      "       [ 0.002968  ]])]\n",
      "gradients_biases:  [array([-0.00018659, -0.00076696, -0.00015874]), array([0.00594873])]\n",
      "Iteration 308, Cost: 0.24870668662266787\n",
      "gradient_weights:  [array([[-0.00047639, -0.00163634, -0.00103228],\n",
      "       [-0.00059937, -0.00276255, -0.00017374]]), array([[-0.00626386],\n",
      "       [-0.00451719],\n",
      "       [ 0.00296869]])]\n",
      "gradients_biases:  [array([-0.00018915, -0.0007688 , -0.00015853]), array([0.00594764])]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 309, Cost: 0.24870086424542037\n",
      "gradient_weights:  [array([[-0.0004826 , -0.00164006, -0.00102944],\n",
      "       [-0.00060727, -0.0027689 , -0.00017333]]), array([[-0.00626131],\n",
      "       [-0.00451862],\n",
      "       [ 0.00296939]])]\n",
      "gradients_biases:  [array([-0.00019171, -0.00077064, -0.00015833]), array([0.00594655])]\n",
      "Iteration 310, Cost: 0.24869504025610845\n",
      "gradient_weights:  [array([[-0.00048882, -0.00164377, -0.00102659],\n",
      "       [-0.00061517, -0.00277525, -0.00017292]]), array([[-0.00625878],\n",
      "       [-0.00452006],\n",
      "       [ 0.0029701 ]])]\n",
      "gradients_biases:  [array([-0.00019428, -0.00077248, -0.00015812]), array([0.00594547])]\n",
      "Iteration 311, Cost: 0.24868921461784524\n",
      "gradient_weights:  [array([[-0.00049502, -0.00164749, -0.00102375],\n",
      "       [-0.00062307, -0.0027816 , -0.00017251]]), array([[-0.00625625],\n",
      "       [-0.00452151],\n",
      "       [ 0.00297082]])]\n",
      "gradients_biases:  [array([-0.00019684, -0.00077432, -0.00015791]), array([0.0059444])]\n",
      "Iteration 312, Cost: 0.24868338729376188\n",
      "gradient_weights:  [array([[-0.00050123, -0.00165121, -0.0010209 ],\n",
      "       [-0.00063097, -0.00278796, -0.0001721 ]]), array([[-0.00625373],\n",
      "       [-0.00452296],\n",
      "       [ 0.00297155]])]\n",
      "gradients_biases:  [array([-0.00019941, -0.00077615, -0.0001577 ]), array([0.00594334])]\n",
      "Iteration 313, Cost: 0.24867755824700746\n",
      "gradient_weights:  [array([[-0.00050743, -0.00165493, -0.00101806],\n",
      "       [-0.00063886, -0.00279432, -0.00017168]]), array([[-0.00625123],\n",
      "       [-0.00452442],\n",
      "       [ 0.00297228]])]\n",
      "gradients_biases:  [array([-0.00020197, -0.00077799, -0.00015748]), array([0.0059423])]\n",
      "Iteration 314, Cost: 0.24867172744074878\n",
      "gradient_weights:  [array([[-0.00051363, -0.00165866, -0.00101521],\n",
      "       [-0.00064675, -0.00280068, -0.00017127]]), array([[-0.00624873],\n",
      "       [-0.00452588],\n",
      "       [ 0.00297303]])]\n",
      "gradients_biases:  [array([-0.00020454, -0.00077983, -0.00015727]), array([0.00594126])]\n",
      "Iteration 315, Cost: 0.2486658948381698\n",
      "gradient_weights:  [array([[-0.00051982, -0.00166239, -0.00101237],\n",
      "       [-0.00065463, -0.00280705, -0.00017085]]), array([[-0.00624624],\n",
      "       [-0.00452735],\n",
      "       [ 0.00297379]])]\n",
      "gradients_biases:  [array([-0.00020711, -0.00078167, -0.00015705]), array([0.00594023])]\n",
      "Iteration 316, Cost: 0.24866006040247174\n",
      "gradient_weights:  [array([[-0.00052601, -0.00166612, -0.00100952],\n",
      "       [-0.00066252, -0.00281342, -0.00017044]]), array([[-0.00624377],\n",
      "       [-0.00452883],\n",
      "       [ 0.00297456]])]\n",
      "gradients_biases:  [array([-0.00020967, -0.00078351, -0.00015683]), array([0.00593921])]\n",
      "Iteration 317, Cost: 0.24865422409687232\n",
      "gradient_weights:  [array([[-0.0005322 , -0.00166985, -0.00100667],\n",
      "       [-0.0006704 , -0.00281979, -0.00017002]]), array([[-0.0062413 ],\n",
      "       [-0.00453031],\n",
      "       [ 0.00297533]])]\n",
      "gradients_biases:  [array([-0.00021224, -0.00078534, -0.00015661]), array([0.0059382])]\n",
      "Iteration 318, Cost: 0.24864838588460608\n",
      "gradient_weights:  [array([[-0.00053838, -0.00167359, -0.00100383],\n",
      "       [-0.00067827, -0.00282617, -0.0001696 ]]), array([[-0.00623884],\n",
      "       [-0.0045318 ],\n",
      "       [ 0.00297612]])]\n",
      "gradients_biases:  [array([-0.00021481, -0.00078718, -0.00015639]), array([0.0059372])]\n",
      "Iteration 319, Cost: 0.24864254572892341\n",
      "gradient_weights:  [array([[-0.00054455, -0.00167733, -0.00100098],\n",
      "       [-0.00068615, -0.00283255, -0.00016919]]), array([[-0.0062364 ],\n",
      "       [-0.00453329],\n",
      "       [ 0.00297691]])]\n",
      "gradients_biases:  [array([-0.00021737, -0.00078902, -0.00015616]), array([0.00593621])]\n",
      "Iteration 320, Cost: 0.24863670359309095\n",
      "gradient_weights:  [array([[-0.00055073, -0.00168107, -0.00099814],\n",
      "       [-0.00069402, -0.00283893, -0.00016877]]), array([[-0.00623396],\n",
      "       [-0.00453479],\n",
      "       [ 0.00297772]])]\n",
      "gradients_biases:  [array([-0.00021994, -0.00079085, -0.00015594]), array([0.00593523])]\n",
      "Iteration 321, Cost: 0.24863085944039082\n",
      "gradient_weights:  [array([[-0.0005569 , -0.00168481, -0.00099529],\n",
      "       [-0.00070189, -0.00284532, -0.00016835]]), array([[-0.00623154],\n",
      "       [-0.00453629],\n",
      "       [ 0.00297853]])]\n",
      "gradients_biases:  [array([-0.00022251, -0.00079269, -0.00015571]), array([0.00593426])]\n",
      "Iteration 322, Cost: 0.2486250132341206\n",
      "gradient_weights:  [array([[-0.00056307, -0.00168856, -0.00099244],\n",
      "       [-0.00070975, -0.00285171, -0.00016793]]), array([[-0.00622912],\n",
      "       [-0.0045378 ],\n",
      "       [ 0.00297936]])]\n",
      "gradients_biases:  [array([-0.00022508, -0.00079453, -0.00015548]), array([0.0059333])]\n",
      "Iteration 323, Cost: 0.24861916493759306\n",
      "gradient_weights:  [array([[-0.00056923, -0.00169231, -0.00098959],\n",
      "       [-0.00071762, -0.0028581 , -0.00016751]]), array([[-0.00622671],\n",
      "       [-0.00453932],\n",
      "       [ 0.00298019]])]\n",
      "gradients_biases:  [array([-0.00022765, -0.00079636, -0.00015525]), array([0.00593235])]\n",
      "Iteration 324, Cost: 0.24861331451413554\n",
      "gradient_weights:  [array([[-0.00057539, -0.00169606, -0.00098675],\n",
      "       [-0.00072548, -0.00286449, -0.00016708]]), array([[-0.00622432],\n",
      "       [-0.00454084],\n",
      "       [ 0.00298104]])]\n",
      "gradients_biases:  [array([-0.00023022, -0.0007982 , -0.00015502]), array([0.00593141])]\n",
      "Iteration 325, Cost: 0.24860746192709027\n",
      "gradient_weights:  [array([[-0.00058154, -0.00169982, -0.0009839 ],\n",
      "       [-0.00073333, -0.00287089, -0.00016666]]), array([[-0.00622193],\n",
      "       [-0.00454237],\n",
      "       [ 0.00298189]])]\n",
      "gradients_biases:  [array([-0.00023278, -0.00080003, -0.00015479]), array([0.00593048])]\n",
      "Iteration 326, Cost: 0.24860160713981358\n",
      "gradient_weights:  [array([[-0.00058769, -0.00170357, -0.00098105],\n",
      "       [-0.00074119, -0.0028773 , -0.00016624]]), array([[-0.00621956],\n",
      "       [-0.0045439 ],\n",
      "       [ 0.00298275]])]\n",
      "gradients_biases:  [array([-0.00023535, -0.00080187, -0.00015455]), array([0.00592956])]\n",
      "Iteration 327, Cost: 0.24859575011567592\n",
      "gradient_weights:  [array([[-0.00059384, -0.00170733, -0.0009782 ],\n",
      "       [-0.00074904, -0.0028837 , -0.00016581]]), array([[-0.00621719],\n",
      "       [-0.00454544],\n",
      "       [ 0.00298363]])]\n",
      "gradients_biases:  [array([-0.00023792, -0.0008037 , -0.00015431]), array([0.00592865])]\n",
      "Iteration 328, Cost: 0.24858989081806152\n",
      "gradient_weights:  [array([[-0.00059999, -0.0017111 , -0.00097535],\n",
      "       [-0.00075689, -0.00289011, -0.00016539]]), array([[-0.00621483],\n",
      "       [-0.00454698],\n",
      "       [ 0.00298451]])]\n",
      "gradients_biases:  [array([-0.00024049, -0.00080554, -0.00015407]), array([0.00592775])]\n",
      "Iteration 329, Cost: 0.24858402921036793\n",
      "gradient_weights:  [array([[-0.00060613, -0.00171486, -0.0009725 ],\n",
      "       [-0.00076473, -0.00289652, -0.00016496]]), array([[-0.00621249],\n",
      "       [-0.00454853],\n",
      "       [ 0.0029854 ]])]\n",
      "gradients_biases:  [array([-0.00024306, -0.00080737, -0.00015383]), array([0.00592685])]\n",
      "Iteration 330, Cost: 0.24857816525600618\n",
      "gradient_weights:  [array([[-0.00061226, -0.00171863, -0.00096965],\n",
      "       [-0.00077258, -0.00290294, -0.00016453]]), array([[-0.00621015],\n",
      "       [-0.00455009],\n",
      "       [ 0.0029863 ]])]\n",
      "gradients_biases:  [array([-0.00024563, -0.00080921, -0.00015359]), array([0.00592597])]\n",
      "Iteration 331, Cost: 0.24857229891839994\n",
      "gradient_weights:  [array([[-0.0006184 , -0.0017224 , -0.00096681],\n",
      "       [-0.00078042, -0.00290936, -0.00016411]]), array([[-0.00620783],\n",
      "       [-0.00455165],\n",
      "       [ 0.00298721]])]\n",
      "gradients_biases:  [array([-0.0002482 , -0.00081104, -0.00015335]), array([0.0059251])]\n",
      "Iteration 332, Cost: 0.248566430160986\n",
      "gradient_weights:  [array([[-0.00062453, -0.00172617, -0.00096396],\n",
      "       [-0.00078826, -0.00291578, -0.00016368]]), array([[-0.00620551],\n",
      "       [-0.00455322],\n",
      "       [ 0.00298813]])]\n",
      "gradients_biases:  [array([-0.00025077, -0.00081288, -0.0001531 ]), array([0.00592424])]\n",
      "Iteration 333, Cost: 0.24856055894721318\n",
      "gradient_weights:  [array([[-0.00063065, -0.00172995, -0.00096111],\n",
      "       [-0.00079609, -0.00292221, -0.00016325]]), array([[-0.0062032 ],\n",
      "       [-0.00455479],\n",
      "       [ 0.00298906]])]\n",
      "gradients_biases:  [array([-0.00025334, -0.00081471, -0.00015286]), array([0.00592338])]\n",
      "Iteration 334, Cost: 0.24855468524054258\n",
      "gradient_weights:  [array([[-0.00063677, -0.00173373, -0.00095825],\n",
      "       [-0.00080392, -0.00292863, -0.00016282]]), array([[-0.00620091],\n",
      "       [-0.00455637],\n",
      "       [ 0.00299   ]])]\n",
      "gradients_biases:  [array([-0.00025591, -0.00081654, -0.00015261]), array([0.00592254])]\n",
      "Iteration 335, Cost: 0.2485488090044473\n",
      "gradient_weights:  [array([[-0.00064289, -0.00173751, -0.0009554 ],\n",
      "       [-0.00081175, -0.00293507, -0.00016239]]), array([[-0.00619862],\n",
      "       [-0.00455795],\n",
      "       [ 0.00299095]])]\n",
      "gradients_biases:  [array([-0.00025848, -0.00081837, -0.00015236]), array([0.00592171])]\n",
      "Iteration 336, Cost: 0.24854293020241205\n",
      "gradient_weights:  [array([[-0.00064901, -0.00174129, -0.00095255],\n",
      "       [-0.00081958, -0.0029415 , -0.00016196]]), array([[-0.00619635],\n",
      "       [-0.00455954],\n",
      "       [ 0.00299191]])]\n",
      "gradients_biases:  [array([-0.00026105, -0.0008202 , -0.00015211]), array([0.00592088])]\n",
      "Iteration 337, Cost: 0.24853704879793284\n",
      "gradient_weights:  [array([[-0.00065512, -0.00174508, -0.0009497 ],\n",
      "       [-0.00082741, -0.00294794, -0.00016153]]), array([[-0.00619408],\n",
      "       [-0.00456114],\n",
      "       [ 0.00299288]])]\n",
      "gradients_biases:  [array([-0.00026363, -0.00082204, -0.00015185]), array([0.00592007])]\n",
      "Iteration 338, Cost: 0.24853116475451686\n",
      "gradient_weights:  [array([[-0.00066123, -0.00174887, -0.00094685],\n",
      "       [-0.00083523, -0.00295438, -0.00016109]]), array([[-0.00619182],\n",
      "       [-0.00456274],\n",
      "       [ 0.00299386]])]\n",
      "gradients_biases:  [array([-0.0002662 , -0.00082387, -0.0001516 ]), array([0.00591926])]\n",
      "Iteration 339, Cost: 0.24852527803568208\n",
      "gradient_weights:  [array([[-0.00066734, -0.00175266, -0.000944  ],\n",
      "       [-0.00084305, -0.00296083, -0.00016066]]), array([[-0.00618958],\n",
      "       [-0.00456435],\n",
      "       [ 0.00299484]])]\n",
      "gradients_biases:  [array([-0.00026877, -0.0008257 , -0.00015134]), array([0.00591847])]\n",
      "Iteration 340, Cost: 0.24851938860495726\n",
      "gradient_weights:  [array([[-0.00067344, -0.00175646, -0.00094114],\n",
      "       [-0.00085086, -0.00296728, -0.00016022]]), array([[-0.00618734],\n",
      "       [-0.00456597],\n",
      "       [ 0.00299584]])]\n",
      "gradients_biases:  [array([-0.00027134, -0.00082753, -0.00015108]), array([0.00591768])]\n",
      "Iteration 341, Cost: 0.24851349642588116\n",
      "gradient_weights:  [array([[-0.00067954, -0.00176025, -0.00093829],\n",
      "       [-0.00085868, -0.00297373, -0.00015979]]), array([[-0.00618511],\n",
      "       [-0.00456759],\n",
      "       [ 0.00299685]])]\n",
      "gradients_biases:  [array([-0.00027391, -0.00082936, -0.00015082]), array([0.00591691])]\n",
      "Iteration 342, Cost: 0.24850760146200299\n",
      "gradient_weights:  [array([[-0.00068563, -0.00176406, -0.00093544],\n",
      "       [-0.00086649, -0.00298019, -0.00015935]]), array([[-0.00618289],\n",
      "       [-0.00456921],\n",
      "       [ 0.00299786]])]\n",
      "gradients_biases:  [array([-0.00027648, -0.00083119, -0.00015056]), array([0.00591614])]\n",
      "Iteration 343, Cost: 0.2485017036768816\n",
      "gradient_weights:  [array([[-0.00069172, -0.00176786, -0.00093258],\n",
      "       [-0.0008743 , -0.00298664, -0.00015892]]), array([[-0.00618069],\n",
      "       [-0.00457084],\n",
      "       [ 0.00299889]])]\n",
      "gradients_biases:  [array([-0.00027905, -0.00083302, -0.0001503 ]), array([0.00591538])]\n",
      "Iteration 344, Cost: 0.24849580303408547\n",
      "gradient_weights:  [array([[-0.00069781, -0.00177167, -0.00092973],\n",
      "       [-0.00088211, -0.00299311, -0.00015848]]), array([[-0.00617849],\n",
      "       [-0.00457248],\n",
      "       [ 0.00299993]])]\n",
      "gradients_biases:  [array([-0.00028162, -0.00083485, -0.00015004]), array([0.00591464])]\n",
      "Iteration 345, Cost: 0.2484898994971923\n",
      "gradient_weights:  [array([[-0.0007039 , -0.00177547, -0.00092688],\n",
      "       [-0.00088991, -0.00299957, -0.00015804]]), array([[-0.0061763 ],\n",
      "       [-0.00457412],\n",
      "       [ 0.00300097]])]\n",
      "gradients_biases:  [array([-0.00028419, -0.00083667, -0.00014977]), array([0.0059139])]\n",
      "Iteration 346, Cost: 0.24848399302978894\n",
      "gradient_weights:  [array([[-0.00070998, -0.00177929, -0.00092402],\n",
      "       [-0.00089771, -0.00300604, -0.0001576 ]]), array([[-0.00617412],\n",
      "       [-0.00457577],\n",
      "       [ 0.00300202]])]\n",
      "gradients_biases:  [array([-0.00028676, -0.0008385 , -0.0001495 ]), array([0.00591317])]\n",
      "Iteration 347, Cost: 0.2484780835954711\n",
      "gradient_weights:  [array([[-0.00071606, -0.0017831 , -0.00092117],\n",
      "       [-0.00090551, -0.00301251, -0.00015716]]), array([[-0.00617195],\n",
      "       [-0.00457743],\n",
      "       [ 0.00300309]])]\n",
      "gradients_biases:  [array([-0.00028933, -0.00084033, -0.00014923]), array([0.00591245])]\n",
      "Iteration 348, Cost: 0.248472171157843\n",
      "gradient_weights:  [array([[-0.00072213, -0.00178692, -0.00091831],\n",
      "       [-0.00091331, -0.00301899, -0.00015672]]), array([[-0.0061698 ],\n",
      "       [-0.00457909],\n",
      "       [ 0.00300416]])]\n",
      "gradients_biases:  [array([-0.00029191, -0.00084216, -0.00014896]), array([0.00591174])]\n",
      "Iteration 349, Cost: 0.24846625568051717\n",
      "gradient_weights:  [array([[-0.0007282 , -0.00179074, -0.00091546],\n",
      "       [-0.00092111, -0.00302547, -0.00015628]]), array([[-0.00616765],\n",
      "       [-0.00458075],\n",
      "       [ 0.00300525]])]\n",
      "gradients_biases:  [array([-0.00029448, -0.00084398, -0.00014869]), array([0.00591104])]\n",
      "Iteration 350, Cost: 0.24846033712711418\n",
      "gradient_weights:  [array([[-0.00073427, -0.00179456, -0.0009126 ],\n",
      "       [-0.0009289 , -0.00303195, -0.00015584]]), array([[-0.00616551],\n",
      "       [-0.00458243],\n",
      "       [ 0.00300634]])]\n",
      "gradients_biases:  [array([-0.00029705, -0.00084581, -0.00014842]), array([0.00591035])]\n",
      "Iteration 351, Cost: 0.2484544154612624\n",
      "gradient_weights:  [array([[-0.00074034, -0.00179839, -0.00090974],\n",
      "       [-0.00093669, -0.00303844, -0.00015539]]), array([[-0.00616338],\n",
      "       [-0.00458411],\n",
      "       [ 0.00300744]])]\n",
      "gradients_biases:  [array([-0.00029962, -0.00084763, -0.00014814]), array([0.00590967])]\n",
      "Iteration 352, Cost: 0.24844849064659785\n",
      "gradient_weights:  [array([[-0.0007464 , -0.00180222, -0.00090689],\n",
      "       [-0.00094448, -0.00304493, -0.00015495]]), array([[-0.00616126],\n",
      "       [-0.00458579],\n",
      "       [ 0.00300855]])]\n",
      "gradients_biases:  [array([-0.00030219, -0.00084946, -0.00014786]), array([0.005909])]\n",
      "Iteration 353, Cost: 0.2484425626467638\n",
      "gradient_weights:  [array([[-0.00075246, -0.00180605, -0.00090403],\n",
      "       [-0.00095226, -0.00305142, -0.00015451]]), array([[-0.00615915],\n",
      "       [-0.00458748],\n",
      "       [ 0.00300968]])]\n",
      "gradients_biases:  [array([-0.00030476, -0.00085128, -0.00014759]), array([0.00590834])]\n",
      "Iteration 354, Cost: 0.24843663142541073\n",
      "gradient_weights:  [array([[-0.00075852, -0.00180989, -0.00090117],\n",
      "       [-0.00096004, -0.00305792, -0.00015406]]), array([[-0.00615705],\n",
      "       [-0.00458918],\n",
      "       [ 0.00301081]])]\n",
      "gradients_biases:  [array([-0.00030733, -0.00085311, -0.00014731]), array([0.00590769])]\n",
      "Iteration 355, Cost: 0.24843069694619563\n",
      "gradient_weights:  [array([[-0.00076457, -0.00181372, -0.00089831],\n",
      "       [-0.00096783, -0.00306442, -0.00015362]]), array([[-0.00615496],\n",
      "       [-0.00459088],\n",
      "       [ 0.00301195]])]\n",
      "gradients_biases:  [array([-0.0003099 , -0.00085493, -0.00014702]), array([0.00590705])]\n",
      "Iteration 356, Cost: 0.2484247591727824\n",
      "gradient_weights:  [array([[-0.00077062, -0.00181756, -0.00089545],\n",
      "       [-0.0009756 , -0.00307092, -0.00015317]]), array([[-0.00615288],\n",
      "       [-0.00459259],\n",
      "       [ 0.0030131 ]])]\n",
      "gradients_biases:  [array([-0.00031247, -0.00085675, -0.00014674]), array([0.00590642])]\n",
      "Iteration 357, Cost: 0.24841881806884117\n",
      "gradient_weights:  [array([[-0.00077667, -0.00182141, -0.00089259],\n",
      "       [-0.00098338, -0.00307743, -0.00015272]]), array([[-0.0061508 ],\n",
      "       [-0.0045943 ],\n",
      "       [ 0.00301426]])]\n",
      "gradients_biases:  [array([-0.00031504, -0.00085858, -0.00014646]), array([0.00590579])]\n",
      "Iteration 358, Cost: 0.24841287359804803\n",
      "gradient_weights:  [array([[-0.00078271, -0.00182526, -0.00088973],\n",
      "       [-0.00099115, -0.00308394, -0.00015227]]), array([[-0.00614874],\n",
      "       [-0.00459602],\n",
      "       [ 0.00301543]])]\n",
      "gradients_biases:  [array([-0.00031761, -0.0008604 , -0.00014617]), array([0.00590518])]\n",
      "Iteration 359, Cost: 0.24840692572408518\n",
      "gradient_weights:  [array([[-0.00078875, -0.0018291 , -0.00088687],\n",
      "       [-0.00099893, -0.00309045, -0.00015183]]), array([[-0.00614669],\n",
      "       [-0.00459775],\n",
      "       [ 0.00301661]])]\n",
      "gradients_biases:  [array([-0.00032018, -0.00086222, -0.00014588]), array([0.00590457])]\n",
      "Iteration 360, Cost: 0.24840097441064016\n",
      "gradient_weights:  [array([[-0.00079479, -0.00183296, -0.00088401],\n",
      "       [-0.0010067 , -0.00309697, -0.00015138]]), array([[-0.00614465],\n",
      "       [-0.00459948],\n",
      "       [ 0.0030178 ]])]\n",
      "gradients_biases:  [array([-0.00032275, -0.00086404, -0.00014559]), array([0.00590398])]\n",
      "Iteration 361, Cost: 0.2483950196214062\n",
      "gradient_weights:  [array([[-0.00080082, -0.00183681, -0.00088115],\n",
      "       [-0.00101446, -0.00310349, -0.00015093]]), array([[-0.00614262],\n",
      "       [-0.00460122],\n",
      "       [ 0.00301899]])]\n",
      "gradients_biases:  [array([-0.00032532, -0.00086586, -0.0001453 ]), array([0.00590339])]\n",
      "Iteration 362, Cost: 0.24838906132008132\n",
      "gradient_weights:  [array([[-0.00080685, -0.00184067, -0.00087829],\n",
      "       [-0.00102223, -0.00311001, -0.00015047]]), array([[-0.00614059],\n",
      "       [-0.00460296],\n",
      "       [ 0.0030202 ]])]\n",
      "gradients_biases:  [array([-0.00032789, -0.00086768, -0.00014501]), array([0.00590282])]\n",
      "Iteration 363, Cost: 0.24838309947036863\n",
      "gradient_weights:  [array([[-0.00081288, -0.00184453, -0.00087543],\n",
      "       [-0.00102999, -0.00311654, -0.00015002]]), array([[-0.00613858],\n",
      "       [-0.00460471],\n",
      "       [ 0.00302142]])]\n",
      "gradients_biases:  [array([-0.00033046, -0.0008695 , -0.00014471]), array([0.00590225])]\n",
      "Iteration 364, Cost: 0.24837713403597592\n",
      "gradient_weights:  [array([[-0.00081891, -0.0018484 , -0.00087257],\n",
      "       [-0.00103775, -0.00312307, -0.00014957]]), array([[-0.00613658],\n",
      "       [-0.00460647],\n",
      "       [ 0.00302265]])]\n",
      "gradients_biases:  [array([-0.00033302, -0.00087131, -0.00014442]), array([0.0059017])]\n",
      "Iteration 365, Cost: 0.24837116498061534\n",
      "gradient_weights:  [array([[-0.00082493, -0.00185227, -0.00086971],\n",
      "       [-0.00104551, -0.00312961, -0.00014912]]), array([[-0.00613458],\n",
      "       [-0.00460823],\n",
      "       [ 0.00302388]])]\n",
      "gradients_biases:  [array([-0.00033559, -0.00087313, -0.00014412]), array([0.00590115])]\n",
      "Iteration 366, Cost: 0.24836519226800316\n",
      "gradient_weights:  [array([[-0.00083095, -0.00185614, -0.00086684],\n",
      "       [-0.00105327, -0.00313614, -0.00014866]]), array([[-0.0061326 ],\n",
      "       [-0.00460999],\n",
      "       [ 0.00302513]])]\n",
      "gradients_biases:  [array([-0.00033816, -0.00087495, -0.00014382]), array([0.00590061])]\n",
      "Iteration 367, Cost: 0.24835921586185977\n",
      "gradient_weights:  [array([[-0.00083697, -0.00186001, -0.00086398],\n",
      "       [-0.00106102, -0.00314269, -0.00014821]]), array([[-0.00613062],\n",
      "       [-0.00461177],\n",
      "       [ 0.00302638]])]\n",
      "gradients_biases:  [array([-0.00034073, -0.00087676, -0.00014352]), array([0.00590008])]\n",
      "Iteration 368, Cost: 0.24835323572590912\n",
      "gradient_weights:  [array([[-0.00084298, -0.00186389, -0.00086111],\n",
      "       [-0.00106877, -0.00314923, -0.00014776]]), array([[-0.00612866],\n",
      "       [-0.00461355],\n",
      "       [ 0.00302765]])]\n",
      "gradients_biases:  [array([-0.0003433 , -0.00087858, -0.00014322]), array([0.00589956])]\n",
      "Iteration 369, Cost: 0.24834725182387873\n",
      "gradient_weights:  [array([[-0.00084899, -0.00186777, -0.00085825],\n",
      "       [-0.00107652, -0.00315578, -0.0001473 ]]), array([[-0.0061267 ],\n",
      "       [-0.00461533],\n",
      "       [ 0.00302892]])]\n",
      "gradients_biases:  [array([-0.00034586, -0.00088039, -0.00014292]), array([0.00589905])]\n",
      "Iteration 370, Cost: 0.2483412641194993\n",
      "gradient_weights:  [array([[-0.000855  , -0.00187165, -0.00085538],\n",
      "       [-0.00108427, -0.00316233, -0.00014684]]), array([[-0.00612475],\n",
      "       [-0.00461712],\n",
      "       [ 0.00303021]])]\n",
      "gradients_biases:  [array([-0.00034843, -0.00088221, -0.00014261]), array([0.00589855])]\n",
      "Iteration 371, Cost: 0.24833527257650467\n",
      "gradient_weights:  [array([[-0.00086101, -0.00187553, -0.00085252],\n",
      "       [-0.00109202, -0.00316889, -0.00014639]]), array([[-0.00612282],\n",
      "       [-0.00461892],\n",
      "       [ 0.0030315 ]])]\n",
      "gradients_biases:  [array([-0.000351  , -0.00088402, -0.00014231]), array([0.00589806])]\n",
      "Iteration 372, Cost: 0.2483292771586313\n",
      "gradient_weights:  [array([[-0.00086701, -0.00187942, -0.00084965],\n",
      "       [-0.00109976, -0.00317545, -0.00014593]]), array([[-0.00612089],\n",
      "       [-0.00462072],\n",
      "       [ 0.00303281]])]\n",
      "gradients_biases:  [array([-0.00035356, -0.00088584, -0.000142  ]), array([0.00589758])]\n",
      "Iteration 373, Cost: 0.24832327782961844\n",
      "gradient_weights:  [array([[-0.00087301, -0.00188331, -0.00084679],\n",
      "       [-0.0011075 , -0.00318201, -0.00014547]]), array([[-0.00611897],\n",
      "       [-0.00462253],\n",
      "       [ 0.00303412]])]\n",
      "gradients_biases:  [array([-0.00035613, -0.00088765, -0.00014169]), array([0.00589711])]\n",
      "Iteration 374, Cost: 0.24831727455320732\n",
      "gradient_weights:  [array([[-0.00087901, -0.00188721, -0.00084392],\n",
      "       [-0.00111524, -0.00318858, -0.00014501]]), array([[-0.00611706],\n",
      "       [-0.00462435],\n",
      "       [ 0.00303544]])]\n",
      "gradients_biases:  [array([-0.00035869, -0.00088946, -0.00014138]), array([0.00589665])]\n",
      "Iteration 375, Cost: 0.24831126729314174\n",
      "gradient_weights:  [array([[-0.000885  , -0.00189111, -0.00084105],\n",
      "       [-0.00112298, -0.00319515, -0.00014455]]), array([[-0.00611516],\n",
      "       [-0.00462617],\n",
      "       [ 0.00303677]])]\n",
      "gradients_biases:  [array([-0.00036126, -0.00089127, -0.00014107]), array([0.00589619])]\n",
      "Iteration 376, Cost: 0.2483052560131669\n",
      "gradient_weights:  [array([[-0.00089099, -0.00189501, -0.00083818],\n",
      "       [-0.00113072, -0.00320172, -0.00014409]]), array([[-0.00611327],\n",
      "       [-0.004628  ],\n",
      "       [ 0.00303811]])]\n",
      "gradients_biases:  [array([-0.00036383, -0.00089308, -0.00014075]), array([0.00589575])]\n",
      "Iteration 377, Cost: 0.24829924067702996\n",
      "gradient_weights:  [array([[-0.00089698, -0.00189891, -0.00083531],\n",
      "       [-0.00113845, -0.0032083 , -0.00014363]]), array([[-0.00611139],\n",
      "       [-0.00462983],\n",
      "       [ 0.00303947]])]\n",
      "gradients_biases:  [array([-0.00036639, -0.00089489, -0.00014044]), array([0.00589532])]\n",
      "Iteration 378, Cost: 0.2482932212484794\n",
      "gradient_weights:  [array([[-0.00090297, -0.00190282, -0.00083244],\n",
      "       [-0.00114619, -0.00321488, -0.00014317]]), array([[-0.00610952],\n",
      "       [-0.00463167],\n",
      "       [ 0.00304083]])]\n",
      "gradients_biases:  [array([-0.00036895, -0.0008967 , -0.00014012]), array([0.00589489])]\n",
      "Iteration 379, Cost: 0.24828719769126498\n",
      "gradient_weights:  [array([[-0.00090895, -0.00190673, -0.00082958],\n",
      "       [-0.00115392, -0.00322146, -0.00014271]]), array([[-0.00610766],\n",
      "       [-0.00463352],\n",
      "       [ 0.0030422 ]])]\n",
      "gradients_biases:  [array([-0.00037152, -0.0008985 , -0.0001398 ]), array([0.00589447])]\n",
      "Iteration 380, Cost: 0.2482811699691373\n",
      "gradient_weights:  [array([[-0.00091493, -0.00191064, -0.0008267 ],\n",
      "       [-0.00116164, -0.00322805, -0.00014224]]), array([[-0.00610581],\n",
      "       [-0.00463537],\n",
      "       [ 0.00304358]])]\n",
      "gradients_biases:  [array([-0.00037408, -0.00090031, -0.00013948]), array([0.00589407])]\n",
      "Iteration 381, Cost: 0.2482751380458477\n",
      "gradient_weights:  [array([[-0.00092091, -0.00191456, -0.00082383],\n",
      "       [-0.00116937, -0.00323464, -0.00014178]]), array([[-0.00610397],\n",
      "       [-0.00463723],\n",
      "       [ 0.00304497]])]\n",
      "gradients_biases:  [array([-0.00037664, -0.00090212, -0.00013916]), array([0.00589367])]\n",
      "Iteration 382, Cost: 0.24826910188514825\n",
      "gradient_weights:  [array([[-0.00092689, -0.00191848, -0.00082096],\n",
      "       [-0.0011771 , -0.00324123, -0.00014131]]), array([[-0.00610213],\n",
      "       [-0.00463909],\n",
      "       [ 0.00304637]])]\n",
      "gradients_biases:  [array([-0.00037921, -0.00090392, -0.00013884]), array([0.00589328])]\n",
      "Iteration 383, Cost: 0.24826306145079113\n",
      "gradient_weights:  [array([[-0.00093286, -0.0019224 , -0.00081809],\n",
      "       [-0.00118482, -0.00324783, -0.00014085]]), array([[-0.00610031],\n",
      "       [-0.00464096],\n",
      "       [ 0.00304777]])]\n",
      "gradients_biases:  [array([-0.00038177, -0.00090573, -0.00013851]), array([0.00589291])]\n",
      "Iteration 384, Cost: 0.24825701670652872\n",
      "gradient_weights:  [array([[-0.00093883, -0.00192633, -0.00081522],\n",
      "       [-0.00119254, -0.00325443, -0.00014038]]), array([[-0.0060985 ],\n",
      "       [-0.00464283],\n",
      "       [ 0.00304919]])]\n",
      "gradients_biases:  [array([-0.00038433, -0.00090753, -0.00013819]), array([0.00589254])]\n",
      "Iteration 385, Cost: 0.24825096761611326\n",
      "gradient_weights:  [array([[-0.0009448 , -0.00193026, -0.00081235],\n",
      "       [-0.00120026, -0.00326104, -0.00013992]]), array([[-0.00609669],\n",
      "       [-0.00464472],\n",
      "       [ 0.00305062]])]\n",
      "gradients_biases:  [array([-0.00038689, -0.00090933, -0.00013786]), array([0.00589218])]\n",
      "Iteration 386, Cost: 0.24824491414329664\n",
      "gradient_weights:  [array([[-0.00095076, -0.00193419, -0.00080947],\n",
      "       [-0.00120798, -0.00326765, -0.00013945]]), array([[-0.0060949 ],\n",
      "       [-0.0046466 ],\n",
      "       [ 0.00305206]])]\n",
      "gradients_biases:  [array([-0.00038945, -0.00091114, -0.00013753]), array([0.00589183])]\n",
      "Iteration 387, Cost: 0.24823885625182998\n",
      "gradient_weights:  [array([[-0.00095673, -0.00193813, -0.0008066 ],\n",
      "       [-0.00121569, -0.00327426, -0.00013898]]), array([[-0.00609311],\n",
      "       [-0.0046485 ],\n",
      "       [ 0.0030535 ]])]\n",
      "gradients_biases:  [array([-0.00039201, -0.00091294, -0.0001372 ]), array([0.00589148])]\n",
      "Iteration 388, Cost: 0.24823279390546396\n",
      "gradient_weights:  [array([[-0.00096269, -0.00194207, -0.00080372],\n",
      "       [-0.0012234 , -0.00328088, -0.00013851]]), array([[-0.00609133],\n",
      "       [-0.0046504 ],\n",
      "       [ 0.00305496]])]\n",
      "gradients_biases:  [array([-0.00039457, -0.00091474, -0.00013687]), array([0.00589115])]\n",
      "Iteration 389, Cost: 0.2482267270679481\n",
      "gradient_weights:  [array([[-0.00096864, -0.00194601, -0.00080085],\n",
      "       [-0.00123112, -0.0032875 , -0.00013805]]), array([[-0.00608957],\n",
      "       [-0.0046523 ],\n",
      "       [ 0.00305643]])]\n",
      "gradients_biases:  [array([-0.00039713, -0.00091654, -0.00013653]), array([0.00589083])]\n",
      "Iteration 390, Cost: 0.24822065570303065\n",
      "gradient_weights:  [array([[-0.0009746 , -0.00194995, -0.00079797],\n",
      "       [-0.00123883, -0.00329412, -0.00013758]]), array([[-0.00608781],\n",
      "       [-0.00465421],\n",
      "       [ 0.0030579 ]])]\n",
      "gradients_biases:  [array([-0.00039969, -0.00091833, -0.0001362 ]), array([0.00589052])]\n",
      "Iteration 391, Cost: 0.24821457977445843\n",
      "gradient_weights:  [array([[-0.00098055, -0.0019539 , -0.0007951 ],\n",
      "       [-0.00124654, -0.00330075, -0.00013711]]), array([[-0.00608606],\n",
      "       [-0.00465613],\n",
      "       [ 0.00305939]])]\n",
      "gradients_biases:  [array([-0.00040225, -0.00092013, -0.00013586]), array([0.00589021])]\n",
      "Iteration 392, Cost: 0.24820849924597668\n",
      "gradient_weights:  [array([[-0.0009865 , -0.00195785, -0.00079222],\n",
      "       [-0.00125424, -0.00330738, -0.00013663]]), array([[-0.00608432],\n",
      "       [-0.00465806],\n",
      "       [ 0.00306088]])]\n",
      "gradients_biases:  [array([-0.00040481, -0.00092193, -0.00013552]), array([0.00588992])]\n",
      "Iteration 393, Cost: 0.24820241408132898\n",
      "gradient_weights:  [array([[-0.00099245, -0.00196181, -0.00078934],\n",
      "       [-0.00126195, -0.00331401, -0.00013616]]), array([[-0.00608259],\n",
      "       [-0.00465999],\n",
      "       [ 0.00306239]])]\n",
      "gradients_biases:  [array([-0.00040736, -0.00092372, -0.00013519]), array([0.00588963])]\n",
      "Iteration 394, Cost: 0.24819632424425633\n",
      "gradient_weights:  [array([[-0.0009984 , -0.00196577, -0.00078646],\n",
      "       [-0.00126965, -0.00332065, -0.00013569]]), array([[-0.00608087],\n",
      "       [-0.00466192],\n",
      "       [ 0.0030639 ]])]\n",
      "gradients_biases:  [array([-0.00040992, -0.00092552, -0.00013484]), array([0.00588936])]\n",
      "Iteration 395, Cost: 0.24819022969849797\n",
      "gradient_weights:  [array([[-0.00100434, -0.00196973, -0.00078358],\n",
      "       [-0.00127735, -0.00332729, -0.00013522]]), array([[-0.00607916],\n",
      "       [-0.00466387],\n",
      "       [ 0.00306542]])]\n",
      "gradients_biases:  [array([-0.00041248, -0.00092731, -0.0001345 ]), array([0.00588909])]\n",
      "Iteration 396, Cost: 0.24818413040779042\n",
      "gradient_weights:  [array([[-0.00101028, -0.00197369, -0.0007807 ],\n",
      "       [-0.00128505, -0.00333394, -0.00013475]]), array([[-0.00607746],\n",
      "       [-0.00466581],\n",
      "       [ 0.00306696]])]\n",
      "gradients_biases:  [array([-0.00041503, -0.00092911, -0.00013416]), array([0.00588883])]\n",
      "Iteration 397, Cost: 0.24817802633586744\n",
      "gradient_weights:  [array([[-0.00101622, -0.00197766, -0.00077782],\n",
      "       [-0.00129275, -0.00334059, -0.00013427]]), array([[-0.00607576],\n",
      "       [-0.00466777],\n",
      "       [ 0.0030685 ]])]\n",
      "gradients_biases:  [array([-0.00041759, -0.0009309 , -0.00013381]), array([0.00588858])]\n",
      "Iteration 398, Cost: 0.24817191744646\n",
      "gradient_weights:  [array([[-0.00102215, -0.00198163, -0.00077494],\n",
      "       [-0.00130045, -0.00334724, -0.0001338 ]]), array([[-0.00607408],\n",
      "       [-0.00466973],\n",
      "       [ 0.00307005]])]\n",
      "gradients_biases:  [array([-0.00042014, -0.00093269, -0.00013347]), array([0.00588834])]\n",
      "Iteration 399, Cost: 0.248165803703296\n",
      "gradient_weights:  [array([[-0.00102809, -0.00198561, -0.00077206],\n",
      "       [-0.00130814, -0.0033539 , -0.00013332]]), array([[-0.0060724 ],\n",
      "       [-0.0046717 ],\n",
      "       [ 0.00307161]])]\n",
      "gradients_biases:  [array([-0.00042269, -0.00093448, -0.00013312]), array([0.00588811])]\n",
      "Iteration 400, Cost: 0.24815968507009983\n",
      "gradient_weights:  [array([[-0.00103402, -0.00198958, -0.00076918],\n",
      "       [-0.00131584, -0.00336056, -0.00013285]]), array([[-0.00607074],\n",
      "       [-0.00467367],\n",
      "       [ 0.00307318]])]\n",
      "gradients_biases:  [array([-0.00042525, -0.00093627, -0.00013277]), array([0.00588789])]\n",
      "Iteration 401, Cost: 0.2481535615105926\n",
      "gradient_weights:  [array([[-0.00103995, -0.00199356, -0.0007663 ],\n",
      "       [-0.00132353, -0.00336722, -0.00013237]]), array([[-0.00606908],\n",
      "       [-0.00467565],\n",
      "       [ 0.00307476]])]\n",
      "gradients_biases:  [array([-0.0004278 , -0.00093806, -0.00013242]), array([0.00588768])]\n",
      "Iteration 402, Cost: 0.24814743298849148\n",
      "gradient_weights:  [array([[-0.00104588, -0.00199755, -0.00076341],\n",
      "       [-0.00133122, -0.00337389, -0.00013189]]), array([[-0.00606744],\n",
      "       [-0.00467763],\n",
      "       [ 0.00307635]])]\n",
      "gradients_biases:  [array([-0.00043035, -0.00093985, -0.00013206]), array([0.00588748])]\n",
      "Iteration 403, Cost: 0.24814129946750985\n",
      "gradient_weights:  [array([[-0.0010518 , -0.00200154, -0.00076053],\n",
      "       [-0.00133891, -0.00338056, -0.00013141]]), array([[-0.0060658 ],\n",
      "       [-0.00467962],\n",
      "       [ 0.00307795]])]\n",
      "gradients_biases:  [array([-0.0004329 , -0.00094163, -0.00013171]), array([0.00588729])]\n",
      "Iteration 404, Cost: 0.24813516091135684\n",
      "gradient_weights:  [array([[-0.00105772, -0.00200553, -0.00075765],\n",
      "       [-0.00134659, -0.00338723, -0.00013094]]), array([[-0.00606417],\n",
      "       [-0.00468162],\n",
      "       [ 0.00307956]])]\n",
      "gradients_biases:  [array([-0.00043545, -0.00094342, -0.00013136]), array([0.0058871])]\n",
      "Iteration 405, Cost: 0.24812901728373732\n",
      "gradient_weights:  [array([[-0.00106364, -0.00200952, -0.00075476],\n",
      "       [-0.00135428, -0.00339391, -0.00013046]]), array([[-0.00606255],\n",
      "       [-0.00468363],\n",
      "       [ 0.00308118]])]\n",
      "gradients_biases:  [array([-0.000438 , -0.0009452, -0.000131 ]), array([0.00588693])]\n",
      "Iteration 406, Cost: 0.2481228685483517\n",
      "gradient_weights:  [array([[-0.00106956, -0.00201352, -0.00075188],\n",
      "       [-0.00136196, -0.00340059, -0.00012998]]), array([[-0.00606094],\n",
      "       [-0.00468564],\n",
      "       [ 0.00308281]])]\n",
      "gradients_biases:  [array([-0.00044055, -0.00094698, -0.00013064]), array([0.00588676])]\n",
      "Iteration 407, Cost: 0.2481167146688954\n",
      "gradient_weights:  [array([[-0.00107548, -0.00201752, -0.00074899],\n",
      "       [-0.00136965, -0.00340728, -0.0001295 ]]), array([[-0.00605934],\n",
      "       [-0.00468765],\n",
      "       [ 0.00308445]])]\n",
      "gradients_biases:  [array([-0.0004431 , -0.00094877, -0.00013028]), array([0.00588661])]\n",
      "Iteration 408, Cost: 0.24811055560905926\n",
      "gradient_weights:  [array([[-0.00108139, -0.00202153, -0.0007461 ],\n",
      "       [-0.00137733, -0.00341397, -0.00012902]]), array([[-0.00605775],\n",
      "       [-0.00468967],\n",
      "       [ 0.0030861 ]])]\n",
      "gradients_biases:  [array([-0.00044565, -0.00095055, -0.00012992]), array([0.00588646])]\n",
      "Iteration 409, Cost: 0.24810439133252854\n",
      "gradient_weights:  [array([[-0.0010873 , -0.00202553, -0.00074321],\n",
      "       [-0.00138501, -0.00342066, -0.00012854]]), array([[-0.00605617],\n",
      "       [-0.0046917 ],\n",
      "       [ 0.00308775]])]\n",
      "gradients_biases:  [array([-0.00044819, -0.00095233, -0.00012956]), array([0.00588632])]\n",
      "Iteration 410, Cost: 0.24809822180298347\n",
      "gradient_weights:  [array([[-0.00109321, -0.00202955, -0.00074033],\n",
      "       [-0.00139269, -0.00342736, -0.00012805]]), array([[-0.00605459],\n",
      "       [-0.00469374],\n",
      "       [ 0.00308942]])]\n",
      "gradients_biases:  [array([-0.00045074, -0.00095411, -0.00012919]), array([0.00588619])]\n",
      "Iteration 411, Cost: 0.24809204698409873\n",
      "gradient_weights:  [array([[-0.00109912, -0.00203356, -0.00073744],\n",
      "       [-0.00140036, -0.00343406, -0.00012757]]), array([[-0.00605303],\n",
      "       [-0.00469578],\n",
      "       [ 0.0030911 ]])]\n",
      "gradients_biases:  [array([-0.00045328, -0.00095588, -0.00012883]), array([0.00588607])]\n",
      "Iteration 412, Cost: 0.24808586683954323\n",
      "gradient_weights:  [array([[-0.00110502, -0.00203758, -0.00073455],\n",
      "       [-0.00140804, -0.00344076, -0.00012709]]), array([[-0.00605147],\n",
      "       [-0.00469782],\n",
      "       [ 0.00309278]])]\n",
      "gradients_biases:  [array([-0.00045583, -0.00095766, -0.00012846]), array([0.00588596])]\n",
      "Iteration 413, Cost: 0.24807968133297995\n",
      "gradient_weights:  [array([[-0.00111093, -0.0020416 , -0.00073166],\n",
      "       [-0.00141571, -0.00344747, -0.0001266 ]]), array([[-0.00604993],\n",
      "       [-0.00469988],\n",
      "       [ 0.00309448]])]\n",
      "gradients_biases:  [array([-0.00045837, -0.00095944, -0.00012809]), array([0.00588586])]\n",
      "Iteration 414, Cost: 0.24807349042806554\n",
      "gradient_weights:  [array([[-0.00111683, -0.00204563, -0.00072877],\n",
      "       [-0.00142338, -0.00345418, -0.00012612]]), array([[-0.00604839],\n",
      "       [-0.00470193],\n",
      "       [ 0.00309618]])]\n",
      "gradients_biases:  [array([-0.00046092, -0.00096121, -0.00012772]), array([0.00588577])]\n",
      "Iteration 415, Cost: 0.2480672940884508\n",
      "gradient_weights:  [array([[-0.00112273, -0.00204965, -0.00072587],\n",
      "       [-0.00143105, -0.0034609 , -0.00012564]]), array([[-0.00604686],\n",
      "       [-0.004704  ],\n",
      "       [ 0.0030979 ]])]\n",
      "gradients_biases:  [array([-0.00046346, -0.00096298, -0.00012735]), array([0.00588568])]\n",
      "Iteration 416, Cost: 0.24806109227777964\n",
      "gradient_weights:  [array([[-0.00112862, -0.00205369, -0.00072298],\n",
      "       [-0.00143872, -0.00346761, -0.00012515]]), array([[-0.00604534],\n",
      "       [-0.00470607],\n",
      "       [ 0.00309962]])]\n",
      "gradients_biases:  [array([-0.000466  , -0.00096476, -0.00012698]), array([0.00588561])]\n",
      "Iteration 417, Cost: 0.24805488495968944\n",
      "gradient_weights:  [array([[-0.00113452, -0.00205772, -0.00072009],\n",
      "       [-0.00144639, -0.00347434, -0.00012466]]), array([[-0.00604384],\n",
      "       [-0.00470815],\n",
      "       [ 0.00310135]])]\n",
      "gradients_biases:  [array([-0.00046854, -0.00096653, -0.0001266 ]), array([0.00588554])]\n",
      "Iteration 418, Cost: 0.24804867209781065\n",
      "gradient_weights:  [array([[-0.00114041, -0.00206176, -0.00071719],\n",
      "       [-0.00145406, -0.00348106, -0.00012418]]), array([[-0.00604233],\n",
      "       [-0.00471023],\n",
      "       [ 0.0031031 ]])]\n",
      "gradients_biases:  [array([-0.00047108, -0.0009683 , -0.00012623]), array([0.00588549])]\n",
      "Iteration 419, Cost: 0.2480424536557666\n",
      "gradient_weights:  [array([[-0.0011463 , -0.0020658 , -0.0007143 ],\n",
      "       [-0.00146173, -0.00348779, -0.00012369]]), array([[-0.00604084],\n",
      "       [-0.00471232],\n",
      "       [ 0.00310485]])]\n",
      "gradients_biases:  [array([-0.00047362, -0.00097007, -0.00012585]), array([0.00588544])]\n",
      "Iteration 420, Cost: 0.24803622959717342\n",
      "gradient_weights:  [array([[-0.00115219, -0.00206985, -0.0007114 ],\n",
      "       [-0.00146939, -0.00349453, -0.0001232 ]]), array([[-0.00603936],\n",
      "       [-0.00471442],\n",
      "       [ 0.00310661]])]\n",
      "gradients_biases:  [array([-0.00047616, -0.00097183, -0.00012547]), array([0.0058854])]\n",
      "Iteration 421, Cost: 0.2480299998856397\n",
      "gradient_weights:  [array([[-0.00115808, -0.0020739 , -0.00070851],\n",
      "       [-0.00147705, -0.00350127, -0.00012272]]), array([[-0.00603789],\n",
      "       [-0.00471652],\n",
      "       [ 0.00310838]])]\n",
      "gradients_biases:  [array([-0.00047869, -0.0009736 , -0.00012509]), array([0.00588538])]\n",
      "Iteration 422, Cost: 0.2480237644847666\n",
      "gradient_weights:  [array([[-0.00116396, -0.00207795, -0.00070561],\n",
      "       [-0.00148471, -0.00350801, -0.00012223]]), array([[-0.00603642],\n",
      "       [-0.00471863],\n",
      "       [ 0.00311016]])]\n",
      "gradients_biases:  [array([-0.00048123, -0.00097537, -0.00012471]), array([0.00588536])]\n",
      "Iteration 423, Cost: 0.24801752335814714\n",
      "gradient_weights:  [array([[-0.00116984, -0.002082  , -0.00070272],\n",
      "       [-0.00149238, -0.00351475, -0.00012174]]), array([[-0.00603497],\n",
      "       [-0.00472074],\n",
      "       [ 0.00311195]])]\n",
      "gradients_biases:  [array([-0.00048376, -0.00097713, -0.00012433]), array([0.00588535])]\n",
      "Iteration 424, Cost: 0.24801127646936652\n",
      "gradient_weights:  [array([[-0.00117573, -0.00208606, -0.00069982],\n",
      "       [-0.00150003, -0.0035215 , -0.00012125]]), array([[-0.00603352],\n",
      "       [-0.00472286],\n",
      "       [ 0.00311376]])]\n",
      "gradients_biases:  [array([-0.0004863 , -0.00097889, -0.00012394]), array([0.00588534])]\n",
      "Iteration 425, Cost: 0.2480050237820018\n",
      "gradient_weights:  [array([[-0.0011816 , -0.00209013, -0.00069692],\n",
      "       [-0.00150769, -0.00352825, -0.00012076]]), array([[-0.00603208],\n",
      "       [-0.00472499],\n",
      "       [ 0.00311557]])]\n",
      "gradients_biases:  [array([-0.00048883, -0.00098065, -0.00012356]), array([0.00588535])]\n",
      "Iteration 426, Cost: 0.2479987652596214\n",
      "gradient_weights:  [array([[-0.00118748, -0.00209419, -0.00069402],\n",
      "       [-0.00151535, -0.00353501, -0.00012027]]), array([[-0.00603066],\n",
      "       [-0.00472713],\n",
      "       [ 0.00311738]])]\n",
      "gradients_biases:  [array([-0.00049137, -0.00098241, -0.00012317]), array([0.00588537])]\n",
      "Iteration 427, Cost: 0.2479925008657854\n",
      "gradient_weights:  [array([[-0.00119336, -0.00209826, -0.00069112],\n",
      "       [-0.001523  , -0.00354177, -0.00011978]]), array([[-0.00602924],\n",
      "       [-0.00472926],\n",
      "       [ 0.00311921]])]\n",
      "gradients_biases:  [array([-0.0004939 , -0.00098417, -0.00012278]), array([0.00588539])]\n",
      "Iteration 428, Cost: 0.24798623056404512\n",
      "gradient_weights:  [array([[-0.00119923, -0.00210234, -0.00068822],\n",
      "       [-0.00153066, -0.00354854, -0.00011928]]), array([[-0.00602783],\n",
      "       [-0.00473141],\n",
      "       [ 0.00312105]])]\n",
      "gradients_biases:  [array([-0.00049643, -0.00098593, -0.00012239]), array([0.00588543])]\n",
      "Iteration 429, Cost: 0.24797995431794276\n",
      "gradient_weights:  [array([[-0.0012051 , -0.00210641, -0.00068532],\n",
      "       [-0.00153831, -0.0035553 , -0.00011879]]), array([[-0.00602643],\n",
      "       [-0.00473356],\n",
      "       [ 0.0031229 ]])]\n",
      "gradients_biases:  [array([-0.00049896, -0.00098769, -0.000122  ]), array([0.00588547])]\n",
      "Iteration 430, Cost: 0.24797367209101168\n",
      "gradient_weights:  [array([[-0.00121097, -0.0021105 , -0.00068241],\n",
      "       [-0.00154596, -0.00356207, -0.0001183 ]]), array([[-0.00602503],\n",
      "       [-0.00473572],\n",
      "       [ 0.00312476]])]\n",
      "gradients_biases:  [array([-0.00050149, -0.00098944, -0.00012161]), array([0.00588553])]\n",
      "Iteration 431, Cost: 0.24796738384677564\n",
      "gradient_weights:  [array([[-0.00121684, -0.00211458, -0.00067951],\n",
      "       [-0.00155361, -0.00356885, -0.0001178 ]]), array([[-0.00602365],\n",
      "       [-0.00473789],\n",
      "       [ 0.00312663]])]\n",
      "gradients_biases:  [array([-0.00050401, -0.00099119, -0.00012122]), array([0.00588559])]\n",
      "Iteration 432, Cost: 0.2479610895487492\n",
      "gradient_weights:  [array([[-0.00122271, -0.00211867, -0.00067661],\n",
      "       [-0.00156126, -0.00357563, -0.00011731]]), array([[-0.00602228],\n",
      "       [-0.00474006],\n",
      "       [ 0.0031285 ]])]\n",
      "gradients_biases:  [array([-0.00050654, -0.00099295, -0.00012082]), array([0.00588566])]\n",
      "Iteration 433, Cost: 0.24795478916043717\n",
      "gradient_weights:  [array([[-0.00122857, -0.00212276, -0.0006737 ],\n",
      "       [-0.00156891, -0.00358241, -0.00011682]]), array([[-0.00602091],\n",
      "       [-0.00474223],\n",
      "       [ 0.00313039]])]\n",
      "gradients_biases:  [array([-0.00050907, -0.0009947 , -0.00012042]), array([0.00588574])]\n",
      "Iteration 434, Cost: 0.24794848264533448\n",
      "gradient_weights:  [array([[-0.00123444, -0.00212685, -0.0006708 ],\n",
      "       [-0.00157656, -0.0035892 , -0.00011632]]), array([[-0.00601955],\n",
      "       [-0.00474442],\n",
      "       [ 0.00313228]])]\n",
      "gradients_biases:  [array([-0.00051159, -0.00099645, -0.00012003]), array([0.00588583])]\n",
      "Iteration 435, Cost: 0.24794216996692614\n",
      "gradient_weights:  [array([[-0.0012403 , -0.00213095, -0.00066789],\n",
      "       [-0.00158421, -0.00359599, -0.00011583]]), array([[-0.00601821],\n",
      "       [-0.00474661],\n",
      "       [ 0.00313419]])]\n",
      "gradients_biases:  [array([-0.00051412, -0.0009982 , -0.00011963]), array([0.00588593])]\n",
      "Iteration 436, Cost: 0.24793585108868693\n",
      "gradient_weights:  [array([[-0.00124616, -0.00213505, -0.00066498],\n",
      "       [-0.00159185, -0.00360278, -0.00011533]]), array([[-0.00601687],\n",
      "       [-0.0047488 ],\n",
      "       [ 0.0031361 ]])]\n",
      "gradients_biases:  [array([-0.00051664, -0.00099994, -0.00011923]), array([0.00588603])]\n",
      "Iteration 437, Cost: 0.2479295259740813\n",
      "gradient_weights:  [array([[-0.00125201, -0.00213916, -0.00066208],\n",
      "       [-0.0015995 , -0.00360958, -0.00011483]]), array([[-0.00601554],\n",
      "       [-0.004751  ],\n",
      "       [ 0.00313803]])]\n",
      "gradients_biases:  [array([-0.00051916, -0.00100169, -0.00011882]), array([0.00588615])]\n",
      "Iteration 438, Cost: 0.24792319458656314\n",
      "gradient_weights:  [array([[-0.00125787, -0.00214327, -0.00065917],\n",
      "       [-0.00160714, -0.00361638, -0.00011434]]), array([[-0.00601422],\n",
      "       [-0.00475321],\n",
      "       [ 0.00313996]])]\n",
      "gradients_biases:  [array([-0.00052168, -0.00100343, -0.00011842]), array([0.00588628])]\n",
      "Iteration 439, Cost: 0.24791685688957582\n",
      "gradient_weights:  [array([[-0.00126372, -0.00214738, -0.00065626],\n",
      "       [-0.00161478, -0.00362319, -0.00011384]]), array([[-0.00601291],\n",
      "       [-0.00475543],\n",
      "       [ 0.00314191]])]\n",
      "gradients_biases:  [array([-0.0005242 , -0.00100517, -0.00011802]), array([0.00588641])]\n",
      "Iteration 440, Cost: 0.2479105128465515\n",
      "gradient_weights:  [array([[-0.00126958, -0.0021515 , -0.00065335],\n",
      "       [-0.00162242, -0.00363   , -0.00011334]]), array([[-0.0060116 ],\n",
      "       [-0.00475765],\n",
      "       [ 0.00314386]])]\n",
      "gradients_biases:  [array([-0.00052672, -0.00100692, -0.00011761]), array([0.00588655])]\n",
      "Iteration 441, Cost: 0.24790416242091162\n",
      "gradient_weights:  [array([[-0.00127543, -0.00215562, -0.00065044],\n",
      "       [-0.00163006, -0.00363681, -0.00011284]]), array([[-0.00601031],\n",
      "       [-0.00475988],\n",
      "       [ 0.00314582]])]\n",
      "gradients_biases:  [array([-0.00052924, -0.00100866, -0.0001172 ]), array([0.0058867])]\n",
      "Iteration 442, Cost: 0.24789780557606628\n",
      "gradient_weights:  [array([[-0.00128128, -0.00215974, -0.00064753],\n",
      "       [-0.0016377 , -0.00364363, -0.00011234]]), array([[-0.00600902],\n",
      "       [-0.00476211],\n",
      "       [ 0.00314779]])]\n",
      "gradients_biases:  [array([-0.00053175, -0.00101039, -0.00011679]), array([0.00588687])]\n",
      "Iteration 443, Cost: 0.24789144227541413\n",
      "gradient_weights:  [array([[-0.00128713, -0.00216387, -0.00064462],\n",
      "       [-0.00164534, -0.00365045, -0.00011184]]), array([[-0.00600775],\n",
      "       [-0.00476435],\n",
      "       [ 0.00314978]])]\n",
      "gradients_biases:  [array([-0.00053427, -0.00101213, -0.00011638]), array([0.00588704])]\n",
      "Iteration 444, Cost: 0.24788507248234248\n",
      "gradient_weights:  [array([[-0.00129297, -0.002168  , -0.0006417 ],\n",
      "       [-0.00165297, -0.00365728, -0.00011134]]), array([[-0.00600648],\n",
      "       [-0.00476659],\n",
      "       [ 0.00315177]])]\n",
      "gradients_biases:  [array([-0.00053678, -0.00101387, -0.00011597]), array([0.00588721])]\n",
      "Iteration 445, Cost: 0.24787869616022679\n",
      "gradient_weights:  [array([[-0.00129882, -0.00217214, -0.00063879],\n",
      "       [-0.00166061, -0.0036641 , -0.00011084]]), array([[-0.00600522],\n",
      "       [-0.00476885],\n",
      "       [ 0.00315377]])]\n",
      "gradients_biases:  [array([-0.0005393 , -0.0010156 , -0.00011556]), array([0.0058874])]\n",
      "Iteration 446, Cost: 0.24787231327243045\n",
      "gradient_weights:  [array([[-0.00130466, -0.00217627, -0.00063587],\n",
      "       [-0.00166824, -0.00367094, -0.00011034]]), array([[-0.00600397],\n",
      "       [-0.00477111],\n",
      "       [ 0.00315578]])]\n",
      "gradients_biases:  [array([-0.00054181, -0.00101733, -0.00011514]), array([0.0058876])]\n",
      "Iteration 447, Cost: 0.2478659237823052\n",
      "gradient_weights:  [array([[-0.0013105 , -0.00218042, -0.00063296],\n",
      "       [-0.00167588, -0.00367777, -0.00010984]]), array([[-0.00600273],\n",
      "       [-0.00477337],\n",
      "       [ 0.0031578 ]])]\n",
      "gradients_biases:  [array([-0.00054432, -0.00101906, -0.00011473]), array([0.00588781])]\n",
      "Iteration 448, Cost: 0.2478595276531903\n",
      "gradient_weights:  [array([[-0.00131634, -0.00218456, -0.00063004],\n",
      "       [-0.00168351, -0.00368461, -0.00010934]]), array([[-0.0060015 ],\n",
      "       [-0.00477564],\n",
      "       [ 0.00315983]])]\n",
      "gradients_biases:  [array([-0.00054683, -0.00102079, -0.00011431]), array([0.00588802])]\n",
      "Iteration 449, Cost: 0.24785312484841265\n",
      "gradient_weights:  [array([[-0.00132218, -0.00218871, -0.00062713],\n",
      "       [-0.00169114, -0.00369146, -0.00010884]]), array([[-0.00600027],\n",
      "       [-0.00477792],\n",
      "       [ 0.00316187]])]\n",
      "gradients_biases:  [array([-0.00054934, -0.00102252, -0.00011389]), array([0.00588824])]\n",
      "Iteration 450, Cost: 0.24784671533128672\n",
      "gradient_weights:  [array([[-0.00132802, -0.00219286, -0.00062421],\n",
      "       [-0.00169877, -0.00369831, -0.00010833]]), array([[-0.00599906],\n",
      "       [-0.0047802 ],\n",
      "       [ 0.00316392]])]\n",
      "gradients_biases:  [array([-0.00055185, -0.00102425, -0.00011347]), array([0.00588848])]\n",
      "Iteration 451, Cost: 0.24784029906511423\n",
      "gradient_weights:  [array([[-0.00133386, -0.00219702, -0.00062129],\n",
      "       [-0.0017064 , -0.00370516, -0.00010783]]), array([[-0.00599785],\n",
      "       [-0.00478249],\n",
      "       [ 0.00316597]])]\n",
      "gradients_biases:  [array([-0.00055435, -0.00102597, -0.00011305]), array([0.00588872])]\n",
      "Iteration 452, Cost: 0.24783387601318402\n",
      "gradient_weights:  [array([[-0.00133969, -0.00220118, -0.00061837],\n",
      "       [-0.00171403, -0.00371201, -0.00010733]]), array([[-0.00599666],\n",
      "       [-0.00478479],\n",
      "       [ 0.00316804]])]\n",
      "gradients_biases:  [array([-0.00055686, -0.0010277 , -0.00011263]), array([0.00588897])]\n",
      "Iteration 453, Cost: 0.2478274461387719\n",
      "gradient_weights:  [array([[-0.00134552, -0.00220535, -0.00061545],\n",
      "       [-0.00172166, -0.00371887, -0.00010682]]), array([[-0.00599547],\n",
      "       [-0.00478709],\n",
      "       [ 0.00317012]])]\n",
      "gradients_biases:  [array([-0.00055936, -0.00102942, -0.0001122 ]), array([0.00588923])]\n",
      "Iteration 454, Cost: 0.24782100940514046\n",
      "gradient_weights:  [array([[-0.00135135, -0.00220951, -0.00061253],\n",
      "       [-0.00172929, -0.00372574, -0.00010632]]), array([[-0.00599429],\n",
      "       [-0.0047894 ],\n",
      "       [ 0.00317221]])]\n",
      "gradients_biases:  [array([-0.00056186, -0.00103114, -0.00011178]), array([0.0058895])]\n",
      "Iteration 455, Cost: 0.2478145657755391\n",
      "gradient_weights:  [array([[-0.00135718, -0.00221369, -0.00060961],\n",
      "       [-0.00173691, -0.0037326 , -0.00010581]]), array([[-0.00599312],\n",
      "       [-0.00479172],\n",
      "       [ 0.0031743 ]])]\n",
      "gradients_biases:  [array([-0.00056436, -0.00103286, -0.00011135]), array([0.00588977])]\n",
      "Iteration 456, Cost: 0.24780811521320356\n",
      "gradient_weights:  [array([[-0.00136301, -0.00221786, -0.00060668],\n",
      "       [-0.00174454, -0.00373947, -0.00010531]]), array([[-0.00599196],\n",
      "       [-0.00479404],\n",
      "       [ 0.00317641]])]\n",
      "gradients_biases:  [array([-0.00056687, -0.00103457, -0.00011092]), array([0.00589006])]\n",
      "Iteration 457, Cost: 0.24780165768135604\n",
      "gradient_weights:  [array([[-0.00136884, -0.00222204, -0.00060376],\n",
      "       [-0.00175216, -0.00374635, -0.0001048 ]]), array([[-0.0059908 ],\n",
      "       [-0.00479637],\n",
      "       [ 0.00317852]])]\n",
      "gradients_biases:  [array([-0.00056936, -0.00103629, -0.00011049]), array([0.00589035])]\n",
      "Iteration 458, Cost: 0.24779519314320486\n",
      "gradient_weights:  [array([[-0.00137467, -0.00222622, -0.00060084],\n",
      "       [-0.00175979, -0.00375323, -0.00010429]]), array([[-0.00598966],\n",
      "       [-0.0047987 ],\n",
      "       [ 0.00318065]])]\n",
      "gradients_biases:  [array([-0.00057186, -0.001038  , -0.00011006]), array([0.00589066])]\n",
      "Iteration 459, Cost: 0.24778872156194437\n",
      "gradient_weights:  [array([[-0.00138049, -0.00223041, -0.00059791],\n",
      "       [-0.00176741, -0.00376011, -0.00010379]]), array([[-0.00598852],\n",
      "       [-0.00480104],\n",
      "       [ 0.00318278]])]\n",
      "gradients_biases:  [array([-0.00057436, -0.00103971, -0.00010963]), array([0.00589097])]\n",
      "Iteration 460, Cost: 0.2477822429007549\n",
      "gradient_weights:  [array([[-0.00138631, -0.0022346 , -0.00059499],\n",
      "       [-0.00177503, -0.003767  , -0.00010328]]), array([[-0.00598739],\n",
      "       [-0.00480339],\n",
      "       [ 0.00318493]])]\n",
      "gradients_biases:  [array([-0.00057685, -0.00104142, -0.00010919]), array([0.00589129])]\n",
      "Iteration 461, Cost: 0.24777575712280236\n",
      "gradient_weights:  [array([[-0.00139214, -0.0022388 , -0.00059206],\n",
      "       [-0.00178265, -0.00377389, -0.00010277]]), array([[-0.00598628],\n",
      "       [-0.00480574],\n",
      "       [ 0.00318708]])]\n",
      "gradients_biases:  [array([-0.00057935, -0.00104313, -0.00010876]), array([0.00589162])]\n",
      "Iteration 462, Cost: 0.2477692641912384\n",
      "gradient_weights:  [array([[-0.00139796, -0.00224299, -0.00058913],\n",
      "       [-0.00179027, -0.00378078, -0.00010226]]), array([[-0.00598517],\n",
      "       [-0.0048081 ],\n",
      "       [ 0.00318925]])]\n",
      "gradients_biases:  [array([-0.00058184, -0.00104484, -0.00010832]), array([0.00589196])]\n",
      "Iteration 463, Cost: 0.24776276406919995\n",
      "gradient_weights:  [array([[-0.00140378, -0.0022472 , -0.0005862 ],\n",
      "       [-0.00179789, -0.00378768, -0.00010175]]), array([[-0.00598406],\n",
      "       [-0.00481047],\n",
      "       [ 0.00319142]])]\n",
      "gradients_biases:  [array([-0.00058433, -0.00104655, -0.00010789]), array([0.00589231])]\n",
      "Iteration 464, Cost: 0.24775625671980914\n",
      "gradient_weights:  [array([[-0.00140959, -0.0022514 , -0.00058327],\n",
      "       [-0.00180551, -0.00379458, -0.00010124]]), array([[-0.00598297],\n",
      "       [-0.00481284],\n",
      "       [ 0.0031936 ]])]\n",
      "gradients_biases:  [array([-0.00058682, -0.00104825, -0.00010745]), array([0.00589267])]\n",
      "Iteration 465, Cost: 0.2477497421061735\n",
      "gradient_weights:  [array([[-0.00141541, -0.00225561, -0.00058034],\n",
      "       [-0.00181313, -0.00380149, -0.00010074]]), array([[-0.00598189],\n",
      "       [-0.00481522],\n",
      "       [ 0.00319579]])]\n",
      "gradients_biases:  [array([-0.00058931, -0.00104995, -0.00010701]), array([0.00589303])]\n",
      "Iteration 466, Cost: 0.24774322019138545\n",
      "gradient_weights:  [array([[-0.00142123, -0.00225983, -0.00057741],\n",
      "       [-0.00182075, -0.0038084 , -0.00010023]]), array([[-0.00598081],\n",
      "       [-0.00481761],\n",
      "       [ 0.00319799]])]\n",
      "gradients_biases:  [array([-0.00059179, -0.00105165, -0.00010656]), array([0.00589341])]\n",
      "Iteration 467, Cost: 0.24773669093852208\n",
      "gradient_weights:  [array([[-1.42704014e-03, -2.26404259e-03, -5.74480855e-04],\n",
      "       [-1.82836649e-03, -3.81530948e-03, -9.97150122e-05]]), array([[-0.00597975],\n",
      "       [-0.00482   ],\n",
      "       [ 0.00320021]])]\n",
      "gradients_biases:  [array([-0.00059428, -0.00105335, -0.00010612]), array([0.00589379])]\n",
      "Iteration 468, Cost: 0.24773015431064516\n",
      "gradient_weights:  [array([[-1.43285323e-03, -2.26826367e-03, -5.71548211e-04],\n",
      "       [-1.83598269e-03, -3.82222695e-03, -9.92042743e-05]]), array([[-0.00597869],\n",
      "       [-0.0048224 ],\n",
      "       [ 0.00320243]])]\n",
      "gradients_biases:  [array([-0.00059676, -0.00105505, -0.00010568]), array([0.00589418])]\n",
      "Iteration 469, Cost: 0.24772361027080125\n",
      "gradient_weights:  [array([[-1.43866512e-03, -2.27248859e-03, -5.68614714e-04],\n",
      "       [-1.84359819e-03, -3.82914831e-03, -9.86931323e-05]]), array([[-0.00597764],\n",
      "       [-0.0048248 ],\n",
      "       [ 0.00320466]])]\n",
      "gradients_biases:  [array([-0.00059925, -0.00105674, -0.00010523]), array([0.00589459])]\n",
      "Iteration 470, Cost: 0.24771705878202105\n",
      "gradient_weights:  [array([[-1.44447582e-03, -2.27671736e-03, -5.65680360e-04],\n",
      "       [-1.85121300e-03, -3.83607356e-03, -9.81815894e-05]]), array([[-0.0059766 ],\n",
      "       [-0.00482721],\n",
      "       [ 0.0032069 ]])]\n",
      "gradients_biases:  [array([-0.00060173, -0.00105843, -0.00010478]), array([0.005895])]\n",
      "Iteration 471, Cost: 0.24771049980731963\n",
      "gradient_weights:  [array([[-1.45028535e-03, -2.28094998e-03, -5.62745146e-04],\n",
      "       [-1.85882713e-03, -3.84300271e-03, -9.76696492e-05]]), array([[-0.00597557],\n",
      "       [-0.00482963],\n",
      "       [ 0.00320915]])]\n",
      "gradients_biases:  [array([-0.00060421, -0.00106012, -0.00010434]), array([0.00589541])]\n",
      "Iteration 472, Cost: 0.2477039333096962\n",
      "gradient_weights:  [array([[-1.45609371e-03, -2.28518647e-03, -5.59809070e-04],\n",
      "       [-1.86644059e-03, -3.84993576e-03, -9.71573151e-05]]), array([[-0.00597454],\n",
      "       [-0.00483205],\n",
      "       [ 0.0032114 ]])]\n",
      "gradients_biases:  [array([-0.00060669, -0.00106181, -0.00010389]), array([0.00589584])]\n",
      "Iteration 473, Cost: 0.2476973592521336\n",
      "gradient_weights:  [array([[-1.46190094e-03, -2.28942683e-03, -5.56872129e-04],\n",
      "       [-1.87405340e-03, -3.85687270e-03, -9.66445905e-05]]), array([[-0.00597353],\n",
      "       [-0.00483448],\n",
      "       [ 0.00321367]])]\n",
      "gradients_biases:  [array([-0.00060916, -0.0010635 , -0.00010344]), array([0.00589628])]\n",
      "Iteration 474, Cost: 0.24769077759759903\n",
      "gradient_weights:  [array([[-1.46770703e-03, -2.29367106e-03, -5.53934321e-04],\n",
      "       [-1.88166556e-03, -3.86381355e-03, -9.61314789e-05]]), array([[-0.00597252],\n",
      "       [-0.00483692],\n",
      "       [ 0.00321595]])]\n",
      "gradients_biases:  [array([-0.00061164, -0.00106519, -0.00010298]), array([0.00589672])]\n",
      "Iteration 475, Cost: 0.24768418830904293\n",
      "gradient_weights:  [array([[-1.47351202e-03, -2.29791918e-03, -5.50995644e-04],\n",
      "       [-1.88927710e-03, -3.87075830e-03, -9.56179838e-05]]), array([[-0.00597153],\n",
      "       [-0.00483936],\n",
      "       [ 0.00321824]])]\n",
      "gradients_biases:  [array([-0.00061411, -0.00106687, -0.00010253]), array([0.00589718])]\n",
      "Iteration 476, Cost: 0.24767759134939954\n",
      "gradient_weights:  [array([[-1.47931590e-03, -2.30217120e-03, -5.48056094e-04],\n",
      "       [-1.89688801e-03, -3.87770695e-03, -9.51041087e-05]]), array([[-0.00597054],\n",
      "       [-0.00484181],\n",
      "       [ 0.00322053]])]\n",
      "gradients_biases:  [array([-0.00061659, -0.00106855, -0.00010208]), array([0.00589764])]\n",
      "Iteration 477, Cost: 0.2476709866815862\n",
      "gradient_weights:  [array([[-1.48511869e-03, -2.30642711e-03, -5.45115669e-04],\n",
      "       [-1.90449831e-03, -3.88465951e-03, -9.45898571e-05]]), array([[-0.00596956],\n",
      "       [-0.00484427],\n",
      "       [ 0.00322284]])]\n",
      "gradients_biases:  [array([-0.00061906, -0.00107023, -0.00010162]), array([0.00589811])]\n",
      "Iteration 478, Cost: 0.24766437426850393\n",
      "gradient_weights:  [array([[-1.49092042e-03, -2.31068693e-03, -5.42174368e-04],\n",
      "       [-1.91210802e-03, -3.89161598e-03, -9.40752325e-05]]), array([[-0.00596859],\n",
      "       [-0.00484673],\n",
      "       [ 0.00322516]])]\n",
      "gradients_biases:  [array([-0.00062153, -0.00107191, -0.00010116]), array([0.00589859])]\n",
      "Iteration 479, Cost: 0.24765775407303653\n",
      "gradient_weights:  [array([[-1.49672108e-03, -2.31495066e-03, -5.39232186e-04],\n",
      "       [-1.91971714e-03, -3.89857636e-03, -9.35602384e-05]]), array([[-0.00596762],\n",
      "       [-0.0048492 ],\n",
      "       [ 0.00322748]])]\n",
      "gradients_biases:  [array([-0.00062399, -0.00107359, -0.0001007 ]), array([0.00589908])]\n",
      "Iteration 480, Cost: 0.24765112605805084\n",
      "gradient_weights:  [array([[-1.50252071e-03, -2.31921831e-03, -5.36289123e-04],\n",
      "       [-1.92732568e-03, -3.90554065e-03, -9.30448785e-05]]), array([[-0.00596667],\n",
      "       [-0.00485167],\n",
      "       [ 0.00322982]])]\n",
      "gradients_biases:  [array([-0.00062646, -0.00107526, -0.00010024]), array([0.00589958])]\n",
      "Iteration 481, Cost: 0.24764449018639678\n",
      "gradient_weights:  [array([[-1.50831931e-03, -2.32348989e-03, -5.33345176e-04],\n",
      "       [-1.93493365e-03, -3.91250886e-03, -9.25291562e-05]]), array([[-0.00596572],\n",
      "       [-0.00485415],\n",
      "       [ 0.00323216]])]\n",
      "gradients_biases:  [array([-6.28926664e-04, -1.07693436e-03, -9.97815614e-05]), array([0.00590009])]\n",
      "Iteration 482, Cost: 0.2476378464209067\n",
      "gradient_weights:  [array([[-1.51411689e-03, -2.32776541e-03, -5.30400342e-04],\n",
      "       [-1.94254108e-03, -3.91948098e-03, -9.20130752e-05]]), array([[-0.00596478],\n",
      "       [-0.00485664],\n",
      "       [ 0.00323452]])]\n",
      "gradients_biases:  [array([-6.31390236e-04, -1.07860481e-03, -9.93188962e-05]), array([0.0059006])]\n",
      "Iteration 483, Cost: 0.24763119472439582\n",
      "gradient_weights:  [array([[-1.51991347e-03, -2.33204487e-03, -5.27454618e-04],\n",
      "       [-1.95014796e-03, -3.92645703e-03, -9.14966390e-05]]), array([[-0.00596386],\n",
      "       [-0.00485913],\n",
      "       [ 0.00323688]])]\n",
      "gradients_biases:  [array([-6.33852252e-04, -1.08027329e-03, -9.88549318e-05]), array([0.00590113])]\n",
      "Iteration 484, Cost: 0.24762453505966148\n",
      "gradient_weights:  [array([[-1.52570907e-03, -2.33632827e-03, -5.24508004e-04],\n",
      "       [-1.95775431e-03, -3.93343699e-03, -9.09798513e-05]]), array([[-0.00596294],\n",
      "       [-0.00486164],\n",
      "       [ 0.00323925]])]\n",
      "gradients_biases:  [array([-6.36312701e-04, -1.08193979e-03, -9.83896702e-05]), array([0.00590166])]\n",
      "Iteration 485, Cost: 0.24761786738948363\n",
      "gradient_weights:  [array([[-1.53150369e-03, -2.34061563e-03, -5.21560496e-04],\n",
      "       [-1.96536013e-03, -3.94042087e-03, -9.04627156e-05]]), array([[-0.00596202],\n",
      "       [-0.00486414],\n",
      "       [ 0.00324164]])]\n",
      "gradients_biases:  [array([-6.38771571e-04, -1.08360430e-03, -9.79231131e-05]), array([0.0059022])]\n",
      "Iteration 486, Cost: 0.24761119167662426\n",
      "gradient_weights:  [array([[-1.53729736e-03, -2.34490696e-03, -5.18612093e-04],\n",
      "       [-1.97296545e-03, -3.94740867e-03, -8.99452355e-05]]), array([[-0.00596112],\n",
      "       [-0.00486666],\n",
      "       [ 0.00324403]])]\n",
      "gradients_biases:  [array([-6.41228851e-04, -1.08526681e-03, -9.74552624e-05]), array([0.00590275])]\n",
      "Iteration 487, Cost: 0.2476045078838276\n",
      "gradient_weights:  [array([[-1.54309008e-03, -2.34920225e-03, -5.15662791e-04],\n",
      "       [-1.98057027e-03, -3.95440040e-03, -8.94274148e-05]]), array([[-0.00596023],\n",
      "       [-0.00486918],\n",
      "       [ 0.00324643]])]\n",
      "gradients_biases:  [array([-6.43684529e-04, -1.08692730e-03, -9.69861200e-05]), array([0.00590331])]\n",
      "Iteration 488, Cost: 0.2475978159738195\n",
      "gradient_weights:  [array([[-1.54888188e-03, -2.35350152e-03, -5.12712589e-04],\n",
      "       [-1.98817460e-03, -3.96139606e-03, -8.89092571e-05]]), array([[-0.00595934],\n",
      "       [-0.0048717 ],\n",
      "       [ 0.00324884]])]\n",
      "gradients_biases:  [array([-6.46138594e-04, -1.08858575e-03, -9.65156878e-05]), array([0.00590388])]\n",
      "Iteration 489, Cost: 0.24759111590930794\n",
      "gradient_weights:  [array([[-1.55467276e-03, -2.35780478e-03, -5.09761485e-04],\n",
      "       [-1.99577845e-03, -3.96839564e-03, -8.83907660e-05]]), array([[-0.00595846],\n",
      "       [-0.00487423],\n",
      "       [ 0.00325126]])]\n",
      "gradients_biases:  [array([-6.48591034e-04, -1.09024217e-03, -9.60439677e-05]), array([0.00590446])]\n",
      "Iteration 490, Cost: 0.24758440765298242\n",
      "gradient_weights:  [array([[-1.56046274e-03, -2.36211202e-03, -5.06809476e-04],\n",
      "       [-2.00338184e-03, -3.97539915e-03, -8.78719453e-05]]), array([[-0.00595759],\n",
      "       [-0.00487677],\n",
      "       [ 0.00325369]])]\n",
      "gradients_biases:  [array([-6.51041838e-04, -1.09189652e-03, -9.55709614e-05]), array([0.00590504])]\n",
      "Iteration 491, Cost: 0.247577691167514\n",
      "gradient_weights:  [array([[-1.56625183e-03, -2.36642327e-03, -5.03856561e-04],\n",
      "       [-2.01098477e-03, -3.98240659e-03, -8.73527985e-05]]), array([[-0.00595673],\n",
      "       [-0.00487932],\n",
      "       [ 0.00325613]])]\n",
      "gradients_biases:  [array([-6.53490995e-04, -1.09354880e-03, -9.50966709e-05]), array([0.00590564])]\n",
      "Iteration 492, Cost: 0.24757096641555518\n",
      "gradient_weights:  [array([[-1.57204004e-03, -2.37073851e-03, -5.00902736e-04],\n",
      "       [-2.01858726e-03, -3.98941797e-03, -8.68333296e-05]]), array([[-0.00595588],\n",
      "       [-0.00488187],\n",
      "       [ 0.00325858]])]\n",
      "gradients_biases:  [array([-6.55938492e-04, -1.09519900e-03, -9.46210980e-05]), array([0.00590624])]\n",
      "Iteration 493, Cost: 0.24756423335973982\n",
      "gradient_weights:  [array([[-1.57782740e-03, -2.37505777e-03, -4.97948001e-04],\n",
      "       [-2.02618931e-03, -3.99643327e-03, -8.63135421e-05]]), array([[-0.00595503],\n",
      "       [-0.00488443],\n",
      "       [ 0.00326104]])]\n",
      "gradients_biases:  [array([-6.58384318e-04, -1.09684709e-03, -9.41442447e-05]), array([0.00590685])]\n",
      "Iteration 494, Cost: 0.24755749196268292\n",
      "gradient_weights:  [array([[-1.58361391e-03, -2.37938105e-03, -4.94992353e-04],\n",
      "       [-2.03379093e-03, -4.00345252e-03, -8.57934398e-05]]), array([[-0.0059542 ],\n",
      "       [-0.00488699],\n",
      "       [ 0.00326351]])]\n",
      "gradients_biases:  [array([-6.60828462e-04, -1.09849307e-03, -9.36661127e-05]), array([0.00590747])]\n",
      "Iteration 495, Cost: 0.24755074218698053\n",
      "gradient_weights:  [array([[-1.58939959e-03, -2.38370834e-03, -4.92035791e-04],\n",
      "       [-2.04139215e-03, -4.01047569e-03, -8.52730265e-05]]), array([[-0.00595337],\n",
      "       [-0.00488957],\n",
      "       [ 0.00326599]])]\n",
      "gradients_biases:  [array([-6.63270912e-04, -1.10013693e-03, -9.31867040e-05]), array([0.0059081])]\n",
      "Iteration 496, Cost: 0.2475439839952096\n",
      "gradient_weights:  [array([[-1.59518444e-03, -2.38803968e-03, -4.89078311e-04],\n",
      "       [-2.04899296e-03, -4.01750281e-03, -8.47523060e-05]]), array([[-0.00595255],\n",
      "       [-0.00489214],\n",
      "       [ 0.00326848]])]\n",
      "gradients_biases:  [array([-6.65711657e-04, -1.10177864e-03, -9.27060204e-05]), array([0.00590874])]\n",
      "Iteration 497, Cost: 0.24753721734992812\n",
      "gradient_weights:  [array([[-1.60096850e-03, -2.39237504e-03, -4.86119912e-04],\n",
      "       [-2.05659337e-03, -4.02453386e-03, -8.42312819e-05]]), array([[-0.00595174],\n",
      "       [-0.00489473],\n",
      "       [ 0.00327098]])]\n",
      "gradients_biases:  [array([-6.68150684e-04, -1.10341819e-03, -9.22240637e-05]), array([0.00590938])]\n",
      "Iteration 498, Cost: 0.24753044221367446\n",
      "gradient_weights:  [array([[-1.60675176e-03, -2.39671446e-03, -4.83160593e-04],\n",
      "       [-2.06419341e-03, -4.03156885e-03, -8.37099582e-05]]), array([[-0.00595094],\n",
      "       [-0.00489732],\n",
      "       [ 0.00327348]])]\n",
      "gradients_biases:  [array([-6.70587983e-04, -1.10505558e-03, -9.17408359e-05]), array([0.00591004])]\n",
      "Iteration 499, Cost: 0.24752365854896768\n",
      "gradient_weights:  [array([[-1.61253425e-03, -2.40105792e-03, -4.80200351e-04],\n",
      "       [-2.07179307e-03, -4.03860779e-03, -8.31883387e-05]]), array([[-0.00595015],\n",
      "       [-0.00489992],\n",
      "       [ 0.003276  ]])]\n",
      "gradients_biases:  [array([-6.73023541e-04, -1.10669078e-03, -9.12563389e-05]), array([0.0059107])]\n",
      "Iteration 500, Cost: 0.2475168663183076\n",
      "gradient_weights:  [array([[-1.61831596e-03, -2.40540545e-03, -4.77239184e-04],\n",
      "       [-2.07939237e-03, -4.04565066e-03, -8.26664270e-05]]), array([[-0.00594936],\n",
      "       [-0.00490252],\n",
      "       [ 0.00327853]])]\n",
      "gradients_biases:  [array([-6.75457347e-04, -1.10832378e-03, -9.07705744e-05]), array([0.00591137])]\n",
      "Iteration 501, Cost: 0.2475100654841739\n",
      "gradient_weights:  [array([[-1.62409693e-03, -2.40975704e-03, -4.74277090e-04],\n",
      "       [-2.08699132e-03, -4.05269748e-03, -8.21442272e-05]]), array([[-0.00594858],\n",
      "       [-0.00490513],\n",
      "       [ 0.00328106]])]\n",
      "gradients_biases:  [array([-6.77889389e-04, -1.10995457e-03, -9.02835444e-05]), array([0.00591205])]\n",
      "Iteration 502, Cost: 0.24750325600902687\n",
      "gradient_weights:  [array([[-1.62987717e-03, -2.41411270e-03, -4.71314068e-04],\n",
      "       [-2.09458992e-03, -4.05974824e-03, -8.16217431e-05]]), array([[-0.00594782],\n",
      "       [-0.00490775],\n",
      "       [ 0.00328361]])]\n",
      "gradients_biases:  [array([-6.80319656e-04, -1.11158314e-03, -8.97952507e-05]), array([0.00591274])]\n",
      "Iteration 503, Cost: 0.24749643785530667\n",
      "gradient_weights:  [array([[-1.63565667e-03, -2.41847243e-03, -4.68350116e-04],\n",
      "       [-2.10218820e-03, -4.06680294e-03, -8.10989785e-05]]), array([[-0.00594706],\n",
      "       [-0.00491037],\n",
      "       [ 0.00328616]])]\n",
      "gradients_biases:  [array([-6.82748136e-04, -1.11320946e-03, -8.93056952e-05]), array([0.00591344])]\n",
      "Iteration 504, Cost: 0.24748961098543376\n",
      "gradient_weights:  [array([[-1.64143547e-03, -2.42283626e-03, -4.65385232e-04],\n",
      "       [-2.10978615e-03, -4.07386160e-03, -8.05759373e-05]]), array([[-0.0059463 ],\n",
      "       [-0.004913  ],\n",
      "       [ 0.00328873]])]\n",
      "gradients_biases:  [array([-6.85174817e-04, -1.11483353e-03, -8.88148797e-05]), array([0.00591415])]\n",
      "Iteration 505, Cost: 0.24748277536180813\n",
      "gradient_weights:  [array([[-1.64721357e-03, -2.42720417e-03, -4.62419414e-04],\n",
      "       [-2.11738380e-03, -4.08092419e-03, -8.00526234e-05]]), array([[-0.00594556],\n",
      "       [-0.00491564],\n",
      "       [ 0.0032913 ]])]\n",
      "gradients_biases:  [array([-6.87599687e-04, -1.11645533e-03, -8.83228063e-05]), array([0.00591486])]\n",
      "Iteration 506, Cost: 0.24747593094680992\n",
      "gradient_weights:  [array([[-1.65299099e-03, -2.43157618e-03, -4.59452661e-04],\n",
      "       [-2.12498114e-03, -4.08799074e-03, -7.95290408e-05]]), array([[-0.00594483],\n",
      "       [-0.00491828],\n",
      "       [ 0.00329388]])]\n",
      "gradients_biases:  [array([-6.90022736e-04, -1.11807485e-03, -8.78294766e-05]), array([0.00591559])]\n",
      "Iteration 507, Cost: 0.24746907770279863\n",
      "gradient_weights:  [array([[-1.65876773e-03, -2.43595230e-03, -4.56484970e-04],\n",
      "       [-2.13257819e-03, -4.09506123e-03, -7.90051934e-05]]), array([[-0.0059441 ],\n",
      "       [-0.00492093],\n",
      "       [ 0.00329648]])]\n",
      "gradients_biases:  [array([-6.92443950e-04, -1.11969207e-03, -8.73348926e-05]), array([0.00591632])]\n",
      "Iteration 508, Cost: 0.24746221559211345\n",
      "gradient_weights:  [array([[-1.66454382e-03, -2.44033252e-03, -4.53516340e-04],\n",
      "       [-2.14017496e-03, -4.10213568e-03, -7.84810850e-05]]), array([[-0.00594338],\n",
      "       [-0.00492358],\n",
      "       [ 0.00329908]])]\n",
      "gradients_biases:  [array([-6.94863319e-04, -1.12130698e-03, -8.68390562e-05]), array([0.00591706])]\n",
      "Iteration 509, Cost: 0.24745534457707305\n",
      "gradient_weights:  [array([[-1.67031926e-03, -2.44471687e-03, -4.50546770e-04],\n",
      "       [-2.14777145e-03, -4.10921407e-03, -7.79567198e-05]]), array([[-0.00594267],\n",
      "       [-0.00492624],\n",
      "       [ 0.00330169]])]\n",
      "gradients_biases:  [array([-6.97280831e-04, -1.12291956e-03, -8.63419693e-05]), array([0.00591781])]\n",
      "Iteration 510, Cost: 0.24744846461997544\n",
      "gradient_weights:  [array([[-1.67609407e-03, -2.44910533e-03, -4.47576257e-04],\n",
      "       [-2.15536769e-03, -4.11629641e-03, -7.74321015e-05]]), array([[-0.00594197],\n",
      "       [-0.00492891],\n",
      "       [ 0.00330431]])]\n",
      "gradients_biases:  [array([-6.99696473e-04, -1.12452980e-03, -8.58436337e-05]), array([0.00591857])]\n",
      "Iteration 511, Cost: 0.24744157568309783\n",
      "gradient_weights:  [array([[-1.68186827e-03, -2.45349793e-03, -4.44604801e-04],\n",
      "       [-2.16296367e-03, -4.12338271e-03, -7.69072344e-05]]), array([[-0.00594128],\n",
      "       [-0.00493159],\n",
      "       [ 0.00330694]])]\n",
      "gradients_biases:  [array([-7.02110234e-04, -1.12613768e-03, -8.53440513e-05]), array([0.00591934])]\n",
      "Iteration 512, Cost: 0.2474346777286966\n",
      "gradient_weights:  [array([[-1.68764185e-03, -2.45789466e-03, -4.41632399e-04],\n",
      "       [-2.17055941e-03, -4.13047295e-03, -7.63821222e-05]]), array([[-0.00594059],\n",
      "       [-0.00493427],\n",
      "       [ 0.00330958]])]\n",
      "gradients_biases:  [array([-7.04522103e-04, -1.12774319e-03, -8.48432239e-05]), array([0.00592011])]\n",
      "Iteration 513, Cost: 0.24742777071900707\n",
      "gradient_weights:  [array([[-1.69341485e-03, -2.46229554e-03, -4.38659050e-04],\n",
      "       [-2.17815492e-03, -4.13756715e-03, -7.58567692e-05]]), array([[-0.00593992],\n",
      "       [-0.00493696],\n",
      "       [ 0.00331224]])]\n",
      "gradients_biases:  [array([-7.06932067e-04, -1.12934632e-03, -8.43411536e-05]), array([0.0059209])]\n",
      "Iteration 514, Cost: 0.24742085461624364\n",
      "gradient_weights:  [array([[-1.69918727e-03, -2.46670057e-03, -4.35684752e-04],\n",
      "       [-2.18575021e-03, -4.14466531e-03, -7.53311793e-05]]), array([[-0.00593925],\n",
      "       [-0.00493965],\n",
      "       [ 0.00331489]])]\n",
      "gradients_biases:  [array([-7.09340114e-04, -1.13094705e-03, -8.38378421e-05]), array([0.00592169])]\n",
      "Iteration 515, Cost: 0.24741392938259937\n",
      "gradient_weights:  [array([[-1.70495912e-03, -2.47110975e-03, -4.32709505e-04],\n",
      "       [-2.19334528e-03, -4.15176741e-03, -7.48053565e-05]]), array([[-0.00593859],\n",
      "       [-0.00494235],\n",
      "       [ 0.00331756]])]\n",
      "gradients_biases:  [array([-7.11746233e-04, -1.13254537e-03, -8.33332913e-05]), array([0.00592249])]\n",
      "Iteration 516, Cost: 0.2474069949802462\n",
      "gradient_weights:  [array([[-1.71073042e-03, -2.47552309e-03, -4.29733305e-04],\n",
      "       [-2.20094015e-03, -4.15887347e-03, -7.42793050e-05]]), array([[-0.00593793],\n",
      "       [-0.00494506],\n",
      "       [ 0.00332024]])]\n",
      "gradients_biases:  [array([-7.14150413e-04, -1.13414126e-03, -8.28275031e-05]), array([0.0059233])]\n",
      "Iteration 517, Cost: 0.2474000513713345\n",
      "gradient_weights:  [array([[-1.71650118e-03, -2.47994060e-03, -4.26756153e-04],\n",
      "       [-2.20853483e-03, -4.16598348e-03, -7.37530287e-05]]), array([[-0.00593729],\n",
      "       [-0.00494777],\n",
      "       [ 0.00332293]])]\n",
      "gradients_biases:  [array([-7.16552640e-04, -1.13573470e-03, -8.23204794e-05]), array([0.00592412])]\n",
      "Iteration 518, Cost: 0.24739309851799324\n",
      "gradient_weights:  [array([[-1.72227141e-03, -2.48436229e-03, -4.23778046e-04],\n",
      "       [-2.21612932e-03, -4.17309745e-03, -7.32265319e-05]]), array([[-0.00593666],\n",
      "       [-0.0049505 ],\n",
      "       [ 0.00332563]])]\n",
      "gradients_biases:  [array([-7.18952903e-04, -1.13732568e-03, -8.18122222e-05]), array([0.00592495])]\n",
      "Iteration 519, Cost: 0.2473861363823299\n",
      "gradient_weights:  [array([[-1.72804112e-03, -2.48878816e-03, -4.20798983e-04],\n",
      "       [-2.22372363e-03, -4.18021538e-03, -7.26998187e-05]]), array([[-0.00593603],\n",
      "       [-0.00495322],\n",
      "       [ 0.00332834]])]\n",
      "gradients_biases:  [array([-7.21351190e-04, -1.13891420e-03, -8.13027332e-05]), array([0.00592579])]\n",
      "Iteration 520, Cost: 0.24737916492643014\n",
      "gradient_weights:  [array([[-1.73381034e-03, -2.49321821e-03, -4.17818963e-04],\n",
      "       [-2.23131778e-03, -4.18733726e-03, -7.21728931e-05]]), array([[-0.00593541],\n",
      "       [-0.00495596],\n",
      "       [ 0.00333105]])]\n",
      "gradients_biases:  [array([-7.23747490e-04, -1.14050022e-03, -8.07920143e-05]), array([0.00592663])]\n",
      "Iteration 521, Cost: 0.24737218411235798\n",
      "gradient_weights:  [array([[-1.73957906e-03, -2.49765246e-03, -4.14837984e-04],\n",
      "       [-2.23891178e-03, -4.19446309e-03, -7.16457593e-05]]), array([[-0.0059348 ],\n",
      "       [-0.0049587 ],\n",
      "       [ 0.00333378]])]\n",
      "gradients_biases:  [array([-7.26141790e-04, -1.14208374e-03, -8.02800676e-05]), array([0.00592748])]\n",
      "Iteration 522, Cost: 0.2473651939021555\n",
      "gradient_weights:  [array([[-1.74534731e-03, -2.50209090e-03, -4.11856046e-04],\n",
      "       [-2.24650562e-03, -4.20159288e-03, -7.11184215e-05]]), array([[-0.0059342 ],\n",
      "       [-0.00496144],\n",
      "       [ 0.00333652]])]\n",
      "gradients_biases:  [array([-7.28534079e-04, -1.14366474e-03, -7.97668948e-05]), array([0.00592834])]\n",
      "Iteration 523, Cost: 0.24735819425784264\n",
      "gradient_weights:  [array([[-1.75111510e-03, -2.50653355e-03, -4.08873146e-04],\n",
      "       [-2.25409933e-03, -4.20872663e-03, -7.05908838e-05]]), array([[-0.0059336 ],\n",
      "       [-0.0049642 ],\n",
      "       [ 0.00333926]])]\n",
      "gradients_biases:  [array([-7.30924344e-04, -1.14524321e-03, -7.92524978e-05]), array([0.00592921])]\n",
      "Iteration 524, Cost: 0.24735118514141763\n",
      "gradient_weights:  [array([[-1.75688243e-03, -2.51098042e-03, -4.05889284e-04],\n",
      "       [-2.26169291e-03, -4.21586433e-03, -7.00631505e-05]]), array([[-0.00593302],\n",
      "       [-0.00496696],\n",
      "       [ 0.00334202]])]\n",
      "gradients_biases:  [array([-7.33312573e-04, -1.14681913e-03, -7.87368786e-05]), array([0.00593009])]\n",
      "Iteration 525, Cost: 0.24734416651485625\n",
      "gradient_weights:  [array([[-1.76264932e-03, -2.51543150e-03, -4.02904457e-04],\n",
      "       [-2.26928636e-03, -4.22300599e-03, -6.95352258e-05]]), array([[-0.00593244],\n",
      "       [-0.00496972],\n",
      "       [ 0.00334478]])]\n",
      "gradients_biases:  [array([-7.35698755e-04, -1.14839249e-03, -7.82200391e-05]), array([0.00593098])]\n",
      "Iteration 526, Cost: 0.24733713834011223\n",
      "gradient_weights:  [array([[-1.76841579e-03, -2.51988680e-03, -3.99918666e-04],\n",
      "       [-2.27687971e-03, -4.23015161e-03, -6.90071139e-05]]), array([[-0.00593187],\n",
      "       [-0.00497249],\n",
      "       [ 0.00334756]])]\n",
      "gradients_biases:  [array([-7.38082877e-04, -1.14996327e-03, -7.77019811e-05]), array([0.00593188])]\n",
      "Iteration 527, Cost: 0.2473301005791168\n",
      "gradient_weights:  [array([[-1.77418184e-03, -2.52434634e-03, -3.96931909e-04],\n",
      "       [-2.28447296e-03, -4.23730118e-03, -6.84788190e-05]]), array([[-0.00593131],\n",
      "       [-0.00497527],\n",
      "       [ 0.00335034]])]\n",
      "gradients_biases:  [array([-7.40464928e-04, -1.15153145e-03, -7.71827066e-05]), array([0.00593278])]\n",
      "Iteration 528, Cost: 0.24732305319377879\n",
      "gradient_weights:  [array([[-1.77994750e-03, -2.52881011e-03, -3.93944184e-04],\n",
      "       [-2.29206611e-03, -4.24445471e-03, -6.79503454e-05]]), array([[-0.00593076],\n",
      "       [-0.00497806],\n",
      "       [ 0.00335313]])]\n",
      "gradients_biases:  [array([-7.42844896e-04, -1.15309703e-03, -7.66622174e-05]), array([0.00593369])]\n",
      "Iteration 529, Cost: 0.2473159961459847\n",
      "gradient_weights:  [array([[-1.78571276e-03, -2.53327812e-03, -3.90955491e-04],\n",
      "       [-2.29965918e-03, -4.25161220e-03, -6.74216973e-05]]), array([[-0.00593021],\n",
      "       [-0.00498085],\n",
      "       [ 0.00335594]])]\n",
      "gradients_biases:  [array([-7.45222767e-04, -1.15465998e-03, -7.61405155e-05]), array([0.00593461])]\n",
      "Iteration 530, Cost: 0.2473089293975982\n",
      "gradient_weights:  [array([[-1.79147765e-03, -2.53775038e-03, -3.87965828e-04],\n",
      "       [-2.30725217e-03, -4.25877364e-03, -6.68928792e-05]]), array([[-0.00592967],\n",
      "       [-0.00498365],\n",
      "       [ 0.00335875]])]\n",
      "gradients_biases:  [array([-7.47598531e-04, -1.15622029e-03, -7.56176028e-05]), array([0.00593554])]\n",
      "Iteration 531, Cost: 0.2473018529104603\n",
      "gradient_weights:  [array([[-1.79724217e-03, -2.54222689e-03, -3.84975195e-04],\n",
      "       [-2.31484510e-03, -4.26593904e-03, -6.63638952e-05]]), array([[-0.00592915],\n",
      "       [-0.00498646],\n",
      "       [ 0.00336157]])]\n",
      "gradients_biases:  [array([-7.49972175e-04, -1.15777795e-03, -7.50934812e-05]), array([0.00593648])]\n",
      "Iteration 532, Cost: 0.24729476664638927\n",
      "gradient_weights:  [array([[-1.80300634e-03, -2.54670766e-03, -3.81983590e-04],\n",
      "       [-2.32243797e-03, -4.27310839e-03, -6.58347498e-05]]), array([[-0.00592863],\n",
      "       [-0.00498927],\n",
      "       [ 0.0033644 ]])]\n",
      "gradients_biases:  [array([-7.52343688e-04, -1.15933294e-03, -7.45681526e-05]), array([0.00593743])]\n",
      "Iteration 533, Cost: 0.24728767056718068\n",
      "gradient_weights:  [array([[-1.80877017e-03, -2.55119270e-03, -3.78991012e-04],\n",
      "       [-2.33003079e-03, -4.28028170e-03, -6.53054471e-05]]), array([[-0.00592811],\n",
      "       [-0.00499209],\n",
      "       [ 0.00336724]])]\n",
      "gradients_biases:  [array([-7.54713056e-04, -1.16088524e-03, -7.40416189e-05]), array([0.00593839])]\n",
      "Iteration 534, Cost: 0.24728056463460676\n",
      "gradient_weights:  [array([[-1.81453367e-03, -2.55568201e-03, -3.75997461e-04],\n",
      "       [-2.33762357e-03, -4.28745897e-03, -6.47759917e-05]]), array([[-0.00592761],\n",
      "       [-0.00499491],\n",
      "       [ 0.00337009]])]\n",
      "gradients_biases:  [array([-7.57080268e-04, -1.16243484e-03, -7.35138821e-05]), array([0.00593935])]\n",
      "Iteration 535, Cost: 0.24727344881041707\n",
      "gradient_weights:  [array([[-1.82029686e-03, -2.56017560e-03, -3.73002934e-04],\n",
      "       [-2.34521632e-03, -4.29464019e-03, -6.42463879e-05]]), array([[-0.00592711],\n",
      "       [-0.00499774],\n",
      "       [ 0.00337295]])]\n",
      "gradients_biases:  [array([-7.59445312e-04, -1.16398173e-03, -7.29849440e-05]), array([0.00594032])]\n",
      "Iteration 536, Cost: 0.24726632305633792\n",
      "gradient_weights:  [array([[-1.82605974e-03, -2.56467347e-03, -3.70007432e-04],\n",
      "       [-2.35280905e-03, -4.30182537e-03, -6.37166401e-05]]), array([[-0.00592663],\n",
      "       [-0.00500058],\n",
      "       [ 0.00337582]])]\n",
      "gradients_biases:  [array([-7.61808175e-04, -1.16552588e-03, -7.24548067e-05]), array([0.0059413])]\n",
      "Iteration 537, Cost: 0.24725918733407248\n",
      "gradient_weights:  [array([[-1.83182233e-03, -2.56917563e-03, -3.67010954e-04],\n",
      "       [-2.36040177e-03, -4.30901450e-03, -6.31867527e-05]]), array([[-0.00592615],\n",
      "       [-0.00500343],\n",
      "       [ 0.0033787 ]])]\n",
      "gradients_biases:  [array([-7.64168846e-04, -1.16706729e-03, -7.19234719e-05]), array([0.00594229])]\n",
      "Iteration 538, Cost: 0.2472520416053004\n",
      "gradient_weights:  [array([[-1.83758464e-03, -2.57368208e-03, -3.64013498e-04],\n",
      "       [-2.36799448e-03, -4.31620758e-03, -6.26567300e-05]]), array([[-0.00592567],\n",
      "       [-0.00500628],\n",
      "       [ 0.00338159]])]\n",
      "gradients_biases:  [array([-7.66527312e-04, -1.16860593e-03, -7.13909417e-05]), array([0.00594329])]\n",
      "Iteration 539, Cost: 0.2472448858316783\n",
      "gradient_weights:  [array([[-1.84334669e-03, -2.57819283e-03, -3.61015064e-04],\n",
      "       [-2.37558719e-03, -4.32340462e-03, -6.21265767e-05]]), array([[-0.00592521],\n",
      "       [-0.00500914],\n",
      "       [ 0.00338449]])]\n",
      "gradients_biases:  [array([-7.68883561e-04, -1.17014179e-03, -7.08572180e-05]), array([0.0059443])]\n",
      "Iteration 540, Cost: 0.24723771997483923\n",
      "gradient_weights:  [array([[-1.84910848e-03, -2.58270789e-03, -3.58015651e-04],\n",
      "       [-2.38317991e-03, -4.33060561e-03, -6.15962971e-05]]), array([[-0.00592476],\n",
      "       [-0.005012  ],\n",
      "       [ 0.0033874 ]])]\n",
      "gradients_biases:  [array([-7.71237580e-04, -1.17167486e-03, -7.03223027e-05]), array([0.00594531])]\n",
      "Iteration 541, Cost: 0.24723054399639272\n",
      "gradient_weights:  [array([[-1.85487002e-03, -2.58722727e-03, -3.55015258e-04],\n",
      "       [-2.39077265e-03, -4.33781055e-03, -6.10658957e-05]]), array([[-0.00592431],\n",
      "       [-0.00501487],\n",
      "       [ 0.00339032]])]\n",
      "gradients_biases:  [array([-7.73589358e-04, -1.17320512e-03, -6.97861977e-05]), array([0.00594633])]\n",
      "Iteration 542, Cost: 0.24722335785792468\n",
      "gradient_weights:  [array([[-1.86063134e-03, -2.59175095e-03, -3.52013884e-04],\n",
      "       [-2.39836543e-03, -4.34501945e-03, -6.05353770e-05]]), array([[-0.00592387],\n",
      "       [-0.00501775],\n",
      "       [ 0.00339324]])]\n",
      "gradients_biases:  [array([-7.75938883e-04, -1.17473255e-03, -6.92489050e-05]), array([0.00594737])]\n",
      "Iteration 543, Cost: 0.24721616152099735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient_weights:  [array([[-1.86639243e-03, -2.59627897e-03, -3.49011528e-04],\n",
      "       [-2.40595823e-03, -4.35223229e-03, -6.00047455e-05]]), array([[-0.00592344],\n",
      "       [-0.00502063],\n",
      "       [ 0.00339618]])]\n",
      "gradients_biases:  [array([-7.78286141e-04, -1.17625714e-03, -6.87104266e-05]), array([0.00594841])]\n",
      "Iteration 544, Cost: 0.2472089549471494\n",
      "gradient_weights:  [array([[-1.87215332e-03, -2.60081131e-03, -3.46008191e-04],\n",
      "       [-2.41355109e-03, -4.35944909e-03, -5.94740057e-05]]), array([[-0.00592301],\n",
      "       [-0.00502352],\n",
      "       [ 0.00339913]])]\n",
      "gradients_biases:  [array([-7.80631121e-04, -1.17777887e-03, -6.81707643e-05]), array([0.00594945])]\n",
      "Iteration 545, Cost: 0.24720173809789553\n",
      "gradient_weights:  [array([[-1.87791400e-03, -2.60534798e-03, -3.43003871e-04],\n",
      "       [-2.42114399e-03, -4.36666984e-03, -5.89431623e-05]]), array([[-0.0059226 ],\n",
      "       [-0.00502642],\n",
      "       [ 0.00340208]])]\n",
      "gradients_biases:  [array([-7.82973810e-04, -1.17929773e-03, -6.76299202e-05]), array([0.00595051])]\n",
      "Iteration 546, Cost: 0.24719451093472658\n",
      "gradient_weights:  [array([[-1.88367451e-03, -2.60988900e-03, -3.39998567e-04],\n",
      "       [-2.42873696e-03, -4.37389453e-03, -5.84122197e-05]]), array([[-0.00592219],\n",
      "       [-0.00502932],\n",
      "       [ 0.00340505]])]\n",
      "gradients_biases:  [array([-7.85314196e-04, -1.18081369e-03, -6.70878962e-05]), array([0.00595158])]\n",
      "Iteration 547, Cost: 0.24718727341910945\n",
      "gradient_weights:  [array([[-1.88943484e-03, -2.61443436e-03, -3.36992279e-04],\n",
      "       [-2.43632999e-03, -4.38112317e-03, -5.78811826e-05]]), array([[-0.00592179],\n",
      "       [-0.00503223],\n",
      "       [ 0.00340802]])]\n",
      "gradients_biases:  [array([-7.87652267e-04, -1.18232675e-03, -6.65446941e-05]), array([0.00595265])]\n",
      "Iteration 548, Cost: 0.247180025512487\n",
      "gradient_weights:  [array([[-1.89519501e-03, -2.61898407e-03, -3.33985007e-04],\n",
      "       [-2.44392311e-03, -4.38835576e-03, -5.73500555e-05]]), array([[-0.0059214 ],\n",
      "       [-0.00503515],\n",
      "       [ 0.003411  ]])]\n",
      "gradients_biases:  [array([-7.89988010e-04, -1.18383689e-03, -6.60003161e-05]), array([0.00595373])]\n",
      "Iteration 549, Cost: 0.24717276717627806\n",
      "gradient_weights:  [array([[-1.90095503e-03, -2.62353814e-03, -3.30976749e-04],\n",
      "       [-2.45151630e-03, -4.39559230e-03, -5.68188431e-05]]), array([[-0.00592102],\n",
      "       [-0.00503808],\n",
      "       [ 0.003414  ]])]\n",
      "gradients_biases:  [array([-7.92321413e-04, -1.18534410e-03, -6.54547641e-05]), array([0.00595482])]\n",
      "Iteration 550, Cost: 0.24716549837187723\n",
      "gradient_weights:  [array([[-1.90671491e-03, -2.62809657e-03, -3.27967505e-04],\n",
      "       [-2.45910960e-03, -4.40283278e-03, -5.62875500e-05]]), array([[-0.00592065],\n",
      "       [-0.00504101],\n",
      "       [ 0.003417  ]])]\n",
      "gradients_biases:  [array([-7.94652463e-04, -1.18684834e-03, -6.49080399e-05]), array([0.00595592])]\n",
      "Iteration 551, Cost: 0.247158219060655\n",
      "gradient_weights:  [array([[-1.91247466e-03, -2.63265936e-03, -3.24957276e-04],\n",
      "       [-2.46670299e-03, -4.41007721e-03, -5.57561809e-05]]), array([[-0.00592028],\n",
      "       [-0.00504394],\n",
      "       [ 0.00342001]])]\n",
      "gradients_biases:  [array([-7.96981148e-04, -1.18834962e-03, -6.43601457e-05]), array([0.00595703])]\n",
      "Iteration 552, Cost: 0.24715092920395731\n",
      "gradient_weights:  [array([[-1.91823430e-03, -2.63722653e-03, -3.21946059e-04],\n",
      "       [-2.47429649e-03, -4.41732558e-03, -5.52247404e-05]]), array([[-0.00591992],\n",
      "       [-0.00504689],\n",
      "       [ 0.00342304]])]\n",
      "gradients_biases:  [array([-7.99307456e-04, -1.18984791e-03, -6.38110833e-05]), array([0.00595814])]\n",
      "Iteration 553, Cost: 0.24714362876310608\n",
      "gradient_weights:  [array([[-1.92399384e-03, -2.64179808e-03, -3.18933855e-04],\n",
      "       [-2.48189011e-03, -4.42457789e-03, -5.46932332e-05]]), array([[-0.00591957],\n",
      "       [-0.00504984],\n",
      "       [ 0.00342607]])]\n",
      "gradients_biases:  [array([-8.01631374e-04, -1.19134320e-03, -6.32608547e-05]), array([0.00595927])]\n",
      "Iteration 554, Cost: 0.24713631769939845\n",
      "gradient_weights:  [array([[-1.92975328e-03, -2.64637401e-03, -3.15920664e-04],\n",
      "       [-2.48948385e-03, -4.43183415e-03, -5.41616641e-05]]), array([[-0.00591923],\n",
      "       [-0.00505279],\n",
      "       [ 0.00342911]])]\n",
      "gradients_biases:  [array([-8.03952890e-04, -1.19283546e-03, -6.27094620e-05]), array([0.0059604])]\n",
      "Iteration 555, Cost: 0.24712899597410737\n",
      "gradient_weights:  [array([[-1.93551264e-03, -2.65095433e-03, -3.12906484e-04],\n",
      "       [-2.49707772e-03, -4.43909434e-03, -5.36300377e-05]]), array([[-0.00591889],\n",
      "       [-0.00505575],\n",
      "       [ 0.00343216]])]\n",
      "gradients_biases:  [array([-8.06271991e-04, -1.19432470e-03, -6.21569070e-05]), array([0.00596154])]\n",
      "Iteration 556, Cost: 0.24712166354848114\n",
      "gradient_weights:  [array([[-1.94127193e-03, -2.65553904e-03, -3.09891316e-04],\n",
      "       [-2.50467174e-03, -4.44635847e-03, -5.30983589e-05]]), array([[-0.00591856],\n",
      "       [-0.00505872],\n",
      "       [ 0.00343522]])]\n",
      "gradients_biases:  [array([-8.08588665e-04, -1.19581088e-03, -6.16031919e-05]), array([0.00596269])]\n",
      "Iteration 557, Cost: 0.24711432038374348\n",
      "gradient_weights:  [array([[-1.94703116e-03, -2.66012815e-03, -3.06875160e-04],\n",
      "       [-2.51226590e-03, -4.45362654e-03, -5.25666323e-05]]), array([[-0.00591825],\n",
      "       [-0.0050617 ],\n",
      "       [ 0.00343829]])]\n",
      "gradients_biases:  [array([-8.10902899e-04, -1.19729399e-03, -6.10483185e-05]), array([0.00596385])]\n",
      "Iteration 558, Cost: 0.24710696644109317\n",
      "gradient_weights:  [array([[-1.95279034e-03, -2.66472167e-03, -3.03858014e-04],\n",
      "       [-2.51986022e-03, -4.46089855e-03, -5.20348627e-05]]), array([[-0.00591794],\n",
      "       [-0.00506468],\n",
      "       [ 0.00344137]])]\n",
      "gradients_biases:  [array([-8.13214681e-04, -1.19877402e-03, -6.04922888e-05]), array([0.00596502])]\n",
      "Iteration 559, Cost: 0.24709960168170483\n",
      "gradient_weights:  [array([[-1.95854949e-03, -2.66931960e-03, -3.00839879e-04],\n",
      "       [-2.52745470e-03, -4.46817449e-03, -5.15030550e-05]]), array([[-0.00591763],\n",
      "       [-0.00506767],\n",
      "       [ 0.00344446]])]\n",
      "gradients_biases:  [array([-8.15523998e-04, -1.20025094e-03, -5.99351049e-05]), array([0.00596619])]\n",
      "Iteration 560, Cost: 0.2470922260667277\n",
      "gradient_weights:  [array([[-1.96430861e-03, -2.67392194e-03, -2.97820755e-04],\n",
      "       [-2.53504935e-03, -4.47545436e-03, -5.09712140e-05]]), array([[-0.00591734],\n",
      "       [-0.00507067],\n",
      "       [ 0.00344756]])]\n",
      "gradients_biases:  [array([-8.17830837e-04, -1.20172475e-03, -5.93767688e-05]), array([0.00596737])]\n",
      "Iteration 561, Cost: 0.24708483955728652\n",
      "gradient_weights:  [array([[-1.97006772e-03, -2.67852870e-03, -2.94800640e-04],\n",
      "       [-2.54264417e-03, -4.48273817e-03, -5.04393445e-05]]), array([[-0.00591705],\n",
      "       [-0.00507367],\n",
      "       [ 0.00345066]])]\n",
      "gradients_biases:  [array([-8.20135186e-04, -1.20319543e-03, -5.88172824e-05]), array([0.00596856])]\n",
      "Iteration 562, Cost: 0.24707744211448113\n",
      "gradient_weights:  [array([[-1.97582682e-03, -2.68313988e-03, -2.91779536e-04],\n",
      "       [-2.55023919e-03, -4.49002590e-03, -4.99074514e-05]]), array([[-0.00591677],\n",
      "       [-0.00507668],\n",
      "       [ 0.00345378]])]\n",
      "gradients_biases:  [array([-8.22437032e-04, -1.20466295e-03, -5.82566479e-05]), array([0.00596976])]\n",
      "Iteration 563, Cost: 0.2470700336993863\n",
      "gradient_weights:  [array([[-1.98158593e-03, -2.68775550e-03, -2.88757442e-04],\n",
      "       [-2.55783439e-03, -4.49731757e-03, -4.93755395e-05]]), array([[-0.0059165 ],\n",
      "       [-0.0050797 ],\n",
      "       [ 0.00345691]])]\n",
      "gradients_biases:  [array([-8.24736363e-04, -1.20612731e-03, -5.76948670e-05]), array([0.00597097])]\n",
      "Iteration 564, Cost: 0.24706261427305182\n",
      "gradient_weights:  [array([[-1.98734506e-03, -2.69237555e-03, -2.85734358e-04],\n",
      "       [-2.56542979e-03, -4.50461316e-03, -4.88436137e-05]]), array([[-0.00591624],\n",
      "       [-0.00508272],\n",
      "       [ 0.00346005]])]\n",
      "gradients_biases:  [array([-8.27033166e-04, -1.20758848e-03, -5.71319420e-05]), array([0.00597219])]\n",
      "Iteration 565, Cost: 0.24705518379650238\n",
      "gradient_weights:  [array([[-1.99310422e-03, -2.69700004e-03, -2.82710283e-04],\n",
      "       [-2.57302540e-03, -4.51191268e-03, -4.83116790e-05]]), array([[-0.00591598],\n",
      "       [-0.00508575],\n",
      "       [ 0.00346319]])]\n",
      "gradients_biases:  [array([-8.29327428e-04, -1.20904645e-03, -5.65678748e-05]), array([0.00597341])]\n",
      "Iteration 566, Cost: 0.24704774223073772\n",
      "gradient_weights:  [array([[-1.99886342e-03, -2.70162898e-03, -2.79685218e-04],\n",
      "       [-2.58062123e-03, -4.51921612e-03, -4.77797402e-05]]), array([[-0.00591573],\n",
      "       [-0.00508878],\n",
      "       [ 0.00346635]])]\n",
      "gradients_biases:  [array([-8.31619137e-04, -1.21050120e-03, -5.60026675e-05]), array([0.00597465])]\n",
      "Iteration 567, Cost: 0.2470402895367323\n",
      "gradient_weights:  [array([[-2.00462267e-03, -2.70626237e-03, -2.76659163e-04],\n",
      "       [-2.58821727e-03, -4.52652348e-03, -4.72478023e-05]]), array([[-0.0059155 ],\n",
      "       [-0.00509182],\n",
      "       [ 0.00346951]])]\n",
      "gradients_biases:  [array([-8.33908280e-04, -1.21195272e-03, -5.54363220e-05]), array([0.00597589])]\n",
      "Iteration 568, Cost: 0.24703282567543527\n",
      "gradient_weights:  [array([[-2.01038198e-03, -2.71090021e-03, -2.73632117e-04],\n",
      "       [-2.59581355e-03, -4.53383477e-03, -4.67158704e-05]]), array([[-0.00591526],\n",
      "       [-0.00509487],\n",
      "       [ 0.00347269]])]\n",
      "gradients_biases:  [array([-8.36194844e-04, -1.21340098e-03, -5.48688404e-05]), array([0.00597714])]\n",
      "Iteration 569, Cost: 0.24702535060777092\n",
      "gradient_weights:  [array([[-2.01614136e-03, -2.71554251e-03, -2.70604081e-04],\n",
      "       [-2.60341006e-03, -4.54114997e-03, -4.61839493e-05]]), array([[-0.00591504],\n",
      "       [-0.00509793],\n",
      "       [ 0.00347587]])]\n",
      "gradients_biases:  [array([-8.38478817e-04, -1.21484598e-03, -5.43002247e-05]), array([0.00597839])]\n",
      "Iteration 570, Cost: 0.24701786429463762\n",
      "gradient_weights:  [array([[-2.02190083e-03, -2.72018928e-03, -2.67575055e-04],\n",
      "       [-2.61100681e-03, -4.54846909e-03, -4.56520441e-05]]), array([[-0.00591482],\n",
      "       [-0.00510099],\n",
      "       [ 0.00347906]])]\n",
      "gradients_biases:  [array([-8.40760185e-04, -1.21628769e-03, -5.37304771e-05]), array([0.00597966])]\n",
      "Iteration 571, Cost: 0.24701036669690904\n",
      "gradient_weights:  [array([[-2.02766039e-03, -2.72484051e-03, -2.64545038e-04],\n",
      "       [-2.61860381e-03, -4.55579213e-03, -4.51201598e-05]]), array([[-0.00591462],\n",
      "       [-0.00510406],\n",
      "       [ 0.00348227]])]\n",
      "gradients_biases:  [array([-8.43038936e-04, -1.21772610e-03, -5.31595994e-05]), array([0.00598094])]\n",
      "Iteration 572, Cost: 0.2470028577754329\n",
      "gradient_weights:  [array([[-2.03342006e-03, -2.72949622e-03, -2.61514032e-04],\n",
      "       [-2.62620107e-03, -4.56311908e-03, -4.45883015e-05]]), array([[-0.00591442],\n",
      "       [-0.00510713],\n",
      "       [ 0.00348548]])]\n",
      "gradients_biases:  [array([-8.45315057e-04, -1.21916119e-03, -5.25875939e-05]), array([0.00598222])]\n",
      "Iteration 573, Cost: 0.24699533749103192\n",
      "gradient_weights:  [array([[-2.03917984e-03, -2.73415641e-03, -2.58482036e-04],\n",
      "       [-2.63379858e-03, -4.57044994e-03, -4.40564742e-05]]), array([[-0.00591422],\n",
      "       [-0.00511021],\n",
      "       [ 0.0034887 ]])]\n",
      "gradients_biases:  [array([-8.47588535e-04, -1.22059294e-03, -5.20144624e-05]), array([0.00598351])]\n",
      "Iteration 574, Cost: 0.24698780580450297\n",
      "gradient_weights:  [array([[-2.04493975e-03, -2.73882109e-03, -2.55449050e-04],\n",
      "       [-2.64139637e-03, -4.57778470e-03, -4.35246830e-05]]), array([[-0.00591404],\n",
      "       [-0.0051133 ],\n",
      "       [ 0.00349193]])]\n",
      "gradients_biases:  [array([-8.49859358e-04, -1.22202133e-03, -5.14402072e-05]), array([0.00598481])]\n",
      "Iteration 575, Cost: 0.2469802626766177\n",
      "gradient_weights:  [array([[-2.05069979e-03, -2.74349025e-03, -2.52415074e-04],\n",
      "       [-2.64899444e-03, -4.58512338e-03, -4.29929330e-05]]), array([[-0.00591386],\n",
      "       [-0.0051164 ],\n",
      "       [ 0.00349517]])]\n",
      "gradients_biases:  [array([-8.52127513e-04, -1.22344636e-03, -5.08648302e-05]), array([0.00598612])]\n",
      "Iteration 576, Cost: 0.24697270806812202\n",
      "gradient_weights:  [array([[-2.05645998e-03, -2.74816391e-03, -2.49380110e-04],\n",
      "       [-2.65659279e-03, -4.59246596e-03, -4.24612293e-05]]), array([[-0.0059137 ],\n",
      "       [-0.0051195 ],\n",
      "       [ 0.00349842]])]\n",
      "gradients_biases:  [array([-8.54392986e-04, -1.22486800e-03, -5.02883335e-05]), array([0.00598743])]\n",
      "Iteration 577, Cost: 0.2469651419397363\n",
      "gradient_weights:  [array([[-2.06222033e-03, -2.75284206e-03, -2.46344157e-04],\n",
      "       [-2.66419143e-03, -4.59981244e-03, -4.19295771e-05]]), array([[-0.00591353],\n",
      "       [-0.00512261],\n",
      "       [ 0.00350169]])]\n",
      "gradients_biases:  [array([-8.56655765e-04, -1.22628623e-03, -4.97107192e-05]), array([0.00598876])]\n",
      "Iteration 578, Cost: 0.24695756425215526\n",
      "gradient_weights:  [array([[-2.06798084e-03, -2.75752472e-03, -2.43307215e-04],\n",
      "       [-2.67179036e-03, -4.60716283e-03, -4.13979815e-05]]), array([[-0.00591338],\n",
      "       [-0.00512572],\n",
      "       [ 0.00350495]])]\n",
      "gradients_biases:  [array([-8.58915837e-04, -1.22770104e-03, -4.91319894e-05]), array([0.00599009])]\n",
      "Iteration 579, Cost: 0.24694997496604784\n",
      "gradient_weights:  [array([[-2.07374153e-03, -2.76221189e-03, -2.40269284e-04],\n",
      "       [-2.67938960e-03, -4.61451711e-03, -4.08664477e-05]]), array([[-0.00591324],\n",
      "       [-0.00512884],\n",
      "       [ 0.00350823]])]\n",
      "gradients_biases:  [array([-8.61173189e-04, -1.22911241e-03, -4.85521461e-05]), array([0.00599143])]\n",
      "Iteration 580, Cost: 0.24694237404205743\n",
      "gradient_weights:  [array([[-2.07950241e-03, -2.76690357e-03, -2.37230366e-04],\n",
      "       [-2.68698915e-03, -4.62187529e-03, -4.03349809e-05]]), array([[-0.0059131 ],\n",
      "       [-0.00513197],\n",
      "       [ 0.00351152]])]\n",
      "gradients_biases:  [array([-8.63427808e-04, -1.23052033e-03, -4.79711915e-05]), array([0.00599278])]\n",
      "Iteration 581, Cost: 0.24693476144080156\n",
      "gradient_weights:  [array([[-2.08526349e-03, -2.77159976e-03, -2.34190461e-04],\n",
      "       [-2.69458901e-03, -4.62923736e-03, -3.98035864e-05]]), array([[-0.00591297],\n",
      "       [-0.0051351 ],\n",
      "       [ 0.00351482]])]\n",
      "gradients_biases:  [array([-8.65679680e-04, -1.23192476e-03, -4.73891276e-05]), array([0.00599414])]\n",
      "Iteration 582, Cost: 0.24692713712287204\n",
      "gradient_weights:  [array([[-2.09102478e-03, -2.77630048e-03, -2.31149568e-04],\n",
      "       [-2.70218920e-03, -4.63660332e-03, -3.92722693e-05]]), array([[-0.00591285],\n",
      "       [-0.00513825],\n",
      "       [ 0.00351813]])]\n",
      "gradients_biases:  [array([-8.67928794e-04, -1.23332571e-03, -4.68059565e-05]), array([0.0059955])]\n",
      "Iteration 583, Cost: 0.2469195010488348\n",
      "gradient_weights:  [array([[-2.09678628e-03, -2.78100572e-03, -2.28107690e-04],\n",
      "       [-2.70978971e-03, -4.64397317e-03, -3.87410350e-05]]), array([[-0.00591273],\n",
      "       [-0.00514139],\n",
      "       [ 0.00352145]])]\n",
      "gradients_biases:  [array([-8.70175136e-04, -1.23472314e-03, -4.62216804e-05]), array([0.00599688])]\n",
      "Iteration 584, Cost: 0.24691185317922984\n",
      "gradient_weights:  [array([[-2.10254801e-03, -2.78571549e-03, -2.25064825e-04],\n",
      "       [-2.71739056e-03, -4.65134691e-03, -3.82098887e-05]]), array([[-0.00591263],\n",
      "       [-0.00514455],\n",
      "       [ 0.00352477]])]\n",
      "gradients_biases:  [array([-8.72418693e-04, -1.23611705e-03, -4.56363013e-05]), array([0.00599826])]\n",
      "Iteration 585, Cost: 0.24690419347457143\n",
      "gradient_weights:  [array([[-2.10830998e-03, -2.79042979e-03, -2.22020974e-04],\n",
      "       [-2.72499175e-03, -4.65872453e-03, -3.76788356e-05]]), array([[-0.00591253],\n",
      "       [-0.00514771],\n",
      "       [ 0.00352811]])]\n",
      "gradients_biases:  [array([-8.74659452e-04, -1.23750741e-03, -4.50498213e-05]), array([0.00599965])]\n",
      "Iteration 586, Cost: 0.246896521895348\n",
      "gradient_weights:  [array([[-2.11407220e-03, -2.79514864e-03, -2.18976139e-04],\n",
      "       [-2.73259329e-03, -4.66610603e-03, -3.71478812e-05]]), array([[-0.00591244],\n",
      "       [-0.00515088],\n",
      "       [ 0.00353145]])]\n",
      "gradients_biases:  [array([-8.76897400e-04, -1.23889421e-03, -4.44622427e-05]), array([0.00600105])]\n",
      "Iteration 587, Cost: 0.24688883840202172\n",
      "gradient_weights:  [array([[-2.11983468e-03, -2.79987203e-03, -2.15930320e-04],\n",
      "       [-2.74019518e-03, -4.67349141e-03, -3.66170308e-05]]), array([[-0.00591236],\n",
      "       [-0.00515405],\n",
      "       [ 0.00353481]])]\n",
      "gradients_biases:  [array([-8.79132524e-04, -1.24027743e-03, -4.38735674e-05]), array([0.00600245])]\n",
      "Iteration 588, Cost: 0.24688114295502914\n",
      "gradient_weights:  [array([[-2.12559743e-03, -2.80459996e-03, -2.12883517e-04],\n",
      "       [-2.74779743e-03, -4.68088066e-03, -3.60862897e-05]]), array([[-0.00591228],\n",
      "       [-0.00515723],\n",
      "       [ 0.00353817]])]\n",
      "gradients_biases:  [array([-8.81364810e-04, -1.24165705e-03, -4.32837976e-05]), array([0.00600387])]\n",
      "Iteration 589, Cost: 0.2468734355147805\n",
      "gradient_weights:  [array([[-2.13136045e-03, -2.80933245e-03, -2.09835731e-04],\n",
      "       [-2.75540005e-03, -4.68827379e-03, -3.55556633e-05]]), array([[-0.00591221],\n",
      "       [-0.00516042],\n",
      "       [ 0.00354155]])]\n",
      "gradients_biases:  [array([-8.83594246e-04, -1.24303306e-03, -4.26929355e-05]), array([0.00600529])]\n",
      "Iteration 590, Cost: 0.24686571604166027\n",
      "gradient_weights:  [array([[-2.13712377e-03, -2.81406949e-03, -2.06786962e-04],\n",
      "       [-2.76300304e-03, -4.69567079e-03, -3.50251571e-05]]), array([[-0.00591215],\n",
      "       [-0.00516361],\n",
      "       [ 0.00354493]])]\n",
      "gradients_biases:  [array([-8.85820818e-04, -1.24440543e-03, -4.21009832e-05]), array([0.00600672])]\n",
      "Iteration 591, Cost: 0.24685798449602675\n",
      "gradient_weights:  [array([[-2.14288738e-03, -2.81881110e-03, -2.03737213e-04],\n",
      "       [-2.77060641e-03, -4.70307165e-03, -3.44947763e-05]]), array([[-0.0059121 ],\n",
      "       [-0.00516681],\n",
      "       [ 0.00354832]])]\n",
      "gradients_biases:  [array([-8.88044513e-04, -1.24577415e-03, -4.15079428e-05]), array([0.00600816])]\n",
      "Iteration 592, Cost: 0.24685024083821222\n",
      "gradient_weights:  [array([[-2.14865129e-03, -2.82355726e-03, -2.00686482e-04],\n",
      "       [-2.77821016e-03, -4.71047638e-03, -3.39645266e-05]]), array([[-0.00591206],\n",
      "       [-0.00517002],\n",
      "       [ 0.00355173]])]\n",
      "gradients_biases:  [array([-8.90265318e-04, -1.24713920e-03, -4.09138165e-05]), array([0.00600961])]\n",
      "Iteration 593, Cost: 0.24684248502852268\n",
      "gradient_weights:  [array([[-2.15441553e-03, -2.82830800e-03, -1.97634772e-04],\n",
      "       [-2.78581430e-03, -4.71788497e-03, -3.34344132e-05]]), array([[-0.00591202],\n",
      "       [-0.00517323],\n",
      "       [ 0.00355514]])]\n",
      "gradients_biases:  [array([-8.92483221e-04, -1.24850056e-03, -4.03186064e-05]), array([0.00601107])]\n",
      "Iteration 594, Cost: 0.24683471702723842\n",
      "gradient_weights:  [array([[-2.16018009e-03, -2.83306331e-03, -1.94582083e-04],\n",
      "       [-2.79341884e-03, -4.72529742e-03, -3.29044418e-05]]), array([[-0.00591199],\n",
      "       [-0.00517645],\n",
      "       [ 0.00355856]])]\n",
      "gradients_biases:  [array([-8.94698206e-04, -1.24985822e-03, -3.97223148e-05]), array([0.00601253])]\n",
      "Iteration 595, Cost: 0.24682693679461318\n",
      "gradient_weights:  [array([[-2.16594499e-03, -2.83782320e-03, -1.91528415e-04],\n",
      "       [-2.80102377e-03, -4.73271373e-03, -3.23746179e-05]]), array([[-0.00591197],\n",
      "       [-0.00517968],\n",
      "       [ 0.00356199]])]\n",
      "gradients_biases:  [array([-8.96910263e-04, -1.25121215e-03, -3.91249437e-05]), array([0.006014])]\n",
      "Iteration 596, Cost: 0.24681914429087481\n",
      "gradient_weights:  [array([[-2.17171024e-03, -2.84258767e-03, -1.88473770e-04],\n",
      "       [-2.80862912e-03, -4.74013389e-03, -3.18449468e-05]]), array([[-0.00591196],\n",
      "       [-0.00518291],\n",
      "       [ 0.00356543]])]\n",
      "gradients_biases:  [array([-8.99119376e-04, -1.25256234e-03, -3.85264954e-05]), array([0.00601548])]\n",
      "Iteration 597, Cost: 0.2468113394762248\n",
      "gradient_weights:  [array([[-2.17747584e-03, -2.84735672e-03, -1.85418149e-04],\n",
      "       [-2.81623488e-03, -4.74755789e-03, -3.13154343e-05]]), array([[-0.00591195],\n",
      "       [-0.00518616],\n",
      "       [ 0.00356888]])]\n",
      "gradients_biases:  [array([-9.01325533e-04, -1.25390878e-03, -3.79269719e-05]), array([0.00601697])]\n",
      "Iteration 598, Cost: 0.2468035223108387\n",
      "gradient_weights:  [array([[-2.18324181e-03, -2.85213036e-03, -1.82361553e-04],\n",
      "       [-2.82384105e-03, -4.75498574e-03, -3.07860858e-05]]), array([[-0.00591196],\n",
      "       [-0.0051894 ],\n",
      "       [ 0.00357234]])]\n",
      "gradients_biases:  [array([-9.03528720e-04, -1.25525143e-03, -3.73263756e-05]), array([0.00601847])]\n",
      "Iteration 599, Cost: 0.24679569275486557\n",
      "gradient_weights:  [array([[-2.18900815e-03, -2.85690860e-03, -1.79303982e-04],\n",
      "       [-2.83144765e-03, -4.76241744e-03, -3.02569070e-05]]), array([[-0.00591197],\n",
      "       [-0.00519266],\n",
      "       [ 0.00357581]])]\n",
      "gradients_biases:  [array([-9.05728925e-04, -1.25659029e-03, -3.67247086e-05]), array([0.00601997])]\n",
      "Iteration 600, Cost: 0.2467878507684284\n",
      "gradient_weights:  [array([[-2.19477487e-03, -2.86169143e-03, -1.76245437e-04],\n",
      "       [-2.83905468e-03, -4.76985297e-03, -2.97279035e-05]]), array([[-0.00591198],\n",
      "       [-0.00519592],\n",
      "       [ 0.00357929]])]\n",
      "gradients_biases:  [array([-9.07926133e-04, -1.25792534e-03, -3.61219731e-05]), array([0.00602149])]\n",
      "Iteration 601, Cost: 0.24677999631162403\n",
      "gradient_weights:  [array([[-2.20054199e-03, -2.86647886e-03, -1.73185921e-04],\n",
      "       [-2.84666215e-03, -4.77729234e-03, -2.91990808e-05]]), array([[-0.00591201],\n",
      "       [-0.00519918],\n",
      "       [ 0.00358278]])]\n",
      "gradients_biases:  [array([-9.10120332e-04, -1.25925655e-03, -3.55181712e-05]), array([0.00602301])]\n",
      "Iteration 602, Cost: 0.24677212934452297\n",
      "gradient_weights:  [array([[-2.20630951e-03, -2.87127090e-03, -1.70125433e-04],\n",
      "       [-2.85427005e-03, -4.78473555e-03, -2.86704446e-05]]), array([[-0.00591204],\n",
      "       [-0.00520246],\n",
      "       [ 0.00358627]])]\n",
      "gradients_biases:  [array([-9.12311508e-04, -1.26058391e-03, -3.49133053e-05]), array([0.00602454])]\n",
      "Iteration 603, Cost: 0.24676424982716932\n",
      "gradient_weights:  [array([[-2.21207745e-03, -2.87606755e-03, -1.67063976e-04],\n",
      "       [-2.86187840e-03, -4.79218258e-03, -2.81420007e-05]]), array([[-0.00591208],\n",
      "       [-0.00520574],\n",
      "       [ 0.00358978]])]\n",
      "gradients_biases:  [array([-9.14499648e-04, -1.26190741e-03, -3.43073774e-05]), array([0.00602607])]\n",
      "Iteration 604, Cost: 0.24675635771958127\n",
      "gradient_weights:  [array([[-2.21784580e-03, -2.88086881e-03, -1.64001549e-04],\n",
      "       [-2.86948720e-03, -4.79963344e-03, -2.76137546e-05]]), array([[-0.00591213],\n",
      "       [-0.00520902],\n",
      "       [ 0.0035933 ]])]\n",
      "gradients_biases:  [array([-9.16684737e-04, -1.26322702e-03, -3.37003899e-05]), array([0.00602762])]\n",
      "Iteration 605, Cost: 0.24674845298175027\n",
      "gradient_weights:  [array([[-2.22361459e-03, -2.88567469e-03, -1.60938156e-04],\n",
      "       [-2.87709646e-03, -4.80708812e-03, -2.70857122e-05]]), array([[-0.00591219],\n",
      "       [-0.00521232],\n",
      "       [ 0.00359682]])]\n",
      "gradients_biases:  [array([-9.18866764e-04, -1.26454272e-03, -3.30923450e-05]), array([0.00602917])]\n",
      "Iteration 606, Cost: 0.24674053557364203\n",
      "gradient_weights:  [array([[-2.22938381e-03, -2.89048518e-03, -1.57873795e-04],\n",
      "       [-2.88470618e-03, -4.81454662e-03, -2.65578791e-05]]), array([[-0.00591225],\n",
      "       [-0.00521562],\n",
      "       [ 0.00360036]])]\n",
      "gradients_biases:  [array([-9.21045714e-04, -1.26585449e-03, -3.24832448e-05]), array([0.00603073])]\n",
      "Iteration 607, Cost: 0.2467326054551956\n",
      "gradient_weights:  [array([[-2.23515349e-03, -2.89530030e-03, -1.54808470e-04],\n",
      "       [-2.89231636e-03, -4.82200894e-03, -2.60302612e-05]]), array([[-0.00591232],\n",
      "       [-0.00521892],\n",
      "       [ 0.0036039 ]])]\n",
      "gradients_biases:  [array([-9.23221573e-04, -1.26716233e-03, -3.18730917e-05]), array([0.0060323])]\n",
      "Iteration 608, Cost: 0.24672466258632375\n",
      "gradient_weights:  [array([[-2.24092362e-03, -2.90012005e-03, -1.51742181e-04],\n",
      "       [-2.89992702e-03, -4.82947507e-03, -2.55028641e-05]]), array([[-0.0059124 ],\n",
      "       [-0.00522223],\n",
      "       [ 0.00360746]])]\n",
      "gradients_biases:  [array([-9.25394329e-04, -1.26846620e-03, -3.12618878e-05]), array([0.00603388])]\n",
      "Iteration 609, Cost: 0.2467167069269132\n",
      "gradient_weights:  [array([[-2.24669422e-03, -2.90494443e-03, -1.48674929e-04],\n",
      "       [-2.90753815e-03, -4.83694500e-03, -2.49756936e-05]]), array([[-0.00591249],\n",
      "       [-0.00522555],\n",
      "       [ 0.00361102]])]\n",
      "gradients_biases:  [array([-9.27563968e-04, -1.26976610e-03, -3.06496355e-05]), array([0.00603547])]\n",
      "Iteration 610, Cost: 0.246708738436824\n",
      "gradient_weights:  [array([[-2.25246529e-03, -2.90977344e-03, -1.45606717e-04],\n",
      "       [-2.91514977e-03, -4.84441874e-03, -2.44487557e-05]]), array([[-0.00591258],\n",
      "       [-0.00522888],\n",
      "       [ 0.00361459]])]\n",
      "gradients_biases:  [array([-9.29730475e-04, -1.27106199e-03, -3.00363370e-05]), array([0.00603706])]\n",
      "Iteration 611, Cost: 0.24670075707589023\n",
      "gradient_weights:  [array([[-2.25823685e-03, -2.91460709e-03, -1.42537545e-04],\n",
      "       [-2.92276187e-03, -4.85189628e-03, -2.39220562e-05]]), array([[-0.00591268],\n",
      "       [-0.00523221],\n",
      "       [ 0.00361818]])]\n",
      "gradients_biases:  [array([-9.31893839e-04, -1.27235387e-03, -2.94219945e-05]), array([0.00603866])]\n",
      "Iteration 612, Cost: 0.24669276280391955\n",
      "gradient_weights:  [array([[-2.26400890e-03, -2.91944538e-03, -1.39467414e-04],\n",
      "       [-2.93037447e-03, -4.85937762e-03, -2.33956008e-05]]), array([[-0.00591279],\n",
      "       [-0.00523555],\n",
      "       [ 0.00362177]])]\n",
      "gradients_biases:  [array([-9.34054044e-04, -1.27364171e-03, -2.88066103e-05]), array([0.00604028])]\n",
      "Iteration 613, Cost: 0.2466847555806932\n",
      "gradient_weights:  [array([[-2.26978146e-03, -2.92428832e-03, -1.36396327e-04],\n",
      "       [-2.93798756e-03, -4.86686275e-03, -2.28693955e-05]]), array([[-0.00591291],\n",
      "       [-0.0052389 ],\n",
      "       [ 0.00362537]])]\n",
      "gradients_biases:  [array([-9.36211078e-04, -1.27492550e-03, -2.81901867e-05]), array([0.00604189])]\n",
      "Iteration 614, Cost: 0.2466767353659663\n",
      "gradient_weights:  [array([[-2.27555452e-03, -2.92913591e-03, -1.33324285e-04],\n",
      "       [-2.94560115e-03, -4.87435166e-03, -2.23434463e-05]]), array([[-0.00591303],\n",
      "       [-0.00524225],\n",
      "       [ 0.00362898]])]\n",
      "gradients_biases:  [array([-9.38364926e-04, -1.27620522e-03, -2.75727260e-05]), array([0.00604352])]\n",
      "Iteration 615, Cost: 0.24666870211946754\n",
      "gradient_weights:  [array([[-2.28132811e-03, -2.93398815e-03, -1.30251290e-04],\n",
      "       [-2.95321525e-03, -4.88184437e-03, -2.18177590e-05]]), array([[-0.00591316],\n",
      "       [-0.00524561],\n",
      "       [ 0.0036326 ]])]\n",
      "gradients_biases:  [array([-9.40515576e-04, -1.27748084e-03, -2.69542305e-05]), array([0.00604515])]\n",
      "Iteration 616, Cost: 0.2466606558008994\n",
      "gradient_weights:  [array([[-2.28710222e-03, -2.93884504e-03, -1.27177342e-04],\n",
      "       [-2.96082986e-03, -4.88934085e-03, -2.12923395e-05]]), array([[-0.0059133 ],\n",
      "       [-0.00524898],\n",
      "       [ 0.00363623]])]\n",
      "gradients_biases:  [array([-9.42663013e-04, -1.27875235e-03, -2.63347024e-05]), array([0.0060468])]\n",
      "Iteration 617, Cost: 0.24665259636993808\n",
      "gradient_weights:  [array([[-2.29287687e-03, -2.94370659e-03, -1.24102444e-04],\n",
      "       [-2.96844499e-03, -4.89684111e-03, -2.07671939e-05]]), array([[-0.00591345],\n",
      "       [-0.00525235],\n",
      "       [ 0.00363987]])]\n",
      "gradients_biases:  [array([-9.44807224e-04, -1.28001973e-03, -2.57141441e-05]), array([0.00604845])]\n",
      "Iteration 618, Cost: 0.2466445237862333\n",
      "gradient_weights:  [array([[-2.29865207e-03, -2.94857281e-03, -1.21026598e-04],\n",
      "       [-2.97606063e-03, -4.90434514e-03, -2.02423282e-05]]), array([[-0.00591361],\n",
      "       [-0.00525573],\n",
      "       [ 0.00364352]])]\n",
      "gradients_biases:  [array([-9.46948195e-04, -1.28128297e-03, -2.50925579e-05]), array([0.00605011])]\n",
      "Iteration 619, Cost: 0.2466364380094088\n",
      "gradient_weights:  [array([[-2.30442782e-03, -2.95344369e-03, -1.17949804e-04],\n",
      "       [-2.98367681e-03, -4.91185294e-03, -1.97177484e-05]]), array([[-0.00591377],\n",
      "       [-0.00525911],\n",
      "       [ 0.00364718]])]\n",
      "gradients_biases:  [array([-9.49085912e-04, -1.28254203e-03, -2.44699461e-05]), array([0.00605177])]\n",
      "Iteration 620, Cost: 0.24662833899906178\n",
      "gradient_weights:  [array([[-2.31020414e-03, -2.95831924e-03, -1.14872066e-04],\n",
      "       [-2.99129351e-03, -4.91936450e-03, -1.91934604e-05]]), array([[-0.00591394],\n",
      "       [-0.00526251],\n",
      "       [ 0.00365085]])]\n",
      "gradients_biases:  [array([-9.51220361e-04, -1.28379691e-03, -2.38463110e-05]), array([0.00605345])]\n",
      "Iteration 621, Cost: 0.2466202267147633\n",
      "gradient_weights:  [array([[-2.31598102e-03, -2.96319946e-03, -1.11793384e-04],\n",
      "       [-2.99891074e-03, -4.92687982e-03, -1.86694705e-05]]), array([[-0.00591411],\n",
      "       [-0.00526591],\n",
      "       [ 0.00365452]])]\n",
      "gradients_biases:  [array([-9.53351530e-04, -1.28504758e-03, -2.32216550e-05]), array([0.00605513])]\n",
      "Iteration 622, Cost: 0.24661210111605822\n",
      "gradient_weights:  [array([[-2.32175849e-03, -2.96808436e-03, -1.08713760e-04],\n",
      "       [-3.00652851e-03, -4.93439890e-03, -1.81457846e-05]]), array([[-0.0059143 ],\n",
      "       [-0.00526931],\n",
      "       [ 0.00365821]])]\n",
      "gradients_biases:  [array([-9.55479403e-04, -1.28629403e-03, -2.25959804e-05]), array([0.00605682])]\n",
      "Iteration 623, Cost: 0.24660396216246505\n",
      "gradient_weights:  [array([[-2.32753654e-03, -2.97297393e-03, -1.05633196e-04],\n",
      "       [-3.01414683e-03, -4.94192173e-03, -1.76224089e-05]]), array([[-0.00591449],\n",
      "       [-0.00527272],\n",
      "       [ 0.0036619 ]])]\n",
      "gradients_biases:  [array([-9.57603968e-04, -1.28753624e-03, -2.19692895e-05]), array([0.00605852])]\n",
      "Iteration 624, Cost: 0.24659580981347606\n",
      "gradient_weights:  [array([[-2.33331519e-03, -2.97786819e-03, -1.02551695e-04],\n",
      "       [-3.02176569e-03, -4.94944830e-03, -1.70993494e-05]]), array([[-0.00591469],\n",
      "       [-0.00527614],\n",
      "       [ 0.00366561]])]\n",
      "gradients_biases:  [array([-9.59725209e-04, -1.28877418e-03, -2.13415847e-05]), array([0.00606023])]\n",
      "Iteration 625, Cost: 0.24658764402855762\n",
      "gradient_weights:  [array([[-2.33909445e-03, -2.98276713e-03, -9.94692570e-05],\n",
      "       [-3.02938510e-03, -4.95697862e-03, -1.65766124e-05]]), array([[-0.0059149 ],\n",
      "       [-0.00527957],\n",
      "       [ 0.00366932]])]\n",
      "gradients_biases:  [array([-9.61843115e-04, -1.29000784e-03, -2.07128684e-05]), array([0.00606194])]\n",
      "Iteration 626, Cost: 0.24657946476714934\n",
      "gradient_weights:  [array([[-2.34487432e-03, -2.98767076e-03, -9.63858854e-05],\n",
      "       [-3.03700507e-03, -4.96451267e-03, -1.60542041e-05]]), array([[-0.00591511],\n",
      "       [-0.005283  ],\n",
      "       [ 0.00367305]])]\n",
      "gradients_biases:  [array([-9.63957669e-04, -1.29123719e-03, -2.00831429e-05]), array([0.00606367])]\n",
      "Iteration 627, Cost: 0.2465712719886651\n",
      "gradient_weights:  [array([[-2.35065480e-03, -2.99257908e-03, -9.33015818e-05],\n",
      "       [-3.04462560e-03, -4.97205046e-03, -1.55321306e-05]]), array([[-0.00591533],\n",
      "       [-0.00528644],\n",
      "       [ 0.00367678]])]\n",
      "gradients_biases:  [array([-9.66068860e-04, -1.29246223e-03, -1.94524105e-05]), array([0.0060654])]\n",
      "Iteration 628, Cost: 0.24656306565249247\n",
      "gradient_weights:  [array([[-2.35643592e-03, -2.99749209e-03, -9.02163482e-05],\n",
      "       [-3.05224670e-03, -4.97959198e-03, -1.50103981e-05]]), array([[-0.00591556],\n",
      "       [-0.00528988],\n",
      "       [ 0.00368053]])]\n",
      "gradients_biases:  [array([-9.68176672e-04, -1.29368292e-03, -1.88206738e-05]), array([0.00606714])]\n",
      "Iteration 629, Cost: 0.24655484571799285\n",
      "gradient_weights:  [array([[-2.36221768e-03, -3.00240980e-03, -8.71301866e-05],\n",
      "       [-3.05986836e-03, -4.98713721e-03, -1.44890129e-05]]), array([[-0.0059158 ],\n",
      "       [-0.00529334],\n",
      "       [ 0.00368428]])]\n",
      "gradients_biases:  [array([-9.70281092e-04, -1.29489926e-03, -1.81879351e-05]), array([0.00606888])]\n",
      "Iteration 630, Cost: 0.24654661214450163\n",
      "gradient_weights:  [array([[-2.36800008e-03, -3.00733220e-03, -8.40430991e-05],\n",
      "       [-3.06749059e-03, -4.99468617e-03, -1.39679812e-05]]), array([[-0.00591605],\n",
      "       [-0.00529679],\n",
      "       [ 0.00368804]])]\n",
      "gradients_biases:  [array([-9.72382105e-04, -1.29611121e-03, -1.75541967e-05]), array([0.00607064])]\n",
      "Iteration 631, Cost: 0.2465383648913279\n",
      "gradient_weights:  [array([[-2.37378314e-03, -3.01225932e-03, -8.09550879e-05],\n",
      "       [-3.07511340e-03, -5.00223884e-03, -1.34473094e-05]]), array([[-0.0059163 ],\n",
      "       [-0.00530026],\n",
      "       [ 0.00369181]])]\n",
      "gradients_biases:  [array([-9.74479699e-04, -1.29731877e-03, -1.69194610e-05]), array([0.0060724])]\n",
      "Iteration 632, Cost: 0.24653010391775476\n",
      "gradient_weights:  [array([[-2.37956685e-03, -3.01719113e-03, -7.78661552e-05],\n",
      "       [-3.08273678e-03, -5.00979522e-03, -1.29270037e-05]]), array([[-0.00591656],\n",
      "       [-0.00530373],\n",
      "       [ 0.00369559]])]\n",
      "gradients_biases:  [array([-9.76573858e-04, -1.29852191e-03, -1.62837306e-05]), array([0.00607417])]\n",
      "Iteration 633, Cost: 0.24652182918303936\n",
      "gradient_weights:  [array([[-2.38535124e-03, -3.02212765e-03, -7.47763031e-05],\n",
      "       [-3.09036076e-03, -5.01735530e-03, -1.24070706e-05]]), array([[-0.00591682],\n",
      "       [-0.00530721],\n",
      "       [ 0.00369938]])]\n",
      "gradients_biases:  [array([-9.78664569e-04, -1.29972061e-03, -1.56470078e-05]), array([0.00607595])]\n",
      "Iteration 634, Cost: 0.24651354064641265\n",
      "gradient_weights:  [array([[-2.39113630e-03, -3.02706889e-03, -7.16855340e-05],\n",
      "       [-3.09798532e-03, -5.02491908e-03, -1.18875162e-05]]), array([[-0.0059171 ],\n",
      "       [-0.00531069],\n",
      "       [ 0.00370318]])]\n",
      "gradients_biases:  [array([-9.80751818e-04, -1.30091485e-03, -1.50092950e-05]), array([0.00607774])]\n",
      "Iteration 635, Cost: 0.2465052382670796\n",
      "gradient_weights:  [array([[-2.39692205e-03, -3.03201483e-03, -6.85938500e-05],\n",
      "       [-3.10561047e-03, -5.03248655e-03, -1.13683471e-05]]), array([[-0.00591738],\n",
      "       [-0.00531418],\n",
      "       [ 0.00370699]])]\n",
      "gradients_biases:  [array([-9.82835591e-04, -1.30210462e-03, -1.43705946e-05]), array([0.00607953])]\n",
      "Iteration 636, Cost: 0.24649692200421935\n",
      "gradient_weights:  [array([[-2.40270850e-03, -3.03696550e-03, -6.55012536e-05],\n",
      "       [-3.11323621e-03, -5.04005772e-03, -1.08495696e-05]]), array([[-0.00591767],\n",
      "       [-0.00531768],\n",
      "       [ 0.00371081]])]\n",
      "gradients_biases:  [array([-9.84915873e-04, -1.30328989e-03, -1.37309092e-05]), array([0.00608134])]\n",
      "Iteration 637, Cost: 0.24648859181698485\n",
      "gradient_weights:  [array([[-2.40849564e-03, -3.04192088e-03, -6.24077470e-05],\n",
      "       [-3.12086255e-03, -5.04763256e-03, -1.03311902e-05]]), array([[-0.00591797],\n",
      "       [-0.00532119],\n",
      "       [ 0.00371464]])]\n",
      "gradients_biases:  [array([-9.86992650e-04, -1.30447065e-03, -1.30902411e-05]), array([0.00608315])]\n",
      "Iteration 638, Cost: 0.24648024766450327\n",
      "gradient_weights:  [array([[-2.41428349e-03, -3.04688098e-03, -5.93133327e-05],\n",
      "       [-3.12848950e-03, -5.05521109e-03, -9.81321529e-06]]), array([[-0.00591827],\n",
      "       [-0.0053247 ],\n",
      "       [ 0.00371847]])]\n",
      "gradients_biases:  [array([-9.89065909e-04, -1.30564686e-03, -1.24485929e-05]), array([0.00608496])]\n",
      "Iteration 639, Cost: 0.24647188950587587\n",
      "gradient_weights:  [array([[-2.42007206e-03, -3.05184581e-03, -5.62180131e-05],\n",
      "       [-3.13611705e-03, -5.06279329e-03, -9.29565134e-06]]), array([[-0.00591858],\n",
      "       [-0.00532822],\n",
      "       [ 0.00372232]])]\n",
      "gradients_biases:  [array([-9.91135635e-04, -1.30681853e-03, -1.18059669e-05]), array([0.00608679])]\n",
      "Iteration 640, Cost: 0.246463517300178\n",
      "gradient_weights:  [array([[-2.42586135e-03, -3.05681536e-03, -5.31217908e-05],\n",
      "       [-3.14374521e-03, -5.07037916e-03, -8.77850487e-06]]), array([[-0.0059189 ],\n",
      "       [-0.00533174],\n",
      "       [ 0.00372618]])]\n",
      "gradients_biases:  [array([-9.93201815e-04, -1.30798561e-03, -1.11623657e-05]), array([0.00608863])]\n",
      "Iteration 641, Cost: 0.24645513100645922\n",
      "gradient_weights:  [array([[-2.43165137e-03, -3.06178964e-03, -5.00246681e-05],\n",
      "       [-3.15137399e-03, -5.07796869e-03, -8.26178239e-06]]), array([[-0.00591923],\n",
      "       [-0.00533527],\n",
      "       [ 0.00373004]])]\n",
      "gradients_biases:  [array([-9.95264432e-04, -1.30914810e-03, -1.05177918e-05]), array([0.00609047])]\n",
      "Iteration 642, Cost: 0.24644673058374328\n",
      "gradient_weights:  [array([[-2.43744213e-03, -3.06676865e-03, -4.69266476e-05],\n",
      "       [-3.15900338e-03, -5.08556187e-03, -7.74549045e-06]]), array([[-0.00591956],\n",
      "       [-0.00533881],\n",
      "       [ 0.00373392]])]\n",
      "gradients_biases:  [array([-9.97323475e-04, -1.31030598e-03, -9.87224752e-06]), array([0.00609232])]\n",
      "Iteration 643, Cost: 0.24643831599102817\n",
      "gradient_weights:  [array([[-2.44323364e-03, -3.07175240e-03, -4.38277320e-05],\n",
      "       [-3.16663338e-03, -5.09315871e-03, -7.22963561e-06]]), array([[-0.0059199 ],\n",
      "       [-0.00534235],\n",
      "       [ 0.0037378 ]])]\n",
      "gradients_biases:  [array([-9.99378928e-04, -1.31145922e-03, -9.22573551e-06]), array([0.00609418])]\n",
      "Iteration 644, Cost: 0.24642988718728617\n",
      "gradient_weights:  [array([[-2.44902590e-03, -3.07674088e-03, -4.07279238e-05],\n",
      "       [-3.17426402e-03, -5.10075920e-03, -6.71422447e-06]]), array([[-0.00592025],\n",
      "       [-0.0053459 ],\n",
      "       [ 0.0037417 ]])]\n",
      "gradients_biases:  [array([-1.00143078e-03, -1.31260780e-03, -8.57825824e-06]), array([0.00609604])]\n",
      "Iteration 645, Cost: 0.24642144413146366\n",
      "gradient_weights:  [array([[-2.45481893e-03, -3.08173410e-03, -3.76272257e-05],\n",
      "       [-3.18189527e-03, -5.10836332e-03, -6.19926363e-06]]), array([[-0.00592061],\n",
      "       [-0.00534946],\n",
      "       [ 0.0037456 ]])]\n",
      "gradients_biases:  [array([-1.00347901e-03, -1.31375171e-03, -7.92981822e-06]), array([0.00609792])]\n",
      "Iteration 646, Cost: 0.24641298678248186\n",
      "gradient_weights:  [array([[-2.46061272e-03, -3.08673206e-03, -3.45256405e-05],\n",
      "       [-3.18952716e-03, -5.11597108e-03, -5.68475975e-06]]), array([[-0.00592097],\n",
      "       [-0.00535303],\n",
      "       [ 0.00374951]])]\n",
      "gradients_biases:  [array([-1.00552361e-03, -1.31489093e-03, -7.28041796e-06]), array([0.0060998])]\n",
      "Iteration 647, Cost: 0.24640451509923578\n",
      "gradient_weights:  [array([[-2.46640728e-03, -3.09173477e-03, -3.14231708e-05],\n",
      "       [-3.19715968e-03, -5.12358248e-03, -5.17071948e-06]]), array([[-0.00592134],\n",
      "       [-0.0053566 ],\n",
      "       [ 0.00375343]])]\n",
      "gradients_biases:  [array([-1.00756456e-03, -1.31602543e-03, -6.63006002e-06]), array([0.00610169])]\n",
      "Iteration 648, Cost: 0.24639602904059532\n",
      "gradient_weights:  [array([[-2.47220263e-03, -3.09674221e-03, -2.83198194e-05],\n",
      "       [-3.20479284e-03, -5.13119749e-03, -4.65714950e-06]]), array([[-0.00592172],\n",
      "       [-0.00536017],\n",
      "       [ 0.00375736]])]\n",
      "gradients_biases:  [array([-1.00960185e-03, -1.31715519e-03, -5.97874691e-06]), array([0.00610359])]\n",
      "Iteration 649, Cost: 0.24638752856540463\n",
      "gradient_weights:  [array([[-2.47799877e-03, -3.10175441e-03, -2.52155892e-05],\n",
      "       [-3.21242663e-03, -5.13881612e-03, -4.14405653e-06]]), array([[-0.0059221 ],\n",
      "       [-0.00536376],\n",
      "       [ 0.0037613 ]])]\n",
      "gradients_biases:  [array([-1.01163546e-03, -1.31828020e-03, -5.32648120e-06]), array([0.00610549])]\n",
      "Iteration 650, Cost: 0.24637901363248244\n",
      "gradient_weights:  [array([[-2.48379571e-03, -3.10677136e-03, -2.21104829e-05],\n",
      "       [-3.22006106e-03, -5.14643837e-03, -3.63144730e-06]]), array([[-0.00592249],\n",
      "       [-0.00536735],\n",
      "       [ 0.00376525]])]\n",
      "gradients_biases:  [array([-1.01366539e-03, -1.31940043e-03, -4.67326543e-06]), array([0.0061074])]\n",
      "Iteration 651, Cost: 0.24637048420062194\n",
      "gradient_weights:  [array([[-2.48959345e-03, -3.11179305e-03, -1.90045036e-05],\n",
      "       [-3.22769614e-03, -5.15406422e-03, -3.11932857e-06]]), array([[-0.00592289],\n",
      "       [-0.00537094],\n",
      "       [ 0.00376921]])]\n",
      "gradients_biases:  [array([-1.01569161e-03, -1.32051587e-03, -4.01910217e-06]), array([0.00610933])]\n",
      "Iteration 652, Cost: 0.24636194022859115\n",
      "gradient_weights:  [array([[-2.49539201e-03, -3.11681950e-03, -1.58976540e-05],\n",
      "       [-3.23533187e-03, -5.16169367e-03, -2.60770712e-06]]), array([[-0.0059233 ],\n",
      "       [-0.00537455],\n",
      "       [ 0.00377318]])]\n",
      "gradients_biases:  [array([-1.01771411e-03, -1.32162649e-03, -3.36399400e-06]), array([0.00611125])]\n",
      "Iteration 653, Cost: 0.2463533816751325\n",
      "gradient_weights:  [array([[-2.50119138e-03, -3.12185071e-03, -1.27899373e-05],\n",
      "       [-3.24296825e-03, -5.16932672e-03, -2.09658975e-06]]), array([[-0.00592372],\n",
      "       [-0.00537816],\n",
      "       [ 0.00377716]])]\n",
      "gradients_biases:  [array([-1.01973288e-03, -1.32273227e-03, -2.70794350e-06]), array([0.00611319])]\n",
      "Iteration 654, Cost: 0.24634480849896326\n",
      "gradient_weights:  [array([[-2.50699158e-03, -3.12688667e-03, -9.68135638e-06],\n",
      "       [-3.25060528e-03, -5.17696336e-03, -1.58598330e-06]]), array([[-0.00592414],\n",
      "       [-0.00538177],\n",
      "       [ 0.00378115]])]\n",
      "gradients_biases:  [array([-1.02174790e-03, -1.32383320e-03, -2.05095324e-06]), array([0.00611514])]\n",
      "Iteration 655, Cost: 0.24633622065877536\n",
      "gradient_weights:  [array([[-2.51279261e-03, -3.13192739e-03, -6.57191430e-06],\n",
      "       [-3.25824296e-03, -5.18460357e-03, -1.07589462e-06]]), array([[-0.00592457],\n",
      "       [-0.0053854 ],\n",
      "       [ 0.00378514]])]\n",
      "gradients_biases:  [array([-1.02375916e-03, -1.32492925e-03, -1.39302585e-06]), array([0.00611709])]\n",
      "Iteration 656, Cost: 0.2463276181132356\n",
      "gradient_weights:  [array([[-2.51859448e-03, -3.13697288e-03, -3.46161415e-06],\n",
      "       [-3.26588131e-03, -5.19224737e-03, -5.66330585e-07]]), array([[-0.005925  ],\n",
      "       [-0.00538903],\n",
      "       [ 0.00378915]])]\n",
      "gradients_biases:  [array([-1.02576664e-03, -1.32602040e-03, -7.34163915e-07]), array([0.00611905])]\n",
      "Iteration 657, Cost: 0.24631900082098568\n",
      "gradient_weights:  [array([[-2.52439719e-03, -3.14202312e-03, -3.50459068e-07],\n",
      "       [-3.27352031e-03, -5.19989473e-03, -5.72981028e-08]]), array([[-0.00592545],\n",
      "       [-0.00539266],\n",
      "       [ 0.00379316]])]\n",
      "gradients_biases:  [array([-1.02777034e-03, -1.32710663e-03, -7.43700607e-08]), array([0.00612102])]\n",
      "Iteration 658, Cost: 0.24631036874064205\n",
      "gradient_weights:  [array([[-2.53020076e-03, -3.14707814e-03,  2.76154779e-06],\n",
      "       [-3.28115998e-03, -5.20754566e-03,  4.51195900e-07]]), array([[-0.0059259 ],\n",
      "       [-0.00539631],\n",
      "       [ 0.00379719]])]\n",
      "gradients_biases:  [array([-1.02977022e-03, -1.32818792e-03,  5.86353090e-07]), array([0.00612299])]\n",
      "Iteration 659, Cost: 0.24630172183079618\n",
      "gradient_weights:  [array([[-2.53600519e-03, -3.15213791e-03,  5.87440321e-06],\n",
      "       [-3.28880032e-03, -5.21520015e-03,  9.59144473e-07]]), array([[-0.00592636],\n",
      "       [-0.00539996],\n",
      "       [ 0.00380122]])]\n",
      "gradients_biases:  [array([-1.03176629e-03, -1.32926426e-03,  1.24800290e-06]), array([0.00612498])]\n",
      "Iteration 660, Cost: 0.24629306005001467\n",
      "gradient_weights:  [array([[-2.54181048e-03, -3.15720246e-03,  8.98810395e-06],\n",
      "       [-3.29644132e-03, -5.22285819e-03,  1.46654064e-06]]), array([[-0.00592682],\n",
      "       [-0.00540361],\n",
      "       [ 0.00380527]])]\n",
      "gradients_biases:  [array([-1.03375852e-03, -1.33033561e-03,  1.91057674e-06]), array([0.00612697])]\n",
      "Iteration 661, Cost: 0.24628438335683897\n",
      "gradient_weights:  [array([[-2.54761665e-03, -3.16227178e-03,  1.21026467e-05],\n",
      "       [-3.30408299e-03, -5.23051977e-03,  1.97337740e-06]]), array([[-0.00592729],\n",
      "       [-0.00540728],\n",
      "       [ 0.00380932]])]\n",
      "gradients_biases:  [array([-1.03574690e-03, -1.33140197e-03,  2.57407194e-06]), array([0.00612897])]\n",
      "Iteration 662, Cost: 0.24627569170978586\n",
      "gradient_weights:  [array([[-2.55342370e-03, -3.16734587e-03,  1.52180282e-05],\n",
      "       [-3.31172534e-03, -5.23818490e-03,  2.47964772e-06]]), array([[-0.00592777],\n",
      "       [-0.00541094],\n",
      "       [ 0.00381338]])]\n",
      "gradients_biases:  [array([-1.03773142e-03, -1.33246330e-03,  3.23848585e-06]), array([0.00613097])]\n",
      "Iteration 663, Cost: 0.24626698506734715\n",
      "gradient_weights:  [array([[-2.55923163e-03, -3.17242473e-03,  1.83342451e-05],\n",
      "       [-3.31936836e-03, -5.24585355e-03,  2.98534456e-06]]), array([[-0.00592826],\n",
      "       [-0.00541462],\n",
      "       [ 0.00381745]])]\n",
      "gradients_biases:  [array([-1.03971206e-03, -1.33351959e-03,  3.90381579e-06]), array([0.00613299])]\n",
      "Iteration 664, Cost: 0.24625826338799017\n",
      "gradient_weights:  [array([[-2.56504046e-03, -3.17750837e-03,  2.14512938e-05],\n",
      "       [-3.32701207e-03, -5.25352573e-03,  3.49046085e-06]]), array([[-0.00592875],\n",
      "       [-0.0054183 ],\n",
      "       [ 0.00382153]])]\n",
      "gradients_biases:  [array([-1.04168880e-03, -1.33457082e-03,  4.57005910e-06]), array([0.00613501])]\n",
      "Iteration 665, Cost: 0.24624952663015712\n",
      "gradient_weights:  [array([[-2.57085018e-03, -3.18259678e-03,  2.45691711e-05],\n",
      "       [-3.33465645e-03, -5.26120143e-03,  3.99498947e-06]]), array([[-0.00592926],\n",
      "       [-0.00542199],\n",
      "       [ 0.00382562]])]\n",
      "gradients_biases:  [array([-1.04366164e-03, -1.33561697e-03,  5.23721307e-06]), array([0.00613704])]\n",
      "Iteration 666, Cost: 0.24624077475226594\n",
      "gradient_weights:  [array([[-2.57666081e-03, -3.18768998e-03,  2.76878734e-05],\n",
      "       [-3.34230151e-03, -5.26888064e-03,  4.49892332e-06]]), array([[-0.00592977],\n",
      "       [-0.00542569],\n",
      "       [ 0.00382972]])]\n",
      "gradients_biases:  [array([-1.04563056e-03, -1.33665801e-03,  5.90527502e-06]), array([0.00613908])]\n",
      "Iteration 667, Cost: 0.24623200771270987\n",
      "gradient_weights:  [array([[-2.58247235e-03, -3.19278795e-03,  3.08073972e-05],\n",
      "       [-3.34994726e-03, -5.27656336e-03,  5.00225523e-06]]), array([[-0.00593028],\n",
      "       [-0.00542939],\n",
      "       [ 0.00383383]])]\n",
      "gradients_biases:  [array([-1.04759553e-03, -1.33769392e-03,  6.57424224e-06]), array([0.00614113])]\n",
      "Iteration 668, Cost: 0.24622322546985764\n",
      "gradient_weights:  [array([[-2.58828481e-03, -3.19789070e-03,  3.39277389e-05],\n",
      "       [-3.35759370e-03, -5.28424958e-03,  5.50497804e-06]]), array([[-0.0059308 ],\n",
      "       [-0.0054331 ],\n",
      "       [ 0.00383795]])]\n",
      "gradients_biases:  [array([-1.04955656e-03, -1.33872469e-03,  7.24411201e-06]), array([0.00614318])]\n",
      "Iteration 669, Cost: 0.2462144279820535\n",
      "gradient_weights:  [array([[-2.59409820e-03, -3.20299824e-03,  3.70488949e-05],\n",
      "       [-3.36524082e-03, -5.29193929e-03,  6.00708454e-06]]), array([[-0.00593133],\n",
      "       [-0.00543681],\n",
      "       [ 0.00384208]])]\n",
      "gradients_biases:  [array([-1.05151362e-03, -1.33975030e-03,  7.91488161e-06]), array([0.00614524])]\n",
      "Iteration 670, Cost: 0.2462056152076175\n",
      "gradient_weights:  [array([[-2.59991251e-03, -3.20811056e-03,  4.01708616e-05],\n",
      "       [-3.37288863e-03, -5.29963248e-03,  6.50856751e-06]]), array([[-0.00593187],\n",
      "       [-0.00544053],\n",
      "       [ 0.00384621]])]\n",
      "gradients_biases:  [array([-1.05346670e-03, -1.34077072e-03,  8.58654830e-06]), array([0.00614731])]\n",
      "Iteration 671, Cost: 0.24619678710484524\n",
      "gradient_weights:  [array([[-2.60572776e-03, -3.21322766e-03,  4.32936353e-05],\n",
      "       [-3.38053714e-03, -5.30732916e-03,  7.00941970e-06]]), array([[-0.00593242],\n",
      "       [-0.00544426],\n",
      "       [ 0.00385036]])]\n",
      "gradients_biases:  [array([-1.05541578e-03, -1.34178592e-03,  9.25910934e-06]), array([0.00614939])]\n",
      "Iteration 672, Cost: 0.2461879436320081\n",
      "gradient_weights:  [array([[-2.61154396e-03, -3.21834955e-03,  4.64172122e-05],\n",
      "       [-3.38818634e-03, -5.31502930e-03,  7.50963384e-06]]), array([[-0.00593297],\n",
      "       [-0.005448  ],\n",
      "       [ 0.00385452]])]\n",
      "gradients_biases:  [array([-1.05736084e-03, -1.34279590e-03,  9.93256198e-06]), array([0.00615147])]\n",
      "Iteration 673, Cost: 0.2461790847473534\n",
      "gradient_weights:  [array([[-2.61736110e-03, -3.22347623e-03,  4.95415887e-05],\n",
      "       [-3.39583624e-03, -5.32273291e-03,  8.00920263e-06]]), array([[-0.00593353],\n",
      "       [-0.00545174],\n",
      "       [ 0.00385868]])]\n",
      "gradients_biases:  [array([-1.05930189e-03, -1.34380063e-03,  1.06069035e-05]), array([0.00615357])]\n",
      "Iteration 674, Cost: 0.2461702104091043\n",
      "gradient_weights:  [array([[-2.62317920e-03, -3.22860770e-03,  5.26667608e-05],\n",
      "       [-3.40348683e-03, -5.33043998e-03,  8.50811874e-06]]), array([[-0.00593409],\n",
      "       [-0.00545549],\n",
      "       [ 0.00386285]])]\n",
      "gradients_biases:  [array([-1.06123889e-03, -1.34480009e-03,  1.12821310e-05]), array([0.00615567])]\n",
      "Iteration 675, Cost: 0.24616132057545984\n",
      "gradient_weights:  [array([[-2.62899826e-03, -3.23374395e-03,  5.57927248e-05],\n",
      "       [-3.41113813e-03, -5.33815050e-03,  9.00637482e-06]]), array([[-0.00593467],\n",
      "       [-0.00545924],\n",
      "       [ 0.00386704]])]\n",
      "gradients_biases:  [array([-1.06317183e-03, -1.34579425e-03,  1.19582418e-05]), array([0.00615777])]\n",
      "Iteration 676, Cost: 0.24615241520459546\n",
      "gradient_weights:  [array([[-2.63481829e-03, -3.23888500e-03,  5.89194767e-05],\n",
      "       [-3.41879012e-03, -5.34586446e-03,  9.50396349e-06]]), array([[-0.00593525],\n",
      "       [-0.005463  ],\n",
      "       [ 0.00387123]])]\n",
      "gradients_biases:  [array([-1.06510071e-03, -1.34678310e-03,  1.26352331e-05]), array([0.00615989])]\n",
      "Iteration 677, Cost: 0.24614349425466236\n",
      "gradient_weights:  [array([[-2.64063929e-03, -3.24403084e-03,  6.20470126e-05],\n",
      "       [-3.42644282e-03, -5.35358186e-03,  1.00008774e-05]]), array([[-0.00593584],\n",
      "       [-0.00546677],\n",
      "       [ 0.00387543]])]\n",
      "gradients_biases:  [array([-1.06702550e-03, -1.34776662e-03,  1.33131021e-05]), array([0.00616201])]\n",
      "Iteration 678, Cost: 0.24613455768378822\n",
      "gradient_weights:  [array([[-2.64646126e-03, -3.24918147e-03,  6.51753285e-05],\n",
      "       [-3.43409623e-03, -5.36130268e-03,  1.04971090e-05]]), array([[-0.00593643],\n",
      "       [-0.00547054],\n",
      "       [ 0.00387964]])]\n",
      "gradients_biases:  [array([-1.06894618e-03, -1.34874478e-03,  1.39918460e-05]), array([0.00616414])]\n",
      "Iteration 679, Cost: 0.24612560545007683\n",
      "gradient_weights:  [array([[-2.65228423e-03, -3.25433690e-03,  6.83044205e-05],\n",
      "       [-3.44175033e-03, -5.36902693e-03,  1.09926510e-05]]), array([[-0.00593703],\n",
      "       [-0.00547432],\n",
      "       [ 0.00388386]])]\n",
      "gradients_biases:  [array([-1.07086275e-03, -1.34971756e-03,  1.46714620e-05]), array([0.00616628])]\n",
      "Iteration 680, Cost: 0.2461166375116085\n",
      "gradient_weights:  [array([[-2.65810818e-03, -3.25949712e-03,  7.14342844e-05],\n",
      "       [-3.44940515e-03, -5.37675459e-03,  1.14874958e-05]]), array([[-0.00593764],\n",
      "       [-0.00547811],\n",
      "       [ 0.00388809]])]\n",
      "gradients_biases:  [array([-1.07277520e-03, -1.35068494e-03,  1.53519471e-05]), array([0.00616843])]\n",
      "Iteration 681, Cost: 0.24610765382643995\n",
      "gradient_weights:  [array([[-2.66393313e-03, -3.26466213e-03,  7.45649162e-05],\n",
      "       [-3.45706067e-03, -5.38448566e-03,  1.19816359e-05]]), array([[-0.00593826],\n",
      "       [-0.0054819 ],\n",
      "       [ 0.00389233]])]\n",
      "gradients_biases:  [array([-1.07468349e-03, -1.35164690e-03,  1.60332987e-05]), array([0.00617058])]\n",
      "Iteration 682, Cost: 0.2460986543526043\n",
      "gradient_weights:  [array([[-2.66975908e-03, -3.26983194e-03,  7.76963117e-05],\n",
      "       [-3.46471691e-03, -5.39222013e-03,  1.24750638e-05]]), array([[-0.00593888],\n",
      "       [-0.0054857 ],\n",
      "       [ 0.00389658]])]\n",
      "gradients_biases:  [array([-1.07658763e-03, -1.35260343e-03,  1.67155138e-05]), array([0.00617274])]\n",
      "Iteration 683, Cost: 0.24608963904811154\n",
      "gradient_weights:  [array([[-2.67558603e-03, -3.27500655e-03,  8.08284668e-05],\n",
      "       [-3.47237385e-03, -5.39995798e-03,  1.29677720e-05]]), array([[-0.00593951],\n",
      "       [-0.00548951],\n",
      "       [ 0.00390084]])]\n",
      "gradients_biases:  [array([-1.07848758e-03, -1.35355448e-03,  1.73985896e-05]), array([0.00617491])]\n",
      "Iteration 684, Cost: 0.2460806078709482\n",
      "gradient_weights:  [array([[-2.68141400e-03, -3.28018596e-03,  8.39613772e-05],\n",
      "       [-3.48003151e-03, -5.40769923e-03,  1.34597529e-05]]), array([[-0.00594015],\n",
      "       [-0.00549332],\n",
      "       [ 0.0039051 ]])]\n",
      "gradients_biases:  [array([-1.08038335e-03, -1.35450006e-03,  1.80825232e-05]), array([0.00617709])]\n",
      "Iteration 685, Cost: 0.2460715607790776\n",
      "gradient_weights:  [array([[-2.68724299e-03, -3.28537017e-03,  8.70950386e-05],\n",
      "       [-3.48768988e-03, -5.41544385e-03,  1.39509988e-05]]), array([[-0.00594079],\n",
      "       [-0.00549714],\n",
      "       [ 0.00390938]])]\n",
      "gradients_biases:  [array([-1.08227491e-03, -1.35544013e-03,  1.87673118e-05]), array([0.00617927])]\n",
      "Iteration 686, Cost: 0.24606249773044003\n",
      "gradient_weights:  [array([[-2.69307300e-03, -3.29055917e-03,  9.02294468e-05],\n",
      "       [-3.49534897e-03, -5.42319183e-03,  1.44415022e-05]]), array([[-0.00594144],\n",
      "       [-0.00550097],\n",
      "       [ 0.00391366]])]\n",
      "gradients_biases:  [array([-1.08416226e-03, -1.35637467e-03,  1.94529524e-05]), array([0.00618146])]\n",
      "Iteration 687, Cost: 0.24605341868295272\n",
      "gradient_weights:  [array([[-2.69890404e-03, -3.29575298e-03,  9.33645974e-05],\n",
      "       [-3.50300877e-03, -5.43094318e-03,  1.49312553e-05]]), array([[-0.0059421 ],\n",
      "       [-0.0055048 ],\n",
      "       [ 0.00391796]])]\n",
      "gradients_biases:  [array([-1.08604536e-03, -1.35730366e-03,  2.01394421e-05]), array([0.00618366])]\n",
      "Iteration 688, Cost: 0.24604432359450984\n",
      "gradient_weights:  [array([[-2.70473611e-03, -3.30095158e-03,  9.65004860e-05],\n",
      "       [-3.51066929e-03, -5.43869788e-03,  1.54202505e-05]]), array([[-0.00594277],\n",
      "       [-0.00550864],\n",
      "       [ 0.00392226]])]\n",
      "gradients_biases:  [array([-1.08792421e-03, -1.35822709e-03,  2.08267781e-05]), array([0.00618587])]\n",
      "Iteration 689, Cost: 0.24603521242298282\n",
      "gradient_weights:  [array([[-2.71056923e-03, -3.30615499e-03,  9.96371082e-05],\n",
      "       [-3.51833053e-03, -5.44645593e-03,  1.59084801e-05]]), array([[-0.00594344],\n",
      "       [-0.00551248],\n",
      "       [ 0.00392658]])]\n",
      "gradients_biases:  [array([-1.08979880e-03, -1.35914492e-03,  2.15149573e-05]), array([0.00618809])]\n",
      "Iteration 690, Cost: 0.24602608512622037\n",
      "gradient_weights:  [array([[-2.71640339e-03, -3.31136320e-03,  1.02774460e-04],\n",
      "       [-3.52599249e-03, -5.45421731e-03,  1.63959364e-05]]), array([[-0.00594412],\n",
      "       [-0.00551634],\n",
      "       [ 0.0039309 ]])]\n",
      "gradients_biases:  [array([-1.09166910e-03, -1.36005714e-03,  2.22039770e-05]), array([0.00619031])]\n",
      "Iteration 691, Cost: 0.24601694166204835\n",
      "gradient_weights:  [array([[-2.72223860e-03, -3.31657621e-03,  1.05912535e-04],\n",
      "       [-3.53365516e-03, -5.46198203e-03,  1.68826115e-05]]), array([[-0.00594481],\n",
      "       [-0.00552019],\n",
      "       [ 0.00393523]])]\n",
      "gradients_biases:  [array([-1.09353511e-03, -1.36096372e-03,  2.28938341e-05]), array([0.00619254])]\n",
      "Iteration 692, Cost: 0.24600778198827028\n",
      "gradient_weights:  [array([[-2.72807486e-03, -3.32179402e-03,  1.09051331e-04],\n",
      "       [-3.54131856e-03, -5.46975006e-03,  1.73684978e-05]]), array([[-0.0059455 ],\n",
      "       [-0.00552406],\n",
      "       [ 0.00393957]])]\n",
      "gradients_biases:  [array([-1.09539681e-03, -1.36186464e-03,  2.35845258e-05]), array([0.00619478])]\n",
      "Iteration 693, Cost: 0.24599860606266694\n",
      "gradient_weights:  [array([[-2.73391218e-03, -3.32701664e-03,  1.12190843e-04],\n",
      "       [-3.54898268e-03, -5.47752141e-03,  1.78535873e-05]]), array([[-0.0059462 ],\n",
      "       [-0.00552793],\n",
      "       [ 0.00394392]])]\n",
      "gradients_biases:  [array([-1.09725418e-03, -1.36275989e-03,  2.42760489e-05]), array([0.00619702])]\n",
      "Iteration 694, Cost: 0.24598941384299702\n",
      "gradient_weights:  [array([[-2.73975057e-03, -3.33224405e-03,  1.15331065e-04],\n",
      "       [-3.55664751e-03, -5.48529606e-03,  1.83378723e-05]]), array([[-0.00594691],\n",
      "       [-0.00553181],\n",
      "       [ 0.00394828]])]\n",
      "gradients_biases:  [array([-1.09910720e-03, -1.36364944e-03,  2.49684006e-05]), array([0.00619927])]\n",
      "Iteration 695, Cost: 0.24598020528699666\n",
      "gradient_weights:  [array([[-2.74559003e-03, -3.33747627e-03,  1.18471993e-04],\n",
      "       [-3.56431308e-03, -5.49307401e-03,  1.88213448e-05]]), array([[-0.00594763],\n",
      "       [-0.00553569],\n",
      "       [ 0.00395265]])]\n",
      "gradients_biases:  [array([-1.10095587e-03, -1.36453327e-03,  2.56615779e-05]), array([0.00620153])]\n",
      "Iteration 696, Cost: 0.24597098035238005\n",
      "gradient_weights:  [array([[-2.75143057e-03, -3.34271330e-03,  1.21613622e-04],\n",
      "       [-3.57197936e-03, -5.50085525e-03,  1.93039970e-05]]), array([[-0.00594835],\n",
      "       [-0.00553958],\n",
      "       [ 0.00395702]])]\n",
      "gradients_biases:  [array([-1.10280016e-03, -1.36541135e-03,  2.63555778e-05]), array([0.0062038])]\n",
      "Iteration 697, Cost: 0.24596173899683915\n",
      "gradient_weights:  [array([[-2.75727218e-03, -3.34795513e-03,  1.24755948e-04],\n",
      "       [-3.57964637e-03, -5.50863977e-03,  1.97858210e-05]]), array([[-0.00594908],\n",
      "       [-0.00554348],\n",
      "       [ 0.00396141]])]\n",
      "gradients_biases:  [array([-1.10464007e-03, -1.36628367e-03,  2.70503973e-05]), array([0.00620608])]\n",
      "Iteration 698, Cost: 0.24595248117804389\n",
      "gradient_weights:  [array([[-2.76311488e-03, -3.35320176e-03,  1.27898966e-04],\n",
      "       [-3.58731410e-03, -5.51642756e-03,  2.02668088e-05]]), array([[-0.00594981],\n",
      "       [-0.00554739],\n",
      "       [ 0.00396581]])]\n",
      "gradients_biases:  [array([-1.10647557e-03, -1.36715020e-03,  2.77460334e-05]), array([0.00620836])]\n",
      "Iteration 699, Cost: 0.2459432068536424\n",
      "gradient_weights:  [array([[-2.76895868e-03, -3.35845320e-03,  1.31042671e-04],\n",
      "       [-3.59498256e-03, -5.52421861e-03,  2.07469525e-05]]), array([[-0.00595056],\n",
      "       [-0.0055513 ],\n",
      "       [ 0.00397021]])]\n",
      "gradients_biases:  [array([-1.10830666e-03, -1.36801092e-03,  2.84424830e-05]), array([0.00621065])]\n",
      "Iteration 700, Cost: 0.2459339159812612\n",
      "gradient_weights:  [array([[-2.77480356e-03, -3.36370943e-03,  1.34187058e-04],\n",
      "       [-3.60265174e-03, -5.53201292e-03,  2.12262440e-05]]), array([[-0.00595131],\n",
      "       [-0.00555521],\n",
      "       [ 0.00397463]])]\n",
      "gradients_biases:  [array([-1.11013330e-03, -1.36886581e-03,  2.91397432e-05]), array([0.00621295])]\n",
      "Iteration 701, Cost: 0.24592460851850478\n",
      "gradient_weights:  [array([[-2.78064955e-03, -3.36897048e-03,  1.37332121e-04],\n",
      "       [-3.61032165e-03, -5.53981048e-03,  2.17046753e-05]]), array([[-0.00595206],\n",
      "       [-0.00555914],\n",
      "       [ 0.00397905]])]\n",
      "gradients_biases:  [array([-1.11195550e-03, -1.36971484e-03,  2.98378108e-05]), array([0.00621525])]\n",
      "Iteration 702, Cost: 0.24591528442295635\n",
      "gradient_weights:  [array([[-2.78649663e-03, -3.37423633e-03,  1.40477857e-04],\n",
      "       [-3.61799228e-03, -5.54761127e-03,  2.21822383e-05]]), array([[-0.00595283],\n",
      "       [-0.00556307],\n",
      "       [ 0.00398348]])]\n",
      "gradients_biases:  [array([-1.11377324e-03, -1.37055801e-03,  3.05366828e-05]), array([0.00621757])]\n",
      "Iteration 703, Cost: 0.24590594365217766\n",
      "gradient_weights:  [array([[-2.79234483e-03, -3.37950698e-03,  1.43624260e-04],\n",
      "       [-3.62566364e-03, -5.55541530e-03,  2.26589251e-05]]), array([[-0.0059536 ],\n",
      "       [-0.005567  ],\n",
      "       [ 0.00398792]])]\n",
      "gradients_biases:  [array([-1.11558649e-03, -1.37139527e-03,  3.12363562e-05]), array([0.00621989])]\n",
      "Iteration 704, Cost: 0.24589658616370885\n",
      "gradient_weights:  [array([[-2.79819414e-03, -3.38478243e-03,  1.46771324e-04],\n",
      "       [-3.63333573e-03, -5.56322254e-03,  2.31347274e-05]]), array([[-0.00595437],\n",
      "       [-0.00557094],\n",
      "       [ 0.00399238]])]\n",
      "gradients_biases:  [array([-1.11739524e-03, -1.37222662e-03,  3.19368279e-05]), array([0.00622222])]\n",
      "Iteration 705, Cost: 0.24588721191506918\n",
      "gradient_weights:  [array([[-2.80404457e-03, -3.39006269e-03,  1.49919046e-04],\n",
      "       [-3.64100854e-03, -5.57103300e-03,  2.36096372e-05]]), array([[-0.00595516],\n",
      "       [-0.00557489],\n",
      "       [ 0.00399684]])]\n",
      "gradients_biases:  [array([-1.11919949e-03, -1.37305202e-03,  3.26380948e-05]), array([0.00622455])]\n",
      "Iteration 706, Cost: 0.2458778208637566\n",
      "gradient_weights:  [array([[-2.80989612e-03, -3.39534775e-03,  1.53067419e-04],\n",
      "       [-3.64868207e-03, -5.57884666e-03,  2.40836463e-05]]), array([[-0.00595595],\n",
      "       [-0.00557885],\n",
      "       [ 0.00400131]])]\n",
      "gradients_biases:  [array([-1.12099920e-03, -1.37387146e-03,  3.33401538e-05]), array([0.00622689])]\n",
      "Iteration 707, Cost: 0.24586841296724798\n",
      "gradient_weights:  [array([[-2.81574880e-03, -3.40063762e-03,  1.56216438e-04],\n",
      "       [-3.65635634e-03, -5.58666351e-03,  2.45567465e-05]]), array([[-0.00595675],\n",
      "       [-0.00558281],\n",
      "       [ 0.00400578]])]\n",
      "gradients_biases:  [array([-1.12279438e-03, -1.37468492e-03,  3.40430017e-05]), array([0.00622924])]\n",
      "Iteration 708, Cost: 0.24585898818299945\n",
      "gradient_weights:  [array([[-2.82160261e-03, -3.40593228e-03,  1.59366099e-04],\n",
      "       [-3.66403133e-03, -5.59448355e-03,  2.50289296e-05]]), array([[-0.00595755],\n",
      "       [-0.00558678],\n",
      "       [ 0.00401027]])]\n",
      "gradients_biases:  [array([-1.12458499e-03, -1.37549236e-03,  3.47466355e-05]), array([0.0062316])]\n",
      "Iteration 709, Cost: 0.24584954646844637\n",
      "gradient_weights:  [array([[-2.82745756e-03, -3.41123175e-03,  1.62516396e-04],\n",
      "       [-3.67170704e-03, -5.60230677e-03,  2.55001874e-05]]), array([[-0.00595837],\n",
      "       [-0.00559075],\n",
      "       [ 0.00401477]])]\n",
      "gradients_biases:  [array([-1.12637104e-03, -1.37629378e-03,  3.54510521e-05]), array([0.00623397])]\n",
      "Iteration 710, Cost: 0.24584008778100325\n",
      "gradient_weights:  [array([[-2.83331364e-03, -3.41653602e-03,  1.65667323e-04],\n",
      "       [-3.67938349e-03, -5.61013315e-03,  2.59705115e-05]]), array([[-0.00595919],\n",
      "       [-0.00559473],\n",
      "       [ 0.00401927]])]\n",
      "gradients_biases:  [array([-1.12815249e-03, -1.37708915e-03,  3.61562482e-05]), array([0.00623634])]\n",
      "Iteration 711, Cost: 0.24583061207806423\n",
      "gradient_weights:  [array([[-2.83917087e-03, -3.42184509e-03,  1.68818875e-04],\n",
      "       [-3.68706065e-03, -5.61796270e-03,  2.64398937e-05]]), array([[-0.00596001],\n",
      "       [-0.00559872],\n",
      "       [ 0.00402379]])]\n",
      "gradients_biases:  [array([-1.12992934e-03, -1.37787844e-03,  3.68622208e-05]), array([0.00623872])]\n",
      "Iteration 712, Cost: 0.24582111931700296\n",
      "gradient_weights:  [array([[-2.84502924e-03, -3.42715897e-03,  1.71971047e-04],\n",
      "       [-3.69473855e-03, -5.62579539e-03,  2.69083257e-05]]), array([[-0.00596085],\n",
      "       [-0.00560272],\n",
      "       [ 0.00402831]])]\n",
      "gradients_biases:  [array([-1.13170156e-03, -1.37866164e-03,  3.75689667e-05]), array([0.00624111])]\n",
      "Iteration 713, Cost: 0.24581160945517283\n",
      "gradient_weights:  [array([[-2.85088877e-03, -3.43247764e-03,  1.75123833e-04],\n",
      "       [-3.70241717e-03, -5.63363122e-03,  2.73757991e-05]]), array([[-0.00596169],\n",
      "       [-0.00560672],\n",
      "       [ 0.00403285]])]\n",
      "gradients_biases:  [array([-1.13346915e-03, -1.37943871e-03,  3.82764826e-05]), array([0.0062435])]\n",
      "Iteration 714, Cost: 0.24580208244990692\n",
      "gradient_weights:  [array([[-2.85674946e-03, -3.43780111e-03,  1.78277228e-04],\n",
      "       [-3.71009651e-03, -5.64147019e-03,  2.78423056e-05]]), array([[-0.00596253],\n",
      "       [-0.00561072],\n",
      "       [ 0.00403739]])]\n",
      "gradients_biases:  [array([-1.13523209e-03, -1.38020965e-03,  3.89847655e-05]), array([0.00624591])]\n",
      "Iteration 715, Cost: 0.24579253825851838\n",
      "gradient_weights:  [array([[-2.86261130e-03, -3.44312939e-03,  1.81431225e-04],\n",
      "       [-3.71777658e-03, -5.64931227e-03,  2.83078367e-05]]), array([[-0.00596339],\n",
      "       [-0.00561474],\n",
      "       [ 0.00404194]])]\n",
      "gradients_biases:  [array([-1.13699037e-03, -1.38097442e-03,  3.96938121e-05]), array([0.00624831])]\n",
      "Iteration 716, Cost: 0.24578297683830047\n",
      "gradient_weights:  [array([[-2.86847431e-03, -3.44846246e-03,  1.84585821e-04],\n",
      "       [-3.72545737e-03, -5.65715747e-03,  2.87723841e-05]]), array([[-0.00596425],\n",
      "       [-0.00561876],\n",
      "       [ 0.0040465 ]])]\n",
      "gradients_biases:  [array([-1.13874395e-03, -1.38173301e-03,  4.04036192e-05]), array([0.00625073])]\n",
      "Iteration 717, Cost: 0.24577339814652643\n",
      "gradient_weights:  [array([[-2.87433849e-03, -3.45380032e-03,  1.87741008e-04],\n",
      "       [-3.73313888e-03, -5.66500577e-03,  2.92359392e-05]]), array([[-0.00596511],\n",
      "       [-0.00562278],\n",
      "       [ 0.00405107]])]\n",
      "gradients_biases:  [array([-1.14049284e-03, -1.38248538e-03,  4.11141836e-05]), array([0.00625316])]\n",
      "Iteration 718, Cost: 0.24576380214045002\n",
      "gradient_weights:  [array([[-2.88020384e-03, -3.45914299e-03,  1.90896782e-04],\n",
      "       [-3.74082112e-03, -5.67285717e-03,  2.96984935e-05]]), array([[-0.00596599],\n",
      "       [-0.00562681],\n",
      "       [ 0.00405565]])]\n",
      "gradients_biases:  [array([-1.14223702e-03, -1.38323153e-03,  4.18255020e-05]), array([0.00625559])]\n",
      "Iteration 719, Cost: 0.24575418877730526\n",
      "gradient_weights:  [array([[-2.88607037e-03, -3.46449045e-03,  1.94053135e-04],\n",
      "       [-3.74850408e-03, -5.68071165e-03,  3.01600387e-05]]), array([[-0.00596687],\n",
      "       [-0.00563085],\n",
      "       [ 0.00406024]])]\n",
      "gradients_biases:  [array([-1.14397647e-03, -1.38397142e-03,  4.25375713e-05]), array([0.00625803])]\n",
      "Iteration 720, Cost: 0.245744558014307\n",
      "gradient_weights:  [array([[-2.89193807e-03, -3.46984271e-03,  1.97210064e-04],\n",
      "       [-3.75618777e-03, -5.68856920e-03,  3.06205661e-05]]), array([[-0.00596776],\n",
      "       [-0.0056349 ],\n",
      "       [ 0.00406484]])]\n",
      "gradients_biases:  [array([-1.14571117e-03, -1.38470504e-03,  4.32503881e-05]), array([0.00626048])]\n",
      "Iteration 721, Cost: 0.24573490980865054\n",
      "gradient_weights:  [array([[-2.89780696e-03, -3.47519976e-03,  2.00367561e-04],\n",
      "       [-3.76387217e-03, -5.69642982e-03,  3.10800671e-05]]), array([[-0.00596865],\n",
      "       [-0.00563895],\n",
      "       [ 0.00406944]])]\n",
      "gradients_biases:  [array([-1.14744110e-03, -1.38543236e-03,  4.39639492e-05]), array([0.00626293])]\n",
      "Iteration 722, Cost: 0.24572524411751212\n",
      "gradient_weights:  [array([[-2.90367704e-03, -3.48056160e-03,  2.03525621e-04],\n",
      "       [-3.77155729e-03, -5.70429350e-03,  3.15385332e-05]]), array([[-0.00596955],\n",
      "       [-0.00564301],\n",
      "       [ 0.00407406]])]\n",
      "gradients_biases:  [array([-1.14916626e-03, -1.38615336e-03,  4.46782514e-05]), array([0.00626539])]\n",
      "Iteration 723, Cost: 0.24571556089804897\n",
      "gradient_weights:  [array([[-2.90954831e-03, -3.48592824e-03,  2.06684238e-04],\n",
      "       [-3.77924314e-03, -5.71216022e-03,  3.19959558e-05]]), array([[-0.00597046],\n",
      "       [-0.00564707],\n",
      "       [ 0.00407868]])]\n",
      "gradients_biases:  [array([-1.15088663e-03, -1.38686801e-03,  4.53932912e-05]), array([0.00626786])]\n",
      "Iteration 724, Cost: 0.2457058601073994\n",
      "gradient_weights:  [array([[-2.91542077e-03, -3.49129966e-03,  2.09843406e-04],\n",
      "       [-3.78692970e-03, -5.72002998e-03,  3.24523262e-05]]), array([[-0.00597138],\n",
      "       [-0.00565114],\n",
      "       [ 0.00408332]])]\n",
      "gradients_biases:  [array([-1.15260219e-03, -1.38757630e-03,  4.61090655e-05]), array([0.00627034])]\n",
      "Iteration 725, Cost: 0.24569614170268292\n",
      "gradient_weights:  [array([[-2.92129443e-03, -3.49667588e-03,  2.13003119e-04],\n",
      "       [-3.79461698e-03, -5.72790276e-03,  3.29076357e-05]]), array([[-0.0059723 ],\n",
      "       [-0.00565522],\n",
      "       [ 0.00408796]])]\n",
      "gradients_biases:  [array([-1.15431292e-03, -1.38827820e-03,  4.68255710e-05]), array([0.00627282])]\n",
      "Iteration 726, Cost: 0.2456864056410005\n",
      "gradient_weights:  [array([[-2.92716929e-03, -3.50205689e-03,  2.16163371e-04],\n",
      "       [-3.80230497e-03, -5.73577856e-03,  3.33618756e-05]]), array([[-0.00597323],\n",
      "       [-0.0056593 ],\n",
      "       [ 0.00409261]])]\n",
      "gradients_biases:  [array([-1.15601882e-03, -1.38897369e-03,  4.75428042e-05]), array([0.00627531])]\n",
      "Iteration 727, Cost: 0.24567665187943447\n",
      "gradient_weights:  [array([[-2.93304535e-03, -3.50744268e-03,  2.19324156e-04],\n",
      "       [-3.80999368e-03, -5.74365737e-03,  3.38150372e-05]]), array([[-0.00597416],\n",
      "       [-0.00566339],\n",
      "       [ 0.00409727]])]\n",
      "gradients_biases:  [array([-1.15771985e-03, -1.38966275e-03,  4.82607619e-05]), array([0.00627781])]\n",
      "Iteration 728, Cost: 0.24566688037504913\n",
      "gradient_weights:  [array([[-2.93892263e-03, -3.51283326e-03,  2.22485467e-04],\n",
      "       [-3.81768311e-03, -5.75153917e-03,  3.42671117e-05]]), array([[-0.0059751 ],\n",
      "       [-0.00566749],\n",
      "       [ 0.00410194]])]\n",
      "gradients_biases:  [array([-1.15941602e-03, -1.39034535e-03,  4.89794407e-05]), array([0.00628031])]\n",
      "Iteration 729, Cost: 0.2456570910848901\n",
      "gradient_weights:  [array([[-2.94480111e-03, -3.51822863e-03,  2.25647299e-04],\n",
      "       [-3.82537325e-03, -5.75942396e-03,  3.47180904e-05]]), array([[-0.00597605],\n",
      "       [-0.00567159],\n",
      "       [ 0.00410662]])]\n",
      "gradients_biases:  [array([-1.16110729e-03, -1.39102147e-03,  4.96988373e-05]), array([0.00628283])]\n",
      "Iteration 730, Cost: 0.24564728396598529\n",
      "gradient_weights:  [array([[-2.95068081e-03, -3.52362878e-03,  2.28809644e-04],\n",
      "       [-3.83306410e-03, -5.76731172e-03,  3.51679644e-05]]), array([[-0.00597701],\n",
      "       [-0.0056757 ],\n",
      "       [ 0.00411131]])]\n",
      "gradients_biases:  [array([-1.16279367e-03, -1.39169108e-03,  5.04189482e-05]), array([0.00628535])]\n",
      "Iteration 731, Cost: 0.24563745897534472\n",
      "gradient_weights:  [array([[-2.95656173e-03, -3.52903371e-03,  2.31972498e-04],\n",
      "       [-3.84075566e-03, -5.77520246e-03,  3.56167249e-05]]), array([[-0.00597797],\n",
      "       [-0.00567982],\n",
      "       [ 0.00411601]])]\n",
      "gradients_biases:  [array([-1.16447512e-03, -1.39235418e-03,  5.11397702e-05]), array([0.00628787])]\n",
      "Iteration 732, Cost: 0.24562761606996028\n",
      "gradient_weights:  [array([[-2.96244387e-03, -3.53444342e-03,  2.35135853e-04],\n",
      "       [-3.84844793e-03, -5.78309614e-03,  3.60643630e-05]]), array([[-0.00597894],\n",
      "       [-0.00568394],\n",
      "       [ 0.00412071]])]\n",
      "gradients_biases:  [array([-1.16615163e-03, -1.39301072e-03,  5.18612998e-05]), array([0.00629041])]\n",
      "Iteration 733, Cost: 0.24561775520680654\n",
      "gradient_weights:  [array([[-2.96832723e-03, -3.53985791e-03,  2.38299704e-04],\n",
      "       [-3.85614091e-03, -5.79099278e-03,  3.65108699e-05]]), array([[-0.00597992],\n",
      "       [-0.00568807],\n",
      "       [ 0.00412543]])]\n",
      "gradients_biases:  [array([-1.16782320e-03, -1.39366069e-03,  5.25835337e-05]), array([0.00629295])]\n",
      "Iteration 734, Cost: 0.2456078763428405\n",
      "gradient_weights:  [array([[-2.97421182e-03, -3.54527718e-03,  2.41464043e-04],\n",
      "       [-3.86383460e-03, -5.79889235e-03,  3.69562365e-05]]), array([[-0.0059809 ],\n",
      "       [-0.0056922 ],\n",
      "       [ 0.00413015]])]\n",
      "gradients_biases:  [array([-1.16948979e-03, -1.39430407e-03,  5.33064683e-05]), array([0.0062955])]\n",
      "Iteration 735, Cost: 0.2455979794350018\n",
      "gradient_weights:  [array([[-2.98009764e-03, -3.55070123e-03,  2.44628864e-04],\n",
      "       [-3.87152899e-03, -5.80679484e-03,  3.74004540e-05]]), array([[-0.00598189],\n",
      "       [-0.00569634],\n",
      "       [ 0.00413488]])]\n",
      "gradients_biases:  [array([-1.17115141e-03, -1.39494084e-03,  5.40301003e-05]), array([0.00629806])]\n",
      "Iteration 736, Cost: 0.24558806444021286\n",
      "gradient_weights:  [array([[-2.98598470e-03, -3.55613005e-03,  2.47794161e-04],\n",
      "       [-3.87922408e-03, -5.81470025e-03,  3.78435133e-05]]), array([[-0.00598288],\n",
      "       [-0.00570049],\n",
      "       [ 0.00413963]])]\n",
      "gradients_biases:  [array([-1.17280802e-03, -1.39557096e-03,  5.47544262e-05]), array([0.00630062])]\n",
      "Iteration 737, Cost: 0.24557813131537914\n",
      "gradient_weights:  [array([[-2.99187299e-03, -3.56156364e-03,  2.50959928e-04],\n",
      "       [-3.88691988e-03, -5.82260857e-03,  3.82854055e-05]]), array([[-0.00598389],\n",
      "       [-0.00570464],\n",
      "       [ 0.00414438]])]\n",
      "gradients_biases:  [array([-1.17445962e-03, -1.39619442e-03,  5.54794426e-05]), array([0.00630319])]\n",
      "Iteration 738, Cost: 0.24556818001738911\n",
      "gradient_weights:  [array([[-2.99776252e-03, -3.56700201e-03,  2.54126156e-04],\n",
      "       [-3.89461638e-03, -5.83051978e-03,  3.87261215e-05]]), array([[-0.0059849 ],\n",
      "       [-0.0057088 ],\n",
      "       [ 0.00414914]])]\n",
      "gradients_biases:  [array([-1.17610618e-03, -1.39681120e-03,  5.62051461e-05]), array([0.00630577])]\n",
      "Iteration 739, Cost: 0.24555821050311466\n",
      "gradient_weights:  [array([[-3.00365329e-03, -3.57244515e-03,  2.57292841e-04],\n",
      "       [-3.90231358e-03, -5.83843387e-03,  3.91656523e-05]]), array([[-0.00598591],\n",
      "       [-0.00571297],\n",
      "       [ 0.00415391]])]\n",
      "gradients_biases:  [array([-1.17774770e-03, -1.39742127e-03,  5.69315330e-05]), array([0.00630835])]\n",
      "Iteration 740, Cost: 0.24554822272941085\n",
      "gradient_weights:  [array([[-3.00954531e-03, -3.57789305e-03,  2.60459974e-04],\n",
      "       [-3.91001147e-03, -5.84635083e-03,  3.96039887e-05]]), array([[-0.00598694],\n",
      "       [-0.00571714],\n",
      "       [ 0.00415868]])]\n",
      "gradients_biases:  [array([-1.17938415e-03, -1.39802461e-03,  5.76586001e-05]), array([0.00631095])]\n",
      "Iteration 741, Cost: 0.24553821665311656\n",
      "gradient_weights:  [array([[-3.01543857e-03, -3.58334572e-03,  2.63627550e-04],\n",
      "       [-3.91771006e-03, -5.85427066e-03,  4.00411216e-05]]), array([[-0.00598796],\n",
      "       [-0.00572132],\n",
      "       [ 0.00416347]])]\n",
      "gradients_biases:  [array([-1.18101552e-03, -1.39862119e-03,  5.83863437e-05]), array([0.00631355])]\n",
      "Iteration 742, Cost: 0.2455281922310542\n",
      "gradient_weights:  [array([[-3.02133309e-03, -3.58880315e-03,  2.66795562e-04],\n",
      "       [-3.92540935e-03, -5.86219334e-03,  4.04770419e-05]]), array([[-0.005989  ],\n",
      "       [-0.0057255 ],\n",
      "       [ 0.00416827]])]\n",
      "gradients_biases:  [array([-1.18264180e-03, -1.39921100e-03,  5.91147603e-05]), array([0.00631615])]\n",
      "Iteration 743, Cost: 0.2455181494200304\n",
      "gradient_weights:  [array([[-3.02722885e-03, -3.59426535e-03,  2.69964002e-04],\n",
      "       [-3.93310932e-03, -5.87011886e-03,  4.09117405e-05]]), array([[-0.00599004],\n",
      "       [-0.00572969],\n",
      "       [ 0.00417307]])]\n",
      "gradients_biases:  [array([-1.18426297e-03, -1.39979401e-03,  5.98438465e-05]), array([0.00631877])]\n",
      "Iteration 744, Cost: 0.24550808817683534\n",
      "gradient_weights:  [array([[-3.03312588e-03, -3.59973230e-03,  2.73132864e-04],\n",
      "       [-3.94080999e-03, -5.87804721e-03,  4.13452080e-05]]), array([[-0.00599109],\n",
      "       [-0.00573389],\n",
      "       [ 0.00417789]])]\n",
      "gradients_biases:  [array([-1.18587900e-03, -1.40037019e-03,  6.05735986e-05]), array([0.00632139])]\n",
      "Iteration 745, Cost: 0.24549800845824377\n",
      "gradient_weights:  [array([[-3.03902416e-03, -3.60520401e-03,  2.76302141e-04],\n",
      "       [-3.94851134e-03, -5.88597837e-03,  4.17774353e-05]]), array([[-0.00599215],\n",
      "       [-0.00573809],\n",
      "       [ 0.00418271]])]\n",
      "gradients_biases:  [array([-1.18748990e-03, -1.40093953e-03,  6.13040132e-05]), array([0.00632402])]\n",
      "Iteration 746, Cost: 0.24548791022101474\n",
      "gradient_weights:  [array([[-3.04492370e-03, -3.61068048e-03,  2.79471825e-04],\n",
      "       [-3.95621338e-03, -5.89391235e-03,  4.22084131e-05]]), array([[-0.00599321],\n",
      "       [-0.0057423 ],\n",
      "       [ 0.00418754]])]\n",
      "gradients_biases:  [array([-1.18909563e-03, -1.40150200e-03,  6.20350866e-05]), array([0.00632665])]\n",
      "Iteration 747, Cost: 0.24547779342189174\n",
      "gradient_weights:  [array([[-3.05082450e-03, -3.61616171e-03,  2.82641910e-04],\n",
      "       [-3.96391610e-03, -5.90184912e-03,  4.26381321e-05]]), array([[-0.00599428],\n",
      "       [-0.00574652],\n",
      "       [ 0.00419238]])]\n",
      "gradients_biases:  [array([-1.19069618e-03, -1.40205757e-03,  6.27668154e-05]), array([0.00632929])]\n",
      "Iteration 748, Cost: 0.2454676580176031\n",
      "gradient_weights:  [array([[-3.05672657e-03, -3.62164768e-03,  2.85812390e-04],\n",
      "       [-3.97161951e-03, -5.90978868e-03,  4.30665830e-05]]), array([[-0.00599536],\n",
      "       [-0.00575074],\n",
      "       [ 0.00419723]])]\n",
      "gradients_biases:  [array([-1.19229154e-03, -1.40260623e-03,  6.34991959e-05]), array([0.00633194])]\n",
      "Iteration 749, Cost: 0.2454575039648618\n",
      "gradient_weights:  [array([[-3.06262991e-03, -3.62713840e-03,  2.88983255e-04],\n",
      "       [-3.97932359e-03, -5.91773101e-03,  4.34937564e-05]]), array([[-0.00599644],\n",
      "       [-0.00575497],\n",
      "       [ 0.00420209]])]\n",
      "gradients_biases:  [array([-1.19388170e-03, -1.40314795e-03,  6.42322245e-05]), array([0.0063346])]\n",
      "Iteration 750, Cost: 0.2454473312203661\n",
      "gradient_weights:  [array([[-3.06853451e-03, -3.63263387e-03,  2.92154500e-04],\n",
      "       [-3.98702835e-03, -5.92567611e-03,  4.39196430e-05]]), array([[-0.00599753],\n",
      "       [-0.00575921],\n",
      "       [ 0.00420696]])]\n",
      "gradients_biases:  [array([-1.19546663e-03, -1.40368271e-03,  6.49658976e-05]), array([0.00633727])]\n",
      "Iteration 751, Cost: 0.2454371397407992\n",
      "gradient_weights:  [array([[-3.07444039e-03, -3.63813409e-03,  2.95326117e-04],\n",
      "       [-3.99473378e-03, -5.93362396e-03,  4.43442334e-05]]), array([[-0.00599862],\n",
      "       [-0.00576345],\n",
      "       [ 0.00421184]])]\n",
      "gradients_biases:  [array([-1.19704631e-03, -1.40421048e-03,  6.57002116e-05]), array([0.00633994])]\n",
      "Iteration 752, Cost: 0.2454269294828299\n",
      "gradient_weights:  [array([[-3.08034754e-03, -3.64363904e-03,  2.98498099e-04],\n",
      "       [-4.00243989e-03, -5.94157455e-03,  4.47675181e-05]]), array([[-0.00599973],\n",
      "       [-0.00576769],\n",
      "       [ 0.00421672]])]\n",
      "gradients_biases:  [array([-1.19862074e-03, -1.40473125e-03,  6.64351629e-05]), array([0.00634261])]\n",
      "Iteration 753, Cost: 0.2454167004031123\n",
      "gradient_weights:  [array([[-3.08625597e-03, -3.64914874e-03,  3.01670438e-04],\n",
      "       [-4.01014667e-03, -5.94952787e-03,  4.51894876e-05]]), array([[-0.00600084],\n",
      "       [-0.00577195],\n",
      "       [ 0.00422162]])]\n",
      "gradients_biases:  [array([-1.20018989e-03, -1.40524498e-03,  6.71707478e-05]), array([0.0063453])]\n",
      "Iteration 754, Cost: 0.24540645245828646\n",
      "gradient_weights:  [array([[-3.09216567e-03, -3.65466317e-03,  3.04843127e-04],\n",
      "       [-4.01785412e-03, -5.95748391e-03,  4.56101325e-05]]), array([[-0.00600195],\n",
      "       [-0.00577621],\n",
      "       [ 0.00422652]])]\n",
      "gradients_biases:  [array([-1.20175376e-03, -1.40575166e-03,  6.79069627e-05]), array([0.00634799])]\n",
      "Iteration 755, Cost: 0.24539618560497803\n",
      "gradient_weights:  [array([[-3.09807666e-03, -3.66018234e-03,  3.08016159e-04],\n",
      "       [-4.02556223e-03, -5.96544266e-03,  4.60294433e-05]]), array([[-0.00600307],\n",
      "       [-0.00578047],\n",
      "       [ 0.00423143]])]\n",
      "gradients_biases:  [array([-1.20331232e-03, -1.40625126e-03,  6.86438038e-05]), array([0.00635069])]\n",
      "Iteration 756, Cost: 0.2453858997997989\n",
      "gradient_weights:  [array([[-3.10398893e-03, -3.66570624e-03,  3.11189527e-04],\n",
      "       [-4.03327100e-03, -5.97340410e-03,  4.64474104e-05]]), array([[-0.0060042 ],\n",
      "       [-0.00578474],\n",
      "       [ 0.00423635]])]\n",
      "gradients_biases:  [array([-1.20486555e-03, -1.40674375e-03,  6.93812675e-05]), array([0.0063534])]\n",
      "Iteration 757, Cost: 0.24537559499934725\n",
      "gradient_weights:  [array([[-3.10990248e-03, -3.67123486e-03,  3.14363222e-04],\n",
      "       [-4.04098043e-03, -5.98136823e-03,  4.68640242e-05]]), array([[-0.00600534],\n",
      "       [-0.00578902],\n",
      "       [ 0.00424128]])]\n",
      "gradients_biases:  [array([-1.20641345e-03, -1.40722912e-03,  7.01193502e-05]), array([0.00635611])]\n",
      "Iteration 758, Cost: 0.2453652711602074\n",
      "gradient_weights:  [array([[-3.11581731e-03, -3.67676822e-03,  3.17537236e-04],\n",
      "       [-4.04869052e-03, -5.98933503e-03,  4.72792752e-05]]), array([[-0.00600648],\n",
      "       [-0.00579331],\n",
      "       [ 0.00424622]])]\n",
      "gradients_biases:  [array([-1.20795599e-03, -1.40770735e-03,  7.08580480e-05]), array([0.00635883])]\n",
      "Iteration 759, Cost: 0.2453549282389505\n",
      "gradient_weights:  [array([[-3.12173344e-03, -3.68230630e-03,  3.20711564e-04],\n",
      "       [-4.05640127e-03, -5.99730449e-03,  4.76931537e-05]]), array([[-0.00600763],\n",
      "       [-0.0057976 ],\n",
      "       [ 0.00425117]])]\n",
      "gradients_biases:  [array([-1.20949316e-03, -1.40817840e-03,  7.15973573e-05]), array([0.00636156])]\n",
      "Iteration 760, Cost: 0.24534456619213435\n",
      "gradient_weights:  [array([[-3.12765085e-03, -3.68784910e-03,  3.23886196e-04],\n",
      "       [-4.06411266e-03, -6.00527660e-03,  4.81056499e-05]]), array([[-0.00600878],\n",
      "       [-0.0058019 ],\n",
      "       [ 0.00425613]])]\n",
      "gradients_biases:  [array([-1.21102494e-03, -1.40864226e-03,  7.23372744e-05]), array([0.0063643])]\n",
      "Iteration 761, Cost: 0.24533418497630372\n",
      "gradient_weights:  [array([[-3.13356956e-03, -3.69339662e-03,  3.27061125e-04],\n",
      "       [-4.07182471e-03, -6.01325135e-03,  4.85167544e-05]]), array([[-0.00600994],\n",
      "       [-0.0058062 ],\n",
      "       [ 0.00426109]])]\n",
      "gradients_biases:  [array([-1.21255132e-03, -1.40909890e-03,  7.30777954e-05]), array([0.00636704])]\n",
      "Iteration 762, Cost: 0.24532378454799036\n",
      "gradient_weights:  [array([[-3.13948956e-03, -3.69894885e-03,  3.30236344e-04],\n",
      "       [-4.07953740e-03, -6.02122873e-03,  4.89264572e-05]]), array([[-0.00601111],\n",
      "       [-0.00581051],\n",
      "       [ 0.00426607]])]\n",
      "gradients_biases:  [array([-1.21407228e-03, -1.40954829e-03,  7.38189167e-05]), array([0.00636979])]\n",
      "Iteration 763, Cost: 0.2453133648637135\n",
      "gradient_weights:  [array([[-3.14541085e-03, -3.70450579e-03,  3.33411844e-04],\n",
      "       [-4.08725073e-03, -6.02920871e-03,  4.93347487e-05]]), array([[-0.00601228],\n",
      "       [-0.00581482],\n",
      "       [ 0.00427105]])]\n",
      "gradients_biases:  [array([-1.21558780e-03, -1.40999042e-03,  7.45606345e-05]), array([0.00637254])]\n",
      "Iteration 764, Cost: 0.24530292587997982\n",
      "gradient_weights:  [array([[-3.15133344e-03, -3.71006745e-03,  3.36587617e-04],\n",
      "       [-4.09496471e-03, -6.03719130e-03,  4.97416192e-05]]), array([[-0.00601347],\n",
      "       [-0.00581915],\n",
      "       [ 0.00427604]])]\n",
      "gradients_biases:  [array([-1.21709786e-03, -1.41042526e-03,  7.53029450e-05]), array([0.0063753])]\n",
      "Iteration 765, Cost: 0.2452924675532836\n",
      "gradient_weights:  [array([[-3.15725733e-03, -3.71563381e-03,  3.39763657e-04],\n",
      "       [-4.10267932e-03, -6.04517649e-03,  5.01470587e-05]]), array([[-0.00601465],\n",
      "       [-0.00582347],\n",
      "       [ 0.00428104]])]\n",
      "gradients_biases:  [array([-1.21860246e-03, -1.41085279e-03,  7.60458443e-05]), array([0.00637807])]\n",
      "Iteration 766, Cost: 0.245281989840107\n",
      "gradient_weights:  [array([[-3.16318251e-03, -3.72120487e-03,  3.42939954e-04],\n",
      "       [-4.11039456e-03, -6.05316424e-03,  5.05510575e-05]]), array([[-0.00601585],\n",
      "       [-0.00582781],\n",
      "       [ 0.00428605]])]\n",
      "gradients_biases:  [array([-1.22010157e-03, -1.41127299e-03,  7.67893287e-05]), array([0.00638085])]\n",
      "Iteration 767, Cost: 0.24527149269692025\n",
      "gradient_weights:  [array([[-3.16910899e-03, -3.72678063e-03,  3.46116500e-04],\n",
      "       [-4.11811044e-03, -6.06115457e-03,  5.09536056e-05]]), array([[-0.00601705],\n",
      "       [-0.00583215],\n",
      "       [ 0.00429107]])]\n",
      "gradients_biases:  [array([-1.22159518e-03, -1.41168582e-03,  7.75333944e-05]), array([0.00638363])]\n",
      "Iteration 768, Cost: 0.24526097608018183\n",
      "gradient_weights:  [array([[-3.17503678e-03, -3.73236109e-03,  3.49293288e-04],\n",
      "       [-4.12582694e-03, -6.06914745e-03,  5.13546933e-05]]), array([[-0.00601825],\n",
      "       [-0.00583649],\n",
      "       [ 0.0042961 ]])]\n",
      "gradients_biases:  [array([-1.22308327e-03, -1.41209127e-03,  7.82780375e-05]), array([0.00638642])]\n",
      "Iteration 769, Cost: 0.24525043994633855\n",
      "gradient_weights:  [array([[-3.18096587e-03, -3.73794625e-03,  3.52470310e-04],\n",
      "       [-4.13354407e-03, -6.07714287e-03,  5.17543105e-05]]), array([[-0.00601947],\n",
      "       [-0.00584085],\n",
      "       [ 0.00430113]])]\n",
      "gradients_biases:  [array([-1.22456582e-03, -1.41248932e-03,  7.90232542e-05]), array([0.00638922])]\n",
      "Iteration 770, Cost: 0.24523988425182583\n",
      "gradient_weights:  [array([[-3.18689626e-03, -3.74353609e-03,  3.55647557e-04],\n",
      "       [-4.14126182e-03, -6.08514082e-03,  5.21524474e-05]]), array([[-0.00602069],\n",
      "       [-0.00584521],\n",
      "       [ 0.00430618]])]\n",
      "gradients_biases:  [array([-1.22604282e-03, -1.41287994e-03,  7.97690406e-05]), array([0.00639203])]\n",
      "Iteration 771, Cost: 0.2452293089530681\n",
      "gradient_weights:  [array([[-3.19282796e-03, -3.74913062e-03,  3.58825021e-04],\n",
      "       [-4.14898018e-03, -6.09314129e-03,  5.25490940e-05]]), array([[-0.00602191],\n",
      "       [-0.00584957],\n",
      "       [ 0.00431123]])]\n",
      "gradients_biases:  [array([-1.22751425e-03, -1.41326310e-03,  8.05153928e-05]), array([0.00639484])]\n",
      "Iteration 772, Cost: 0.24521871400647846\n",
      "gradient_weights:  [array([[-3.19876096e-03, -3.75472983e-03,  3.62002694e-04],\n",
      "       [-4.15669916e-03, -6.10114427e-03,  5.29442402e-05]]), array([[-0.00602314],\n",
      "       [-0.00585394],\n",
      "       [ 0.00431629]])]\n",
      "gradients_biases:  [array([-1.22898010e-03, -1.41363878e-03,  8.12623070e-05]), array([0.00639766])]\n",
      "Iteration 773, Cost: 0.24520809936845944\n",
      "gradient_weights:  [array([[-3.20469528e-03, -3.76033372e-03,  3.65180568e-04],\n",
      "       [-4.16441875e-03, -6.10914973e-03,  5.33378759e-05]]), array([[-0.00602438],\n",
      "       [-0.00585832],\n",
      "       [ 0.00432137]])]\n",
      "gradients_biases:  [array([-1.23044035e-03, -1.41400697e-03,  8.20097792e-05]), array([0.00640048])]\n",
      "Iteration 774, Cost: 0.2451974649954029\n",
      "gradient_weights:  [array([[-3.21063090e-03, -3.76594229e-03,  3.68358634e-04],\n",
      "       [-4.17213895e-03, -6.11715768e-03,  5.37299913e-05]]), array([[-0.00602563],\n",
      "       [-0.0058627 ],\n",
      "       [ 0.00432645]])]\n",
      "gradients_biases:  [array([-1.23189498e-03, -1.41436763e-03,  8.27578056e-05]), array([0.00640331])]\n",
      "Iteration 775, Cost: 0.24518681084369026\n",
      "gradient_weights:  [array([[-3.21656783e-03, -3.77155553e-03,  3.71536883e-04],\n",
      "       [-4.17985975e-03, -6.12516810e-03,  5.41205761e-05]]), array([[-0.00602688],\n",
      "       [-0.00586709],\n",
      "       [ 0.00433153]])]\n",
      "gradients_biases:  [array([-1.23334397e-03, -1.41472074e-03,  8.35063823e-05]), array([0.00640615])]\n",
      "Iteration 776, Cost: 0.24517613686969275\n",
      "gradient_weights:  [array([[-3.22250607e-03, -3.77717344e-03,  3.74715308e-04],\n",
      "       [-4.18758116e-03, -6.13318097e-03,  5.45096202e-05]]), array([[-0.00602814],\n",
      "       [-0.00587149],\n",
      "       [ 0.00433663]])]\n",
      "gradients_biases:  [array([-1.23478732e-03, -1.41506628e-03,  8.42555052e-05]), array([0.006409])]\n",
      "Iteration 777, Cost: 0.24516544302977145\n",
      "gradient_weights:  [array([[-3.22844562e-03, -3.78279602e-03,  3.77893900e-04],\n",
      "       [-4.19530315e-03, -6.14119628e-03,  5.48971134e-05]]), array([[-0.0060294 ],\n",
      "       [-0.00587589],\n",
      "       [ 0.00434174]])]\n",
      "gradients_biases:  [array([-1.23622500e-03, -1.41540423e-03,  8.50051704e-05]), array([0.00641185])]\n",
      "Iteration 778, Cost: 0.2451547292802778\n",
      "gradient_weights:  [array([[-3.23438648e-03, -3.78842326e-03,  3.81072650e-04],\n",
      "       [-4.20302575e-03, -6.14921403e-03,  5.52830457e-05]]), array([[-0.00603067],\n",
      "       [-0.00588029],\n",
      "       [ 0.00434685]])]\n",
      "gradients_biases:  [array([-1.23765699e-03, -1.41573455e-03,  8.57553741e-05]), array([0.00641471])]\n",
      "Iteration 779, Cost: 0.2451439955775535\n",
      "gradient_weights:  [array([[-3.24032865e-03, -3.79405515e-03,  3.84251549e-04],\n",
      "       [-4.21074893e-03, -6.15723419e-03,  5.56674067e-05]]), array([[-0.00603195],\n",
      "       [-0.00588471],\n",
      "       [ 0.00435198]])]\n",
      "gradients_biases:  [array([-1.23908328e-03, -1.41605724e-03,  8.65061121e-05]), array([0.00641758])]\n",
      "Iteration 780, Cost: 0.2451332418779309\n",
      "gradient_weights:  [array([[-3.24627214e-03, -3.79969170e-03,  3.87430589e-04],\n",
      "       [-4.21847269e-03, -6.16525676e-03,  5.60501863e-05]]), array([[-0.00603323],\n",
      "       [-0.00588913],\n",
      "       [ 0.00435711]])]\n",
      "gradients_biases:  [array([-1.24050386e-03, -1.41637226e-03,  8.72573805e-05]), array([0.00642045])]\n",
      "Iteration 781, Cost: 0.24512246813773314\n",
      "gradient_weights:  [array([[-3.25221694e-03, -3.80533290e-03,  3.90609762e-04],\n",
      "       [-4.22619704e-03, -6.17328172e-03,  5.64313742e-05]]), array([[-0.00603452],\n",
      "       [-0.00589355],\n",
      "       [ 0.00436225]])]\n",
      "gradients_biases:  [array([-1.24191870e-03, -1.41667959e-03,  8.80091754e-05]), array([0.00642333])]\n",
      "Iteration 782, Cost: 0.24511167431327435\n",
      "gradient_weights:  [array([[-3.25816306e-03, -3.81097875e-03,  3.93789058e-04],\n",
      "       [-4.23392196e-03, -6.18130906e-03,  5.68109600e-05]]), array([[-0.00603582],\n",
      "       [-0.00589798],\n",
      "       [ 0.0043674 ]])]\n",
      "gradients_biases:  [array([-1.24332779e-03, -1.41697920e-03,  8.87614926e-05]), array([0.00642622])]\n",
      "Iteration 783, Cost: 0.2451008603608597\n",
      "gradient_weights:  [array([[-3.26411049e-03, -3.81662924e-03,  3.96968469e-04],\n",
      "       [-4.24164746e-03, -6.18933876e-03,  5.71889335e-05]]), array([[-0.00603712],\n",
      "       [-0.00590242],\n",
      "       [ 0.00437256]])]\n",
      "gradients_biases:  [array([-1.24473112e-03, -1.41727108e-03,  8.95143282e-05]), array([0.00642911])]\n",
      "Iteration 784, Cost: 0.245090026236786\n",
      "gradient_weights:  [array([[-3.27005924e-03, -3.82228436e-03,  4.00147985e-04],\n",
      "       [-4.24937352e-03, -6.19737082e-03,  5.75652842e-05]]), array([[-0.00603843],\n",
      "       [-0.00590687],\n",
      "       [ 0.00437773]])]\n",
      "gradients_biases:  [array([-1.24612866e-03, -1.41755520e-03,  9.02676781e-05]), array([0.00643201])]\n",
      "Iteration 785, Cost: 0.24507917189734157\n",
      "gradient_weights:  [array([[-3.27600930e-03, -3.82794412e-03,  4.03327598e-04],\n",
      "       [-4.25710015e-03, -6.20540522e-03,  5.79400019e-05]]), array([[-0.00603975],\n",
      "       [-0.00591132],\n",
      "       [ 0.0043829 ]])]\n",
      "gradients_biases:  [array([-1.24752041e-03, -1.41783154e-03,  9.10215383e-05]), array([0.00643492])]\n",
      "Iteration 786, Cost: 0.24506829729880653\n",
      "gradient_weights:  [array([[-3.28196068e-03, -3.83360851e-03,  4.06507300e-04],\n",
      "       [-4.26482734e-03, -6.21344195e-03,  5.83130760e-05]]), array([[-0.00604107],\n",
      "       [-0.00591577],\n",
      "       [ 0.00438809]])]\n",
      "gradients_biases:  [array([-1.24890633e-03, -1.41810006e-03,  9.17759047e-05]), array([0.00643783])]\n",
      "Iteration 787, Cost: 0.2450574023974532\n",
      "gradient_weights:  [array([[-3.28791338e-03, -3.83927753e-03,  4.09687080e-04],\n",
      "       [-4.27255509e-03, -6.22148099e-03,  5.86844962e-05]]), array([[-0.0060424 ],\n",
      "       [-0.00592023],\n",
      "       [ 0.00439328]])]\n",
      "gradients_biases:  [array([-1.25028643e-03, -1.41836076e-03,  9.25307733e-05]), array([0.00644075])]\n",
      "Iteration 788, Cost: 0.24504648714954574\n",
      "gradient_weights:  [array([[-3.29386739e-03, -3.84495117e-03,  4.12866930e-04],\n",
      "       [-4.28028339e-03, -6.22952234e-03,  5.90542519e-05]]), array([[-0.00604373],\n",
      "       [-0.0059247 ],\n",
      "       [ 0.00439848]])]\n",
      "gradients_biases:  [array([-1.25166067e-03, -1.41861360e-03,  9.32861398e-05]), array([0.00644368])]\n",
      "Iteration 789, Cost: 0.24503555151134132\n",
      "gradient_weights:  [array([[-3.29982272e-03, -3.85062942e-03,  4.16046840e-04],\n",
      "       [-4.28801224e-03, -6.23756597e-03,  5.94223327e-05]]), array([[-0.00604507],\n",
      "       [-0.00592918],\n",
      "       [ 0.00440369]])]\n",
      "gradients_biases:  [array([-1.25302905e-03, -1.41885855e-03,  9.40420003e-05]), array([0.00644661])]\n",
      "Iteration 790, Cost: 0.24502459543908936\n",
      "gradient_weights:  [array([[-3.30577937e-03, -3.85631229e-03,  4.19226802e-04],\n",
      "       [-4.29574164e-03, -6.24561188e-03,  5.97887281e-05]]), array([[-0.00604642],\n",
      "       [-0.00593366],\n",
      "       [ 0.00440891]])]\n",
      "gradients_biases:  [array([-1.25439155e-03, -1.41909561e-03,  9.47983506e-05]), array([0.00644955])]\n",
      "Iteration 791, Cost: 0.24501361888903236\n",
      "gradient_weights:  [array([[-3.31173734e-03, -3.86199976e-03,  4.22406807e-04],\n",
      "       [-4.30347157e-03, -6.25366004e-03,  6.01534274e-05]]), array([[-0.00604777],\n",
      "       [-0.00593814],\n",
      "       [ 0.00441414]])]\n",
      "gradients_biases:  [array([-1.25574814e-03, -1.41932474e-03,  9.55551866e-05]), array([0.0064525])]\n",
      "Iteration 792, Cost: 0.2450026218174059\n",
      "gradient_weights:  [array([[-3.31769662e-03, -3.86769184e-03,  4.25586844e-04],\n",
      "       [-4.31120204e-03, -6.26171046e-03,  6.05164201e-05]]), array([[-0.00604913],\n",
      "       [-0.00594263],\n",
      "       [ 0.00441938]])]\n",
      "gradients_biases:  [array([-1.25709882e-03, -1.41954592e-03,  9.63125040e-05]), array([0.00645546])]\n",
      "Iteration 793, Cost: 0.2449916041804388\n",
      "gradient_weights:  [array([[-3.32365722e-03, -3.87338852e-03,  4.28766905e-04],\n",
      "       [-4.31893304e-03, -6.26976312e-03,  6.08776956e-05]]), array([[-0.00605049],\n",
      "       [-0.00594713],\n",
      "       [ 0.00442462]])]\n",
      "gradients_biases:  [array([-1.25844357e-03, -1.41975912e-03,  9.70702989e-05]), array([0.00645842])]\n",
      "Iteration 794, Cost: 0.24498056593435366\n",
      "gradient_weights:  [array([[-3.32961914e-03, -3.87908980e-03,  4.31946980e-04],\n",
      "       [-4.32666456e-03, -6.27781799e-03,  6.12372432e-05]]), array([[-0.00605187],\n",
      "       [-0.00595164],\n",
      "       [ 0.00442987]])]\n",
      "gradients_biases:  [array([-1.25978237e-03, -1.41996433e-03,  9.78285670e-05]), array([0.00646139])]\n",
      "Iteration 795, Cost: 0.24496950703536657\n",
      "gradient_weights:  [array([[-3.33558238e-03, -3.88479566e-03,  4.35127061e-04],\n",
      "       [-4.33439661e-03, -6.28587507e-03,  6.15950522e-05]]), array([[-0.00605324],\n",
      "       [-0.00595615],\n",
      "       [ 0.00443514]])]\n",
      "gradients_biases:  [array([-1.26111520e-03, -1.42016151e-03,  9.85873040e-05]), array([0.00646436])]\n",
      "Iteration 796, Cost: 0.24495842743968776\n",
      "gradient_weights:  [array([[-3.34154693e-03, -3.89050611e-03,  4.38307136e-04],\n",
      "       [-4.34212917e-03, -6.29393435e-03,  6.19511120e-05]]), array([[-0.00605463],\n",
      "       [-0.00596066],\n",
      "       [ 0.00444041]])]\n",
      "gradients_biases:  [array([-1.26244205e-03, -1.42035065e-03,  9.93465059e-05]), array([0.00646734])]\n",
      "Iteration 797, Cost: 0.24494732710352157\n",
      "gradient_weights:  [array([[-3.34751280e-03, -3.89622115e-03,  4.41487198e-04],\n",
      "       [-4.34986224e-03, -6.30199581e-03,  6.23054118e-05]]), array([[-0.00605602],\n",
      "       [-0.00596518],\n",
      "       [ 0.00444569]])]\n",
      "gradients_biases:  [array([-0.00126376, -0.00142053,  0.00010011]), array([0.00647033])]\n",
      "Iteration 798, Cost: 0.2449362059830669\n",
      "gradient_weights:  [array([[-3.35347999e-03, -3.90194075e-03,  4.44667236e-04],\n",
      "       [-4.35759583e-03, -6.31005943e-03,  6.26579407e-05]]), array([[-0.00605742],\n",
      "       [-0.00596971],\n",
      "       [ 0.00445098]])]\n",
      "gradients_biases:  [array([-0.00126508, -0.0014207 ,  0.00010087]), array([0.00647332])]\n",
      "Iteration 799, Cost: 0.24492506403451736\n",
      "gradient_weights:  [array([[-3.35944850e-03, -3.90766493e-03,  4.47847240e-04],\n",
      "       [-4.36532991e-03, -6.31812521e-03,  6.30086882e-05]]), array([[-0.00605882],\n",
      "       [-0.00597424],\n",
      "       [ 0.00445627]])]\n",
      "gradients_biases:  [array([-0.00126639, -0.00142087,  0.00010163]), array([0.00647632])]\n",
      "Iteration 800, Cost: 0.2449139012140611\n",
      "gradient_weights:  [array([[-3.36541832e-03, -3.91339368e-03,  4.51027202e-04],\n",
      "       [-4.37306449e-03, -6.32619313e-03,  6.33576432e-05]]), array([[-0.00606023],\n",
      "       [-0.00597878],\n",
      "       [ 0.00446158]])]\n",
      "gradients_biases:  [array([-0.00126769, -0.00142103,  0.00010239]), array([0.00647933])]\n",
      "Iteration 801, Cost: 0.24490271747788173\n",
      "gradient_weights:  [array([[-3.37138945e-03, -3.91912699e-03,  4.54207111e-04],\n",
      "       [-4.38079957e-03, -6.33426318e-03,  6.37047950e-05]]), array([[-0.00606164],\n",
      "       [-0.00598333],\n",
      "       [ 0.00446689]])]\n",
      "gradients_biases:  [array([-0.00126899, -0.00142117,  0.00010315]), array([0.00648235])]\n",
      "Iteration 802, Cost: 0.24489151278215815\n",
      "gradient_weights:  [array([[-3.37736191e-03, -3.92486485e-03,  4.57386957e-04],\n",
      "       [-4.38853513e-03, -6.34233534e-03,  6.40501326e-05]]), array([[-0.00606306],\n",
      "       [-0.00598788],\n",
      "       [ 0.00447221]])]\n",
      "gradients_biases:  [array([-0.00127028, -0.00142132,  0.00010391]), array([0.00648537])]\n",
      "Iteration 803, Cost: 0.24488028708306464\n",
      "gradient_weights:  [array([[-3.38333567e-03, -3.93060726e-03,  4.60566731e-04],\n",
      "       [-4.39627117e-03, -6.35040960e-03,  6.43936453e-05]]), array([[-0.00606449],\n",
      "       [-0.00599244],\n",
      "       [ 0.00447754]])]\n",
      "gradients_biases:  [array([-0.00127156, -0.00142145,  0.00010467]), array([0.00648839])]\n",
      "Iteration 804, Cost: 0.24486904033677148\n",
      "gradient_weights:  [array([[-3.38931076e-03, -3.93635423e-03,  4.63746422e-04],\n",
      "       [-4.40400770e-03, -6.35848594e-03,  6.47353219e-05]]), array([[-0.00606593],\n",
      "       [-0.005997  ],\n",
      "       [ 0.00448288]])]\n",
      "gradients_biases:  [array([-0.00127284, -0.00142157,  0.00010544]), array([0.00649143])]\n",
      "Iteration 805, Cost: 0.2448577724994449\n",
      "gradient_weights:  [array([[-3.39528715e-03, -3.94210573e-03,  4.66926022e-04],\n",
      "       [-4.41174469e-03, -6.36656435e-03,  6.50751516e-05]]), array([[-0.00606737],\n",
      "       [-0.00600157],\n",
      "       [ 0.00448823]])]\n",
      "gradients_biases:  [array([-0.00127411, -0.00142169,  0.0001062 ]), array([0.00649447])]\n",
      "Iteration 806, Cost: 0.24484648352724742\n",
      "gradient_weights:  [array([[-3.40126486e-03, -3.94786177e-03,  4.70105519e-04],\n",
      "       [-4.41948215e-03, -6.37464482e-03,  6.54131233e-05]]), array([[-0.00606881],\n",
      "       [-0.00600615],\n",
      "       [ 0.00449359]])]\n",
      "gradients_biases:  [array([-0.00127538, -0.00142179,  0.00010696]), array([0.00649752])]\n",
      "Iteration 807, Cost: 0.24483517337633798\n",
      "gradient_weights:  [array([[-3.40724388e-03, -3.95362234e-03,  4.73284904e-04],\n",
      "       [-4.42722008e-03, -6.38272733e-03,  6.57492261e-05]]), array([[-0.00607026],\n",
      "       [-0.00601073],\n",
      "       [ 0.00449895]])]\n",
      "gradients_biases:  [array([-0.00127664, -0.00142189,  0.00010773]), array([0.00650057])]\n",
      "Iteration 808, Cost: 0.24482384200287235\n",
      "gradient_weights:  [array([[-3.41322421e-03, -3.95938744e-03,  4.76464166e-04],\n",
      "       [-4.43495846e-03, -6.39081187e-03,  6.60834488e-05]]), array([[-0.00607172],\n",
      "       [-0.00601531],\n",
      "       [ 0.00450433]])]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradients_biases:  [array([-0.00127789, -0.00142198,  0.00010849]), array([0.00650363])]\n",
      "Iteration 809, Cost: 0.24481248936300323\n",
      "gradient_weights:  [array([[-3.41920585e-03, -3.96515705e-03,  4.79643296e-04],\n",
      "       [-4.44269729e-03, -6.39889843e-03,  6.64157804e-05]]), array([[-0.00607319],\n",
      "       [-0.00601991],\n",
      "       [ 0.00450971]])]\n",
      "gradients_biases:  [array([-0.00127914, -0.00142207,  0.00010926]), array([0.0065067])]\n",
      "Iteration 810, Cost: 0.24480111541288063\n",
      "gradient_weights:  [array([[-3.42518881e-03, -3.97093118e-03,  4.82822284e-04],\n",
      "       [-4.45043657e-03, -6.40698698e-03,  6.67462097e-05]]), array([[-0.00607466],\n",
      "       [-0.00602451],\n",
      "       [ 0.0045151 ]])]\n",
      "gradients_biases:  [array([-0.00128038, -0.00142214,  0.00011002]), array([0.00650977])]\n",
      "Iteration 811, Cost: 0.24478972010865174\n",
      "gradient_weights:  [array([[-3.43117307e-03, -3.97670982e-03,  4.86001118e-04],\n",
      "       [-4.45817629e-03, -6.41507752e-03,  6.70747257e-05]]), array([[-0.00607613],\n",
      "       [-0.00602911],\n",
      "       [ 0.0045205 ]])]\n",
      "gradients_biases:  [array([-0.00128162, -0.00142221,  0.00011079]), array([0.00651285])]\n",
      "Iteration 812, Cost: 0.24477830340646173\n",
      "gradient_weights:  [array([[-3.43715864e-03, -3.98249297e-03,  4.89179789e-04],\n",
      "       [-4.46591644e-03, -6.42317003e-03,  6.74013171e-05]]), array([[-0.00607762],\n",
      "       [-0.00603372],\n",
      "       [ 0.0045259 ]])]\n",
      "gradients_biases:  [array([-0.00128284, -0.00142226,  0.00011155]), array([0.00651594])]\n",
      "Iteration 813, Cost: 0.2447668652624535\n",
      "gradient_weights:  [array([[-3.44314551e-03, -3.98828061e-03,  4.92358287e-04],\n",
      "       [-4.47365703e-03, -6.43126450e-03,  6.77259727e-05]]), array([[-0.0060791 ],\n",
      "       [-0.00603834],\n",
      "       [ 0.00453132]])]\n",
      "gradients_biases:  [array([-0.00128407, -0.00142231,  0.00011232]), array([0.00651903])]\n",
      "Iteration 814, Cost: 0.24475540563276832\n",
      "gradient_weights:  [array([[-3.44913370e-03, -3.99407275e-03,  4.95536600e-04],\n",
      "       [-4.48139803e-03, -6.43936092e-03,  6.80486813e-05]]), array([[-0.0060806 ],\n",
      "       [-0.00604296],\n",
      "       [ 0.00453674]])]\n",
      "gradients_biases:  [array([-0.00128528, -0.00142235,  0.00011309]), array([0.00652213])]\n",
      "Iteration 815, Cost: 0.24474392447354562\n",
      "gradient_weights:  [array([[-3.45512318e-03, -3.99986937e-03,  4.98714719e-04],\n",
      "       [-4.48913946e-03, -6.44745926e-03,  6.83694316e-05]]), array([[-0.0060821 ],\n",
      "       [-0.00604759],\n",
      "       [ 0.00454218]])]\n",
      "gradients_biases:  [array([-0.00128649, -0.00142238,  0.00011385]), array([0.00652523])]\n",
      "Iteration 816, Cost: 0.24473242174092363\n",
      "gradient_weights:  [array([[-3.46111397e-03, -4.00567047e-03,  5.01892633e-04],\n",
      "       [-4.49688130e-03, -6.45555951e-03,  6.86882124e-05]]), array([[-0.00608361],\n",
      "       [-0.00605222],\n",
      "       [ 0.00454762]])]\n",
      "gradients_biases:  [array([-0.0012877 , -0.00142241,  0.00011462]), array([0.00652835])]\n",
      "Iteration 817, Cost: 0.24472089739103953\n",
      "gradient_weights:  [array([[-3.46710607e-03, -4.01147605e-03,  5.05070331e-04],\n",
      "       [-4.50462355e-03, -6.46366167e-03,  6.90050122e-05]]), array([[-0.00608512],\n",
      "       [-0.00605686],\n",
      "       [ 0.00455307]])]\n",
      "gradients_biases:  [array([-0.00128889, -0.00142242,  0.00011539]), array([0.00653146])]\n",
      "Iteration 818, Cost: 0.24470935138002953\n",
      "gradient_weights:  [array([[-3.47309946e-03, -4.01728610e-03,  5.08247803e-04],\n",
      "       [-4.51236619e-03, -6.47176571e-03,  6.93198199e-05]]), array([[-0.00608664],\n",
      "       [-0.0060615 ],\n",
      "       [ 0.00455852]])]\n",
      "gradients_biases:  [array([-0.00129009, -0.00142243,  0.00011616]), array([0.00653459])]\n",
      "Iteration 819, Cost: 0.24469778366402933\n",
      "gradient_weights:  [array([[-3.47909415e-03, -4.02310062e-03,  5.11425038e-04],\n",
      "       [-4.52010924e-03, -6.47987161e-03,  6.96326239e-05]]), array([[-0.00608817],\n",
      "       [-0.00606615],\n",
      "       [ 0.00456399]])]\n",
      "gradients_biases:  [array([-0.00129127, -0.00142243,  0.00011693]), array([0.00653772])]\n",
      "Iteration 820, Cost: 0.2446861941991741\n",
      "gradient_weights:  [array([[-3.48509014e-03, -4.02891959e-03,  5.14602026e-04],\n",
      "       [-4.52785267e-03, -6.48797938e-03,  6.99434129e-05]]), array([[-0.0060897 ],\n",
      "       [-0.00607081],\n",
      "       [ 0.00456946]])]\n",
      "gradients_biases:  [array([-0.00129245, -0.00142241,  0.0001177 ]), array([0.00654086])]\n",
      "Iteration 821, Cost: 0.24467458294159922\n",
      "gradient_weights:  [array([[-3.49108743e-03, -4.03474301e-03,  5.17778755e-04],\n",
      "       [-4.53559648e-03, -6.49608898e-03,  7.02521754e-05]]), array([[-0.00609123],\n",
      "       [-0.00607547],\n",
      "       [ 0.00457495]])]\n",
      "gradients_biases:  [array([-0.00129362, -0.00142239,  0.00011847]), array([0.006544])]\n",
      "Iteration 822, Cost: 0.24466294984743991\n",
      "gradient_weights:  [array([[-3.49708601e-03, -4.04057088e-03,  5.20955214e-04],\n",
      "       [-4.54334068e-03, -6.50420041e-03,  7.05588999e-05]]), array([[-0.00609278],\n",
      "       [-0.00608014],\n",
      "       [ 0.00458044]])]\n",
      "gradients_biases:  [array([-0.00129479, -0.00142237,  0.00011923]), array([0.00654715])]\n",
      "Iteration 823, Cost: 0.2446512948728319\n",
      "gradient_weights:  [array([[-3.50308589e-03, -4.04640319e-03,  5.24131394e-04],\n",
      "       [-4.55108524e-03, -6.51231365e-03,  7.08635750e-05]]), array([[-0.00609433],\n",
      "       [-0.00608482],\n",
      "       [ 0.00458594]])]\n",
      "gradients_biases:  [array([-0.00129594, -0.00142233,  0.00012   ]), array([0.00655031])]\n",
      "Iteration 824, Cost: 0.24463961797391165\n",
      "gradient_weights:  [array([[-3.50908705e-03, -4.05223993e-03,  5.27307283e-04],\n",
      "       [-4.55883017e-03, -6.52042869e-03,  7.11661892e-05]]), array([[-0.00609588],\n",
      "       [-0.0060895 ],\n",
      "       [ 0.00459144]])]\n",
      "gradients_biases:  [array([-0.0012971 , -0.00142228,  0.00012078]), array([0.00655347])]\n",
      "Iteration 825, Cost: 0.24462791910681636\n",
      "gradient_weights:  [array([[-3.51508951e-03, -4.05808110e-03,  5.30482869e-04],\n",
      "       [-4.56657546e-03, -6.52854551e-03,  7.14667308e-05]]), array([[-0.00609744],\n",
      "       [-0.00609418],\n",
      "       [ 0.00459696]])]\n",
      "gradients_biases:  [array([-0.00129824, -0.00142223,  0.00012155]), array([0.00655664])]\n",
      "Iteration 826, Cost: 0.24461619822768438\n",
      "gradient_weights:  [array([[-3.52109326e-03, -4.06392669e-03,  5.33658142e-04],\n",
      "       [-4.57432110e-03, -6.53666410e-03,  7.17651883e-05]]), array([[-0.00609901],\n",
      "       [-0.00609887],\n",
      "       [ 0.00460249]])]\n",
      "gradients_biases:  [array([-0.00129938, -0.00142217,  0.00012232]), array([0.00655982])]\n",
      "Iteration 827, Cost: 0.24460445529265584\n",
      "gradient_weights:  [array([[-3.52709829e-03, -4.06977670e-03,  5.36833092e-04],\n",
      "       [-4.58206709e-03, -6.54478443e-03,  7.20615501e-05]]), array([[-0.00610058],\n",
      "       [-0.00610357],\n",
      "       [ 0.00460802]])]\n",
      "gradients_biases:  [array([-0.00130052, -0.00142209,  0.00012309]), array([0.006563])]\n",
      "Iteration 828, Cost: 0.244592690257872\n",
      "gradient_weights:  [array([[-3.53310461e-03, -4.07563111e-03,  5.40007705e-04],\n",
      "       [-4.58981342e-03, -6.55290650e-03,  7.23558045e-05]]), array([[-0.00610216],\n",
      "       [-0.00610827],\n",
      "       [ 0.00461356]])]\n",
      "gradients_biases:  [array([-0.00130164, -0.00142201,  0.00012386]), array([0.00656619])]\n",
      "Iteration 829, Cost: 0.24458090307947633\n",
      "gradient_weights:  [array([[-3.53911221e-03, -4.08148993e-03,  5.43181972e-04],\n",
      "       [-4.59756008e-03, -6.56103030e-03,  7.26479398e-05]]), array([[-0.00610375],\n",
      "       [-0.00611298],\n",
      "       [ 0.00461911]])]\n",
      "gradients_biases:  [array([-0.00130276, -0.00142192,  0.00012463]), array([0.00656938])]\n",
      "Iteration 830, Cost: 0.2445690937136146\n",
      "gradient_weights:  [array([[-3.54512109e-03, -4.08735314e-03,  5.46355882e-04],\n",
      "       [-4.60530707e-03, -6.56915580e-03,  7.29379444e-05]]), array([[-0.00610534],\n",
      "       [-0.00611769],\n",
      "       [ 0.00462467]])]\n",
      "gradients_biases:  [array([-0.00130388, -0.00142182,  0.00012541]), array([0.00657259])]\n",
      "Iteration 831, Cost: 0.24455726211643475\n",
      "gradient_weights:  [array([[-3.55113125e-03, -4.09322074e-03,  5.49529422e-04],\n",
      "       [-4.61305439e-03, -6.57728299e-03,  7.32258065e-05]]), array([[-0.00610694],\n",
      "       [-0.00612241],\n",
      "       [ 0.00463023]])]\n",
      "gradients_biases:  [array([-0.00130498, -0.00142172,  0.00012618]), array([0.00657579])]\n",
      "Iteration 832, Cost: 0.24454540824408771\n",
      "gradient_weights:  [array([[-3.55714269e-03, -4.09909273e-03,  5.52702582e-04],\n",
      "       [-4.62080202e-03, -6.58541185e-03,  7.35115144e-05]]), array([[-0.00610854],\n",
      "       [-0.00612714],\n",
      "       [ 0.00463581]])]\n",
      "gradients_biases:  [array([-0.00130608, -0.0014216 ,  0.00012695]), array([0.00657901])]\n",
      "Iteration 833, Cost: 0.24453353205272713\n",
      "gradient_weights:  [array([[-3.56315540e-03, -4.10496909e-03,  5.55875349e-04],\n",
      "       [-4.62854995e-03, -6.59354238e-03,  7.37950562e-05]]), array([[-0.00611015],\n",
      "       [-0.00613187],\n",
      "       [ 0.00464139]])]\n",
      "gradients_biases:  [array([-0.00130718, -0.00142147,  0.00012772]), array([0.00658223])]\n",
      "Iteration 834, Cost: 0.24452163349850997\n",
      "gradient_weights:  [array([[-3.56916938e-03, -4.11084982e-03,  5.59047714e-04],\n",
      "       [-4.63629819e-03, -6.60167455e-03,  7.40764202e-05]]), array([[-0.00611176],\n",
      "       [-0.00613661],\n",
      "       [ 0.00464698]])]\n",
      "gradients_biases:  [array([-0.00130826, -0.00142134,  0.0001285 ]), array([0.00658545])]\n",
      "Iteration 835, Cost: 0.24450971253759685\n",
      "gradient_weights:  [array([[-3.57518464e-03, -4.11673491e-03,  5.62219663e-04],\n",
      "       [-4.64404672e-03, -6.60980835e-03,  7.43555945e-05]]), array([[-0.00611338],\n",
      "       [-0.00614135],\n",
      "       [ 0.00465258]])]\n",
      "gradients_biases:  [array([-0.00130934, -0.0014212 ,  0.00012927]), array([0.00658868])]\n",
      "Iteration 836, Cost: 0.2444977691261519\n",
      "gradient_weights:  [array([[-3.58120116e-03, -4.12262436e-03,  5.65391186e-04],\n",
      "       [-4.65179554e-03, -6.61794376e-03,  7.46325672e-05]]), array([[-0.00611501],\n",
      "       [-0.0061461 ],\n",
      "       [ 0.00465818]])]\n",
      "gradients_biases:  [array([-0.00131042, -0.00142105,  0.00013005]), array([0.00659192])]\n",
      "Iteration 837, Cost: 0.24448580322034347\n",
      "gradient_weights:  [array([[-3.58721895e-03, -4.12851816e-03,  5.68562270e-04],\n",
      "       [-4.65954465e-03, -6.62608077e-03,  7.49073264e-05]]), array([[-0.00611664],\n",
      "       [-0.00615085],\n",
      "       [ 0.0046638 ]])]\n",
      "gradients_biases:  [array([-0.00131149, -0.00142088,  0.00013082]), array([0.00659517])]\n",
      "Iteration 838, Cost: 0.24447381477634422\n",
      "gradient_weights:  [array([[-3.59323800e-03, -4.13441629e-03,  5.71732905e-04],\n",
      "       [-4.66729402e-03, -6.63421937e-03,  7.51798601e-05]]), array([[-0.00611828],\n",
      "       [-0.00615561],\n",
      "       [ 0.00466942]])]\n",
      "gradients_biases:  [array([-0.00131255, -0.00142071,  0.0001316 ]), array([0.00659842])]\n",
      "Iteration 839, Cost: 0.24446180375033108\n",
      "gradient_weights:  [array([[-3.59925832e-03, -4.14031877e-03,  5.74903078e-04],\n",
      "       [-4.67504367e-03, -6.64235953e-03,  7.54501565e-05]]), array([[-0.00611992],\n",
      "       [-0.00616037],\n",
      "       [ 0.00467505]])]\n",
      "gradients_biases:  [array([-0.0013136 , -0.00142053,  0.00013237]), array([0.00660168])]\n",
      "Iteration 840, Cost: 0.24444977009848623\n",
      "gradient_weights:  [array([[-3.60527989e-03, -4.14622557e-03,  5.78072778e-04],\n",
      "       [-4.68279358e-03, -6.65050125e-03,  7.57182035e-05]]), array([[-0.00612157],\n",
      "       [-0.00616514],\n",
      "       [ 0.00468069]])]\n",
      "gradients_biases:  [array([-0.00131465, -0.00142035,  0.00013315]), array([0.00660494])]\n",
      "Iteration 841, Cost: 0.24443771377699663\n",
      "gradient_weights:  [array([[-3.61130272e-03, -4.15213669e-03,  5.81241993e-04],\n",
      "       [-4.69054374e-03, -6.65864450e-03,  7.59839890e-05]]), array([[-0.00612323],\n",
      "       [-0.00616992],\n",
      "       [ 0.00468634]])]\n",
      "gradients_biases:  [array([-0.00131569, -0.00142015,  0.00013392]), array([0.00660821])]\n",
      "Iteration 842, Cost: 0.24442563474205486\n",
      "gradient_weights:  [array([[-3.61732680e-03, -4.15805213e-03,  5.84410710e-04],\n",
      "       [-4.69829415e-03, -6.66678927e-03,  7.62475011e-05]]), array([[-0.00612489],\n",
      "       [-0.0061747 ],\n",
      "       [ 0.004692  ]])]\n",
      "gradients_biases:  [array([-0.00131672, -0.00141994,  0.0001347 ]), array([0.00661148])]\n",
      "Iteration 843, Cost: 0.24441353294985896\n",
      "gradient_weights:  [array([[-3.62335213e-03, -4.16397187e-03,  5.87578918e-04],\n",
      "       [-4.70604479e-03, -6.67493554e-03,  7.65087277e-05]]), array([[-0.00612656],\n",
      "       [-0.00617949],\n",
      "       [ 0.00469766]])]\n",
      "gradients_biases:  [array([-0.00131775, -0.00141973,  0.00013547]), array([0.00661476])]\n",
      "Iteration 844, Cost: 0.24440140835661295\n",
      "gradient_weights:  [array([[-3.62937871e-03, -4.16989591e-03,  5.90746606e-04],\n",
      "       [-4.71379568e-03, -6.68308331e-03,  7.67676566e-05]]), array([[-0.00612823],\n",
      "       [-0.00618428],\n",
      "       [ 0.00470334]])]\n",
      "gradients_biases:  [array([-0.00131877, -0.0014195 ,  0.00013625]), array([0.00661805])]\n",
      "Iteration 845, Cost: 0.24438926091852708\n",
      "gradient_weights:  [array([[-3.63540653e-03, -4.17582425e-03,  5.93913760e-04],\n",
      "       [-4.72154678e-03, -6.69123254e-03,  7.70242757e-05]]), array([[-0.00612991],\n",
      "       [-0.00618908],\n",
      "       [ 0.00470902]])]\n",
      "gradients_biases:  [array([-0.00131978, -0.00141927,  0.00013703]), array([0.00662135])]\n",
      "Iteration 846, Cost: 0.24437709059181817\n",
      "gradient_weights:  [array([[-3.64143560e-03, -4.18175687e-03,  5.97080368e-04],\n",
      "       [-4.72929811e-03, -6.69938324e-03,  7.72785728e-05]]), array([[-0.00613159],\n",
      "       [-0.00619388],\n",
      "       [ 0.0047147 ]])]\n",
      "gradients_biases:  [array([-0.00132079, -0.00141902,  0.0001378 ]), array([0.00662465])]\n",
      "Iteration 847, Cost: 0.24436489733270952\n",
      "gradient_weights:  [array([[-3.64746591e-03, -4.18769377e-03,  6.00246420e-04],\n",
      "       [-4.73704965e-03, -6.70753537e-03,  7.75305357e-05]]), array([[-0.00613329],\n",
      "       [-0.00619869],\n",
      "       [ 0.0047204 ]])]\n",
      "gradients_biases:  [array([-0.00132179, -0.00141877,  0.00013858]), array([0.00662795])]\n",
      "Iteration 848, Cost: 0.2443526810974317\n",
      "gradient_weights:  [array([[-3.65349745e-03, -4.19363493e-03,  6.03411901e-04],\n",
      "       [-4.74480139e-03, -6.71568894e-03,  7.77801523e-05]]), array([[-0.00613498],\n",
      "       [-0.00620351],\n",
      "       [ 0.00472611]])]\n",
      "gradients_biases:  [array([-0.00132278, -0.00141851,  0.00013936]), array([0.00663126])]\n",
      "Iteration 849, Cost: 0.2443404418422225\n",
      "gradient_weights:  [array([[-3.65953023e-03, -4.19958036e-03,  6.06576801e-04],\n",
      "       [-4.75255333e-03, -6.72384391e-03,  7.80274102e-05]]), array([[-0.00613668],\n",
      "       [-0.00620833],\n",
      "       [ 0.00473182]])]\n",
      "gradients_biases:  [array([-0.00132376, -0.00141824,  0.00014013]), array([0.00663458])]\n",
      "Iteration 850, Cost: 0.24432817952332736\n",
      "gradient_weights:  [array([[-3.66556424e-03, -4.20553005e-03,  6.09741106e-04],\n",
      "       [-4.76030546e-03, -6.73200027e-03,  7.82722972e-05]]), array([[-0.00613839],\n",
      "       [-0.00621315],\n",
      "       [ 0.00473754]])]\n",
      "gradients_biases:  [array([-0.00132474, -0.00141796,  0.00014091]), array([0.00663791])]\n",
      "Iteration 851, Cost: 0.24431589409699936\n",
      "gradient_weights:  [array([[-3.67159947e-03, -4.21148398e-03,  6.12904805e-04],\n",
      "       [-4.76805777e-03, -6.74015801e-03,  7.85148010e-05]]), array([[-0.00614011],\n",
      "       [-0.00621798],\n",
      "       [ 0.00474327]])]\n",
      "gradients_biases:  [array([-0.00132572, -0.00141767,  0.00014169]), array([0.00664124])]\n",
      "Iteration 852, Cost: 0.24430358551950004\n",
      "gradient_weights:  [array([[-3.67763593e-03, -4.21744215e-03,  6.16067884e-04],\n",
      "       [-4.77581025e-03, -6.74831711e-03,  7.87549091e-05]]), array([[-0.00614182],\n",
      "       [-0.00622282],\n",
      "       [ 0.00474901]])]\n",
      "gradients_biases:  [array([-0.00132668, -0.00141737,  0.00014247]), array([0.00664457])]\n",
      "Iteration 853, Cost: 0.24429125374709904\n",
      "gradient_weights:  [array([[-3.68367361e-03, -4.22340456e-03,  6.19230332e-04],\n",
      "       [-4.78356290e-03, -6.75647756e-03,  7.89926093e-05]]), array([[-0.00614355],\n",
      "       [-0.00622766],\n",
      "       [ 0.00475475]])]\n",
      "gradients_biases:  [array([-0.00132764, -0.00141706,  0.00014325]), array([0.00664791])]\n",
      "Iteration 854, Cost: 0.24427889873607517\n",
      "gradient_weights:  [array([[-3.68971251e-03, -4.22937119e-03,  6.22392135e-04],\n",
      "       [-4.79131572e-03, -6.76463934e-03,  7.92278891e-05]]), array([[-0.00614528],\n",
      "       [-0.00623251],\n",
      "       [ 0.0047605 ]])]\n",
      "gradients_biases:  [array([-0.00132859, -0.00141674,  0.00014402]), array([0.00665126])]\n",
      "Iteration 855, Cost: 0.24426652044271574\n",
      "gradient_weights:  [array([[-3.69575263e-03, -4.23534203e-03,  6.25553282e-04],\n",
      "       [-4.79906868e-03, -6.77280242e-03,  7.94607362e-05]]), array([[-0.00614702],\n",
      "       [-0.00623736],\n",
      "       [ 0.00476626]])]\n",
      "gradients_biases:  [array([-0.00132953, -0.00141641,  0.0001448 ]), array([0.00665462])]\n",
      "Iteration 856, Cost: 0.2442541188233176\n",
      "gradient_weights:  [array([[-3.70179395e-03, -4.24131709e-03,  6.28713760e-04],\n",
      "       [-4.80682179e-03, -6.78096681e-03,  7.96911380e-05]]), array([[-0.00614876],\n",
      "       [-0.00624222],\n",
      "       [ 0.00477203]])]\n",
      "gradients_biases:  [array([-0.00133047, -0.00141607,  0.00014558]), array([0.00665798])]\n",
      "Iteration 857, Cost: 0.24424169383418717\n",
      "gradient_weights:  [array([[-3.70783649e-03, -4.24729634e-03,  6.31873555e-04],\n",
      "       [-4.81457503e-03, -6.78913247e-03,  7.99190820e-05]]), array([[-0.00615051],\n",
      "       [-0.00624709],\n",
      "       [ 0.00477781]])]\n",
      "gradients_biases:  [array([-0.0013314 , -0.00141573,  0.00014636]), array([0.00666134])]\n",
      "Iteration 858, Cost: 0.2442292454316407\n",
      "gradient_weights:  [array([[-3.71388023e-03, -4.25327979e-03,  6.35032655e-04],\n",
      "       [-4.82232841e-03, -6.79729940e-03,  8.01445559e-05]]), array([[-0.00615226],\n",
      "       [-0.00625196],\n",
      "       [ 0.0047836 ]])]\n",
      "gradients_biases:  [array([-0.00133232, -0.00141537,  0.00014714]), array([0.00666471])]\n",
      "Iteration 859, Cost: 0.24421677357200447\n",
      "gradient_weights:  [array([[-3.71992517e-03, -4.25926742e-03,  6.38191047e-04],\n",
      "       [-4.83008190e-03, -6.80546757e-03,  8.03675469e-05]]), array([[-0.00615402],\n",
      "       [-0.00625683],\n",
      "       [ 0.00478939]])]\n",
      "gradients_biases:  [array([-0.00133324, -0.00141501,  0.00014792]), array([0.00666809])]\n",
      "Iteration 860, Cost: 0.24420427821161542\n",
      "gradient_weights:  [array([[-3.72597130e-03, -4.26525923e-03,  6.41348719e-04],\n",
      "       [-4.83783551e-03, -6.81363698e-03,  8.05880426e-05]]), array([[-0.00615578],\n",
      "       [-0.00626171],\n",
      "       [ 0.00479519]])]\n",
      "gradients_biases:  [array([-0.00133415, -0.00141463,  0.0001487 ]), array([0.00667147])]\n",
      "Iteration 861, Cost: 0.24419175930682097\n",
      "gradient_weights:  [array([[-3.73201863e-03, -4.27125520e-03,  6.44505658e-04],\n",
      "       [-4.84558922e-03, -6.82180760e-03,  8.08060303e-05]]), array([[-0.00615756],\n",
      "       [-0.0062666 ],\n",
      "       [ 0.004801  ]])]\n",
      "gradients_biases:  [array([-0.00133505, -0.00141425,  0.00014948]), array([0.00667486])]\n",
      "Iteration 862, Cost: 0.24417921681397975\n",
      "gradient_weights:  [array([[-3.73806715e-03, -4.27725534e-03,  6.47661849e-04],\n",
      "       [-4.85334303e-03, -6.82997941e-03,  8.10214975e-05]]), array([[-0.00615933],\n",
      "       [-0.00627149],\n",
      "       [ 0.00480682]])]\n",
      "gradients_biases:  [array([-0.00133595, -0.00141385,  0.00015026]), array([0.00667826])]\n",
      "Iteration 863, Cost: 0.24416665068946147\n",
      "gradient_weights:  [array([[-3.74411686e-03, -4.28325963e-03,  6.50817281e-04],\n",
      "       [-4.86109693e-03, -6.83815241e-03,  8.12344313e-05]]), array([[-0.00616111],\n",
      "       [-0.00627639],\n",
      "       [ 0.00481264]])]\n",
      "gradients_biases:  [array([-0.00133683, -0.00141345,  0.00015104]), array([0.00668166])]\n",
      "Iteration 864, Cost: 0.24415406088964775\n",
      "gradient_weights:  [array([[-3.75016775e-03, -4.28926806e-03,  6.53971941e-04],\n",
      "       [-4.86885091e-03, -6.84632657e-03,  8.14448193e-05]]), array([[-0.0061629 ],\n",
      "       [-0.00628129],\n",
      "       [ 0.00481847]])]\n",
      "gradients_biases:  [array([-0.00133772, -0.00141303,  0.00015182]), array([0.00668507])]\n",
      "Iteration 865, Cost: 0.24414144737093182\n",
      "gradient_weights:  [array([[-3.75621982e-03, -4.29528063e-03,  6.57125814e-04],\n",
      "       [-4.87660496e-03, -6.85450188e-03,  8.16526486e-05]]), array([[-0.00616469],\n",
      "       [-0.0062862 ],\n",
      "       [ 0.00482431]])]\n",
      "gradients_biases:  [array([-0.00133859, -0.00141261,  0.0001526 ]), array([0.00668848])]\n",
      "Iteration 866, Cost: 0.24412881008971926\n",
      "gradient_weights:  [array([[-3.76227307e-03, -4.30129733e-03,  6.60278888e-04],\n",
      "       [-4.88435908e-03, -6.86267832e-03,  8.18579066e-05]]), array([[-0.00616649],\n",
      "       [-0.00629111],\n",
      "       [ 0.00483016]])]\n",
      "gradients_biases:  [array([-0.00133946, -0.00141218,  0.00015338]), array([0.0066919])]\n",
      "Iteration 867, Cost: 0.24411614900242795\n",
      "gradient_weights:  [array([[-3.76832748e-03, -4.30731814e-03,  6.63431151e-04],\n",
      "       [-4.89211325e-03, -6.87085587e-03,  8.20605803e-05]]), array([[-0.0061683 ],\n",
      "       [-0.00629603],\n",
      "       [ 0.00483602]])]\n",
      "gradients_biases:  [array([-0.00134031, -0.00141173,  0.00015416]), array([0.00669532])]\n",
      "Iteration 868, Cost: 0.2441034640654888\n",
      "gradient_weights:  [array([[-3.77438306e-03, -4.31334307e-03,  6.66582587e-04],\n",
      "       [-4.89986747e-03, -6.87903452e-03,  8.22606571e-05]]), array([[-0.00617011],\n",
      "       [-0.00630095],\n",
      "       [ 0.00484188]])]\n",
      "gradients_biases:  [array([-0.00134117, -0.00141128,  0.00015494]), array([0.00669875])]\n",
      "Iteration 869, Cost: 0.2440907552353456\n",
      "gradient_weights:  [array([[-3.78043981e-03, -4.31937209e-03,  6.69733185e-04],\n",
      "       [-4.90762174e-03, -6.88721426e-03,  8.24581242e-05]]), array([[-0.00617192],\n",
      "       [-0.00630588],\n",
      "       [ 0.00484775]])]\n",
      "gradients_biases:  [array([-0.00134201, -0.00141082,  0.00015572]), array([0.00670219])]\n",
      "Iteration 870, Cost: 0.2440780224684556\n",
      "gradient_weights:  [array([[-3.78649772e-03, -4.32540521e-03,  6.72882930e-04],\n",
      "       [-4.91537603e-03, -6.89539505e-03,  8.26529686e-05]]), array([[-0.00617374],\n",
      "       [-0.00631081],\n",
      "       [ 0.00485363]])]\n",
      "gradients_biases:  [array([-0.00134285, -0.00141035,  0.0001565 ]), array([0.00670563])]\n",
      "Iteration 871, Cost: 0.24406526572128961\n",
      "gradient_weights:  [array([[-3.79255678e-03, -4.33144241e-03,  6.76031809e-04],\n",
      "       [-4.92313035e-03, -6.90357689e-03,  8.28451774e-05]]), array([[-0.00617557],\n",
      "       [-0.00631575],\n",
      "       [ 0.00485952]])]\n",
      "gradients_biases:  [array([-0.00134368, -0.00140987,  0.00015729]), array([0.00670908])]\n",
      "Iteration 872, Cost: 0.24405248495033255\n",
      "gradient_weights:  [array([[-3.79861699e-03, -4.33748369e-03,  6.79179808e-04],\n",
      "       [-4.93088468e-03, -6.91175976e-03,  8.30347379e-05]]), array([[-0.0061774 ],\n",
      "       [-0.0063207 ],\n",
      "       [ 0.00486542]])]\n",
      "gradients_biases:  [array([-0.0013445 , -0.00140937,  0.00015807]), array([0.00671253])]\n",
      "Iteration 873, Cost: 0.24403968011208366\n",
      "gradient_weights:  [array([[-3.80467834e-03, -4.34352903e-03,  6.82326915e-04],\n",
      "       [-4.93863902e-03, -6.91994365e-03,  8.32216369e-05]]), array([[-0.00617924],\n",
      "       [-0.00632565],\n",
      "       [ 0.00487132]])]\n",
      "gradients_biases:  [array([-0.00134532, -0.00140887,  0.00015885]), array([0.00671599])]\n",
      "Iteration 874, Cost: 0.24402685116305645\n",
      "gradient_weights:  [array([[-3.81074084e-03, -4.34957844e-03,  6.85473115e-04],\n",
      "       [-4.94639336e-03, -6.92812853e-03,  8.34058616e-05]]), array([[-0.00618108],\n",
      "       [-0.0063306 ],\n",
      "       [ 0.00487723]])]\n",
      "gradients_biases:  [array([-0.00134613, -0.00140836,  0.00015963]), array([0.00671945])]\n",
      "Iteration 875, Cost: 0.24401399805977955\n",
      "gradient_weights:  [array([[-3.81680448e-03, -4.35563189e-03,  6.88618395e-04],\n",
      "       [-4.95414769e-03, -6.93631439e-03,  8.35873990e-05]]), array([[-0.00618293],\n",
      "       [-0.00633556],\n",
      "       [ 0.00488315]])]\n",
      "gradients_biases:  [array([-0.00134693, -0.00140784,  0.00016041]), array([0.00672292])]\n",
      "Iteration 876, Cost: 0.2440011207587968\n",
      "gradient_weights:  [array([[-3.82286925e-03, -4.36168938e-03,  6.91762740e-04],\n",
      "       [-4.96190200e-03, -6.94450121e-03,  8.37662360e-05]]), array([[-0.00618479],\n",
      "       [-0.00634053],\n",
      "       [ 0.00488908]])]\n",
      "gradients_biases:  [array([-0.00134772, -0.00140731,  0.00016119]), array([0.0067264])]\n",
      "Iteration 877, Cost: 0.24398821921666733\n",
      "gradient_weights:  [array([[-3.82893515e-03, -4.36775090e-03,  6.94906138e-04],\n",
      "       [-4.96965628e-03, -6.95268897e-03,  8.39423596e-05]]), array([[-0.00618665],\n",
      "       [-0.0063455 ],\n",
      "       [ 0.00489501]])]\n",
      "gradients_biases:  [array([-0.00134851, -0.00140677,  0.00016197]), array([0.00672988])]\n",
      "Iteration 878, Cost: 0.24397529338996626\n",
      "gradient_weights:  [array([[-3.83500217e-03, -4.37381645e-03,  6.98048574e-04],\n",
      "       [-4.97741052e-03, -6.96087767e-03,  8.41157567e-05]]), array([[-0.00618851],\n",
      "       [-0.00635047],\n",
      "       [ 0.00490095]])]\n",
      "gradients_biases:  [array([-0.00134929, -0.00140621,  0.00016276]), array([0.00673337])]\n",
      "Iteration 879, Cost: 0.24396234323528465\n",
      "gradient_weights:  [array([[-3.84107031e-03, -4.37988600e-03,  7.01190034e-04],\n",
      "       [-4.98516472e-03, -6.96906727e-03,  8.42864142e-05]]), array([[-0.00619038],\n",
      "       [-0.00635546],\n",
      "       [ 0.0049069 ]])]\n",
      "gradients_biases:  [array([-0.00135006, -0.00140565,  0.00016354]), array([0.00673686])]\n",
      "Iteration 880, Cost: 0.24394936870923\n",
      "gradient_weights:  [array([[-3.84713957e-03, -4.38595956e-03,  7.04330504e-04],\n",
      "       [-4.99291886e-03, -6.97725776e-03,  8.44543190e-05]]), array([[-0.00619226],\n",
      "       [-0.00636044],\n",
      "       [ 0.00491286]])]\n",
      "gradients_biases:  [array([-0.00135083, -0.00140508,  0.00016432]), array([0.00674036])]\n",
      "Iteration 881, Cost: 0.24393636976842653\n",
      "gradient_weights:  [array([[-3.85320994e-03, -4.39203712e-03,  7.07469971e-04],\n",
      "       [-5.00067294e-03, -6.98544914e-03,  8.46194579e-05]]), array([[-0.00619414],\n",
      "       [-0.00636543],\n",
      "       [ 0.00491882]])]\n",
      "gradients_biases:  [array([-0.00135158, -0.0014045 ,  0.0001651 ]), array([0.00674386])]\n",
      "Iteration 882, Cost: 0.2439233463695154\n",
      "gradient_weights:  [array([[-3.85928142e-03, -4.39811866e-03,  7.10608420e-04],\n",
      "       [-5.00842695e-03, -6.99364136e-03,  8.47818176e-05]]), array([[-0.00619603],\n",
      "       [-0.00637043],\n",
      "       [ 0.0049248 ]])]\n",
      "gradients_biases:  [array([-0.00135233, -0.00140391,  0.00016588]), array([0.00674737])]\n",
      "Iteration 883, Cost: 0.24391029846915535\n",
      "gradient_weights:  [array([[-3.86535399e-03, -4.40420418e-03,  7.13745837e-04],\n",
      "       [-5.01618088e-03, -7.00183443e-03,  8.49413851e-05]]), array([[-0.00619792],\n",
      "       [-0.00637543],\n",
      "       [ 0.00493078]])]\n",
      "gradients_biases:  [array([-0.00135307, -0.00140331,  0.00016667]), array([0.00675089])]\n",
      "Iteration 884, Cost: 0.24389722602402242\n",
      "gradient_weights:  [array([[-3.87142766e-03, -4.41029366e-03,  7.16882208e-04],\n",
      "       [-5.02393472e-03, -7.01002833e-03,  8.50981470e-05]]), array([[-0.00619982],\n",
      "       [-0.00638044],\n",
      "       [ 0.00493676]])]\n",
      "gradients_biases:  [array([-0.00135381, -0.0014027 ,  0.00016745]), array([0.00675441])]\n",
      "Iteration 885, Cost: 0.2438841289908108\n",
      "gradient_weights:  [array([[-3.87750243e-03, -4.41638710e-03,  7.20017519e-04],\n",
      "       [-5.03168846e-03, -7.01822303e-03,  8.52520901e-05]]), array([[-0.00620172],\n",
      "       [-0.00638545],\n",
      "       [ 0.00494276]])]\n",
      "gradients_biases:  [array([-0.00135454, -0.00140207,  0.00016823]), array([0.00675794])]\n",
      "Iteration 886, Cost: 0.24387100732623285\n",
      "gradient_weights:  [array([[-3.88357828e-03, -4.42248449e-03,  7.23151755e-04],\n",
      "       [-5.03944209e-03, -7.02641852e-03,  8.54032010e-05]]), array([[-0.00620363],\n",
      "       [-0.00639047],\n",
      "       [ 0.00494876]])]\n",
      "gradients_biases:  [array([-0.00135526, -0.00140144,  0.00016901]), array([0.00676147])]\n",
      "Iteration 887, Cost: 0.24385786098701948\n",
      "gradient_weights:  [array([[-3.88965521e-03, -4.42858581e-03,  7.26284901e-04],\n",
      "       [-5.04719561e-03, -7.03461478e-03,  8.55514665e-05]]), array([[-0.00620554],\n",
      "       [-0.00639549],\n",
      "       [ 0.00495477]])]\n",
      "gradients_biases:  [array([-0.00135597, -0.0014008 ,  0.00016979]), array([0.006765])]\n",
      "Iteration 888, Cost: 0.2438446899299206\n",
      "gradient_weights:  [array([[-3.89573321e-03, -4.43469107e-03,  7.29416945e-04],\n",
      "       [-5.05494900e-03, -7.04281179e-03,  8.56968731e-05]]), array([[-0.00620746],\n",
      "       [-0.00640052],\n",
      "       [ 0.00496079]])]\n",
      "gradients_biases:  [array([-0.00135667, -0.00140015,  0.00017058]), array([0.00676855])]\n",
      "Iteration 889, Cost: 0.24383149411170513\n",
      "gradient_weights:  [array([[-3.90181229e-03, -4.44080024e-03,  7.32547871e-04],\n",
      "       [-5.06270225e-03, -7.05100954e-03,  8.58394076e-05]]), array([[-0.00620939],\n",
      "       [-0.00640555],\n",
      "       [ 0.00496682]])]\n",
      "gradients_biases:  [array([-0.00135737, -0.00139948,  0.00017136]), array([0.0067721])]\n",
      "Iteration 890, Cost: 0.24381827348916169\n",
      "gradient_weights:  [array([[-3.90789244e-03, -4.44691333e-03,  7.35677664e-04],\n",
      "       [-5.07045536e-03, -7.05920800e-03,  8.59790564e-05]]), array([[-0.00621132],\n",
      "       [-0.00641059],\n",
      "       [ 0.00497285]])]\n",
      "gradients_biases:  [array([-0.00135806, -0.00139881,  0.00017214]), array([0.00677565])]\n",
      "Iteration 891, Cost: 0.24380502801909856\n",
      "gradient_weights:  [array([[-3.91397364e-03, -4.45303031e-03,  7.38806310e-04],\n",
      "       [-5.07820831e-03, -7.06740717e-03,  8.61158062e-05]]), array([[-0.00621325],\n",
      "       [-0.00641563],\n",
      "       [ 0.00497889]])]\n",
      "gradients_biases:  [array([-0.00135874, -0.00139813,  0.00017292]), array([0.00677921])]\n",
      "Iteration 892, Cost: 0.24379175765834427\n",
      "gradient_weights:  [array([[-3.92005590e-03, -4.45915118e-03,  7.41933795e-04],\n",
      "       [-5.08596110e-03, -7.07560702e-03,  8.62496435e-05]]), array([[-0.00621519],\n",
      "       [-0.00642068],\n",
      "       [ 0.00498494]])]\n",
      "gradients_biases:  [array([-0.00135942, -0.00139743,  0.00017371]), array([0.00678277])]\n",
      "Iteration 893, Cost: 0.24377846236374773\n",
      "gradient_weights:  [array([[-3.92613921e-03, -4.46527593e-03,  7.45060103e-04],\n",
      "       [-5.09371372e-03, -7.08380754e-03,  8.63805547e-05]]), array([[-0.00621714],\n",
      "       [-0.00642574],\n",
      "       [ 0.004991  ]])]\n",
      "gradients_biases:  [array([-0.00136008, -0.00139673,  0.00017449]), array([0.00678634])]\n",
      "Iteration 894, Cost: 0.2437651420921787\n",
      "gradient_weights:  [array([[-3.93222357e-03, -4.47140455e-03,  7.48185221e-04],\n",
      "       [-5.10146616e-03, -7.09200870e-03,  8.65085264e-05]]), array([[-0.00621909],\n",
      "       [-0.00643079],\n",
      "       [ 0.00499706]])]\n",
      "gradients_biases:  [array([-0.00136074, -0.00139602,  0.00017527]), array([0.00678992])]\n",
      "Iteration 895, Cost: 0.24375179680052794\n",
      "gradient_weights:  [array([[-3.93830896e-03, -4.47753704e-03,  7.51309132e-04],\n",
      "       [-5.10921840e-03, -7.10021050e-03,  8.66335451e-05]]), array([[-0.00622105],\n",
      "       [-0.00643586],\n",
      "       [ 0.00500313]])]\n",
      "gradients_biases:  [array([-0.0013614 , -0.00139529,  0.00017605]), array([0.0067935])]\n",
      "Iteration 896, Cost: 0.2437384264457077\n",
      "gradient_weights:  [array([[-3.94439539e-03, -4.48367337e-03,  7.54431823e-04],\n",
      "       [-5.11697045e-03, -7.10841290e-03,  8.67555971e-05]]), array([[-0.00622301],\n",
      "       [-0.00644092],\n",
      "       [ 0.00500921]])]\n",
      "gradients_biases:  [array([-0.00136204, -0.00139456,  0.00017683]), array([0.00679708])]\n",
      "Iteration 897, Cost: 0.24372503098465192\n",
      "gradient_weights:  [array([[-3.95048285e-03, -4.48981354e-03,  7.57553279e-04],\n",
      "       [-5.12472228e-03, -7.11661590e-03,  8.68746689e-05]]), array([[-0.00622498],\n",
      "       [-0.006446  ],\n",
      "       [ 0.0050153 ]])]\n",
      "gradients_biases:  [array([-0.00136268, -0.00139381,  0.00017762]), array([0.00680068])]\n",
      "Iteration 898, Cost: 0.2437116103743166\n",
      "gradient_weights:  [array([[-3.95657133e-03, -4.49595754e-03,  7.60673484e-04],\n",
      "       [-5.13247389e-03, -7.12481948e-03,  8.69907467e-05]]), array([[-0.00622695],\n",
      "       [-0.00645108],\n",
      "       [ 0.00502139]])]\n",
      "gradients_biases:  [array([-0.00136331, -0.00139306,  0.0001784 ]), array([0.00680427])]\n",
      "Iteration 899, Cost: 0.2436981645716801\n",
      "gradient_weights:  [array([[-3.96266083e-03, -4.50210536e-03,  7.63792423e-04],\n",
      "       [-5.14022528e-03, -7.13302361e-03,  8.71038171e-05]]), array([[-0.00622893],\n",
      "       [-0.00645616],\n",
      "       [ 0.0050275 ]])]\n",
      "gradients_biases:  [array([-0.00136393, -0.00139229,  0.00017918]), array([0.00680788])]\n",
      "Iteration 900, Cost: 0.2436846935337435\n",
      "gradient_weights:  [array([[-3.96875134e-03, -4.50825699e-03,  7.66910082e-04],\n",
      "       [-5.14797642e-03, -7.14122829e-03,  8.72138662e-05]]), array([[-0.00623091],\n",
      "       [-0.00646125],\n",
      "       [ 0.0050336 ]])]\n",
      "gradients_biases:  [array([-0.00136454, -0.00139151,  0.00017996]), array([0.00681148])]\n",
      "Iteration 901, Cost: 0.2436711972175309\n",
      "gradient_weights:  [array([[-3.97484286e-03, -4.51441242e-03,  7.70026445e-04],\n",
      "       [-5.15572731e-03, -7.14943349e-03,  8.73208804e-05]]), array([[-0.0062329 ],\n",
      "       [-0.00646634],\n",
      "       [ 0.00503972]])]\n",
      "gradients_biases:  [array([-0.00136515, -0.00139073,  0.00018075]), array([0.0068151])]\n",
      "Iteration 902, Cost: 0.2436576755800897\n",
      "gradient_weights:  [array([[-3.98093537e-03, -4.52057164e-03,  7.73141497e-04],\n",
      "       [-5.16347795e-03, -7.15763919e-03,  8.74248460e-05]]), array([[-0.00623489],\n",
      "       [-0.00647144],\n",
      "       [ 0.00504584]])]\n",
      "gradients_biases:  [array([-0.00136574, -0.00138993,  0.00018153]), array([0.00681871])]\n",
      "Iteration 903, Cost: 0.24364412857849083\n",
      "gradient_weights:  [array([[-3.98702888e-03, -4.52673464e-03,  7.76255223e-04],\n",
      "       [-5.17122831e-03, -7.16584538e-03,  8.75257492e-05]]), array([[-0.00623689],\n",
      "       [-0.00647654],\n",
      "       [ 0.00505197]])]\n",
      "gradients_biases:  [array([-0.00136633, -0.00138912,  0.00018231]), array([0.00682234])]\n",
      "Iteration 904, Cost: 0.24363055616982965\n",
      "gradient_weights:  [array([[-3.99312338e-03, -4.53290140e-03,  7.79367607e-04],\n",
      "       [-5.17897840e-03, -7.17405203e-03,  8.76235761e-05]]), array([[-0.0062389 ],\n",
      "       [-0.00648165],\n",
      "       [ 0.00505811]])]\n",
      "gradients_biases:  [array([-0.00136692, -0.0013883 ,  0.00018309]), array([0.00682597])]\n",
      "Iteration 905, Cost: 0.24361695831122535\n",
      "gradient_weights:  [array([[-3.99921886e-03, -4.53907193e-03,  7.82478635e-04],\n",
      "       [-5.18672819e-03, -7.18225914e-03,  8.77183130e-05]]), array([[-0.00624091],\n",
      "       [-0.00648676],\n",
      "       [ 0.00506426]])]\n",
      "gradients_biases:  [array([-0.00136749, -0.00138747,  0.00018387]), array([0.0068296])]\n",
      "Iteration 906, Cost: 0.24360333495982195\n",
      "gradient_weights:  [array([[-4.00531532e-03, -4.54524620e-03,  7.85588291e-04],\n",
      "       [-5.19447769e-03, -7.19046668e-03,  8.78099461e-05]]), array([[-0.00624292],\n",
      "       [-0.00649188],\n",
      "       [ 0.00507041]])]\n",
      "gradients_biases:  [array([-0.00136806, -0.00138663,  0.00018465]), array([0.00683324])]\n",
      "Iteration 907, Cost: 0.24358968607278847\n",
      "gradient_weights:  [array([[-4.01141274e-03, -4.55142420e-03,  7.88696559e-04],\n",
      "       [-5.20222688e-03, -7.19867463e-03,  8.78984614e-05]]), array([[-0.00624494],\n",
      "       [-0.006497  ],\n",
      "       [ 0.00507657]])]\n",
      "gradients_biases:  [array([-0.00136862, -0.00138578,  0.00018544]), array([0.00683688])]\n",
      "Iteration 908, Cost: 0.2435760116073193\n",
      "gradient_weights:  [array([[-4.01751114e-03, -4.55760594e-03,  7.91803424e-04],\n",
      "       [-5.20997575e-03, -7.20688298e-03,  8.79838452e-05]]), array([[-0.00624697],\n",
      "       [-0.00650213],\n",
      "       [ 0.00508274]])]\n",
      "gradients_biases:  [array([-0.00136917, -0.00138492,  0.00018622]), array([0.00684053])]\n",
      "Iteration 909, Cost: 0.24356231152063407\n",
      "gradient_weights:  [array([[-4.02361049e-03, -4.56379138e-03,  7.94908870e-04],\n",
      "       [-5.21772429e-03, -7.21509170e-03,  8.80660834e-05]]), array([[-0.006249  ],\n",
      "       [-0.00650726],\n",
      "       [ 0.00508891]])]\n",
      "gradients_biases:  [array([-0.00136971, -0.00138405,  0.000187  ]), array([0.00684419])]\n",
      "Iteration 910, Cost: 0.24354858576997895\n",
      "gradient_weights:  [array([[-4.02971079e-03, -4.56998054e-03,  7.98012882e-04],\n",
      "       [-5.22547250e-03, -7.22330079e-03,  8.81451621e-05]]), array([[-0.00625103],\n",
      "       [-0.0065124 ],\n",
      "       [ 0.0050951 ]])]\n",
      "gradients_biases:  [array([-0.00137025, -0.00138317,  0.00018778]), array([0.00684785])]\n",
      "Iteration 911, Cost: 0.24353483431262585\n",
      "gradient_weights:  [array([[-4.03581203e-03, -4.57617338e-03,  8.01115444e-04],\n",
      "       [-5.23322035e-03, -7.23151021e-03,  8.82210674e-05]]), array([[-0.00625307],\n",
      "       [-0.00651754],\n",
      "       [ 0.00510129]])]\n",
      "gradients_biases:  [array([-0.00137077, -0.00138227,  0.00018856]), array([0.00685151])]\n",
      "Iteration 912, Cost: 0.24352105710587357\n",
      "gradient_weights:  [array([[-4.04191422e-03, -4.58236991e-03,  8.04216540e-04],\n",
      "       [-5.24096784e-03, -7.23971996e-03,  8.82937852e-05]]), array([[-0.00625512],\n",
      "       [-0.00652269],\n",
      "       [ 0.00510748]])]\n",
      "gradients_biases:  [array([-0.00137129, -0.00138137,  0.00018934]), array([0.00685518])]\n",
      "Iteration 913, Cost: 0.24350725410704793\n",
      "gradient_weights:  [array([[-4.04801734e-03, -4.58857011e-03,  8.07316155e-04],\n",
      "       [-5.24871497e-03, -7.24793002e-03,  8.83633016e-05]]), array([[-0.00625717],\n",
      "       [-0.00652784],\n",
      "       [ 0.00511369]])]\n",
      "gradients_biases:  [array([-0.0013718 , -0.00138045,  0.00019012]), array([0.00685886])]\n",
      "Iteration 914, Cost: 0.24349342527350165\n",
      "gradient_weights:  [array([[-4.05412139e-03, -4.59477397e-03,  8.10414273e-04],\n",
      "       [-5.25646171e-03, -7.25614035e-03,  8.84296024e-05]]), array([[-0.00625923],\n",
      "       [-0.006533  ],\n",
      "       [ 0.0051199 ]])]\n",
      "gradients_biases:  [array([-0.00137231, -0.00137953,  0.0001909 ]), array([0.00686254])]\n",
      "Iteration 915, Cost: 0.24347957056261543\n",
      "gradient_weights:  [array([[-4.06022635e-03, -4.60098148e-03,  8.13510877e-04],\n",
      "       [-5.26420806e-03, -7.26435096e-03,  8.84926737e-05]]), array([[-0.00626129],\n",
      "       [-0.00653816],\n",
      "       [ 0.00512612]])]\n",
      "gradients_biases:  [array([-0.0013728 , -0.00137859,  0.00019168]), array([0.00686622])]\n",
      "Iteration 916, Cost: 0.24346568993179774\n",
      "gradient_weights:  [array([[-4.06633223e-03, -4.60719263e-03,  8.16605953e-04],\n",
      "       [-5.27195401e-03, -7.27256181e-03,  8.85525013e-05]]), array([[-0.00626335],\n",
      "       [-0.00654333],\n",
      "       [ 0.00513234]])]\n",
      "gradients_biases:  [array([-0.00137329, -0.00137764,  0.00019247]), array([0.00686991])]\n",
      "Iteration 917, Cost: 0.24345178333848533\n",
      "gradient_weights:  [array([[-4.07243902e-03, -4.61340741e-03,  8.19699483e-04],\n",
      "       [-5.27969955e-03, -7.28077290e-03,  8.86090710e-05]]), array([[-0.00626542],\n",
      "       [-0.0065485 ],\n",
      "       [ 0.00513858]])]\n",
      "gradients_biases:  [array([-0.00137377, -0.00137669,  0.00019325]), array([0.00687361])]\n",
      "Iteration 918, Cost: 0.24343785074014357\n",
      "gradient_weights:  [array([[-4.07854671e-03, -4.61962581e-03,  8.22791452e-04],\n",
      "       [-5.28744467e-03, -7.28898420e-03,  8.86623688e-05]]), array([[-0.0062675 ],\n",
      "       [-0.00655367],\n",
      "       [ 0.00514482]])]\n",
      "gradients_biases:  [array([-0.00137424, -0.00137572,  0.00019403]), array([0.00687731])]\n",
      "Iteration 919, Cost: 0.24342389209426674\n",
      "gradient_weights:  [array([[-4.08465529e-03, -4.62584781e-03,  8.25881844e-04],\n",
      "       [-5.29518936e-03, -7.29719569e-03,  8.87123805e-05]]), array([[-0.00626958],\n",
      "       [-0.00655886],\n",
      "       [ 0.00515106]])]\n",
      "gradients_biases:  [array([-0.00137471, -0.00137474,  0.00019481]), array([0.00688101])]\n",
      "Iteration 920, Cost: 0.24340990735837834\n",
      "gradient_weights:  [array([[-4.09076476e-03, -4.63207340e-03,  8.28970642e-04],\n",
      "       [-5.30293360e-03, -7.30540735e-03,  8.87590918e-05]]), array([[-0.00627167],\n",
      "       [-0.00656404],\n",
      "       [ 0.00515732]])]\n",
      "gradients_biases:  [array([-0.00137516, -0.00137375,  0.00019559]), array([0.00688472])]\n",
      "Iteration 921, Cost: 0.2433958964900318\n",
      "gradient_weights:  [array([[-4.09687511e-03, -4.63830258e-03,  8.32057831e-04],\n",
      "       [-5.31067739e-03, -7.31361917e-03,  8.88024885e-05]]), array([[-0.00627376],\n",
      "       [-0.00656923],\n",
      "       [ 0.00516358]])]\n",
      "gradients_biases:  [array([-0.00137561, -0.00137275,  0.00019637]), array([0.00688844])]\n",
      "Iteration 922, Cost: 0.24338185944680996\n",
      "gradient_weights:  [array([[-4.10298633e-03, -4.64453533e-03,  8.35143394e-04],\n",
      "       [-5.31842072e-03, -7.32183113e-03,  8.88425564e-05]]), array([[-0.00627586],\n",
      "       [-0.00657443],\n",
      "       [ 0.00516985]])]\n",
      "gradients_biases:  [array([-0.00137605, -0.00137173,  0.00019715]), array([0.00689216])]\n",
      "Iteration 923, Cost: 0.2433677961863264\n",
      "gradient_weights:  [array([[-4.10909842e-03, -4.65077163e-03,  8.38227315e-04],\n",
      "       [-5.32616357e-03, -7.33004321e-03,  8.88792812e-05]]), array([[-0.00627796],\n",
      "       [-0.00657962],\n",
      "       [ 0.00517612]])]\n",
      "gradients_biases:  [array([-0.00137648, -0.00137071,  0.00019793]), array([0.00689588])]\n",
      "Iteration 924, Cost: 0.24335370666622502\n",
      "gradient_weights:  [array([[-4.11521136e-03, -4.65701149e-03,  8.41309577e-04],\n",
      "       [-5.33390594e-03, -7.33825539e-03,  8.89126486e-05]]), array([[-0.00628006],\n",
      "       [-0.00658483],\n",
      "       [ 0.00518241]])]\n",
      "gradients_biases:  [array([-0.00137691, -0.00136967,  0.0001987 ]), array([0.00689961])]\n",
      "Iteration 925, Cost: 0.2433395908441809\n",
      "gradient_weights:  [array([[-4.12132516e-03, -4.66325489e-03,  8.44390164e-04],\n",
      "       [-5.34164781e-03, -7.34646765e-03,  8.89426443e-05]]), array([[-0.00628218],\n",
      "       [-0.00659004],\n",
      "       [ 0.0051887 ]])]\n",
      "gradients_biases:  [array([-0.00137732, -0.00136863,  0.00019948]), array([0.00690335])]\n",
      "Iteration 926, Cost: 0.2433254486779003\n",
      "gradient_weights:  [array([[-4.12743981e-03, -4.66950181e-03,  8.47469059e-04],\n",
      "       [-5.34938917e-03, -7.35467997e-03,  8.89692539e-05]]), array([[-0.00628429],\n",
      "       [-0.00659525],\n",
      "       [ 0.00519499]])]\n",
      "gradients_biases:  [array([-0.00137773, -0.00136757,  0.00020026]), array([0.00690709])]\n",
      "Iteration 927, Cost: 0.24331128012512102\n",
      "gradient_weights:  [array([[-4.13355530e-03, -4.67575224e-03,  8.50546246e-04],\n",
      "       [-5.35713001e-03, -7.36289234e-03,  8.89924631e-05]]), array([[-0.00628641],\n",
      "       [-0.00660047],\n",
      "       [ 0.0052013 ]])]\n",
      "gradients_biases:  [array([-0.00137813, -0.0013665 ,  0.00020104]), array([0.00691083])]\n",
      "Iteration 928, Cost: 0.24329708514361303\n",
      "gradient_weights:  [array([[-4.13967161e-03, -4.68200618e-03,  8.53621709e-04],\n",
      "       [-5.36487033e-03, -7.37110473e-03,  8.90122574e-05]]), array([[-0.00628854],\n",
      "       [-0.00660569],\n",
      "       [ 0.00520761]])]\n",
      "gradients_biases:  [array([-0.00137852, -0.00136543,  0.00020182]), array([0.00691458])]\n",
      "Iteration 929, Cost: 0.24328286369117846\n",
      "gradient_weights:  [array([[-4.14578875e-03, -4.68826361e-03,  8.56695431e-04],\n",
      "       [-5.37261010e-03, -7.37931713e-03,  8.90286224e-05]]), array([[-0.00629067],\n",
      "       [-0.00661092],\n",
      "       [ 0.00521393]])]\n",
      "gradients_biases:  [array([-0.0013789 , -0.00136434,  0.0002026 ]), array([0.00691834])]\n",
      "Iteration 930, Cost: 0.24326861572565212\n",
      "gradient_weights:  [array([[-4.15190671e-03, -4.69452451e-03,  8.59767394e-04],\n",
      "       [-5.38034933e-03, -7.38752952e-03,  8.90415437e-05]]), array([[-0.00629281],\n",
      "       [-0.00661615],\n",
      "       [ 0.00522025]])]\n",
      "gradients_biases:  [array([-0.00137928, -0.00136323,  0.00020338]), array([0.0069221])]\n",
      "Iteration 931, Cost: 0.24325434120490189\n",
      "gradient_weights:  [array([[-4.15802548e-03, -4.70078889e-03,  8.62837583e-04],\n",
      "       [-5.38808799e-03, -7.39574188e-03,  8.90510068e-05]]), array([[-0.00629495],\n",
      "       [-0.00662139],\n",
      "       [ 0.00522659]])]\n",
      "gradients_biases:  [array([-0.00137964, -0.00136212,  0.00020415]), array([0.00692586])]\n",
      "Iteration 932, Cost: 0.243240040086829\n",
      "gradient_weights:  [array([[-4.16414504e-03, -4.70705671e-03,  8.65905980e-04],\n",
      "       [-5.39582608e-03, -7.40395419e-03,  8.90569972e-05]]), array([[-0.00629709],\n",
      "       [-0.00662663],\n",
      "       [ 0.00523292]])]\n",
      "gradients_biases:  [array([-0.00138   , -0.001361  ,  0.00020493]), array([0.00692963])]\n",
      "Iteration 933, Cost: 0.24322571232936813\n",
      "gradient_weights:  [array([[-4.17026541e-03, -4.71332799e-03,  8.68972569e-04],\n",
      "       [-5.40356359e-03, -7.41216642e-03,  8.90595003e-05]]), array([[-0.00629925],\n",
      "       [-0.00663188],\n",
      "       [ 0.00523927]])]\n",
      "gradients_biases:  [array([-0.00138035, -0.00135986,  0.00020571]), array([0.0069334])]\n",
      "Iteration 934, Cost: 0.24321135789048826\n",
      "gradient_weights:  [array([[-4.17638656e-03, -4.71960269e-03,  8.72037332e-04],\n",
      "       [-5.41130050e-03, -7.42037857e-03,  8.90585017e-05]]), array([[-0.0063014 ],\n",
      "       [-0.00663713],\n",
      "       [ 0.00524562]])]\n",
      "gradients_biases:  [array([-0.00138069, -0.00135872,  0.00020649]), array([0.00693718])]\n",
      "Iteration 935, Cost: 0.24319697672819246\n",
      "gradient_weights:  [array([[-4.18250849e-03, -4.72588081e-03,  8.75100253e-04],\n",
      "       [-5.41903681e-03, -7.42859062e-03,  8.90539867e-05]]), array([[-0.00630356],\n",
      "       [-0.00664238],\n",
      "       [ 0.00525198]])]\n",
      "gradients_biases:  [array([-0.00138103, -0.00135756,  0.00020726]), array([0.00694096])]\n",
      "Iteration 936, Cost: 0.24318256880051883\n",
      "gradient_weights:  [array([[-4.18863120e-03, -4.73216234e-03,  8.78161315e-04],\n",
      "       [-5.42677249e-03, -7.43680254e-03,  8.90459408e-05]]), array([[-0.00630573],\n",
      "       [-0.00664764],\n",
      "       [ 0.00525835]])]\n",
      "gradients_biases:  [array([-0.00138135, -0.00135639,  0.00020804]), array([0.00694475])]\n",
      "Iteration 937, Cost: 0.24316813406554022\n",
      "gradient_weights:  [array([[-4.19475467e-03, -4.73844727e-03,  8.81220500e-04],\n",
      "       [-5.43450755e-03, -7.44501431e-03,  8.90343493e-05]]), array([[-0.0063079 ],\n",
      "       [-0.0066529 ],\n",
      "       [ 0.00526472]])]\n",
      "gradients_biases:  [array([-0.00138167, -0.00135521,  0.00020882]), array([0.00694854])]\n",
      "Iteration 938, Cost: 0.24315367248136502\n",
      "gradient_weights:  [array([[-4.20087889e-03, -4.74473557e-03,  8.84277792e-04],\n",
      "       [-5.44224197e-03, -7.45322592e-03,  8.90191977e-05]]), array([[-0.00631007],\n",
      "       [-0.00665817],\n",
      "       [ 0.00527111]])]\n",
      "gradients_biases:  [array([-0.00138198, -0.00135402,  0.00020959]), array([0.00695234])]\n",
      "Iteration 939, Cost: 0.2431391840061373\n",
      "gradient_weights:  [array([[-4.20700387e-03, -4.75102725e-03,  8.87333173e-04],\n",
      "       [-5.44997574e-03, -7.46143735e-03,  8.90004711e-05]]), array([[-0.00631225],\n",
      "       [-0.00666345],\n",
      "       [ 0.00527749]])]\n",
      "gradients_biases:  [array([-0.00138228, -0.00135282,  0.00021037]), array([0.00695614])]\n",
      "Iteration 940, Cost: 0.24312466859803739\n",
      "gradient_weights:  [array([[-4.21312958e-03, -4.75732229e-03,  8.90386626e-04],\n",
      "       [-5.45770885e-03, -7.46964857e-03,  8.89781550e-05]]), array([[-0.00631444],\n",
      "       [-0.00666872],\n",
      "       [ 0.00528389]])]\n",
      "gradients_biases:  [array([-0.00138257, -0.00135161,  0.00021114]), array([0.00695995])]\n",
      "Iteration 941, Cost: 0.24311012621528166\n",
      "gradient_weights:  [array([[-4.21925604e-03, -4.76362067e-03,  8.93438133e-04],\n",
      "       [-5.46544128e-03, -7.47785957e-03,  8.89522346e-05]]), array([[-0.00631663],\n",
      "       [-0.00667401],\n",
      "       [ 0.00529029]])]\n",
      "gradients_biases:  [array([-0.00138285, -0.00135038,  0.00021192]), array([0.00696376])]\n",
      "Iteration 942, Cost: 0.2430955568161237\n",
      "gradient_weights:  [array([[-4.22538321e-03, -4.76992239e-03,  8.96487679e-04],\n",
      "       [-5.47317303e-03, -7.48607034e-03,  8.89226952e-05]]), array([[-0.00631882],\n",
      "       [-0.00667929],\n",
      "       [ 0.0052967 ]])]\n",
      "gradients_biases:  [array([-0.00138313, -0.00134915,  0.0002127 ]), array([0.00696758])]\n",
      "Iteration 943, Cost: 0.2430809603588539\n",
      "gradient_weights:  [array([[-4.23151111e-03, -4.77622743e-03,  8.99535244e-04],\n",
      "       [-5.48090407e-03, -7.49428084e-03,  8.88895221e-05]]), array([[-0.00632102],\n",
      "       [-0.00668458],\n",
      "       [ 0.00530311]])]\n",
      "gradients_biases:  [array([-0.00138339, -0.0013479 ,  0.00021347]), array([0.0069714])]\n",
      "Iteration 944, Cost: 0.2430663368018003\n",
      "gradient_weights:  [array([[-4.23763971e-03, -4.78253578e-03,  9.02580811e-04],\n",
      "       [-5.48863442e-03, -7.50249107e-03,  8.88527003e-05]]), array([[-0.00632323],\n",
      "       [-0.00668988],\n",
      "       [ 0.00530953]])]\n",
      "gradients_biases:  [array([-0.00138365, -0.00134664,  0.00021424]), array([0.00697522])]\n",
      "Iteration 945, Cost: 0.24305168610332878\n",
      "gradient_weights:  [array([[-4.24376902e-03, -4.78884742e-03,  9.05624364e-04],\n",
      "       [-5.49636404e-03, -7.51070100e-03,  8.88122152e-05]]), array([[-0.00632544],\n",
      "       [-0.00669518],\n",
      "       [ 0.00531596]])]\n",
      "gradients_biases:  [array([-0.0013839 , -0.00134537,  0.00021502]), array([0.00697905])]\n",
      "Iteration 946, Cost: 0.24303700822184301\n",
      "gradient_weights:  [array([[-4.24989903e-03, -4.79516235e-03,  9.08665884e-04],\n",
      "       [-5.50409293e-03, -7.51891061e-03,  8.87680520e-05]]), array([[-0.00632765],\n",
      "       [-0.00670048],\n",
      "       [ 0.0053224 ]])]\n",
      "gradients_biases:  [array([-0.00138414, -0.00134409,  0.00021579]), array([0.00698288])]\n",
      "Iteration 947, Cost: 0.24302230311578576\n",
      "gradient_weights:  [array([[-4.25602971e-03, -4.80148055e-03,  9.11705354e-04],\n",
      "       [-5.51182107e-03, -7.52711989e-03,  8.87201956e-05]]), array([[-0.00632987],\n",
      "       [-0.00670579],\n",
      "       [ 0.00532884]])]\n",
      "gradients_biases:  [array([-0.00138438, -0.0013428 ,  0.00021657]), array([0.00698672])]\n",
      "Iteration 948, Cost: 0.24300757074363818\n",
      "gradient_weights:  [array([[-4.26216108e-03, -4.80780200e-03,  9.14742756e-04],\n",
      "       [-5.51954846e-03, -7.53532881e-03,  8.86686314e-05]]), array([[-0.00633209],\n",
      "       [-0.0067111 ],\n",
      "       [ 0.00533529]])]\n",
      "gradients_biases:  [array([-0.0013846 , -0.00134149,  0.00021734]), array([0.00699056])]\n",
      "Iteration 949, Cost: 0.2429928110639209\n",
      "gradient_weights:  [array([[-4.26829312e-03, -4.81412671e-03,  9.17778073e-04],\n",
      "       [-5.52727509e-03, -7.54353736e-03,  8.86133443e-05]]), array([[-0.00633432],\n",
      "       [-0.00671642],\n",
      "       [ 0.00534174]])]\n",
      "gradients_biases:  [array([-0.00138482, -0.00134018,  0.00021811]), array([0.00699441])]\n",
      "Iteration 950, Cost: 0.24297802403519403\n",
      "gradient_weights:  [array([[-4.27442582e-03, -4.82045464e-03,  9.20811286e-04],\n",
      "       [-5.53500093e-03, -7.55174551e-03,  8.85543195e-05]]), array([[-0.00633655],\n",
      "       [-0.00672174],\n",
      "       [ 0.0053482 ]])]\n",
      "gradients_biases:  [array([-0.00138503, -0.00133885,  0.00021888]), array([0.00699826])]\n",
      "Iteration 951, Cost: 0.24296320961605744\n",
      "gradient_weights:  [array([[-4.28055917e-03, -4.82678580e-03,  9.23842378e-04],\n",
      "       [-5.54272599e-03, -7.55995326e-03,  8.84915421e-05]]), array([[-0.00633879],\n",
      "       [-0.00672707],\n",
      "       [ 0.00535467]])]\n",
      "gradients_biases:  [array([-0.00138522, -0.00133751,  0.00021966]), array([0.00700212])]\n",
      "Iteration 952, Cost: 0.2429483677651516\n",
      "gradient_weights:  [array([[-4.28669316e-03, -4.83312016e-03,  9.26871331e-04],\n",
      "       [-5.55045024e-03, -7.56816056e-03,  8.84249969e-05]]), array([[-0.00634103],\n",
      "       [-0.0067324 ],\n",
      "       [ 0.00536115]])]\n",
      "gradients_biases:  [array([-0.00138542, -0.00133616,  0.00022043]), array([0.00700598])]\n",
      "Iteration 953, Cost: 0.2429334984411572\n",
      "gradient_weights:  [array([[-4.29282780e-03, -4.83945772e-03,  9.29898127e-04],\n",
      "       [-5.55817368e-03, -7.57636742e-03,  8.83546692e-05]]), array([[-0.00634328],\n",
      "       [-0.00673773],\n",
      "       [ 0.00536763]])]\n",
      "gradients_biases:  [array([-0.0013856, -0.0013348,  0.0002212]), array([0.00700985])]\n",
      "Iteration 954, Cost: 0.2429186016027962\n",
      "gradient_weights:  [array([[-4.29896306e-03, -4.84579846e-03,  9.32922749e-04],\n",
      "       [-5.56589630e-03, -7.58457381e-03,  8.82805438e-05]]), array([[-0.00634553],\n",
      "       [-0.00674307],\n",
      "       [ 0.00537412]])]\n",
      "gradients_biases:  [array([-0.00138577, -0.00133342,  0.00022197]), array([0.00701372])]\n",
      "Iteration 955, Cost: 0.24290367720883171\n",
      "gradient_weights:  [array([[-4.30509894e-03, -4.85214237e-03,  9.35945177e-04],\n",
      "       [-5.57361807e-03, -7.59277970e-03,  8.82026057e-05]]), array([[-0.00634779],\n",
      "       [-0.00674841],\n",
      "       [ 0.00538061]])]\n",
      "gradients_biases:  [array([-0.00138594, -0.00133204,  0.00022274]), array([0.00701759])]\n",
      "Iteration 956, Cost: 0.24288872521806854\n",
      "gradient_weights:  [array([[-4.31123543e-03, -4.85848944e-03,  9.38965394e-04],\n",
      "       [-5.58133900e-03, -7.60098509e-03,  8.81208399e-05]]), array([[-0.00635005],\n",
      "       [-0.00675376],\n",
      "       [ 0.00538711]])]\n",
      "gradients_biases:  [array([-0.00138609, -0.00133064,  0.00022351]), array([0.00702147])]\n",
      "Iteration 957, Cost: 0.24287374558935348\n",
      "gradient_weights:  [array([[-4.31737252e-03, -4.86483964e-03,  9.41983382e-04],\n",
      "       [-5.58905907e-03, -7.60918995e-03,  8.80352312e-05]]), array([[-0.00635232],\n",
      "       [-0.00675911],\n",
      "       [ 0.00539362]])]\n",
      "gradients_biases:  [array([-0.00138624, -0.00132923,  0.00022428]), array([0.00702535])]\n",
      "Iteration 958, Cost: 0.24285873828157578\n",
      "gradient_weights:  [array([[-4.32351021e-03, -4.87119298e-03,  9.44999123e-04],\n",
      "       [-5.59677826e-03, -7.61739426e-03,  8.79457647e-05]]), array([[-0.00635459],\n",
      "       [-0.00676447],\n",
      "       [ 0.00540014]])]\n",
      "gradients_biases:  [array([-0.00138638, -0.00132781,  0.00022505]), array([0.00702924])]\n",
      "Iteration 959, Cost: 0.24284370325366736\n",
      "gradient_weights:  [array([[-4.32964848e-03, -4.87754943e-03,  9.48012598e-04],\n",
      "       [-5.60449656e-03, -7.62559800e-03,  8.78524252e-05]]), array([[-0.00635686],\n",
      "       [-0.00676983],\n",
      "       [ 0.00540666]])]\n",
      "gradients_biases:  [array([-0.00138651, -0.00132638,  0.00022582]), array([0.00703313])]\n",
      "Iteration 960, Cost: 0.24282864046460317\n",
      "gradient_weights:  [array([[-4.33578733e-03, -4.88390899e-03,  9.51023789e-04],\n",
      "       [-5.61221397e-03, -7.63380115e-03,  8.77551975e-05]]), array([[-0.00635914],\n",
      "       [-0.00677519],\n",
      "       [ 0.00541319]])]\n",
      "gradients_biases:  [array([-0.00138663, -0.00132493,  0.00022659]), array([0.00703702])]\n",
      "Iteration 961, Cost: 0.24281354987340165\n",
      "gradient_weights:  [array([[-4.34192675e-03, -4.89027164e-03,  9.54032678e-04],\n",
      "       [-5.61993047e-03, -7.64200370e-03,  8.76540664e-05]]), array([[-0.00636142],\n",
      "       [-0.00678056],\n",
      "       [ 0.00541972]])]\n",
      "gradients_biases:  [array([-0.00138675, -0.00132348,  0.00022736]), array([0.00704092])]\n",
      "Iteration 962, Cost: 0.24279843143912497\n",
      "gradient_weights:  [array([[-4.34806672e-03, -4.89663736e-03,  9.57039246e-04],\n",
      "       [-5.62764605e-03, -7.65020561e-03,  8.75490169e-05]]), array([[-0.00636371],\n",
      "       [-0.00678593],\n",
      "       [ 0.00542626]])]\n",
      "gradients_biases:  [array([-0.00138685, -0.00132201,  0.00022813]), array([0.00704483])]\n",
      "Iteration 963, Cost: 0.24278328512087946\n",
      "gradient_weights:  [array([[-4.35420725e-03, -4.90300614e-03,  9.60043475e-04],\n",
      "       [-5.63536069e-03, -7.65840689e-03,  8.74400337e-05]]), array([[-0.00636601],\n",
      "       [-0.00679131],\n",
      "       [ 0.00543281]])]\n",
      "gradients_biases:  [array([-0.00138695, -0.00132053,  0.0002289 ]), array([0.00704874])]\n",
      "Iteration 964, Cost: 0.24276811087781597\n",
      "gradient_weights:  [array([[-4.36034831e-03, -4.90937798e-03,  9.63045347e-04],\n",
      "       [-5.64307438e-03, -7.66660750e-03,  8.73271015e-05]]), array([[-0.0063683 ],\n",
      "       [-0.00679669],\n",
      "       [ 0.00543936]])]\n",
      "gradients_biases:  [array([-0.00138703, -0.00131904,  0.00022966]), array([0.00705265])]\n",
      "Iteration 965, Cost: 0.24275290866913013\n",
      "gradient_weights:  [array([[-4.36648991e-03, -4.91575285e-03,  9.66044842e-04],\n",
      "       [-5.65078712e-03, -7.67480742e-03,  8.72102051e-05]]), array([[-0.0063706 ],\n",
      "       [-0.00680208],\n",
      "       [ 0.00544592]])]\n",
      "gradients_biases:  [array([-0.00138711, -0.00131754,  0.00023043]), array([0.00705657])]\n",
      "Iteration 966, Cost: 0.24273767845406277\n",
      "gradient_weights:  [array([[-4.37263203e-03, -4.92213074e-03,  9.69041942e-04],\n",
      "       [-5.65849888e-03, -7.68300664e-03,  8.70893293e-05]]), array([[-0.00637291],\n",
      "       [-0.00680747],\n",
      "       [ 0.00545249]])]\n",
      "gradients_biases:  [array([-0.00138718, -0.00131602,  0.0002312 ]), array([0.00706049])]\n",
      "Iteration 967, Cost: 0.24272242019190038\n",
      "gradient_weights:  [array([[-4.37877467e-03, -4.92851164e-03,  9.72036629e-04],\n",
      "       [-5.66620967e-03, -7.69120513e-03,  8.69644588e-05]]), array([[-0.00637522],\n",
      "       [-0.00681286],\n",
      "       [ 0.00545906]])]\n",
      "gradients_biases:  [array([-0.00138724, -0.0013145 ,  0.00023196]), array([0.00706441])]\n",
      "Iteration 968, Cost: 0.2427071338419751\n",
      "gradient_weights:  [array([[-4.38491780e-03, -4.93489554e-03,  9.75028884e-04],\n",
      "       [-5.67391945e-03, -7.69940288e-03,  8.68355782e-05]]), array([[-0.00637754],\n",
      "       [-0.00681826],\n",
      "       [ 0.00546564]])]\n",
      "gradients_biases:  [array([-0.0013873 , -0.00131296,  0.00023273]), array([0.00706834])]\n",
      "Iteration 969, Cost: 0.24269181936366568\n",
      "gradient_weights:  [array([[-4.39106144e-03, -4.94128242e-03,  9.78018688e-04],\n",
      "       [-5.68162823e-03, -7.70759987e-03,  8.67026722e-05]]), array([[-0.00637986],\n",
      "       [-0.00682366],\n",
      "       [ 0.00547223]])]\n",
      "gradients_biases:  [array([-0.00138734, -0.00131141,  0.00023349]), array([0.00707227])]\n",
      "Iteration 970, Cost: 0.2426764767163972\n",
      "gradient_weights:  [array([[-4.39720556e-03, -4.94767227e-03,  9.81006022e-04],\n",
      "       [-5.68933598e-03, -7.71579607e-03,  8.65657255e-05]]), array([[-0.00638218],\n",
      "       [-0.00682906],\n",
      "       [ 0.00547882]])]\n",
      "gradients_biases:  [array([-0.00138738, -0.00130985,  0.00023426]), array([0.00707621])]\n",
      "Iteration 971, Cost: 0.24266110585964182\n",
      "gradient_weights:  [array([[-4.40335016e-03, -4.95406507e-03,  9.83990867e-04],\n",
      "       [-5.69704271e-03, -7.72399147e-03,  8.64247227e-05]]), array([[-0.00638451],\n",
      "       [-0.00683447],\n",
      "       [ 0.00548542]])]\n",
      "gradients_biases:  [array([-0.0013874 , -0.00130827,  0.00023502]), array([0.00708015])]\n",
      "Iteration 972, Cost: 0.24264570675291908\n",
      "gradient_weights:  [array([[-4.40949522e-03, -4.96046081e-03,  9.86973205e-04],\n",
      "       [-5.70474838e-03, -7.73218605e-03,  8.62796485e-05]]), array([[-0.00638685],\n",
      "       [-0.00683989],\n",
      "       [ 0.00549202]])]\n",
      "gradients_biases:  [array([-0.00138742, -0.00130669,  0.00023578]), array([0.0070841])]\n",
      "Iteration 973, Cost: 0.24263027935579612\n",
      "gradient_weights:  [array([[-4.41564075e-03, -4.96685948e-03,  9.89953016e-04],\n",
      "       [-5.71245300e-03, -7.74037979e-03,  8.61304873e-05]]), array([[-0.00638918],\n",
      "       [-0.00684531],\n",
      "       [ 0.00549863]])]\n",
      "gradients_biases:  [array([-0.00138743, -0.00130509,  0.00023655]), array([0.00708805])]\n",
      "Iteration 974, Cost: 0.24261482362788825\n",
      "gradient_weights:  [array([[-4.42178672e-03, -4.97326106e-03,  9.92930282e-04],\n",
      "       [-5.72015655e-03, -7.74857266e-03,  8.59772239e-05]]), array([[-0.00639153],\n",
      "       [-0.00685073],\n",
      "       [ 0.00550525]])]\n",
      "gradients_biases:  [array([-0.00138743, -0.00130348,  0.00023731]), array([0.007092])]\n",
      "Iteration 975, Cost: 0.24259933952885887\n",
      "gradient_weights:  [array([[-4.42793313e-03, -4.97966554e-03,  9.95904983e-04],\n",
      "       [-5.72785901e-03, -7.75676465e-03,  8.58198426e-05]]), array([[-0.00639387],\n",
      "       [-0.00685615],\n",
      "       [ 0.00551187]])]\n",
      "gradients_biases:  [array([-0.00138742, -0.00130186,  0.00023807]), array([0.00709596])]\n",
      "Iteration 976, Cost: 0.24258382701842057\n",
      "gradient_weights:  [array([[-4.43407997e-03, -4.98607291e-03,  9.98877100e-04],\n",
      "       [-5.73556038e-03, -7.76495575e-03,  8.56583282e-05]]), array([[-0.00639623],\n",
      "       [-0.00686158],\n",
      "       [ 0.0055185 ]])]\n",
      "gradients_biases:  [array([-0.0013874 , -0.00130022,  0.00023883]), array([0.00709992])]\n",
      "Iteration 977, Cost: 0.24256828605633465\n",
      "gradient_weights:  [array([[-4.44022723e-03, -4.99248314e-03,  1.00184661e-03],\n",
      "       [-5.74326063e-03, -7.77314592e-03,  8.54926650e-05]]), array([[-0.00639858],\n",
      "       [-0.00686702],\n",
      "       [ 0.00552514]])]\n",
      "gradients_biases:  [array([-0.00138737, -0.00129858,  0.0002396 ]), array([0.00710388])]\n",
      "Iteration 978, Cost: 0.2425527166024122\n",
      "gradient_weights:  [array([[-4.44637491e-03, -4.99889624e-03,  1.00481351e-03],\n",
      "       [-5.75095977e-03, -7.78133515e-03,  8.53228376e-05]]), array([[-0.00640094],\n",
      "       [-0.00687246],\n",
      "       [ 0.00553178]])]\n",
      "gradients_biases:  [array([-0.00138734, -0.00129692,  0.00024036]), array([0.00710785])]\n",
      "Iteration 979, Cost: 0.2425371186165139\n",
      "gradient_weights:  [array([[-4.45252298e-03, -5.00531217e-03,  1.00777776e-03],\n",
      "       [-5.75865777e-03, -7.78952342e-03,  8.51488305e-05]]), array([[-0.00640331],\n",
      "       [-0.0068779 ],\n",
      "       [ 0.00553843]])]\n",
      "gradients_biases:  [array([-0.00138729, -0.00129525,  0.00024112]), array([0.00711183])]\n",
      "Iteration 980, Cost: 0.24252149205855078\n",
      "gradient_weights:  [array([[-4.45867144e-03, -5.01173094e-03,  1.01073935e-03],\n",
      "       [-5.76635462e-03, -7.79771071e-03,  8.49706280e-05]]), array([[-0.00640567],\n",
      "       [-0.00688334],\n",
      "       [ 0.00554509]])]\n",
      "gradients_biases:  [array([-0.00138724, -0.00129357,  0.00024188]), array([0.0071158])]\n",
      "Iteration 981, Cost: 0.24250583688848426\n",
      "gradient_weights:  [array([[-4.46482029e-03, -5.01815252e-03,  1.01369826e-03],\n",
      "       [-5.77405032e-03, -7.80589700e-03,  8.47882147e-05]]), array([[-0.00640805],\n",
      "       [-0.00688879],\n",
      "       [ 0.00555175]])]\n",
      "gradients_biases:  [array([-0.00138718, -0.00129187,  0.00024264]), array([0.00711978])]\n",
      "Iteration 982, Cost: 0.24249015306632676\n",
      "gradient_weights:  [array([[-4.47096951e-03, -5.02457690e-03,  1.01665447e-03],\n",
      "       [-5.78174484e-03, -7.81408227e-03,  8.46015749e-05]]), array([[-0.00641042],\n",
      "       [-0.00689425],\n",
      "       [ 0.00555841]])]\n",
      "gradients_biases:  [array([-0.0013871 , -0.00129017,  0.00024339]), array([0.00712377])]\n",
      "Iteration 983, Cost: 0.242474440552142\n",
      "gradient_weights:  [array([[-4.47711909e-03, -5.03100406e-03,  1.01960796e-03],\n",
      "       [-5.78943817e-03, -7.82226650e-03,  8.44106931e-05]]), array([[-0.00641281],\n",
      "       [-0.0068997 ],\n",
      "       [ 0.00556509]])]\n",
      "gradients_biases:  [array([-0.00138702, -0.00128845,  0.00024415]), array([0.00712776])]\n",
      "Iteration 984, Cost: 0.2424586993060452\n",
      "gradient_weights:  [array([[-4.48326902e-03, -5.03743400e-03,  1.02255871e-03],\n",
      "       [-5.79713031e-03, -7.83044967e-03,  8.42155537e-05]]), array([[-0.00641519],\n",
      "       [-0.00690516],\n",
      "       [ 0.00557177]])]\n",
      "gradients_biases:  [array([-0.00138693, -0.00128672,  0.00024491]), array([0.00713175])]\n",
      "Iteration 985, Cost: 0.24244292928820355\n",
      "gradient_weights:  [array([[-4.48941930e-03, -5.04386669e-03,  1.02550671e-03],\n",
      "       [-5.80482123e-03, -7.83863176e-03,  8.40161409e-05]]), array([[-0.00641758],\n",
      "       [-0.00691063],\n",
      "       [ 0.00557845]])]\n",
      "gradients_biases:  [array([-0.00138684, -0.00128497,  0.00024567]), array([0.00713575])]\n",
      "Iteration 986, Cost: 0.2424271304588367\n",
      "gradient_weights:  [array([[-4.49556990e-03, -5.05030212e-03,  1.02845192e-03],\n",
      "       [-5.81251093e-03, -7.84681276e-03,  8.38124392e-05]]), array([[-0.00641998],\n",
      "       [-0.0069161 ],\n",
      "       [ 0.00558514]])]\n",
      "gradients_biases:  [array([-0.00138673, -0.00128322,  0.00024642]), array([0.00713975])]\n",
      "Iteration 987, Cost: 0.24241130277821685\n",
      "gradient_weights:  [array([[-4.50172083e-03, -5.05674029e-03,  1.03139434e-03],\n",
      "       [-5.82019939e-03, -7.85499263e-03,  8.36044329e-05]]), array([[-0.00642238],\n",
      "       [-0.00692157],\n",
      "       [ 0.00559184]])]\n",
      "gradients_biases:  [array([-0.00138661, -0.00128145,  0.00024718]), array([0.00714375])]\n",
      "Iteration 988, Cost: 0.24239544620666936\n",
      "gradient_weights:  [array([[-4.50787207e-03, -5.06318117e-03,  1.03433395e-03],\n",
      "       [-5.82788660e-03, -7.86317137e-03,  8.33921062e-05]]), array([[-0.00642478],\n",
      "       [-0.00692705],\n",
      "       [ 0.00559854]])]\n",
      "gradients_biases:  [array([-0.00138649, -0.00127967,  0.00024793]), array([0.00714776])]\n",
      "Iteration 989, Cost: 0.24237956070457298\n",
      "gradient_weights:  [array([[-4.51402361e-03, -5.06962474e-03,  1.03727071e-03],\n",
      "       [-5.83557255e-03, -7.87134895e-03,  8.31754435e-05]]), array([[-0.00642719],\n",
      "       [-0.00693253],\n",
      "       [ 0.00560525]])]\n",
      "gradients_biases:  [array([-0.00138635, -0.00127788,  0.00024869]), array([0.00715177])]\n",
      "Iteration 990, Cost: 0.24236364623236017\n",
      "gradient_weights:  [array([[-4.52017545e-03, -5.07607100e-03,  1.04020462e-03],\n",
      "       [-5.84325722e-03, -7.87952535e-03,  8.29544291e-05]]), array([[-0.0064296 ],\n",
      "       [-0.00693801],\n",
      "       [ 0.00561197]])]\n",
      "gradients_biases:  [array([-0.00138621, -0.00127608,  0.00024944]), array([0.00715578])]\n",
      "Iteration 991, Cost: 0.24234770275051745\n",
      "gradient_weights:  [array([[-4.52632756e-03, -5.08251994e-03,  1.04313565e-03],\n",
      "       [-5.85094060e-03, -7.88770055e-03,  8.27290471e-05]]), array([[-0.00643202],\n",
      "       [-0.0069435 ],\n",
      "       [ 0.00561869]])]\n",
      "gradients_biases:  [array([-0.00138606, -0.00127426,  0.0002502 ]), array([0.0071598])]\n",
      "Iteration 992, Cost: 0.24233173021958604\n",
      "gradient_weights:  [array([[-4.53247995e-03, -5.08897152e-03,  1.04606379e-03],\n",
      "       [-5.85862267e-03, -7.89587453e-03,  8.24992819e-05]]), array([[-0.00643444],\n",
      "       [-0.00694899],\n",
      "       [ 0.00562541]])]\n",
      "gradients_biases:  [array([-0.00138589, -0.00127243,  0.00025095]), array([0.00716382])]\n",
      "Iteration 993, Cost: 0.24231572860016182\n",
      "gradient_weights:  [array([[-4.53863260e-03, -5.09542575e-03,  1.04898901e-03],\n",
      "       [-5.86630343e-03, -7.90404728e-03,  8.22651177e-05]]), array([[-0.00643686],\n",
      "       [-0.00695449],\n",
      "       [ 0.00563215]])]\n",
      "gradients_biases:  [array([-0.00138572, -0.00127059,  0.0002517 ]), array([0.00716785])]\n",
      "Iteration 994, Cost: 0.2422996978528959\n",
      "gradient_weights:  [array([[-4.54478550e-03, -5.10188260e-03,  1.05191129e-03],\n",
      "       [-5.87398286e-03, -7.91221877e-03,  8.20265386e-05]]), array([[-0.00643929],\n",
      "       [-0.00695998],\n",
      "       [ 0.00563889]])]\n",
      "gradients_biases:  [array([-0.00138554, -0.00126874,  0.00025245]), array([0.00717188])]\n",
      "Iteration 995, Cost: 0.24228363793849508\n",
      "gradient_weights:  [array([[-4.55093863e-03, -5.10834206e-03,  1.05483062e-03],\n",
      "       [-5.88166094e-03, -7.92038898e-03,  8.17835288e-05]]), array([[-0.00644172],\n",
      "       [-0.00696549],\n",
      "       [ 0.00564563]])]\n",
      "gradients_biases:  [array([-0.00138536, -0.00126687,  0.00025321]), array([0.00717591])]\n",
      "Iteration 996, Cost: 0.24226754881772183\n",
      "gradient_weights:  [array([[-4.55709200e-03, -5.11480412e-03,  1.05774697e-03],\n",
      "       [-5.88933767e-03, -7.92855789e-03,  8.15360726e-05]]), array([[-0.00644416],\n",
      "       [-0.00697099],\n",
      "       [ 0.00565238]])]\n",
      "gradients_biases:  [array([-0.00138516, -0.001265  ,  0.00025396]), array([0.00717995])]\n",
      "Iteration 997, Cost: 0.24225143045139513\n",
      "gradient_weights:  [array([[-4.56324559e-03, -5.12126876e-03,  1.06066032e-03],\n",
      "       [-5.89701303e-03, -7.93672549e-03,  8.12841540e-05]]), array([[-0.0064466 ],\n",
      "       [-0.0069765 ],\n",
      "       [ 0.00565914]])]\n",
      "gradients_biases:  [array([-0.00138495, -0.00126311,  0.00025471]), array([0.00718399])]\n",
      "Iteration 998, Cost: 0.24223528280039056\n",
      "gradient_weights:  [array([[-4.56939938e-03, -5.12773597e-03,  1.06357065e-03],\n",
      "       [-5.90468701e-03, -7.94489175e-03,  8.10277573e-05]]), array([[-0.00644904],\n",
      "       [-0.00698202],\n",
      "       [ 0.0056659 ]])]\n",
      "gradients_biases:  [array([-0.00138473, -0.0012612 ,  0.00025545]), array([0.00718803])]\n",
      "Iteration 999, Cost: 0.2422191058256405\n",
      "gradient_weights:  [array([[-4.57555338e-03, -5.13420573e-03,  1.06647795e-03],\n",
      "       [-5.91235959e-03, -7.95305665e-03,  8.07668665e-05]]), array([[-0.00645149],\n",
      "       [-0.00698753],\n",
      "       [ 0.00567267]])]\n",
      "gradients_biases:  [array([-0.00138451, -0.00125929,  0.0002562 ]), array([0.00719208])]\n",
      "Iteration 1000, Cost: 0.242202899488135\n",
      "Accuracy: 0.75\n",
      "F1 Score: 0.8\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        self.weights = [np.random.randn(layers[i], layers[i+1]) for i in range(len(layers)-1)]\n",
    "        self.biases = [np.zeros((1, layers[i+1])) for i in range(len(layers)-1)]\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    def forward_pass(self, X):\n",
    "        activations = [X]\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(activations[-1], w) + b\n",
    "            activations.append(self.sigmoid(z))\n",
    "        return activations\n",
    "    \n",
    "    def backward_pass(self, X, Y, activations):\n",
    "        deltas = [(activations[-1] - Y) * self.sigmoid_derivative(activations[-1])]\n",
    "        for i in range(len(self.layers) - 2, 0, -1):\n",
    "            delta = np.dot(deltas[0], self.weights[i].T) * self.sigmoid_derivative(activations[i])\n",
    "            deltas.insert(0, delta)\n",
    "        return deltas\n",
    "    \n",
    "    def compute_gradients(self, activations, deltas):\n",
    "        gradients_weights = [np.dot(activations[i].T, deltas[i]) for i in range(len(self.layers) - 1)]\n",
    "        gradients_biases = [np.sum(deltas[i], axis=0) for i in range(len(self.layers) - 1)]\n",
    "        print(\"gradient_weights: \",gradients_weights)\n",
    "        print(\"gradients_biases: \",gradients_biases)\n",
    "        return gradients_weights, gradients_biases\n",
    "    \n",
    "    def update_weights(self, gradients_weights, gradients_biases, learning_rate):\n",
    "        self.weights = [w - learning_rate * gw for w, gw in zip(self.weights, gradients_weights)]\n",
    "        self.biases = [b - learning_rate * gb for b, gb in zip(self.biases, gradients_biases)]\n",
    "    \n",
    "    def train(self, X, Y, learning_rate, lam, max_iterations, epsilon):\n",
    "        for iteration in range(max_iterations):\n",
    "            activations = self.forward_pass(X)\n",
    "            deltas = self.backward_pass(X, Y, activations)\n",
    "            gradients_weights, gradients_biases = self.compute_gradients(activations, deltas)\n",
    "            self.update_weights(gradients_weights, gradients_biases, learning_rate)\n",
    "            # Compute cost function\n",
    "            J = np.mean(np.square(activations[-1] - Y))\n",
    "            print(f\"Iteration {iteration+1}, Cost: {J}\")\n",
    "            # Check for convergence\n",
    "            if J < epsilon:\n",
    "                print(\"Converged!\")\n",
    "                break\n",
    "\n",
    "    def accuracy(self, y_true, y_pred):\n",
    "        correct = np.sum(y_true == y_pred)\n",
    "        return correct / len(y_true)\n",
    "\n",
    "    def f1_score(self, y_true, y_pred):\n",
    "        tp = np.sum(y_true & y_pred)\n",
    "        fp = np.sum((~y_true) & y_pred)\n",
    "        fn = np.sum(y_true & (~y_pred))\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        return f1\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        activations = self.forward_pass(X_test)[-1]\n",
    "        y_pred = (activations > 0.5).astype(int)\n",
    "        acc = self.accuracy(y_test, y_pred)\n",
    "        f1 = self.f1_score(y_test, y_pred)\n",
    "        return acc, f1\n",
    "\n",
    "# Example usage\n",
    "# Create a neural network with 2 input neurons, 3 hidden neurons, and 1 output neuron\n",
    "nn = NeuralNetwork([2, 3, 1])\n",
    "\n",
    "# Generate some dummy data for training\n",
    "X_train = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "Y_train = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# Train the neural network\n",
    "nn.train(X_train, Y_train, learning_rate=0.1, lam=0.0, max_iterations=1000, epsilon=0.005)\n",
    "\n",
    "# Generate some dummy data for testing\n",
    "X_test = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "Y_test = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# Evaluate the trained model\n",
    "accuracy, f1_score = nn.evaluate(X_test, Y_test)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1 Score:\", f1_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4c62b1",
   "metadata": {},
   "source": [
    "### ACCURACY 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a59c1b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        self.weights = [np.random.randn(layers[i], layers[i+1]) for i in range(len(layers)-1)]\n",
    "        self.biases = [np.zeros((1, layers[i+1])) for i in range(len(layers)-1)]\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    def forward_pass(self, X):\n",
    "        activations = [X]\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(activations[-1], w) + b\n",
    "            activations.append(self.sigmoid(z))\n",
    "        return activations\n",
    "    \n",
    "    def backward_pass(self, X, Y, activations):\n",
    "        deltas = [(activations[-1] - Y) * self.sigmoid_derivative(activations[-1])]\n",
    "        for i in range(len(self.layers) - 2, 0, -1):\n",
    "            delta = np.dot(deltas[0], self.weights[i].T) * self.sigmoid_derivative(activations[i])\n",
    "            deltas.insert(0, delta)\n",
    "        return deltas\n",
    "    \n",
    "    def compute_gradients(self, activations, deltas):\n",
    "        gradients_weights = [np.dot(activations[i].T, deltas[i]) for i in range(len(self.layers) - 1)]\n",
    "        gradients_biases = [np.sum(deltas[i], axis=0) for i in range(len(self.layers) - 1)]\n",
    "        return gradients_weights, gradients_biases\n",
    "    \n",
    "    def update_weights(self, gradients_weights, gradients_biases, learning_rate):\n",
    "        self.weights = [w - learning_rate * gw for w, gw in zip(self.weights, gradients_weights)]\n",
    "        self.biases = [b - learning_rate * gb for b, gb in zip(self.biases, gradients_biases)]\n",
    "    \n",
    "    def train(self, X, Y, learning_rate, lam, max_iterations, epsilon):\n",
    "        for iteration in range(max_iterations):\n",
    "            activations = self.forward_pass(X)\n",
    "            deltas = self.backward_pass(X, Y, activations)\n",
    "            gradients_weights, gradients_biases = self.compute_gradients(activations, deltas)\n",
    "            self.update_weights(gradients_weights, gradients_biases, learning_rate)\n",
    "            # Compute cost function\n",
    "            J = np.mean(np.square(activations[-1] - Y))\n",
    "            print(f\"Iteration {iteration+1}, Cost: {J}\")\n",
    "            # Check for convergence\n",
    "            if J < epsilon:\n",
    "                print(\"Converged!\")\n",
    "                break\n",
    "\n",
    "    def accuracy(self, y_true, y_pred):\n",
    "        correct = np.sum(np.all(y_true == y_pred, axis=1))\n",
    "        return correct / len(y_true)\n",
    "\n",
    "\n",
    "    def f1_score(self, y_true, y_pred):\n",
    "        tp = np.sum(np.logical_and(y_true, y_pred))\n",
    "        fp = np.sum(np.logical_and(np.logical_not(y_true), y_pred))\n",
    "        fn = np.sum(np.logical_and(y_true, np.logical_not(y_pred)))\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        return f1\n",
    "\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        activations = self.forward_pass(X_test)[-1]\n",
    "        y_pred = (activations > 0.5).astype(int)\n",
    "        acc = self.accuracy(y_test, y_pred)\n",
    "        f1 = self.f1_score(y_test, y_pred)\n",
    "        return acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "617b810a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, Cost: 0.3468199035851143\n",
      "Iteration 2, Cost: 0.33918426229564697\n",
      "Iteration 3, Cost: 0.3333363614834019\n",
      "Iteration 4, Cost: 0.3282025542208706\n",
      "Iteration 5, Cost: 0.32324987337475664\n",
      "Iteration 6, Cost: 0.3181154272309825\n",
      "Iteration 7, Cost: 0.31245690233881734\n",
      "Iteration 8, Cost: 0.30587360866766355\n",
      "Iteration 9, Cost: 0.2978410312556765\n",
      "Iteration 10, Cost: 0.28765528433194004\n",
      "Iteration 11, Cost: 0.27445604475847657\n",
      "Iteration 12, Cost: 0.25755293711560323\n",
      "Iteration 13, Cost: 0.23742692590889788\n",
      "Iteration 14, Cost: 0.21703993292457235\n",
      "Iteration 15, Cost: 0.20081182684289614\n",
      "Iteration 16, Cost: 0.19054264599976498\n",
      "Iteration 17, Cost: 0.18457167382194414\n",
      "Iteration 18, Cost: 0.1807577432799471\n",
      "Iteration 19, Cost: 0.17787482876134175\n",
      "Iteration 20, Cost: 0.17537892847289796\n",
      "Iteration 21, Cost: 0.1730439098065553\n",
      "Iteration 22, Cost: 0.1707755241979251\n",
      "Iteration 23, Cost: 0.16853368731225185\n",
      "Iteration 24, Cost: 0.166301084906767\n",
      "Iteration 25, Cost: 0.1640702972748061\n",
      "Iteration 26, Cost: 0.1618383563776661\n",
      "Iteration 27, Cost: 0.15960436047037124\n",
      "Iteration 28, Cost: 0.15736838650662696\n",
      "Iteration 29, Cost: 0.15513097472584353\n",
      "Iteration 30, Cost: 0.15289287836083676\n",
      "Iteration 31, Cost: 0.15065494427480464\n",
      "Iteration 32, Cost: 0.14841806274709962\n",
      "Iteration 33, Cost: 0.14618315531407572\n",
      "Iteration 34, Cost: 0.14395118289517128\n",
      "Iteration 35, Cost: 0.1417231626372585\n",
      "Iteration 36, Cost: 0.13950018529845887\n",
      "Iteration 37, Cost: 0.13728342736154958\n",
      "Iteration 38, Cost: 0.13507415414769006\n",
      "Iteration 39, Cost: 0.13287371224349168\n",
      "Iteration 40, Cost: 0.13068351154337723\n",
      "Iteration 41, Cost: 0.12850499895945336\n",
      "Iteration 42, Cost: 0.12633962710112473\n",
      "Iteration 43, Cost: 0.1241888217595384\n",
      "Iteration 44, Cost: 0.122053951787112\n",
      "Iteration 45, Cost: 0.1199363040807282\n",
      "Iteration 46, Cost: 0.11783706515345288\n",
      "Iteration 47, Cost: 0.11575730955087657\n",
      "Iteration 48, Cost: 0.11369799439978008\n",
      "Iteration 49, Cost: 0.11165995879955389\n",
      "Iteration 50, Cost: 0.10964392657890586\n",
      "Iteration 51, Cost: 0.10765051105366116\n",
      "Iteration 52, Cost: 0.10568022071752663\n",
      "Iteration 53, Cost: 0.10373346517017526\n",
      "Iteration 54, Cost: 0.10181056096135356\n",
      "Iteration 55, Cost: 0.0999117373649222\n",
      "Iteration 56, Cost: 0.0980371423772901\n",
      "Iteration 57, Cost: 0.09618684945971384\n",
      "Iteration 58, Cost: 0.09436086571735385\n",
      "Iteration 59, Cost: 0.09255914233064312\n",
      "Iteration 60, Cost: 0.09078158811761053\n",
      "Iteration 61, Cost: 0.08902808708481316\n",
      "Iteration 62, Cost: 0.08729852067412153\n",
      "Iteration 63, Cost: 0.08559279506439249\n",
      "Iteration 64, Cost: 0.08391087325585389\n",
      "Iteration 65, Cost: 0.08225281067194816\n",
      "Iteration 66, Cost: 0.08061879163579737\n",
      "Iteration 67, Cost: 0.07900916242984685\n",
      "Iteration 68, Cost: 0.07742445506889924\n",
      "Iteration 69, Cost: 0.0758653950240529\n",
      "Iteration 70, Cost: 0.07433288672196976\n",
      "Iteration 71, Cost: 0.07282797336116284\n",
      "Iteration 72, Cost: 0.07135177243643918\n",
      "Iteration 73, Cost: 0.0699053942821876\n",
      "Iteration 74, Cost: 0.06848985591353528\n",
      "Iteration 75, Cost: 0.0671060043283274\n",
      "Iteration 76, Cost: 0.06575446120491309\n",
      "Iteration 77, Cost: 0.06443559539113278\n",
      "Iteration 78, Cost: 0.0631495229542542\n",
      "Iteration 79, Cost: 0.06189612923796321\n",
      "Iteration 80, Cost: 0.0606751047216351\n",
      "Iteration 81, Cost: 0.05948598649746439\n",
      "Iteration 82, Cost: 0.0583281989498215\n",
      "Iteration 83, Cost: 0.05720108961221768\n",
      "Iteration 84, Cost: 0.05610395836052249\n",
      "Iteration 85, Cost: 0.055036079685484504\n",
      "Iteration 86, Cost: 0.0539967187249657\n",
      "Iteration 87, Cost: 0.052985142156473125\n",
      "Iteration 88, Cost: 0.05200062513047597\n",
      "Iteration 89, Cost: 0.05104245531839182\n",
      "Iteration 90, Cost: 0.0501099349638495\n",
      "Iteration 91, Cost: 0.049202381626416755\n",
      "Iteration 92, Cost: 0.04831912812616168\n",
      "Iteration 93, Cost: 0.04745952204796597\n",
      "Iteration 94, Cost: 0.04662292504818338\n",
      "Iteration 95, Cost: 0.04580871211960258\n",
      "Iteration 96, Cost: 0.045016270908441444\n",
      "Iteration 97, Cost: 0.044245001133864884\n",
      "Iteration 98, Cost: 0.04349431413150099\n",
      "Iteration 99, Cost: 0.04276363252374078\n",
      "Iteration 100, Cost: 0.042052390008215194\n",
      "Iteration 101, Cost: 0.041360031249485424\n",
      "Iteration 102, Cost: 0.04068601185600192\n",
      "Iteration 103, Cost: 0.04002979842360895\n",
      "Iteration 104, Cost: 0.0393908686274719\n",
      "Iteration 105, Cost: 0.03876871134572565\n",
      "Iteration 106, Cost: 0.03816282680000832\n",
      "Iteration 107, Cost: 0.037572726700108\n",
      "Iteration 108, Cost: 0.036997934382052226\n",
      "Iteration 109, Cost: 0.03643798493101042\n",
      "Iteration 110, Cost: 0.03589242528229322\n",
      "Iteration 111, Cost: 0.035360814295484495\n",
      "Iteration 112, Cost: 0.03484272279830932\n",
      "Iteration 113, Cost: 0.03433773359821528\n",
      "Iteration 114, Cost: 0.03384544146082391\n",
      "Iteration 115, Cost: 0.033365453055398016\n",
      "Iteration 116, Cost: 0.03289738686827846\n",
      "Iteration 117, Cost: 0.03244087308588262\n",
      "Iteration 118, Cost: 0.03199555344934153\n",
      "Iteration 119, Cost: 0.031561081083199174\n",
      "Iteration 120, Cost: 0.03113712030082368\n",
      "Iteration 121, Cost: 0.030723346389302915\n",
      "Iteration 122, Cost: 0.03031944537663271\n",
      "Iteration 123, Cost: 0.029925113783972255\n",
      "Iteration 124, Cost: 0.029540058365650594\n",
      "Iteration 125, Cost: 0.02916399583947601\n",
      "Iteration 126, Cost: 0.028796652609736722\n",
      "Iteration 127, Cost: 0.028437764485097793\n",
      "Iteration 128, Cost: 0.028087076393403236\n",
      "Iteration 129, Cost: 0.02774434209519168\n",
      "Iteration 130, Cost: 0.027409323897533585\n",
      "Iteration 131, Cost: 0.027081792369603296\n",
      "Iteration 132, Cost: 0.02676152606121183\n",
      "Iteration 133, Cost: 0.026448311225350497\n",
      "Iteration 134, Cost: 0.026141941545631493\n",
      "Iteration 135, Cost: 0.025842217869360567\n",
      "Iteration 136, Cost: 0.02554894794683992\n",
      "Iteration 137, Cost: 0.025261946177375788\n",
      "Iteration 138, Cost: 0.02498103336235456\n",
      "Iteration 139, Cost: 0.02470603646565389\n",
      "Iteration 140, Cost: 0.024436788381569437\n",
      "Iteration 141, Cost: 0.02417312771036312\n",
      "Iteration 142, Cost: 0.02391489854147517\n",
      "Iteration 143, Cost: 0.023661950244386696\n",
      "Iteration 144, Cost: 0.023414137267073937\n",
      "Iteration 145, Cost: 0.023171318941956304\n",
      "Iteration 146, Cost: 0.022933359299208717\n",
      "Iteration 147, Cost: 0.02270012688728288\n",
      "Iteration 148, Cost: 0.022471494600461776\n",
      "Iteration 149, Cost: 0.022247339513255752\n",
      "Iteration 150, Cost: 0.022027542721436654\n",
      "Iteration 151, Cost: 0.021811989189498285\n",
      "Iteration 152, Cost: 0.02160056760432577\n",
      "Iteration 153, Cost: 0.021393170234853702\n",
      "Iteration 154, Cost: 0.021189692797491895\n",
      "Iteration 155, Cost: 0.020990034327098592\n",
      "Iteration 156, Cost: 0.020794097053283406\n",
      "Iteration 157, Cost: 0.02060178628182544\n",
      "Iteration 158, Cost: 0.020413010280997023\n",
      "Iteration 159, Cost: 0.02022768017258821\n",
      "Iteration 160, Cost: 0.02004570982743319\n",
      "Iteration 161, Cost: 0.019867015765245983\n",
      "Iteration 162, Cost: 0.019691517058579057\n",
      "Iteration 163, Cost: 0.01951913524072536\n",
      "Iteration 164, Cost: 0.019349794217390956\n",
      "Iteration 165, Cost: 0.01918342018197214\n",
      "Iteration 166, Cost: 0.019019941534277884\n",
      "Iteration 167, Cost: 0.018859288802544897\n",
      "Iteration 168, Cost: 0.018701394568599233\n",
      "Iteration 169, Cost: 0.018546193396024886\n",
      "Iteration 170, Cost: 0.018393621761205844\n",
      "Iteration 171, Cost: 0.0182436179871142\n",
      "Iteration 172, Cost: 0.018096122179722805\n",
      "Iteration 173, Cost: 0.017951076166926445\n",
      "Iteration 174, Cost: 0.017808423439860967\n",
      "Iteration 175, Cost: 0.017668109096515002\n",
      "Iteration 176, Cost: 0.017530079787533858\n",
      "Iteration 177, Cost: 0.01739428366411981\n",
      "Iteration 178, Cost: 0.01726067032793769\n",
      "Iteration 179, Cost: 0.017129190782938904\n",
      "Iteration 180, Cost: 0.016999797389021125\n",
      "Iteration 181, Cost: 0.0168724438174448\n",
      "Iteration 182, Cost: 0.016747085007931494\n",
      "Iteration 183, Cost: 0.016623677127372342\n",
      "Iteration 184, Cost: 0.01650217753007856\n",
      "Iteration 185, Cost: 0.01638254471950898\n",
      "Iteration 186, Cost: 0.016264738311412702\n",
      "Iteration 187, Cost: 0.01614871899832774\n",
      "Iteration 188, Cost: 0.01603444851537942\n",
      "Iteration 189, Cost: 0.015921889607324862\n",
      "Iteration 190, Cost: 0.01581100599679218\n",
      "Iteration 191, Cost: 0.015701762353665716\n",
      "Iteration 192, Cost: 0.015594124265570527\n",
      "Iteration 193, Cost: 0.015488058209411616\n",
      "Iteration 194, Cost: 0.015383531523925539\n",
      "Iteration 195, Cost: 0.015280512383203583\n",
      "Iteration 196, Cost: 0.015178969771147917\n",
      "Iteration 197, Cost: 0.015078873456823589\n",
      "Iteration 198, Cost: 0.014980193970670979\n",
      "Iteration 199, Cost: 0.014882902581544892\n",
      "Iteration 200, Cost: 0.01478697127454791\n",
      "Iteration 201, Cost: 0.014692372729627057\n",
      "Iteration 202, Cost: 0.014599080300904236\n",
      "Iteration 203, Cost: 0.014507067996712046\n",
      "Iteration 204, Cost: 0.014416310460307946\n",
      "Iteration 205, Cost: 0.014326782951240805\n",
      "Iteration 206, Cost: 0.014238461327344987\n",
      "Iteration 207, Cost: 0.014151322027338233\n",
      "Iteration 208, Cost: 0.014065342054000519\n",
      "Iteration 209, Cost: 0.013980498957912153\n",
      "Iteration 210, Cost: 0.013896770821730072\n",
      "Iteration 211, Cost: 0.013814136244982433\n",
      "Iteration 212, Cost: 0.013732574329362236\n",
      "Iteration 213, Cost: 0.013652064664501551\n",
      "Iteration 214, Cost: 0.013572587314208706\n",
      "Iteration 215, Cost: 0.0134941228031515\n",
      "Iteration 216, Cost: 0.013416652103970212\n",
      "Iteration 217, Cost: 0.013340156624804762\n",
      "Iteration 218, Cost: 0.013264618197221108\n",
      "Iteration 219, Cost: 0.013190019064522522\n",
      "Iteration 220, Cost: 0.013116341870431918\n",
      "Iteration 221, Cost: 0.013043569648132006\n",
      "Iteration 222, Cost: 0.012971685809650626\n",
      "Iteration 223, Cost: 0.012900674135578914\n",
      "Iteration 224, Cost: 0.01283051876511072\n",
      "Iteration 225, Cost: 0.012761204186391907\n",
      "Iteration 226, Cost: 0.012692715227168723\n",
      "Iteration 227, Cost: 0.012625037045724848\n",
      "Iteration 228, Cost: 0.012558155122097129\n",
      "Iteration 229, Cost: 0.012492055249560303\n",
      "Iteration 230, Cost: 0.012426723526371548\n",
      "Iteration 231, Cost: 0.012362146347765925\n",
      "Iteration 232, Cost: 0.012298310398194115\n",
      "Iteration 233, Cost: 0.0122352026437943\n",
      "Iteration 234, Cost: 0.012172810325090188\n",
      "Iteration 235, Cost: 0.01211112094990761\n",
      "Iteration 236, Cost: 0.01205012228650231\n",
      "Iteration 237, Cost: 0.011989802356891935\n",
      "Iteration 238, Cost: 0.011930149430385305\n",
      "Iteration 239, Cost: 0.011871152017302579\n",
      "Iteration 240, Cost: 0.011812798862879873\n",
      "Iteration 241, Cost: 0.011755078941352306\n",
      "Iteration 242, Cost: 0.011697981450209676\n",
      "Iteration 243, Cost: 0.011641495804619068\n",
      "Iteration 244, Cost: 0.011585611632008956\n",
      "Iteration 245, Cost: 0.011530318766809674\n",
      "Iteration 246, Cost: 0.011475607245345056\n",
      "Iteration 247, Cost: 0.01142146730087056\n",
      "Iteration 248, Cost: 0.011367889358753028\n",
      "Iteration 249, Cost: 0.011314864031787716\n",
      "Iteration 250, Cost: 0.011262382115648116\n",
      "Iteration 251, Cost: 0.011210434584464456\n",
      "Iteration 252, Cost: 0.01115901258652679\n",
      "Iteration 253, Cost: 0.011108107440108792\n",
      "Iteration 254, Cost: 0.011057710629408475\n",
      "Iteration 255, Cost: 0.011007813800602189\n",
      "Iteration 256, Cost: 0.010958408758008426\n",
      "Iteration 257, Cost: 0.010909487460358035\n",
      "Iteration 258, Cost: 0.01086104201716753\n",
      "Iteration 259, Cost: 0.010813064685212441\n",
      "Iteration 260, Cost: 0.01076554786509756\n",
      "Iteration 261, Cost: 0.010718484097921208\n",
      "Iteration 262, Cost: 0.01067186606203069\n",
      "Iteration 263, Cost: 0.010625686569866132\n",
      "Iteration 264, Cost: 0.010579938564890137\n",
      "Iteration 265, Cost: 0.010534615118600658\n",
      "Iteration 266, Cost: 0.010489709427624623\n",
      "Iteration 267, Cost: 0.010445214810889952\n",
      "Iteration 268, Cost: 0.010401124706873611\n",
      "Iteration 269, Cost: 0.01035743267092357\n",
      "Iteration 270, Cost: 0.01031413237265237\n",
      "Iteration 271, Cost: 0.010271217593400396\n",
      "Iteration 272, Cost: 0.010228682223766699\n",
      "Iteration 273, Cost: 0.010186520261205487\n",
      "Iteration 274, Cost: 0.010144725807686425\n",
      "Iteration 275, Cost: 0.01010329306741688\n",
      "Iteration 276, Cost: 0.01006221634462439\n",
      "Iteration 277, Cost: 0.010021490041397654\n",
      "Iteration 278, Cost: 0.009981108655584386\n",
      "Iteration 279, Cost: 0.009941066778744467\n",
      "Iteration 280, Cost: 0.009901359094156832\n",
      "Iteration 281, Cost: 0.00986198037487863\n",
      "Iteration 282, Cost: 0.009822925481855205\n",
      "Iteration 283, Cost: 0.009784189362079504\n",
      "Iteration 284, Cost: 0.00974576704679959\n",
      "Iteration 285, Cost: 0.009707653649772914\n",
      "Iteration 286, Cost: 0.009669844365566114\n",
      "Iteration 287, Cost: 0.009632334467899122\n",
      "Iteration 288, Cost: 0.009595119308032377\n",
      "Iteration 289, Cost: 0.009558194313196017\n",
      "Iteration 290, Cost: 0.009521554985059914\n",
      "Iteration 291, Cost: 0.009485196898243514\n",
      "Iteration 292, Cost: 0.009449115698864422\n",
      "Iteration 293, Cost: 0.009413307103124725\n",
      "Iteration 294, Cost: 0.009377766895934048\n",
      "Iteration 295, Cost: 0.009342490929568474\n",
      "Iteration 296, Cost: 0.009307475122364323\n",
      "Iteration 297, Cost: 0.009272715457445945\n",
      "Iteration 298, Cost: 0.009238207981486696\n",
      "Iteration 299, Cost: 0.009203948803502145\n",
      "Iteration 300, Cost: 0.009169934093674876\n",
      "Iteration 301, Cost: 0.009136160082209913\n",
      "Iteration 302, Cost: 0.00910262305822017\n",
      "Iteration 303, Cost: 0.009069319368641056\n",
      "Iteration 304, Cost: 0.009036245417173603\n",
      "Iteration 305, Cost: 0.009003397663255372\n",
      "Iteration 306, Cost: 0.008970772621058479\n",
      "Iteration 307, Cost: 0.008938366858514088\n",
      "Iteration 308, Cost: 0.008906176996362708\n",
      "Iteration 309, Cost: 0.00887419970722971\n",
      "Iteration 310, Cost: 0.008842431714725424\n",
      "Iteration 311, Cost: 0.008810869792569257\n",
      "Iteration 312, Cost: 0.008779510763737234\n",
      "Iteration 313, Cost: 0.008748351499632444\n",
      "Iteration 314, Cost: 0.008717388919277829\n",
      "Iteration 315, Cost: 0.008686619988530769\n",
      "Iteration 316, Cost: 0.008656041719319016\n",
      "Iteration 317, Cost: 0.008625651168897382\n",
      "Iteration 318, Cost: 0.008595445439124813\n",
      "Iteration 319, Cost: 0.008565421675761263\n",
      "Iteration 320, Cost: 0.008535577067783991\n",
      "Iteration 321, Cost: 0.008505908846722754\n",
      "Iteration 322, Cost: 0.008476414286013544\n",
      "Iteration 323, Cost: 0.008447090700370362\n",
      "Iteration 324, Cost: 0.008417935445174643\n",
      "Iteration 325, Cost: 0.008388945915881919\n",
      "Iteration 326, Cost: 0.008360119547445305\n",
      "Iteration 327, Cost: 0.00833145381375545\n",
      "Iteration 328, Cost: 0.008302946227096503\n",
      "Iteration 329, Cost: 0.00827459433761779\n",
      "Iteration 330, Cost: 0.008246395732820749\n",
      "Iteration 331, Cost: 0.008218348037060862\n",
      "Iteration 332, Cost: 0.008190448911064099\n",
      "Iteration 333, Cost: 0.008162696051457653\n",
      "Iteration 334, Cost: 0.008135087190314505\n",
      "Iteration 335, Cost: 0.00810762009471155\n",
      "Iteration 336, Cost: 0.008080292566300927\n",
      "Iteration 337, Cost: 0.008053102440894216\n",
      "Iteration 338, Cost: 0.008026047588059162\n",
      "Iteration 339, Cost: 0.007999125910728636\n",
      "Iteration 340, Cost: 0.007972335344821481\n",
      "Iteration 341, Cost: 0.007945673858874944\n",
      "Iteration 342, Cost: 0.007919139453688365\n",
      "Iteration 343, Cost: 0.007892730161977824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 344, Cost: 0.00786644404804143\n",
      "Iteration 345, Cost: 0.007840279207434948\n",
      "Iteration 346, Cost: 0.00781423376665744\n",
      "Iteration 347, Cost: 0.007788305882846673\n",
      "Iteration 348, Cost: 0.0077624937434839\n",
      "Iteration 349, Cost: 0.0077367955661077955\n",
      "Iteration 350, Cost: 0.007711209598037203\n",
      "Iteration 351, Cost: 0.007685734116102408\n",
      "Iteration 352, Cost: 0.007660367426384615\n",
      "Iteration 353, Cost: 0.007635107863963374\n",
      "Iteration 354, Cost: 0.007609953792671618\n",
      "Iteration 355, Cost: 0.007584903604858003\n",
      "Iteration 356, Cost: 0.007559955721156318\n",
      "Iteration 357, Cost: 0.007535108590261584\n",
      "Iteration 358, Cost: 0.00751036068871258\n",
      "Iteration 359, Cost: 0.007485710520680528\n",
      "Iteration 360, Cost: 0.0074611566177635565\n",
      "Iteration 361, Cost: 0.007436697538786736\n",
      "Iteration 362, Cost: 0.007412331869607279\n",
      "Iteration 363, Cost: 0.007388058222924726\n",
      "Iteration 364, Cost: 0.0073638752380956905\n",
      "Iteration 365, Cost: 0.007339781580952959\n",
      "Iteration 366, Cost: 0.0073157759436285755\n",
      "Iteration 367, Cost: 0.0072918570443806455\n",
      "Iteration 368, Cost: 0.007268023627423539\n",
      "Iteration 369, Cost: 0.007244274462761206\n",
      "Iteration 370, Cost: 0.0072206083460232525\n",
      "Iteration 371, Cost: 0.007197024098303556\n",
      "Iteration 372, Cost: 0.007173520566001026\n",
      "Iteration 373, Cost: 0.007150096620662283\n",
      "Iteration 374, Cost: 0.0071267511588259036\n",
      "Iteration 375, Cost: 0.00710348310186797\n",
      "Iteration 376, Cost: 0.007080291395848591\n",
      "Iteration 377, Cost: 0.007057175011359132\n",
      "Iteration 378, Cost: 0.007034132943369833\n",
      "Iteration 379, Cost: 0.007011164211077534\n",
      "Iteration 380, Cost: 0.00698826785775325\n",
      "Iteration 381, Cost: 0.0069654429505892484\n",
      "Iteration 382, Cost: 0.006942688580545408\n",
      "Iteration 383, Cost: 0.006920003862194566\n",
      "Iteration 384, Cost: 0.00689738793356656\n",
      "Iteration 385, Cost: 0.006874839955990743\n",
      "Iteration 386, Cost: 0.0068523591139366975\n",
      "Iteration 387, Cost: 0.006829944614852846\n",
      "Iteration 388, Cost: 0.006807595689002834\n",
      "Iteration 389, Cost: 0.006785311589299312\n",
      "Iteration 390, Cost: 0.0067630915911349905\n",
      "Iteration 391, Cost: 0.0067409349922106935\n",
      "Iteration 392, Cost: 0.00671884111236023\n",
      "Iteration 393, Cost: 0.006696809293371875\n",
      "Iteration 394, Cost: 0.006674838898806246\n",
      "Iteration 395, Cost: 0.006652929313810467\n",
      "Iteration 396, Cost: 0.006631079944928362\n",
      "Iteration 397, Cost: 0.006609290219906585\n",
      "Iteration 398, Cost: 0.00658755958749655\n",
      "Iteration 399, Cost: 0.006565887517251973\n",
      "Iteration 400, Cost: 0.006544273499321976\n",
      "Iteration 401, Cost: 0.006522717044239613\n",
      "Iteration 402, Cost: 0.006501217682705746\n",
      "Iteration 403, Cost: 0.0064797749653681775\n",
      "Iteration 404, Cost: 0.006458388462596008\n",
      "Iteration 405, Cost: 0.006437057764249152\n",
      "Iteration 406, Cost: 0.006415782479442997\n",
      "Iteration 407, Cost: 0.006394562236308172\n",
      "Iteration 408, Cost: 0.00637339668174545\n",
      "Iteration 409, Cost: 0.006352285481175786\n",
      "Iteration 410, Cost: 0.006331228318285514\n",
      "Iteration 411, Cost: 0.006310224894766764\n",
      "Iteration 412, Cost: 0.006289274930053145\n",
      "Iteration 413, Cost: 0.006268378161050781\n",
      "Iteration 414, Cost: 0.006247534341864782\n",
      "Iteration 415, Cost: 0.006226743243521226\n",
      "Iteration 416, Cost: 0.0062060046536848235\n",
      "Iteration 417, Cost: 0.006185318376372359\n",
      "Iteration 418, Cost: 0.006164684231662014\n",
      "Iteration 419, Cost: 0.006144102055398797\n",
      "Iteration 420, Cost: 0.006123571698896201\n",
      "Iteration 421, Cost: 0.006103093028634258\n",
      "Iteration 422, Cost: 0.006082665925954181\n",
      "Iteration 423, Cost: 0.006062290286749796\n",
      "Iteration 424, Cost: 0.006041966021155961\n",
      "Iteration 425, Cost: 0.006021693053234154\n",
      "Iteration 426, Cost: 0.006001471320655485\n",
      "Iteration 427, Cost: 0.005981300774381314\n",
      "Iteration 428, Cost: 0.005961181378341724\n",
      "Iteration 429, Cost: 0.005941113109112075\n",
      "Iteration 430, Cost: 0.00592109595558784\n",
      "Iteration 431, Cost: 0.005901129918658005\n",
      "Iteration 432, Cost: 0.005881215010877226\n",
      "Iteration 433, Cost: 0.005861351256136995\n",
      "Iteration 434, Cost: 0.005841538689336045\n",
      "Iteration 435, Cost: 0.00582177735605023\n",
      "Iteration 436, Cost: 0.00580206731220211\n",
      "Iteration 437, Cost: 0.0057824086237304485\n",
      "Iteration 438, Cost: 0.00576280136625988\n",
      "Iteration 439, Cost: 0.005743245624770938\n",
      "Iteration 440, Cost: 0.00572374149327067\n",
      "Iteration 441, Cost: 0.005704289074464038\n",
      "Iteration 442, Cost: 0.005684888479426322\n",
      "Iteration 443, Cost: 0.005665539827276709\n",
      "Iteration 444, Cost: 0.0056462432448532406\n",
      "Iteration 445, Cost: 0.00562699886638934\n",
      "Iteration 446, Cost: 0.005607806833192066\n",
      "Iteration 447, Cost: 0.005588667293322229\n",
      "Iteration 448, Cost: 0.005569580401276594\n",
      "Iteration 449, Cost: 0.0055505463176722634\n",
      "Iteration 450, Cost: 0.0055315652089333875\n",
      "Iteration 451, Cost: 0.005512637246980354\n",
      "Iteration 452, Cost: 0.005493762608921559\n",
      "Iteration 453, Cost: 0.005474941476747901\n",
      "Iteration 454, Cost: 0.005456174037030058\n",
      "Iteration 455, Cost: 0.005437460480618691\n",
      "Iteration 456, Cost: 0.005418801002347651\n",
      "Iteration 457, Cost: 0.005400195800740267\n",
      "Iteration 458, Cost: 0.005381645077718795\n",
      "Iteration 459, Cost: 0.005363149038317112\n",
      "Iteration 460, Cost: 0.005344707890396693\n",
      "Iteration 461, Cost: 0.005326321844365972\n",
      "Iteration 462, Cost: 0.0053079911129030885\n",
      "Iteration 463, Cost: 0.005289715910682123\n",
      "Iteration 464, Cost: 0.00527149645410284\n",
      "Iteration 465, Cost: 0.005253332961023972\n",
      "Iteration 466, Cost: 0.00523522565050011\n",
      "Iteration 467, Cost: 0.005217174742522221\n",
      "Iteration 468, Cost: 0.0051991804577618365\n",
      "Iteration 469, Cost: 0.00518124301731893\n",
      "Iteration 470, Cost: 0.0051633626424735434\n",
      "Iteration 471, Cost: 0.005145539554441174\n",
      "Iteration 472, Cost: 0.005127773974131956\n",
      "Iteration 473, Cost: 0.005110066121913667\n",
      "Iteration 474, Cost: 0.005092416217378626\n",
      "Iteration 475, Cost: 0.005074824479114441\n",
      "Iteration 476, Cost: 0.005057291124478752\n",
      "Iteration 477, Cost: 0.005039816369377883\n",
      "Iteration 478, Cost: 0.005022400428049543\n",
      "Iteration 479, Cost: 0.00500504351284956\n",
      "Iteration 480, Cost: 0.004987745834042709\n",
      "Converged!\n",
      "Accuracy: 0.9722222222222222\n",
      "F1 Score: 0.9863013698630138\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "df_wine = pd.read_csv(\"/Users/noshitha/Downloads/hw3/datasets/hw3_wine.csv\", delimiter=\"\\t\")\n",
    "\n",
    "# Preprocess data\n",
    "X_wine = pd.get_dummies(df_wine.iloc[:, 1:])\n",
    "y_wine = df_wine.iloc[:, 0]\n",
    "\n",
    "# Normalize data\n",
    "y_wine_resized = y_wine.values.reshape(-1, 1)\n",
    "\n",
    "# Initialize the OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Fit and transform the target variable\n",
    "y_encoded = encoder.fit_transform(y_wine_resized)\n",
    "\n",
    "# Define model architectures\n",
    "architectures = [\n",
    "    [X_wine.shape[1], 10, 8, 3]# Example architecture\n",
    "]\n",
    "\n",
    "# Instantiate the NeuralNetwork class with the desired architecture\n",
    "model = NeuralNetwork(architectures[0])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_wine, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize input features\n",
    "# Compute mean and standard deviation for each feature\n",
    "mean = np.mean(X_wine, axis=0)\n",
    "std = np.std(X_wine, axis=0)\n",
    "\n",
    "# Normalize training and test data\n",
    "X_train_normalized = (X_train - mean) / std\n",
    "X_test_normalized = (X_test - mean) / std\n",
    "\n",
    "# Train the neural network\n",
    "model.train(X_train_normalized, y_train, learning_rate=0.01, lam=0.01, max_iterations=1000, epsilon=0.005)\n",
    "\n",
    "\n",
    "# Evaluate the trained model\n",
    "accuracy, f1_score = model.evaluate(X_test_normalized, y_test)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1 Score:\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3617068d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4c71cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fcf3a0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, Cost: 0.22980779640241564\n",
      "Iteration 2, Cost: 0.1763618624147078\n",
      "Iteration 3, Cost: 0.15313726169860026\n",
      "Iteration 4, Cost: 0.14400886704399485\n",
      "Iteration 5, Cost: 0.1382417912669854\n",
      "Iteration 6, Cost: 0.13353205020122735\n",
      "Iteration 7, Cost: 0.12932750762361256\n",
      "Iteration 8, Cost: 0.12545885447220326\n",
      "Iteration 9, Cost: 0.1218559083727014\n",
      "Iteration 10, Cost: 0.11847823540795342\n",
      "Iteration 11, Cost: 0.1152963947122215\n",
      "Iteration 12, Cost: 0.11228648566572338\n",
      "Iteration 13, Cost: 0.10942836292253649\n",
      "Iteration 14, Cost: 0.10670487778673869\n",
      "Iteration 15, Cost: 0.10410141409762848\n",
      "Iteration 16, Cost: 0.10160552286062861\n",
      "Iteration 17, Cost: 0.09920660749764455\n",
      "Iteration 18, Cost: 0.09689564890479624\n",
      "Iteration 19, Cost: 0.09466496685319341\n",
      "Iteration 20, Cost: 0.09250801458218261\n",
      "Iteration 21, Cost: 0.09041920283454082\n",
      "Iteration 22, Cost: 0.08839374938202224\n",
      "Iteration 23, Cost: 0.08642755033583198\n",
      "Iteration 24, Cost: 0.08451707002899278\n",
      "Iteration 25, Cost: 0.0826592468117199\n",
      "Iteration 26, Cost: 0.08085141260080138\n",
      "Iteration 27, Cost: 0.07909122441729154\n",
      "Iteration 28, Cost: 0.07737660642746694\n",
      "Iteration 29, Cost: 0.07570570119127527\n",
      "Iteration 30, Cost: 0.07407682895257112\n",
      "Iteration 31, Cost: 0.07248845390692955\n",
      "Iteration 32, Cost: 0.07093915647778294\n",
      "Iteration 33, Cost: 0.06942761073130435\n",
      "Iteration 34, Cost: 0.06795256616694334\n",
      "Iteration 35, Cost: 0.06651283323003541\n",
      "Iteration 36, Cost: 0.06510727199935391\n",
      "Iteration 37, Cost: 0.06373478360001936\n",
      "Iteration 38, Cost: 0.06239430397665455\n",
      "Iteration 39, Cost: 0.06108479973086969\n",
      "Iteration 40, Cost: 0.059805265780499314\n",
      "Iteration 41, Cost: 0.058554724635893285\n",
      "Iteration 42, Cost: 0.05733222711178403\n",
      "Iteration 43, Cost: 0.05613685430259275\n",
      "Iteration 44, Cost: 0.05496772064515152\n",
      "Iteration 45, Cost: 0.053823977876374336\n",
      "Iteration 46, Cost: 0.05270481966549052\n",
      "Iteration 47, Cost: 0.05160948666310693\n",
      "Iteration 48, Cost: 0.05053727166620672\n",
      "Iteration 49, Cost: 0.049487524554877725\n",
      "Iteration 50, Cost: 0.04845965662091603\n",
      "Iteration 51, Cost: 0.04745314389000572\n",
      "Iteration 52, Cost: 0.046467529047980856\n",
      "Iteration 53, Cost: 0.04550242162636226\n",
      "Iteration 54, Cost: 0.04455749618775388\n",
      "Iteration 55, Cost: 0.04363248837655895\n",
      "Iteration 56, Cost: 0.042727188856215605\n",
      "Iteration 57, Cost: 0.04184143532499716\n",
      "Iteration 58, Cost: 0.040975102967622114\n",
      "Iteration 59, Cost: 0.040128093837454255\n",
      "Iteration 60, Cost: 0.03930032575501127\n",
      "Iteration 61, Cost: 0.038491721340813474\n",
      "Iteration 62, Cost: 0.03770219777136008\n",
      "Iteration 63, Cost: 0.036931657762529185\n",
      "Iteration 64, Cost: 0.03617998215873319\n",
      "Iteration 65, Cost: 0.03544702435696099\n",
      "Iteration 66, Cost: 0.03473260664168634\n",
      "Iteration 67, Cost: 0.034036518366767195\n",
      "Iteration 68, Cost: 0.03335851580686663\n",
      "Iteration 69, Cost: 0.03269832342109803\n",
      "Iteration 70, Cost: 0.03205563622749422\n",
      "Iteration 71, Cost: 0.03143012297575714\n",
      "Iteration 72, Cost: 0.030821429821472283\n",
      "Iteration 73, Cost: 0.030229184239780885\n",
      "Iteration 74, Cost: 0.02965299896239093\n",
      "Iteration 75, Cost: 0.02909247577168961\n",
      "Iteration 76, Cost: 0.028547209034088406\n",
      "Iteration 77, Cost: 0.02801678889787773\n",
      "Iteration 78, Cost: 0.027500804116770562\n",
      "Iteration 79, Cost: 0.026998844488317717\n",
      "Iteration 80, Cost: 0.02651050291680069\n",
      "Iteration 81, Cost: 0.02603537712396442\n",
      "Iteration 82, Cost: 0.02557307103923257\n",
      "Iteration 83, Cost: 0.025123195905094885\n",
      "Iteration 84, Cost: 0.024685371134318353\n",
      "Iteration 85, Cost: 0.024259224954491152\n",
      "Iteration 86, Cost: 0.023844394872944756\n",
      "Iteration 87, Cost: 0.023440527991908534\n",
      "Iteration 88, Cost: 0.02304728120026006\n",
      "Iteration 89, Cost: 0.022664321264733008\n",
      "Iteration 90, Cost: 0.022291324840116592\n",
      "Iteration 91, Cost: 0.02192797841493197\n",
      "Iteration 92, Cost: 0.02157397820635136\n",
      "Iteration 93, Cost: 0.02122903001574894\n",
      "Iteration 94, Cost: 0.02089284905422751\n",
      "Iteration 95, Cost: 0.02056515974572856\n",
      "Iteration 96, Cost: 0.02024569551387399\n",
      "Iteration 97, Cost: 0.019934198557473075\n",
      "Iteration 98, Cost: 0.019630419618624464\n",
      "Iteration 99, Cost: 0.019334117746520385\n",
      "Iteration 100, Cost: 0.01904506005938984\n",
      "Iteration 101, Cost: 0.018763021506475335\n",
      "Iteration 102, Cost: 0.01848778463150158\n",
      "Iteration 103, Cost: 0.018219139338746453\n",
      "Iteration 104, Cost: 0.017956882662547937\n",
      "Iteration 105, Cost: 0.01770081854086333\n",
      "Iteration 106, Cost: 0.01745075759332633\n",
      "Iteration 107, Cost: 0.01720651690411575\n",
      "Iteration 108, Cost: 0.016967919809847586\n",
      "Iteration 109, Cost: 0.016734795692624697\n",
      "Iteration 110, Cost: 0.01650697977831952\n",
      "Iteration 111, Cost: 0.016284312940121202\n",
      "Iteration 112, Cost: 0.016066641507346092\n",
      "Iteration 113, Cost: 0.015853817079486358\n",
      "Iteration 114, Cost: 0.01564569634545413\n",
      "Iteration 115, Cost: 0.015442140907966017\n",
      "Iteration 116, Cost: 0.015243017113003675\n",
      "Iteration 117, Cost: 0.015048195884279652\n",
      "Iteration 118, Cost: 0.014857552562632973\n",
      "Iteration 119, Cost: 0.014670966750275516\n",
      "Iteration 120, Cost: 0.014488322159807408\n",
      "Iteration 121, Cost: 0.01430950646791773\n",
      "Iteration 122, Cost: 0.014134411173684941\n",
      "Iteration 123, Cost: 0.013962931461389867\n",
      "Iteration 124, Cost: 0.013794966067752776\n",
      "Iteration 125, Cost: 0.013630417153504621\n",
      "Iteration 126, Cost: 0.013469190179201484\n",
      "Iteration 127, Cost: 0.01331119378518989\n",
      "Iteration 128, Cost: 0.013156339675629762\n",
      "Iteration 129, Cost: 0.013004542506480882\n",
      "Iteration 130, Cost: 0.012855719777357702\n",
      "Iteration 131, Cost: 0.012709791727156973\n",
      "Iteration 132, Cost: 0.012566681233361873\n",
      "Iteration 133, Cost: 0.012426313714926179\n",
      "Iteration 134, Cost: 0.012288617038641836\n",
      "Iteration 135, Cost: 0.01215352142889314\n",
      "Iteration 136, Cost: 0.012020959380701317\n",
      "Iteration 137, Cost: 0.011890865575963269\n",
      "Iteration 138, Cost: 0.011763176802789186\n",
      "Iteration 139, Cost: 0.01163783187784437\n",
      "Iteration 140, Cost: 0.011514771571601403\n",
      "Iteration 141, Cost: 0.011393938536410185\n",
      "Iteration 142, Cost: 0.0112752772372943\n",
      "Iteration 143, Cost: 0.011158733885383779\n",
      "Iteration 144, Cost: 0.011044256373895571\n",
      "Iteration 145, Cost: 0.010931794216574954\n",
      "Iteration 146, Cost: 0.01082129848851246\n",
      "Iteration 147, Cost: 0.01071272176925296\n",
      "Iteration 148, Cost: 0.010606018088115203\n",
      "Iteration 149, Cost: 0.010501142871642173\n",
      "Iteration 150, Cost: 0.01039805289310443\n",
      "Iteration 151, Cost: 0.01029670622398071\n",
      "Iteration 152, Cost: 0.010197062187342081\n",
      "Iteration 153, Cost: 0.010099081313067892\n",
      "Iteration 154, Cost: 0.010002725294823944\n",
      "Iteration 155, Cost: 0.00990795694873526\n",
      "Iteration 156, Cost: 0.009814740173687973\n",
      "Iteration 157, Cost: 0.009723039913196785\n",
      "Iteration 158, Cost: 0.009632822118776646\n",
      "Iteration 159, Cost: 0.009544053714759106\n",
      "Iteration 160, Cost: 0.009456702564495984\n",
      "Iteration 161, Cost: 0.009370737437894757\n",
      "Iteration 162, Cost: 0.009286127980232184\n",
      "Iteration 163, Cost: 0.00920284468219442\n",
      "Iteration 164, Cost: 0.009120858851093792\n",
      "Iteration 165, Cost: 0.00904014258321424\n",
      "Iteration 166, Cost: 0.00896066873723914\n",
      "Iteration 167, Cost: 0.008882410908717024\n",
      "Iteration 168, Cost: 0.008805343405522312\n",
      "Iteration 169, Cost: 0.008729441224269921\n",
      "Iteration 170, Cost: 0.00865468002764409\n",
      "Iteration 171, Cost: 0.008581036122603403\n",
      "Iteration 172, Cost: 0.008508486439425499\n",
      "Iteration 173, Cost: 0.008437008511556314\n",
      "Iteration 174, Cost: 0.00836658045623023\n",
      "Iteration 175, Cost: 0.008297180955828844\n",
      "Iteration 176, Cost: 0.00822878923994735\n",
      "Iteration 177, Cost: 0.008161385068138818\n",
      "Iteration 178, Cost: 0.008094948713307968\n",
      "Iteration 179, Cost: 0.008029460945727143\n",
      "Iteration 180, Cost: 0.007964903017648317\n",
      "Iteration 181, Cost: 0.007901256648486205\n",
      "Iteration 182, Cost: 0.00783850401054845\n",
      "Iteration 183, Cost: 0.007776627715290059\n",
      "Iteration 184, Cost: 0.007715610800070061\n",
      "Iteration 185, Cost: 0.00765543671538948\n",
      "Iteration 186, Cost: 0.0075960893125905414\n",
      "Iteration 187, Cost: 0.0075375528319978815\n",
      "Iteration 188, Cost: 0.00747981189148347\n",
      "Iteration 189, Cost: 0.007422851475437664\n",
      "Iteration 190, Cost: 0.007366656924129622\n",
      "Iteration 191, Cost: 0.007311213923441081\n",
      "Iteration 192, Cost: 0.0072565084949581405\n",
      "Iteration 193, Cost: 0.0072025269864064555\n",
      "Iteration 194, Cost: 0.0071492560624158204\n",
      "Iteration 195, Cost: 0.007096682695600776\n",
      "Iteration 196, Cost: 0.007044794157944513\n",
      "Iteration 197, Cost: 0.006993578012473828\n",
      "Iteration 198, Cost: 0.006943022105213486\n",
      "Iteration 199, Cost: 0.006893114557408873\n",
      "Iteration 200, Cost: 0.006843843758006283\n",
      "Iteration 201, Cost: 0.0067951983563806695\n",
      "Iteration 202, Cost: 0.006747167255301156\n",
      "Iteration 203, Cost: 0.006699739604125046\n",
      "Iteration 204, Cost: 0.006652904792211394\n",
      "Iteration 205, Cost: 0.006606652442545764\n",
      "Iteration 206, Cost: 0.006560972405567955\n",
      "Iteration 207, Cost: 0.006515854753195072\n",
      "Iteration 208, Cost: 0.006471289773032424\n",
      "Iteration 209, Cost: 0.006427267962765252\n",
      "Iteration 210, Cost: 0.006383780024724473\n",
      "Iteration 211, Cost: 0.006340816860619985\n",
      "Iteration 212, Cost: 0.006298369566435325\n",
      "Iteration 213, Cost: 0.006256429427477759\n",
      "Iteration 214, Cost: 0.006214987913578147\n",
      "Iteration 215, Cost: 0.006174036674435119\n",
      "Iteration 216, Cost: 0.006133567535098411\n",
      "Iteration 217, Cost: 0.006093572491586335\n",
      "Iteration 218, Cost: 0.00605404370663267\n",
      "Iteration 219, Cost: 0.0060149735055583615\n",
      "Iteration 220, Cost: 0.0059763543722636875\n",
      "Iteration 221, Cost: 0.00593817894533667\n",
      "Iteration 222, Cost: 0.005900440014273759\n",
      "Iteration 223, Cost: 0.005863130515808864\n",
      "Iteration 224, Cost: 0.005826243530347116\n",
      "Iteration 225, Cost: 0.005789772278499735\n",
      "Iteration 226, Cost: 0.005753710117716687\n",
      "Iteration 227, Cost: 0.005718050539013763\n",
      "Iteration 228, Cost: 0.005682787163791044\n",
      "Iteration 229, Cost: 0.005647913740739657\n",
      "Iteration 230, Cost: 0.005613424142833983\n",
      "Iteration 231, Cost: 0.005579312364406513\n",
      "Iteration 232, Cost: 0.0055455725183026746\n",
      "Iteration 233, Cost: 0.005512198833113088\n",
      "Iteration 234, Cost: 0.005479185650480732\n",
      "Iteration 235, Cost: 0.005446527422480691\n",
      "Iteration 236, Cost: 0.005414218709070184\n",
      "Iteration 237, Cost: 0.005382254175606629\n",
      "Iteration 238, Cost: 0.005350628590431708\n",
      "Iteration 239, Cost: 0.005319336822519303\n",
      "Iteration 240, Cost: 0.005288373839185401\n",
      "Iteration 241, Cost: 0.005257734703858036\n",
      "Iteration 242, Cost: 0.005227414573905473\n",
      "Iteration 243, Cost: 0.0051974086985208355\n",
      "Iteration 244, Cost: 0.0051677124166615235\n",
      "Iteration 245, Cost: 0.005138321155041733\n",
      "Iteration 246, Cost: 0.005109230426176568\n",
      "Iteration 247, Cost: 0.005080435826476137\n",
      "Iteration 248, Cost: 0.005051933034388243\n",
      "Iteration 249, Cost: 0.005023717808588176\n",
      "Iteration 250, Cost: 0.004995785986214301\n",
      "Converged!\n",
      "Iteration 1, Cost: 0.28393580212589276\n",
      "Iteration 2, Cost: 0.2502238648297742\n",
      "Iteration 3, Cost: 0.2222350222507119\n",
      "Iteration 4, Cost: 0.20262335676043491\n",
      "Iteration 5, Cost: 0.19178033107539505\n",
      "Iteration 6, Cost: 0.18568997832988673\n",
      "Iteration 7, Cost: 0.18140177849026592\n",
      "Iteration 8, Cost: 0.17775861751885122\n",
      "Iteration 9, Cost: 0.17437719041860192\n",
      "Iteration 10, Cost: 0.17113267917239797\n",
      "Iteration 11, Cost: 0.16798374485157738\n",
      "Iteration 12, Cost: 0.1649159418281324\n",
      "Iteration 13, Cost: 0.16192317951246363\n",
      "Iteration 14, Cost: 0.15900169897430114\n",
      "Iteration 15, Cost: 0.15614827468149162\n",
      "Iteration 16, Cost: 0.15335982001335\n",
      "Iteration 17, Cost: 0.1506334066231855\n",
      "Iteration 18, Cost: 0.1479663556029767\n",
      "Iteration 19, Cost: 0.1453562944889299\n",
      "Iteration 20, Cost: 0.14280115959215436\n",
      "Iteration 21, Cost: 0.14029915294303644\n",
      "Iteration 22, Cost: 0.13784867257241482\n",
      "Iteration 23, Cost: 0.13544823607618212\n",
      "Iteration 24, Cost: 0.1330964141602605\n",
      "Iteration 25, Cost: 0.1307917848088951\n",
      "Iteration 26, Cost: 0.12853291166532033\n",
      "Iteration 27, Cost: 0.12631834411195964\n",
      "Iteration 28, Cost: 0.12414663275998312\n",
      "Iteration 29, Cost: 0.12201635292990813\n",
      "Iteration 30, Cost: 0.11992612962980036\n",
      "Iteration 31, Cost: 0.11787465950956477\n",
      "Iteration 32, Cost: 0.11586072737975846\n",
      "Iteration 33, Cost: 0.11388321658084054\n",
      "Iteration 34, Cost: 0.11194111358971276\n",
      "Iteration 35, Cost: 0.11003350781083503\n",
      "Iteration 36, Cost: 0.10815958767446252\n",
      "Iteration 37, Cost: 0.106318634114071\n",
      "Iteration 38, Cost: 0.10451001233976405\n",
      "Iteration 39, Cost: 0.10273316263988598\n",
      "Iteration 40, Cost: 0.10098759076852479\n",
      "Iteration 41, Cost: 0.09927285832809621\n",
      "Iteration 42, Cost: 0.09758857343708453\n",
      "Iteration 43, Cost: 0.09593438188087532\n",
      "Iteration 44, Cost: 0.09430995887416052\n",
      "Iteration 45, Cost: 0.09271500151220761\n",
      "Iteration 46, Cost: 0.0911492219514362\n",
      "Iteration 47, Cost: 0.08961234133391517\n",
      "Iteration 48, Cost: 0.08810408445278982\n",
      "Iteration 49, Cost: 0.0866241751439369\n",
      "Iteration 50, Cost: 0.08517233238142204\n",
      "Iteration 51, Cost: 0.08374826704908929\n",
      "Iteration 52, Cost: 0.08235167935675604\n",
      "Iteration 53, Cost: 0.08098225686629086\n",
      "Iteration 54, Cost: 0.07963967308992835\n",
      "Iteration 55, Cost: 0.07832358662038029\n",
      "Iteration 56, Cost: 0.0770336407496666\n",
      "Iteration 57, Cost: 0.07576946353122226\n",
      "Iteration 58, Cost: 0.07453066823787173\n",
      "Iteration 59, Cost: 0.07331685416680153\n",
      "Iteration 60, Cost: 0.07212760774176404\n",
      "Iteration 61, Cost: 0.07096250386242205\n",
      "Iteration 62, Cost: 0.0698211074509706\n",
      "Iteration 63, Cost: 0.06870297514691703\n",
      "Iteration 64, Cost: 0.0676076571021366\n",
      "Iteration 65, Cost: 0.06653469883004148\n",
      "Iteration 66, Cost: 0.0654836430649342\n",
      "Iteration 67, Cost: 0.06445403159040637\n",
      "Iteration 68, Cost: 0.06344540699905327\n",
      "Iteration 69, Cost: 0.06245731434985665\n",
      "Iteration 70, Cost: 0.06148930269436342\n",
      "Iteration 71, Cost: 0.06054092644823094\n",
      "Iteration 72, Cost: 0.05961174659072079\n",
      "Iteration 73, Cost: 0.05870133168113105\n",
      "Iteration 74, Cost: 0.05780925868771828\n",
      "Iteration 75, Cost: 0.056935113631080086\n",
      "Iteration 76, Cost: 0.056078492049926494\n",
      "Iteration 77, Cost: 0.05523899930235745\n",
      "Iteration 78, Cost: 0.05441625071992808\n",
      "Iteration 79, Cost: 0.05360987163474382\n",
      "Iteration 80, Cost: 0.05281949730151127\n",
      "Iteration 81, Cost: 0.05204477273690221\n",
      "Iteration 82, Cost: 0.05128535249789385\n",
      "Iteration 83, Cost: 0.050540900419119725\n",
      "Iteration 84, Cost: 0.04981108932694089\n",
      "Iteration 85, Cost: 0.04909560074517308\n",
      "Iteration 86, Cost: 0.04839412460441498\n",
      "Iteration 87, Cost: 0.04770635896391431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 88, Cost: 0.047032009752038494\n",
      "Iteration 89, Cost: 0.0463707905287944\n",
      "Iteration 90, Cost: 0.04572242227154054\n",
      "Iteration 91, Cost: 0.04508663318309176\n",
      "Iteration 92, Cost: 0.04446315851984773\n",
      "Iteration 93, Cost: 0.043851740436381244\n",
      "Iteration 94, Cost: 0.04325212784208833\n",
      "Iteration 95, Cost: 0.04266407626501149\n",
      "Iteration 96, Cost: 0.04208734771777396\n",
      "Iteration 97, Cost: 0.041521710560677626\n",
      "Iteration 98, Cost: 0.04096693935738552\n",
      "Iteration 99, Cost: 0.040422814719192006\n",
      "Iteration 100, Cost: 0.0398891231346383\n",
      "Iteration 101, Cost: 0.039365656782111416\n",
      "Iteration 102, Cost: 0.038852213324026226\n",
      "Iteration 103, Cost: 0.03834859568218542\n",
      "Iteration 104, Cost: 0.03785461179489788\n",
      "Iteration 105, Cost: 0.03737007435737015\n",
      "Iteration 106, Cost: 0.03689480054773313\n",
      "Iteration 107, Cost: 0.03642861174179403\n",
      "Iteration 108, Cost: 0.03597133322018989\n",
      "Iteration 109, Cost: 0.0355227938720443\n",
      "Iteration 110, Cost: 0.03508282589948516\n",
      "Iteration 111, Cost: 0.03465126452746541\n",
      "Iteration 112, Cost: 0.03422794772324793\n",
      "Iteration 113, Cost: 0.03381271592967979\n",
      "Iteration 114, Cost: 0.03340541181601286\n",
      "Iteration 115, Cost: 0.03300588004954642\n",
      "Iteration 116, Cost: 0.03261396709080339\n",
      "Iteration 117, Cost: 0.032229521014331824\n",
      "Iteration 118, Cost: 0.03185239135657789\n",
      "Iteration 119, Cost: 0.03148242899163166\n",
      "Iteration 120, Cost: 0.031119486035029436\n",
      "Iteration 121, Cost: 0.03076341577522512\n",
      "Iteration 122, Cost: 0.030414072631835945\n",
      "Iteration 123, Cost: 0.030071312139335723\n",
      "Iteration 124, Cost: 0.029734990954517513\n",
      "Iteration 125, Cost: 0.029404966885779604\n",
      "Iteration 126, Cost: 0.029081098942101386\n",
      "Iteration 127, Cost: 0.028763247399462226\n",
      "Iteration 128, Cost: 0.028451273882410342\n",
      "Iteration 129, Cost: 0.028145041458497444\n",
      "Iteration 130, Cost: 0.027844414743349852\n",
      "Iteration 131, Cost: 0.02754926001423484\n",
      "Iteration 132, Cost: 0.027259445330092436\n",
      "Iteration 133, Cost: 0.026974840656127638\n",
      "Iteration 134, Cost: 0.02669531799118684\n",
      "Iteration 135, Cost: 0.026420751496268727\n",
      "Iteration 136, Cost: 0.026151017622636972\n",
      "Iteration 137, Cost: 0.025885995238106765\n",
      "Iteration 138, Cost: 0.025625565750165866\n",
      "Iteration 139, Cost: 0.025369613224662378\n",
      "Iteration 140, Cost: 0.0251180244988465\n",
      "Iteration 141, Cost: 0.024870689287592954\n",
      "Iteration 142, Cost: 0.02462750028165748\n",
      "Iteration 143, Cost: 0.024388353236839477\n",
      "Iteration 144, Cost: 0.02415314705293682\n",
      "Iteration 145, Cost: 0.023921783841395775\n",
      "Iteration 146, Cost: 0.023694168980583276\n",
      "Iteration 147, Cost: 0.023470211157649225\n",
      "Iteration 148, Cost: 0.0232498223960075\n",
      "Iteration 149, Cost: 0.023032918067554524\n",
      "Iteration 150, Cost: 0.022819416888866986\n",
      "Iteration 151, Cost: 0.022609240900781157\n",
      "Iteration 152, Cost: 0.022402315430956176\n",
      "Iteration 153, Cost: 0.02219856903926339\n",
      "Iteration 154, Cost: 0.02199793344611989\n",
      "Iteration 155, Cost: 0.021800343444191177\n",
      "Iteration 156, Cost: 0.021605736794216755\n",
      "Iteration 157, Cost: 0.021414054106051884\n",
      "Iteration 158, Cost: 0.021225238706354954\n",
      "Iteration 159, Cost: 0.021039236494668198\n",
      "Iteration 160, Cost: 0.02085599578992322\n",
      "Iteration 161, Cost: 0.020675467169637533\n",
      "Iteration 162, Cost: 0.020497603304239886\n",
      "Iteration 163, Cost: 0.020322358789059865\n",
      "Iteration 164, Cost: 0.020149689976533682\n",
      "Iteration 165, Cost: 0.019979554811111015\n",
      "Iteration 166, Cost: 0.01981191266919839\n",
      "Iteration 167, Cost: 0.01964672420625026\n",
      "Iteration 168, Cost: 0.019483951212830516\n",
      "Iteration 169, Cost: 0.019323556481128577\n",
      "Iteration 170, Cost: 0.019165503683043685\n",
      "Iteration 171, Cost: 0.019009757260564995\n",
      "Iteration 172, Cost: 0.01885628232879344\n",
      "Iteration 173, Cost: 0.018705044591589337\n",
      "Iteration 174, Cost: 0.018556010269503213\n",
      "Iteration 175, Cost: 0.018409146039366615\n",
      "Iteration 176, Cost: 0.01826441898469359\n",
      "Iteration 177, Cost: 0.018121796555875532\n",
      "Iteration 178, Cost: 0.017981246539043476\n",
      "Iteration 179, Cost: 0.01784273703241952\n",
      "Iteration 180, Cost: 0.01770623642897818\n",
      "Iteration 181, Cost: 0.017571713404281607\n",
      "Iteration 182, Cost: 0.017439136908431588\n",
      "Iteration 183, Cost: 0.017308476161186744\n",
      "Iteration 184, Cost: 0.017179700649416714\n",
      "Iteration 185, Cost: 0.01705278012619745\n",
      "Iteration 186, Cost: 0.016927684610985785\n",
      "Iteration 187, Cost: 0.016804384390440692\n",
      "Iteration 188, Cost: 0.0166828500195779\n",
      "Iteration 189, Cost: 0.016563052323050352\n",
      "Iteration 190, Cost: 0.01644496239643657\n",
      "Iteration 191, Cost: 0.016328551607491894\n",
      "Iteration 192, Cost: 0.016213791597372563\n",
      "Iteration 193, Cost: 0.016100654281882027\n",
      "Iteration 194, Cost: 0.015989111852812127\n",
      "Iteration 195, Cost: 0.015879136779463208\n",
      "Iteration 196, Cost: 0.01577070181042651\n",
      "Iteration 197, Cost: 0.015663779975703656\n",
      "Iteration 198, Cost: 0.015558344589223064\n",
      "Iteration 199, Cost: 0.015454369251793573\n",
      "Iteration 200, Cost: 0.015351827854514506\n",
      "Iteration 201, Cost: 0.0152506945826394\n",
      "Iteration 202, Cost: 0.015150943919869512\n",
      "Iteration 203, Cost: 0.015052550653034487\n",
      "Iteration 204, Cost: 0.01495548987710108\n",
      "Iteration 205, Cost: 0.014859737000437866\n",
      "Iteration 206, Cost: 0.01476526775025435\n",
      "Iteration 207, Cost: 0.014672058178126645\n",
      "Iteration 208, Cost: 0.014580084665519332\n",
      "Iteration 209, Cost: 0.014489323929213184\n",
      "Iteration 210, Cost: 0.014399753026551684\n",
      "Iteration 211, Cost: 0.014311349360424148\n",
      "Iteration 212, Cost: 0.014224090683910728\n",
      "Iteration 213, Cost: 0.014137955104522548\n",
      "Iteration 214, Cost: 0.014052921087979882\n",
      "Iteration 215, Cost: 0.013968967461480833\n",
      "Iteration 216, Cost: 0.013886073416423212\n",
      "Iteration 217, Cost: 0.013804218510552002\n",
      "Iteration 218, Cost: 0.013723382669514375\n",
      "Iteration 219, Cost: 0.013643546187813155\n",
      "Iteration 220, Cost: 0.01356468972915774\n",
      "Iteration 221, Cost: 0.01348679432621896\n",
      "Iteration 222, Cost: 0.01340984137980073\n",
      "Iteration 223, Cost: 0.013333812657447099\n",
      "Iteration 224, Cost: 0.013258690291507804\n",
      "Iteration 225, Cost: 0.013184456776689451\n",
      "Iteration 226, Cost: 0.013111094967122212\n",
      "Iteration 227, Cost: 0.013038588072974418\n",
      "Iteration 228, Cost: 0.012966919656648675\n",
      "Iteration 229, Cost: 0.01289607362859427\n",
      "Iteration 230, Cost: 0.01282603424277083\n",
      "Iteration 231, Cost: 0.012756786091798089\n",
      "Iteration 232, Cost: 0.012688314101826118\n",
      "Iteration 233, Cost: 0.012620603527159524\n",
      "Iteration 234, Cost: 0.012553639944667785\n",
      "Iteration 235, Cost: 0.012487409248012863\n",
      "Iteration 236, Cost: 0.012421897641723282\n",
      "Iteration 237, Cost: 0.012357091635142585\n",
      "Iteration 238, Cost: 0.012292978036278098\n",
      "Iteration 239, Cost: 0.012229543945574298\n",
      "Iteration 240, Cost: 0.012166776749633235\n",
      "Iteration 241, Cost: 0.012104664114902691\n",
      "Iteration 242, Cost: 0.012043193981351057\n",
      "Iteration 243, Cost: 0.011982354556146138\n",
      "Iteration 244, Cost: 0.011922134307353638\n",
      "Iteration 245, Cost: 0.011862521957669212\n",
      "Iteration 246, Cost: 0.01180350647819689\n",
      "Iteration 247, Cost: 0.011745077082284939\n",
      "Iteration 248, Cost: 0.011687223219429117\n",
      "Iteration 249, Cost: 0.011629934569252008\n",
      "Iteration 250, Cost: 0.011573201035565927\n",
      "Iteration 251, Cost: 0.01151701274052596\n",
      "Iteration 252, Cost: 0.011461360018878604\n",
      "Iteration 253, Cost: 0.011406233412310764\n",
      "Iteration 254, Cost: 0.011351623663902768\n",
      "Iteration 255, Cost: 0.011297521712688737\n",
      "Iteration 256, Cost: 0.011243918688326563\n",
      "Iteration 257, Cost: 0.011190805905879472\n",
      "Iteration 258, Cost: 0.011138174860710425\n",
      "Iteration 259, Cost: 0.011086017223490214\n",
      "Iteration 260, Cost: 0.011034324835319678\n",
      "Iteration 261, Cost: 0.010983089702966003\n",
      "Iteration 262, Cost: 0.010932303994212947\n",
      "Iteration 263, Cost: 0.01088196003332422\n",
      "Iteration 264, Cost: 0.010832050296619343\n",
      "Iteration 265, Cost: 0.010782567408160826\n",
      "Iteration 266, Cost: 0.010733504135551445\n",
      "Iteration 267, Cost: 0.01068485338584025\n",
      "Iteration 268, Cost: 0.010636608201535717\n",
      "Iteration 269, Cost: 0.010588761756724447\n",
      "Iteration 270, Cost: 0.010541307353293674\n",
      "Iteration 271, Cost: 0.010494238417255754\n",
      "Iteration 272, Cost: 0.010447548495172872\n",
      "Iteration 273, Cost: 0.010401231250679975\n",
      "Iteration 274, Cost: 0.010355280461104148\n",
      "Iteration 275, Cost: 0.01030969001417842\n",
      "Iteration 276, Cost: 0.010264453904848182\n",
      "Iteration 277, Cost: 0.01021956623216825\n",
      "Iteration 278, Cost: 0.010175021196288733\n",
      "Iteration 279, Cost: 0.010130813095527874\n",
      "Iteration 280, Cost: 0.010086936323530008\n",
      "Iteration 281, Cost: 0.010043385366506887\n",
      "Iteration 282, Cost: 0.010000154800560665\n",
      "Iteration 283, Cost: 0.009957239289086832\n",
      "Iteration 284, Cost: 0.009914633580255483\n",
      "Iteration 285, Cost: 0.009872332504569406\n",
      "Iteration 286, Cost: 0.009830330972497378\n",
      "Iteration 287, Cost: 0.009788623972181372\n",
      "Iteration 288, Cost: 0.009747206567216164\n",
      "Iteration 289, Cost: 0.009706073894500141\n",
      "Iteration 290, Cost: 0.009665221162156014\n",
      "Iteration 291, Cost: 0.009624643647520298\n",
      "Iteration 292, Cost: 0.009584336695200459\n",
      "Iteration 293, Cost: 0.009544295715198714\n",
      "Iteration 294, Cost: 0.009504516181101565\n",
      "Iteration 295, Cost: 0.009464993628334159\n",
      "Iteration 296, Cost: 0.009425723652478736\n",
      "Iteration 297, Cost: 0.009386701907656425\n",
      "Iteration 298, Cost: 0.009347924104971771\n",
      "Iteration 299, Cost: 0.009309386011019454\n",
      "Iteration 300, Cost: 0.00927108344645267\n",
      "Iteration 301, Cost: 0.009233012284612905\n",
      "Iteration 302, Cost: 0.00919516845022066\n",
      "Iteration 303, Cost: 0.009157547918126948\n",
      "Iteration 304, Cost: 0.009120146712125465\n",
      "Iteration 305, Cost: 0.009082960903825276\n",
      "Iteration 306, Cost: 0.009045986611584088\n",
      "Iteration 307, Cost: 0.00900921999950215\n",
      "Iteration 308, Cost: 0.008972657276477006\n",
      "Iteration 309, Cost: 0.008936294695319298\n",
      "Iteration 310, Cost: 0.008900128551929927\n",
      "Iteration 311, Cost: 0.008864155184539042\n",
      "Iteration 312, Cost: 0.00882837097300722\n",
      "Iteration 313, Cost: 0.008792772338189458\n",
      "Iteration 314, Cost: 0.008757355741362528\n",
      "Iteration 315, Cost: 0.008722117683716418\n",
      "Iteration 316, Cost: 0.008687054705910524\n",
      "Iteration 317, Cost: 0.008652163387695406\n",
      "Iteration 318, Cost: 0.00861744034760101\n",
      "Iteration 319, Cost: 0.008582882242692103\n",
      "Iteration 320, Cost: 0.008548485768392004\n",
      "Iteration 321, Cost: 0.008514247658375444\n",
      "Iteration 322, Cost: 0.008480164684531619\n",
      "Iteration 323, Cost: 0.0084462336569984\n",
      "Iteration 324, Cost: 0.008412451424268736\n",
      "Iteration 325, Cost: 0.008378814873370215\n",
      "Iteration 326, Cost: 0.008345320930118857\n",
      "Iteration 327, Cost: 0.008311966559447976\n",
      "Iteration 328, Cost: 0.008278748765813197\n",
      "Iteration 329, Cost: 0.008245664593674335\n",
      "Iteration 330, Cost: 0.008212711128055024\n",
      "Iteration 331, Cost: 0.008179885495180799\n",
      "Iteration 332, Cost: 0.008147184863196126\n",
      "Iteration 333, Cost: 0.008114606442960992\n",
      "Iteration 334, Cost: 0.008082147488927157\n",
      "Iteration 335, Cost: 0.008049805300094396\n",
      "Iteration 336, Cost: 0.008017577221046469\n",
      "Iteration 337, Cost: 0.007985460643066645\n",
      "Iteration 338, Cost: 0.007953453005332104\n",
      "Iteration 339, Cost: 0.007921551796186426\n",
      "Iteration 340, Cost: 0.007889754554488952\n",
      "Iteration 341, Cost: 0.007858058871039485\n",
      "Iteration 342, Cost: 0.007826462390076455\n",
      "Iteration 343, Cost: 0.00779496281084617\n",
      "Iteration 344, Cost: 0.007763557889240421\n",
      "Iteration 345, Cost: 0.007732245439499153\n",
      "Iteration 346, Cost: 0.007701023335974476\n",
      "Iteration 347, Cost: 0.007669889514951632\n",
      "Iteration 348, Cost: 0.007638841976522076\n",
      "Iteration 349, Cost: 0.007607878786503188\n",
      "Iteration 350, Cost: 0.0075769980783984\n",
      "Iteration 351, Cost: 0.007546198055391049\n",
      "Iteration 352, Cost: 0.0075154769923644355\n",
      "Iteration 353, Cost: 0.007484833237939922\n",
      "Iteration 354, Cost: 0.007454265216524226\n",
      "Iteration 355, Cost: 0.007423771430356352\n",
      "Iteration 356, Cost: 0.007393350461543842\n",
      "Iteration 357, Cost: 0.007363000974077368\n",
      "Iteration 358, Cost: 0.007332721715812009\n",
      "Iteration 359, Cost: 0.0073025115204028955\n",
      "Iteration 360, Cost: 0.007272369309182217\n",
      "Iteration 361, Cost: 0.007242294092964124\n",
      "Iteration 362, Cost: 0.0072122849737634035\n",
      "Iteration 363, Cost: 0.007182341146413473\n",
      "Iteration 364, Cost: 0.007152461900068752\n",
      "Iteration 365, Cost: 0.007122646619576257\n",
      "Iteration 366, Cost: 0.007092894786701082\n",
      "Iteration 367, Cost: 0.007063205981190259\n",
      "Iteration 368, Cost: 0.007033579881659728\n",
      "Iteration 369, Cost: 0.007004016266289186\n",
      "Iteration 370, Cost: 0.006974515013309978\n",
      "Iteration 371, Cost: 0.006945076101271752\n",
      "Iteration 372, Cost: 0.006915699609074157\n",
      "Iteration 373, Cost: 0.006886385715750818\n",
      "Iteration 374, Cost: 0.0068571346999937305\n",
      "Iteration 375, Cost: 0.006827946939407442\n",
      "Iteration 376, Cost: 0.006798822909483771\n",
      "Iteration 377, Cost: 0.00676976318228928\n",
      "Iteration 378, Cost: 0.0067407684248594986\n",
      "Iteration 379, Cost: 0.006711839397295616\n",
      "Iteration 380, Cost: 0.006682976950561475\n",
      "Iteration 381, Cost: 0.006654182023980695\n",
      "Iteration 382, Cost: 0.006625455642436075\n",
      "Iteration 383, Cost: 0.006596798913275606\n",
      "Iteration 384, Cost: 0.00656821302293186\n",
      "Iteration 385, Cost: 0.006539699233263865\n",
      "Iteration 386, Cost: 0.006511258877632959\n",
      "Iteration 387, Cost: 0.0064828933567264765\n",
      "Iteration 388, Cost: 0.006454604134145407\n",
      "Iteration 389, Cost: 0.006426392731774285\n",
      "Iteration 390, Cost: 0.006398260724953792\n",
      "Iteration 391, Cost: 0.006370209737478242\n",
      "Iteration 392, Cost: 0.006342241436442009\n",
      "Iteration 393, Cost: 0.006314357526960342\n",
      "Iteration 394, Cost: 0.006286559746791316\n",
      "Iteration 395, Cost: 0.006258849860886603\n",
      "Iteration 396, Cost: 0.006231229655899493\n",
      "Iteration 397, Cost: 0.006203700934679043\n",
      "Iteration 398, Cost: 0.006176265510779183\n",
      "Iteration 399, Cost: 0.006148925203011629\n",
      "Iteration 400, Cost: 0.006121681830070826\n",
      "Iteration 401, Cost: 0.006094537205258297\n",
      "Iteration 402, Cost: 0.006067493131332932\n",
      "Iteration 403, Cost: 0.006040551395512111\n",
      "Iteration 404, Cost: 0.006013713764647264\n",
      "Iteration 405, Cost: 0.005986981980595447\n",
      "Iteration 406, Cost: 0.0059603577558066945\n",
      "Iteration 407, Cost: 0.005933842769144648\n",
      "Iteration 408, Cost: 0.005907438661955817\n",
      "Iteration 409, Cost: 0.005881147034400302\n",
      "Iteration 410, Cost: 0.005854969442054631\n",
      "Iteration 411, Cost: 0.005828907392794717\n",
      "Iteration 412, Cost: 0.005802962343964631\n",
      "Iteration 413, Cost: 0.005777135699834557\n",
      "Iteration 414, Cost: 0.005751428809348845\n",
      "Iteration 415, Cost: 0.005725842964163083\n",
      "Iteration 416, Cost: 0.005700379396966828\n",
      "Iteration 417, Cost: 0.005675039280086871\n",
      "Iteration 418, Cost: 0.0056498237243640574\n",
      "Iteration 419, Cost: 0.005624733778295091\n",
      "Iteration 420, Cost: 0.0055997704274293415\n",
      "Iteration 421, Cost: 0.005574934594009512\n",
      "Iteration 422, Cost: 0.00555022713684386\n",
      "Iteration 423, Cost: 0.005525648851396929\n",
      "Iteration 424, Cost: 0.005501200470085025\n",
      "Iteration 425, Cost: 0.005476882662762235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 426, Cost: 0.0054526960373824\n",
      "Iteration 427, Cost: 0.005428641140822408\n",
      "Iteration 428, Cost: 0.005404718459852022\n",
      "Iteration 429, Cost: 0.00538092842223575\n",
      "Iteration 430, Cost: 0.005357271397952382\n",
      "Iteration 431, Cost: 0.005333747700518243\n",
      "Iteration 432, Cost: 0.005310357588400612\n",
      "Iteration 433, Cost: 0.005287101266508412\n",
      "Iteration 434, Cost: 0.0052639788877476106\n",
      "Iteration 435, Cost: 0.0052409905546297285\n",
      "Iteration 436, Cost: 0.005218136320922309\n",
      "Iteration 437, Cost: 0.005195416193330943\n",
      "Iteration 438, Cost: 0.005172830133203351\n",
      "Iteration 439, Cost: 0.005150378058246441\n",
      "Iteration 440, Cost: 0.005128059844248302\n",
      "Iteration 441, Cost: 0.005105875326797645\n",
      "Iteration 442, Cost: 0.005083824302993909\n",
      "Iteration 443, Cost: 0.00506190653314208\n",
      "Iteration 444, Cost: 0.0050401217424267\n",
      "Iteration 445, Cost: 0.0050184696225604175\n",
      "Iteration 446, Cost: 0.004996949833402892\n",
      "Converged!\n",
      "Iteration 1, Cost: 0.2528034238755919\n",
      "Iteration 2, Cost: 0.2370540845597029\n",
      "Iteration 3, Cost: 0.22826959485194495\n",
      "Iteration 4, Cost: 0.22205669880974524\n",
      "Iteration 5, Cost: 0.21688038373333116\n",
      "Iteration 6, Cost: 0.21222958046938806\n",
      "Iteration 7, Cost: 0.20792275292844173\n",
      "Iteration 8, Cost: 0.2038849921654813\n",
      "Iteration 9, Cost: 0.20007595134123946\n",
      "Iteration 10, Cost: 0.1964666955978747\n",
      "Iteration 11, Cost: 0.1930327334706417\n",
      "Iteration 12, Cost: 0.18975221012425214\n",
      "Iteration 13, Cost: 0.186605554497859\n",
      "Iteration 14, Cost: 0.18357541814286946\n",
      "Iteration 15, Cost: 0.180646600981292\n",
      "Iteration 16, Cost: 0.17780591932000328\n",
      "Iteration 17, Cost: 0.1750420341407943\n",
      "Iteration 18, Cost: 0.1723452633067537\n",
      "Iteration 19, Cost: 0.1697073945069182\n",
      "Iteration 20, Cost: 0.1671215086202927\n",
      "Iteration 21, Cost: 0.16458181806697514\n",
      "Iteration 22, Cost: 0.1620835215183158\n",
      "Iteration 23, Cost: 0.159622674532912\n",
      "Iteration 24, Cost: 0.1571960747978592\n",
      "Iteration 25, Cost: 0.15480116033490665\n",
      "Iteration 26, Cost: 0.1524359190368835\n",
      "Iteration 27, Cost: 0.15009880807125245\n",
      "Iteration 28, Cost: 0.14778868192463374\n",
      "Iteration 29, Cost: 0.14550472810531864\n",
      "Iteration 30, Cost: 0.14324640973798236\n",
      "Iteration 31, Cost: 0.14101341446102256\n",
      "Iteration 32, Cost: 0.13880560916780865\n",
      "Iteration 33, Cost: 0.13662300022055368\n",
      "Iteration 34, Cost: 0.13446569881512882\n",
      "Iteration 35, Cost: 0.13233389119447786\n",
      "Iteration 36, Cost: 0.130227813405703\n",
      "Iteration 37, Cost: 0.12814773027962312\n",
      "Iteration 38, Cost: 0.1260939182891309\n",
      "Iteration 39, Cost: 0.12406665192021868\n",
      "Iteration 40, Cost: 0.12206619317179261\n",
      "Iteration 41, Cost: 0.12009278379043943\n",
      "Iteration 42, Cost: 0.11814663984572285\n",
      "Iteration 43, Cost: 0.11622794826069906\n",
      "Iteration 44, Cost: 0.1143368649304997\n",
      "Iteration 45, Cost: 0.11247351408776811\n",
      "Iteration 46, Cost: 0.11063798860578895\n",
      "Iteration 47, Cost: 0.10883035096653636\n",
      "Iteration 48, Cost: 0.10705063465978894\n",
      "Iteration 49, Cost: 0.10529884581925218\n",
      "Iteration 50, Cost: 0.1035749649407844\n",
      "Iteration 51, Cost: 0.101878948565048\n",
      "Iteration 52, Cost: 0.10021073084114329\n",
      "Iteration 53, Cost: 0.09857022491820461\n",
      "Iteration 54, Cost: 0.09695732413798683\n",
      "Iteration 55, Cost: 0.0953719030228321\n",
      "Iteration 56, Cost: 0.0938138180700196\n",
      "Iteration 57, Cost: 0.09228290837552537\n",
      "Iteration 58, Cost: 0.09077899611801626\n",
      "Iteration 59, Cost: 0.08930188693797232\n",
      "Iteration 60, Cost: 0.08785137024779281\n",
      "Iteration 61, Cost: 0.08642721950724666\n",
      "Iteration 62, Cost: 0.08502919249533972\n",
      "Iteration 63, Cost: 0.08365703160521083\n",
      "Iteration 64, Cost: 0.08231046418357649\n",
      "Iteration 65, Cost: 0.08098920293097933\n",
      "Iteration 66, Cost: 0.07969294637400114\n",
      "Iteration 67, Cost: 0.0784213794159305\n",
      "Iteration 68, Cost: 0.07717417396827887\n",
      "Iteration 69, Cost: 0.07595098966209551\n",
      "Iteration 70, Cost: 0.07475147463525106\n",
      "Iteration 71, Cost: 0.0735752663897112\n",
      "Iteration 72, Cost: 0.07242199271124017\n",
      "Iteration 73, Cost: 0.0712912726428809\n",
      "Iteration 74, Cost: 0.07018271750286749\n",
      "Iteration 75, Cost: 0.06909593193725345\n",
      "Iteration 76, Cost: 0.06803051499741047\n",
      "Iteration 77, Cost: 0.06698606123259929\n",
      "Iteration 78, Cost: 0.06596216178798493\n",
      "Iteration 79, Cost: 0.06495840549871859\n",
      "Iteration 80, Cost: 0.06397437997100751\n",
      "Iteration 81, Cost: 0.06300967264142025\n",
      "Iteration 82, Cost: 0.062063871806015074\n",
      "Iteration 83, Cost: 0.06113656761122968\n",
      "Iteration 84, Cost: 0.060227352998832744\n",
      "Iteration 85, Cost: 0.059335824597619526\n",
      "Iteration 86, Cost: 0.05846158355494905\n",
      "Iteration 87, Cost: 0.05760423630168665\n",
      "Iteration 88, Cost: 0.05676339524465372\n",
      "Iteration 89, Cost: 0.055938679381319516\n",
      "Iteration 90, Cost: 0.055129714832221316\n",
      "Iteration 91, Cost: 0.054336135287490574\n",
      "Iteration 92, Cost: 0.053557582364909304\n",
      "Iteration 93, Cost: 0.05279370587813433\n",
      "Iteration 94, Cost: 0.0520441640151028\n",
      "Iteration 95, Cost: 0.0513086234281582\n",
      "Iteration 96, Cost: 0.050586759239080334\n",
      "Iteration 97, Cost: 0.04987825496391919\n",
      "Iteration 98, Cost: 0.049182802364259204\n",
      "Iteration 99, Cost: 0.04850010123320106\n",
      "Iteration 100, Cost: 0.04782985912585775\n",
      "Iteration 101, Cost: 0.04717179104543187\n",
      "Iteration 102, Cost: 0.04652561909688823\n",
      "Iteration 103, Cost: 0.04589107212078702\n",
      "Iteration 104, Cost: 0.04526788531994619\n",
      "Iteration 105, Cost: 0.04465579989123138\n",
      "Iteration 106, Cost: 0.04405456267393044\n",
      "Iteration 107, Cost: 0.04346392582489573\n",
      "Iteration 108, Cost: 0.04288364652899436\n",
      "Iteration 109, Cost: 0.04231348675148587\n",
      "Iteration 110, Cost: 0.041753213036857445\n",
      "Iteration 111, Cost: 0.041202596356502745\n",
      "Iteration 112, Cost: 0.040661412005548424\n",
      "Iteration 113, Cost: 0.040129439547214485\n",
      "Iteration 114, Cost: 0.03960646280142845\n",
      "Iteration 115, Cost: 0.039092269873060234\n",
      "Iteration 116, Cost: 0.038586653214142994\n",
      "Iteration 117, Cost: 0.038089409713806095\n",
      "Iteration 118, Cost: 0.037600340809359124\n",
      "Iteration 119, Cost: 0.037119252612001025\n",
      "Iteration 120, Cost: 0.03664595604094054\n",
      "Iteration 121, Cost: 0.03618026696025263\n",
      "Iteration 122, Cost: 0.0357220063135028\n",
      "Iteration 123, Cost: 0.035271000251991515\n",
      "Iteration 124, Cost: 0.034827080253352016\n",
      "Iteration 125, Cost: 0.03439008322812979\n",
      "Iteration 126, Cost: 0.03395985161284\n",
      "Iteration 127, Cost: 0.03353623344880732\n",
      "Iteration 128, Cost: 0.03311908244681248\n",
      "Iteration 129, Cost: 0.03270825803818027\n",
      "Iteration 130, Cost: 0.03230362541342567\n",
      "Iteration 131, Cost: 0.031905055549914026\n",
      "Iteration 132, Cost: 0.03151242523017493\n",
      "Iteration 133, Cost: 0.0311256170525269\n",
      "Iteration 134, Cost: 0.030744519435512253\n",
      "Iteration 135, Cost: 0.030369026617303067\n",
      "Iteration 136, Cost: 0.029999038650716618\n",
      "Iteration 137, Cost: 0.029634461393776457\n",
      "Iteration 138, Cost: 0.029275206494885384\n",
      "Iteration 139, Cost: 0.028921191370661365\n",
      "Iteration 140, Cost: 0.028572339173365387\n",
      "Iteration 141, Cost: 0.02822857874367495\n",
      "Iteration 142, Cost: 0.027889844543402393\n",
      "Iteration 143, Cost: 0.027556076561714472\n",
      "Iteration 144, Cost: 0.027227220187588736\n",
      "Iteration 145, Cost: 0.026903226040762448\n",
      "Iteration 146, Cost: 0.02658404975341459\n",
      "Iteration 147, Cost: 0.0262696516953839\n",
      "Iteration 148, Cost: 0.025959996636952667\n",
      "Iteration 149, Cost: 0.02565505334516297\n",
      "Iteration 150, Cost: 0.02535479411226272\n",
      "Iteration 151, Cost: 0.025059194218116795\n",
      "Iteration 152, Cost: 0.024768231332096248\n",
      "Iteration 153, Cost: 0.024481884863834492\n",
      "Iteration 154, Cost: 0.024200135276011836\n",
      "Iteration 155, Cost: 0.023922963375664914\n",
      "Iteration 156, Cost: 0.023650349603086405\n",
      "Iteration 157, Cost: 0.023382273338899664\n",
      "Iteration 158, Cost: 0.023118712250165454\n",
      "Iteration 159, Cost: 0.02285964169532299\n",
      "Iteration 160, Cost: 0.02260503420543657\n",
      "Iteration 161, Cost: 0.022354859055792087\n",
      "Iteration 162, Cost: 0.022109081937653634\n",
      "Iteration 163, Cost: 0.02186766473530679\n",
      "Iteration 164, Cost: 0.021630565408767895\n",
      "Iteration 165, Cost: 0.021397737978089727\n",
      "Iteration 166, Cost: 0.02116913260134886\n",
      "Iteration 167, Cost: 0.020944695735373373\n",
      "Iteration 168, Cost: 0.020724370366178788\n",
      "Iteration 169, Cost: 0.020508096294943448\n",
      "Iteration 170, Cost: 0.020295810465106333\n",
      "Iteration 171, Cost: 0.020087447316683327\n",
      "Iteration 172, Cost: 0.01988293915500688\n",
      "Iteration 173, Cost: 0.019682216522619183\n",
      "Iteration 174, Cost: 0.019485208564817124\n",
      "Iteration 175, Cost: 0.01929184338120522\n",
      "Iteration 176, Cost: 0.019102048357435307\n",
      "Iteration 177, Cost: 0.01891575047300705\n",
      "Iteration 178, Cost: 0.018732876582509557\n",
      "Iteration 179, Cost: 0.018553353668968868\n",
      "Iteration 180, Cost: 0.018377109069017173\n",
      "Iteration 181, Cost: 0.018204070670424666\n",
      "Iteration 182, Cost: 0.01803416708315124\n",
      "Iteration 183, Cost: 0.017867327785508213\n",
      "Iteration 184, Cost: 0.017703483247297817\n",
      "Iteration 185, Cost: 0.017542565031949352\n",
      "Iteration 186, Cost: 0.01738450587972229\n",
      "Iteration 187, Cost: 0.01722923977402262\n",
      "Iteration 188, Cost: 0.017076701992800616\n",
      "Iteration 189, Cost: 0.016926829146882864\n",
      "Iteration 190, Cost: 0.016779559206953443\n",
      "Iteration 191, Cost: 0.016634831520749134\n",
      "Iteration 192, Cost: 0.01649258682187984\n",
      "Iteration 193, Cost: 0.016352767231534403\n",
      "Iteration 194, Cost: 0.01621531625418689\n",
      "Iteration 195, Cost: 0.01608017876828339\n",
      "Iteration 196, Cost: 0.015947301012764228\n",
      "Iteration 197, Cost: 0.015816630570163637\n",
      "Iteration 198, Cost: 0.015688116346926993\n",
      "Iteration 199, Cost: 0.015561708551495586\n",
      "Iteration 200, Cost: 0.015437358670629111\n",
      "Iteration 201, Cost: 0.015315019444366241\n",
      "Iteration 202, Cost: 0.015194644839962863\n",
      "Iteration 203, Cost: 0.015076190025094839\n",
      "Iteration 204, Cost: 0.014959611340566802\n",
      "Iteration 205, Cost: 0.014844866272729315\n",
      "Iteration 206, Cost: 0.014731913425773395\n",
      "Iteration 207, Cost: 0.014620712494042802\n",
      "Iteration 208, Cost: 0.01451122423448015\n",
      "Iteration 209, Cost: 0.014403410439302322\n",
      "Iteration 210, Cost: 0.014297233908983142\n",
      "Iteration 211, Cost: 0.01419265842560637\n",
      "Iteration 212, Cost: 0.0140896487266397\n",
      "Iteration 213, Cost: 0.013988170479169853\n",
      "Iteration 214, Cost: 0.013888190254629928\n",
      "Iteration 215, Cost: 0.01378967550404284\n",
      "Iteration 216, Cost: 0.013692594533798286\n",
      "Iteration 217, Cost: 0.013596916481975588\n",
      "Iteration 218, Cost: 0.013502611295220264\n",
      "Iteration 219, Cost: 0.013409649706178601\n",
      "Iteration 220, Cost: 0.013318003211491448\n",
      "Iteration 221, Cost: 0.013227644050345955\n",
      "Iteration 222, Cost: 0.013138545183581825\n",
      "Iteration 223, Cost: 0.013050680273347081\n",
      "Iteration 224, Cost: 0.012964023663296733\n",
      "Iteration 225, Cost: 0.012878550359326847\n",
      "Iteration 226, Cost: 0.012794236010835347\n",
      "Iteration 227, Cost: 0.012711056892500358\n",
      "Iteration 228, Cost: 0.012628989886566149\n",
      "Iteration 229, Cost: 0.012548012465626394\n",
      "Iteration 230, Cost: 0.012468102675894125\n",
      "Iteration 231, Cost: 0.012389239120947467\n",
      "Iteration 232, Cost: 0.012311400945940078\n",
      "Iteration 233, Cost: 0.01223456782226523\n",
      "Iteration 234, Cost: 0.012158719932662269\n",
      "Iteration 235, Cost: 0.012083837956754303\n",
      "Iteration 236, Cost: 0.012009903057006029\n",
      "Iteration 237, Cost: 0.011936896865090606\n",
      "Iteration 238, Cost: 0.011864801468654648\n",
      "Iteration 239, Cost: 0.01179359939847056\n",
      "Iteration 240, Cost: 0.011723273615965557\n",
      "Iteration 241, Cost: 0.011653807501116834\n",
      "Iteration 242, Cost: 0.011585184840702674\n",
      "Iteration 243, Cost: 0.011517389816899305\n",
      "Iteration 244, Cost: 0.011450406996213625\n",
      "Iteration 245, Cost: 0.011384221318742112\n",
      "Iteration 246, Cost: 0.011318818087746367\n",
      "Iteration 247, Cost: 0.01125418295953609\n",
      "Iteration 248, Cost: 0.011190301933650344\n",
      "Iteration 249, Cost: 0.01112716134332836\n",
      "Iteration 250, Cost: 0.011064747846261155\n",
      "Iteration 251, Cost: 0.01100304841561567\n",
      "Iteration 252, Cost: 0.010942050331323174\n",
      "Iteration 253, Cost: 0.010881741171623997\n",
      "Iteration 254, Cost: 0.01082210880486083\n",
      "Iteration 255, Cost: 0.010763141381513094\n",
      "Iteration 256, Cost: 0.01070482732646501\n",
      "Iteration 257, Cost: 0.010647155331500275\n",
      "Iteration 258, Cost: 0.010590114348016455\n",
      "Iteration 259, Cost: 0.010533693579952352\n",
      "Iteration 260, Cost: 0.010477882476921855\n",
      "Iteration 261, Cost: 0.010422670727547967\n",
      "Iteration 262, Cost: 0.010368048252990848\n",
      "Iteration 263, Cost: 0.010314005200663997\n",
      "Iteration 264, Cost: 0.010260531938132738\n",
      "Iteration 265, Cost: 0.0102076190471895\n",
      "Iteration 266, Cost: 0.010155257318100443\n",
      "Iteration 267, Cost: 0.01010343774401821\n",
      "Iteration 268, Cost: 0.010052151515555741\n",
      "Iteration 269, Cost: 0.010001390015516214\n",
      "Iteration 270, Cost: 0.00995114481377437\n",
      "Iteration 271, Cost: 0.009901407662304665\n",
      "Iteration 272, Cost: 0.009852170490351694\n",
      "Iteration 273, Cost: 0.009803425399738679\n",
      "Iteration 274, Cost: 0.009755164660309783\n",
      "Iteration 275, Cost: 0.009707380705502266\n",
      "Iteration 276, Cost: 0.009660066128044511\n",
      "Iteration 277, Cost: 0.009613213675776196\n",
      "Iteration 278, Cost: 0.009566816247586958\n",
      "Iteration 279, Cost: 0.009520866889469974\n",
      "Iteration 280, Cost: 0.009475358790687059\n",
      "Iteration 281, Cost: 0.009430285280041983\n",
      "Iteration 282, Cost: 0.009385639822258787\n",
      "Iteration 283, Cost: 0.009341416014462013\n",
      "Iteration 284, Cost: 0.009297607582755834\n",
      "Iteration 285, Cost: 0.00925420837889922\n",
      "Iteration 286, Cost: 0.009211212377074257\n",
      "Iteration 287, Cost: 0.009168613670745037\n",
      "Iteration 288, Cost: 0.009126406469604319\n",
      "Iteration 289, Cost: 0.00908458509660558\n",
      "Iteration 290, Cost: 0.009043143985077877\n",
      "Iteration 291, Cost: 0.009002077675921174\n",
      "Iteration 292, Cost: 0.008961380814879873\n",
      "Iteration 293, Cost: 0.008921048149892212\n",
      "Iteration 294, Cost: 0.008881074528513451\n",
      "Iteration 295, Cost: 0.0088414548954107\n",
      "Iteration 296, Cost: 0.008802184289927373\n",
      "Iteration 297, Cost: 0.008763257843715271\n",
      "Iteration 298, Cost: 0.008724670778432435\n",
      "Iteration 299, Cost: 0.008686418403504844\n",
      "Iteration 300, Cost: 0.008648496113950216\n",
      "Iteration 301, Cost: 0.00861089938826214\n",
      "Iteration 302, Cost: 0.00857362378635285\n",
      "Iteration 303, Cost: 0.008536664947553\n",
      "Iteration 304, Cost: 0.008500018588666855\n",
      "Iteration 305, Cost: 0.00846368050208131\n",
      "Iteration 306, Cost: 0.008427646553927312\n",
      "Iteration 307, Cost: 0.008391912682292115\n",
      "Iteration 308, Cost: 0.008356474895481044\n",
      "Iteration 309, Cost: 0.008321329270327329\n",
      "Iteration 310, Cost: 0.008286471950548711\n",
      "Iteration 311, Cost: 0.008251899145149467\n",
      "Iteration 312, Cost: 0.008217607126866635\n",
      "Iteration 313, Cost: 0.008183592230659157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 314, Cost: 0.008149850852238788\n",
      "Iteration 315, Cost: 0.008116379446641588\n",
      "Iteration 316, Cost: 0.00808317452683881\n",
      "Iteration 317, Cost: 0.008050232662386183\n",
      "Iteration 318, Cost: 0.008017550478110414\n",
      "Iteration 319, Cost: 0.007985124652831913\n",
      "Iteration 320, Cost: 0.007952951918122722\n",
      "Iteration 321, Cost: 0.00792102905709863\n",
      "Iteration 322, Cost: 0.007889352903244534\n",
      "Iteration 323, Cost: 0.007857920339272097\n",
      "Iteration 324, Cost: 0.007826728296008775\n",
      "Iteration 325, Cost: 0.007795773751317339\n",
      "Iteration 326, Cost: 0.007765053729045016\n",
      "Iteration 327, Cost: 0.0077345652980014045\n",
      "Iteration 328, Cost: 0.007704305570964351\n",
      "Iteration 329, Cost: 0.007674271703712954\n",
      "Iteration 330, Cost: 0.007644460894086963\n",
      "Iteration 331, Cost: 0.0076148703810717915\n",
      "Iteration 332, Cost: 0.0075854974439083815\n",
      "Iteration 333, Cost: 0.007556339401227251\n",
      "Iteration 334, Cost: 0.007527393610205977\n",
      "Iteration 335, Cost: 0.0074986574657494726\n",
      "Iteration 336, Cost: 0.007470128399692372\n",
      "Iteration 337, Cost: 0.007441803880022872\n",
      "Iteration 338, Cost: 0.007413681410127464\n",
      "Iteration 339, Cost: 0.007385758528055851\n",
      "Iteration 340, Cost: 0.007358032805805554\n",
      "Iteration 341, Cost: 0.007330501848625578\n",
      "Iteration 342, Cost: 0.007303163294338588\n",
      "Iteration 343, Cost: 0.007276014812681073\n",
      "Iteration 344, Cost: 0.00724905410466096\n",
      "Iteration 345, Cost: 0.007222278901932155\n",
      "Iteration 346, Cost: 0.007195686966185556\n",
      "Iteration 347, Cost: 0.007169276088556006\n",
      "Iteration 348, Cost: 0.007143044089044755\n",
      "Iteration 349, Cost: 0.0071169888159569915\n",
      "Iteration 350, Cost: 0.007091108145353965\n",
      "Iteration 351, Cost: 0.007065399980519314\n",
      "Iteration 352, Cost: 0.007039862251439171\n",
      "Iteration 353, Cost: 0.00701449291429566\n",
      "Iteration 354, Cost: 0.006989289950973393\n",
      "Iteration 355, Cost: 0.006964251368578608\n",
      "Iteration 356, Cost: 0.006939375198970573\n",
      "Iteration 357, Cost: 0.006914659498304935\n",
      "Iteration 358, Cost: 0.006890102346588657\n",
      "Iteration 359, Cost: 0.006865701847246236\n",
      "Iteration 360, Cost: 0.0068414561266968875\n",
      "Iteration 361, Cost: 0.006817363333942393\n",
      "Iteration 362, Cost: 0.006793421640165339\n",
      "Iteration 363, Cost: 0.006769629238337443\n",
      "Iteration 364, Cost: 0.006745984342837726\n",
      "Iteration 365, Cost: 0.006722485189080264\n",
      "Iteration 366, Cost: 0.006699130033151262\n",
      "Iteration 367, Cost: 0.006675917151455246\n",
      "Iteration 368, Cost: 0.00665284484037009\n",
      "Iteration 369, Cost: 0.006629911415910734\n",
      "Iteration 370, Cost: 0.006607115213401311\n",
      "Iteration 371, Cost: 0.0065844545871555395\n",
      "Iteration 372, Cost: 0.006561927910165156\n",
      "Iteration 373, Cost: 0.006539533573796208\n",
      "Iteration 374, Cost: 0.006517269987493067\n",
      "Iteration 375, Cost: 0.006495135578489943\n",
      "Iteration 376, Cost: 0.006473128791529775\n",
      "Iteration 377, Cost: 0.006451248088590337\n",
      "Iteration 378, Cost: 0.0064294919486174275\n",
      "Iteration 379, Cost: 0.006407858867264965\n",
      "Iteration 380, Cost: 0.006386347356641929\n",
      "Iteration 381, Cost: 0.006364955945065938\n",
      "Iteration 382, Cost: 0.006343683176823422\n",
      "Iteration 383, Cost: 0.006322527611936231\n",
      "Iteration 384, Cost: 0.006301487825934604\n",
      "Iteration 385, Cost: 0.00628056240963637\n",
      "Iteration 386, Cost: 0.006259749968932314\n",
      "Iteration 387, Cost: 0.006239049124577611\n",
      "Iteration 388, Cost: 0.006218458511989236\n",
      "Iteration 389, Cost: 0.0061979767810492765\n",
      "Iteration 390, Cost: 0.0061776025959141\n",
      "Iteration 391, Cost: 0.006157334634829244\n",
      "Iteration 392, Cost: 0.006137171589950048\n",
      "Iteration 393, Cost: 0.0061171121671679\n",
      "Iteration 394, Cost: 0.006097155085942065\n",
      "Iteration 395, Cost: 0.0060772990791370584\n",
      "Iteration 396, Cost: 0.006057542892865491\n",
      "Iteration 397, Cost: 0.006037885286336359\n",
      "Iteration 398, Cost: 0.0060183250317087066\n",
      "Iteration 399, Cost: 0.005998860913950678\n",
      "Iteration 400, Cost: 0.00597949173070386\n",
      "Iteration 401, Cost: 0.005960216292152922\n",
      "Iteration 402, Cost: 0.0059410334209004986\n",
      "Iteration 403, Cost: 0.005921941951847312\n",
      "Iteration 404, Cost: 0.00590294073207747\n",
      "Iteration 405, Cost: 0.005884028620748943\n",
      "Iteration 406, Cost: 0.005865204488989191\n",
      "Iteration 407, Cost: 0.00584646721979589\n",
      "Iteration 408, Cost: 0.005827815707942785\n",
      "Iteration 409, Cost: 0.0058092488598905844\n",
      "Iteration 410, Cost: 0.00579076559370294\n",
      "Iteration 411, Cost: 0.00577236483896744\n",
      "Iteration 412, Cost: 0.00575404553672161\n",
      "Iteration 413, Cost: 0.00573580663938392\n",
      "Iteration 414, Cost: 0.005717647110689744\n",
      "Iteration 415, Cost: 0.005699565925632256\n",
      "Iteration 416, Cost: 0.0056815620704082645\n",
      "Iteration 417, Cost: 0.00566363454236892\n",
      "Iteration 418, Cost: 0.005645782349975298\n",
      "Iteration 419, Cost: 0.005628004512758803\n",
      "Iteration 420, Cost: 0.0056103000612864\n",
      "Iteration 421, Cost: 0.00559266803713059\n",
      "Iteration 422, Cost: 0.00557510749284412\n",
      "Iteration 423, Cost: 0.005557617491939397\n",
      "Iteration 424, Cost: 0.005540197108872539\n",
      "Iteration 425, Cost: 0.005522845429032014\n",
      "Iteration 426, Cost: 0.005505561548731848\n",
      "Iteration 427, Cost: 0.005488344575209305\n",
      "Iteration 428, Cost: 0.005471193626626997\n",
      "Iteration 429, Cost: 0.005454107832079368\n",
      "Iteration 430, Cost: 0.005437086331603461\n",
      "Iteration 431, Cost: 0.005420128276193885\n",
      "Iteration 432, Cost: 0.005403232827821948\n",
      "Iteration 433, Cost: 0.005386399159458793\n",
      "Iteration 434, Cost: 0.005369626455102502\n",
      "Iteration 435, Cost: 0.005352913909809035\n",
      "Iteration 436, Cost: 0.005336260729726898\n",
      "Iteration 437, Cost: 0.005319666132135429\n",
      "Iteration 438, Cost: 0.005303129345486574\n",
      "Iteration 439, Cost: 0.005286649609450015\n",
      "Iteration 440, Cost: 0.00527022617496151\n",
      "Iteration 441, Cost: 0.0052538583042743\n",
      "Iteration 442, Cost: 0.005237545271013424\n",
      "Iteration 443, Cost: 0.005221286360232767\n",
      "Iteration 444, Cost: 0.005205080868474664\n",
      "Iteration 445, Cost: 0.005188928103831889\n",
      "Iteration 446, Cost: 0.005172827386011822\n",
      "Iteration 447, Cost: 0.005156778046402581\n",
      "Iteration 448, Cost: 0.005140779428140925\n",
      "Iteration 449, Cost: 0.005124830886181695\n",
      "Iteration 450, Cost: 0.005108931787368544\n",
      "Iteration 451, Cost: 0.005093081510505756\n",
      "Iteration 452, Cost: 0.005077279446430837\n",
      "Iteration 453, Cost: 0.005061524998087692\n",
      "Iteration 454, Cost: 0.005045817580600043\n",
      "Iteration 455, Cost: 0.005030156621344877\n",
      "Iteration 456, Cost: 0.005014541560025569\n",
      "Iteration 457, Cost: 0.004998971848744449\n",
      "Converged!\n",
      "Iteration 1, Cost: 0.3336119939851233\n",
      "Iteration 2, Cost: 0.2961180666493818\n",
      "Iteration 3, Cost: 0.2628628027648418\n",
      "Iteration 4, Cost: 0.24469281728460437\n",
      "Iteration 5, Cost: 0.23504615286458952\n",
      "Iteration 6, Cost: 0.22821364331891938\n",
      "Iteration 7, Cost: 0.22248323146954033\n",
      "Iteration 8, Cost: 0.21741022763428888\n",
      "Iteration 9, Cost: 0.2128666178995732\n",
      "Iteration 10, Cost: 0.20878806006154654\n",
      "Iteration 11, Cost: 0.20511422436237817\n",
      "Iteration 12, Cost: 0.20178258956130488\n",
      "Iteration 13, Cost: 0.19873284759787793\n",
      "Iteration 14, Cost: 0.19591038899564495\n",
      "Iteration 15, Cost: 0.19326745601049014\n",
      "Iteration 16, Cost: 0.19076281828654035\n",
      "Iteration 17, Cost: 0.1883608028567652\n",
      "Iteration 18, Cost: 0.18603014582430552\n",
      "Iteration 19, Cost: 0.1837428768120643\n",
      "Iteration 20, Cost: 0.18147331799001976\n",
      "Iteration 21, Cost: 0.17919722900259655\n",
      "Iteration 22, Cost: 0.17689112275863073\n",
      "Iteration 23, Cost: 0.17453179853198228\n",
      "Iteration 24, Cost: 0.17209618314200192\n",
      "Iteration 25, Cost: 0.16956163722578588\n",
      "Iteration 26, Cost: 0.16690696618328169\n",
      "Iteration 27, Cost: 0.16411444886378015\n",
      "Iteration 28, Cost: 0.16117319066381083\n",
      "Iteration 29, Cost: 0.1580838754073486\n",
      "Iteration 30, Cost: 0.15486431839757095\n",
      "Iteration 31, Cost: 0.15155398253210592\n",
      "Iteration 32, Cost: 0.14821418786852192\n",
      "Iteration 33, Cost: 0.14492050263394668\n",
      "Iteration 34, Cost: 0.14174668068914265\n",
      "Iteration 35, Cost: 0.13874545016954323\n",
      "Iteration 36, Cost: 0.1359358803903639\n",
      "Iteration 37, Cost: 0.13330416672678883\n",
      "Iteration 38, Cost: 0.13081573390212756\n",
      "Iteration 39, Cost: 0.1284301717731972\n",
      "Iteration 40, Cost: 0.12611180638655928\n",
      "Iteration 41, Cost: 0.12383402661106532\n",
      "Iteration 42, Cost: 0.12157922503195057\n",
      "Iteration 43, Cost: 0.11933688406361395\n",
      "Iteration 44, Cost: 0.11710144307843288\n",
      "Iteration 45, Cost: 0.11487059993857267\n",
      "Iteration 46, Cost: 0.11264413778748418\n",
      "Iteration 47, Cost: 0.11042316266332637\n",
      "Iteration 48, Cost: 0.10820961223176777\n",
      "Iteration 49, Cost: 0.10600593014967938\n",
      "Iteration 50, Cost: 0.10381484021552077\n",
      "Iteration 51, Cost: 0.1016391833342135\n",
      "Iteration 52, Cost: 0.09948179757578984\n",
      "Iteration 53, Cost: 0.09734543060259103\n",
      "Iteration 54, Cost: 0.09523267789466557\n",
      "Iteration 55, Cost: 0.09314594192307452\n",
      "Iteration 56, Cost: 0.09108740812510319\n",
      "Iteration 57, Cost: 0.0890590339418978\n",
      "Iteration 58, Cost: 0.08706254758141721\n",
      "Iteration 59, Cost: 0.08509945363921209\n",
      "Iteration 60, Cost: 0.08317104322320132\n",
      "Iteration 61, Cost: 0.08127840673869136\n",
      "Iteration 62, Cost: 0.07942244795619703\n",
      "Iteration 63, Cost: 0.07760389838388597\n",
      "Iteration 64, Cost: 0.07582333129101262\n",
      "Iteration 65, Cost: 0.07408117498146237\n",
      "Iteration 66, Cost: 0.07237772510603253\n",
      "Iteration 67, Cost: 0.07071315593879521\n",
      "Iteration 68, Cost: 0.06908753063702913\n",
      "Iteration 69, Cost: 0.0675008105647456\n",
      "Iteration 70, Cost: 0.0659528637942434\n",
      "Iteration 71, Cost: 0.06444347291454276\n",
      "Iteration 72, Cost: 0.06297234227503416\n",
      "Iteration 73, Cost: 0.06153910478150745\n",
      "Iteration 74, Cost: 0.06014332834358349\n",
      "Iteration 75, Cost: 0.05878452205062734\n",
      "Iteration 76, Cost: 0.05746214213017596\n",
      "Iteration 77, Cost: 0.0561755977209399\n",
      "Iteration 78, Cost: 0.054924256473155665\n",
      "Iteration 79, Cost: 0.05370744997352185\n",
      "Iteration 80, Cost: 0.05252447898064004\n",
      "Iteration 81, Cost: 0.05137461844981866\n",
      "Iteration 82, Cost: 0.05025712232291803\n",
      "Iteration 83, Cost: 0.04917122805899931\n",
      "Iteration 84, Cost: 0.04811616088413415\n",
      "Iteration 85, Cost: 0.04709113774305215\n",
      "Iteration 86, Cost: 0.04609537094062195\n",
      "Iteration 87, Cost: 0.04512807146685587\n",
      "Iteration 88, Cost: 0.044188452004709514\n",
      "Iteration 89, Cost: 0.04327572962506722\n",
      "Iteration 90, Cost: 0.04238912817773917\n",
      "Iteration 91, Cost: 0.041527880390932734\n",
      "Iteration 92, Cost: 0.04069122969447048\n",
      "Iteration 93, Cost: 0.03987843178404132\n",
      "Iteration 94, Cost: 0.03908875594506197\n",
      "Iteration 95, Cost: 0.038321486155386456\n",
      "Iteration 96, Cost: 0.03757592198623685\n",
      "Iteration 97, Cost: 0.03685137932043994\n",
      "Iteration 98, Cost: 0.03614719090644052\n",
      "Iteration 99, Cost: 0.03546270676570728\n",
      "Iteration 100, Cost: 0.03479729447012734\n",
      "Iteration 101, Cost: 0.03415033930486149\n",
      "Iteration 102, Cost: 0.03352124433095473\n",
      "Iteration 103, Cost: 0.03290943036080548\n",
      "Iteration 104, Cost: 0.03231433585842196\n",
      "Iteration 105, Cost: 0.03173541677525717\n",
      "Iteration 106, Cost: 0.03117214633133146\n",
      "Iteration 107, Cost: 0.03062401475033318\n",
      "Iteration 108, Cost: 0.030090528956439728\n",
      "Iteration 109, Cost: 0.029571212239725912\n",
      "Iteration 110, Cost: 0.029065603896225034\n",
      "Iteration 111, Cost: 0.028573258847977734\n",
      "Iteration 112, Cost: 0.0280937472477429\n",
      "Iteration 113, Cost: 0.027626654072449207\n",
      "Iteration 114, Cost: 0.02717157870893152\n",
      "Iteration 115, Cost: 0.026728134535019155\n",
      "Iteration 116, Cost: 0.026295948498617452\n",
      "Iteration 117, Cost: 0.025874660697046865\n",
      "Iteration 118, Cost: 0.025463923958569713\n",
      "Iteration 119, Cost: 0.025063403427739998\n",
      "Iteration 120, Cost: 0.024672776155952193\n",
      "Iteration 121, Cost: 0.024291730698337115\n",
      "Iteration 122, Cost: 0.023919966717953593\n",
      "Iteration 123, Cost: 0.023557194598050165\n",
      "Iteration 124, Cost: 0.023203135063019242\n",
      "Iteration 125, Cost: 0.022857518808533837\n",
      "Iteration 126, Cost: 0.02252008614124234\n",
      "Iteration 127, Cost: 0.02219058662829753\n",
      "Iteration 128, Cost: 0.02186877875691013\n",
      "Iteration 129, Cost: 0.021554429604043406\n",
      "Iteration 130, Cost: 0.02124731451630183\n",
      "Iteration 131, Cost: 0.02094721680001244\n",
      "Iteration 132, Cost: 0.02065392742145106\n",
      "Iteration 133, Cost: 0.020367244717126216\n",
      "Iteration 134, Cost: 0.02008697411400017\n",
      "Iteration 135, Cost: 0.0198129278594983\n",
      "Iteration 136, Cost: 0.019544924761134792\n",
      "Iteration 137, Cost: 0.019282789935563108\n",
      "Iteration 138, Cost: 0.01902635456684373\n",
      "Iteration 139, Cost: 0.018775455673709005\n",
      "Iteration 140, Cost: 0.018529935885594668\n",
      "Iteration 141, Cost: 0.018289643227200024\n",
      "Iteration 142, Cost: 0.01805443091133278\n",
      "Iteration 143, Cost: 0.01782415713979096\n",
      "Iteration 144, Cost: 0.017598684912031794\n",
      "Iteration 145, Cost: 0.017377881841376525\n",
      "Iteration 146, Cost: 0.01716161997850057\n",
      "Iteration 147, Cost: 0.01694977564195948\n",
      "Iteration 148, Cost: 0.016742229255503803\n",
      "Iteration 149, Cost: 0.01653886519193862\n",
      "Iteration 150, Cost: 0.01633957162328751\n",
      "Iteration 151, Cost: 0.016144240377025002\n",
      "Iteration 152, Cost: 0.015952766798146414\n",
      "Iteration 153, Cost: 0.01576504961684913\n",
      "Iteration 154, Cost: 0.015580990821605238\n",
      "Iteration 155, Cost: 0.015400495537411034\n",
      "Iteration 156, Cost: 0.015223471909005307\n",
      "Iteration 157, Cost: 0.015049830988854441\n",
      "Iteration 158, Cost: 0.014879486629708817\n",
      "Iteration 159, Cost: 0.014712355381541415\n",
      "Iteration 160, Cost: 0.014548356392686233\n",
      "Iteration 161, Cost: 0.014387411315000325\n",
      "Iteration 162, Cost: 0.014229444212879987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 163, Cost: 0.01407438147596789\n",
      "Iteration 164, Cost: 0.013922151735394324\n",
      "Iteration 165, Cost: 0.013772685783401833\n",
      "Iteration 166, Cost: 0.013625916496208723\n",
      "Iteration 167, Cost: 0.01348177875997265\n",
      "Iteration 168, Cost: 0.013340209399721536\n",
      "Iteration 169, Cost: 0.01320114711112432\n",
      "Iteration 170, Cost: 0.013064532394979773\n",
      "Iteration 171, Cost: 0.012930307494306703\n",
      "Iteration 172, Cost: 0.012798416333923883\n",
      "Iteration 173, Cost: 0.012668804462413119\n",
      "Iteration 174, Cost: 0.012541418996363303\n",
      "Iteration 175, Cost: 0.012416208566797978\n",
      "Iteration 176, Cost: 0.012293123267693261\n",
      "Iteration 177, Cost: 0.012172114606496927\n",
      "Iteration 178, Cost: 0.012053135456563755\n",
      "Iteration 179, Cost: 0.01193614001142562\n",
      "Iteration 180, Cost: 0.011821083740818835\n",
      "Iteration 181, Cost: 0.011707923348394444\n",
      "Iteration 182, Cost: 0.011596616731040595\n",
      "Iteration 183, Cost: 0.011487122939749269\n",
      "Iteration 184, Cost: 0.011379402141962607\n",
      "Iteration 185, Cost: 0.011273415585337088\n",
      "Iteration 186, Cost: 0.011169125562866337\n",
      "Iteration 187, Cost: 0.011066495379306223\n",
      "Iteration 188, Cost: 0.010965489318848153\n",
      "Iteration 189, Cost: 0.010866072613989065\n",
      "Iteration 190, Cost: 0.01076821141554875\n",
      "Iteration 191, Cost: 0.01067187276378735\n",
      "Iteration 192, Cost: 0.010577024560577991\n",
      "Iteration 193, Cost: 0.010483635542591392\n",
      "Iteration 194, Cost: 0.010391675255451276\n",
      "Iteration 195, Cost: 0.010301114028821083\n",
      "Iteration 196, Cost: 0.010211922952384383\n",
      "Iteration 197, Cost: 0.01012407385268278\n",
      "Iteration 198, Cost: 0.010037539270776904\n",
      "Iteration 199, Cost: 0.009952292440697426\n",
      "Iteration 200, Cost: 0.009868307268654489\n",
      "Iteration 201, Cost: 0.00978555831297537\n",
      "Iteration 202, Cost: 0.009704020764741373\n",
      "Iteration 203, Cost: 0.009623670429096379\n",
      "Iteration 204, Cost: 0.009544483707200381\n",
      "Iteration 205, Cost: 0.009466437578802863\n",
      "Iteration 206, Cost: 0.009389509585411489\n",
      "Iteration 207, Cost: 0.009313677814033127\n",
      "Iteration 208, Cost: 0.009238920881464689\n",
      "Iteration 209, Cost: 0.009165217919112718\n",
      "Iteration 210, Cost: 0.009092548558321157\n",
      "Iteration 211, Cost: 0.009020892916187838\n",
      "Iteration 212, Cost: 0.008950231581850968\n",
      "Iteration 213, Cost: 0.008880545603227732\n",
      "Iteration 214, Cost: 0.008811816474187757\n",
      "Iteration 215, Cost: 0.008744026122145114\n",
      "Iteration 216, Cost: 0.008677156896053025\n",
      "Iteration 217, Cost: 0.00861119155478625\n",
      "Iteration 218, Cost: 0.008546113255896638\n",
      "Iteration 219, Cost: 0.008481905544728056\n",
      "Iteration 220, Cost: 0.00841855234387735\n",
      "Iteration 221, Cost: 0.008356037942988693\n",
      "Iteration 222, Cost: 0.008294346988869043\n",
      "Iteration 223, Cost: 0.008233464475913088\n",
      "Iteration 224, Cost: 0.008173375736826414\n",
      "Iteration 225, Cost: 0.008114066433636182\n",
      "Iteration 226, Cost: 0.008055522548978985\n",
      "Iteration 227, Cost: 0.007997730377655976\n",
      "Iteration 228, Cost: 0.007940676518445846\n",
      "Iteration 229, Cost: 0.007884347866166451\n",
      "Iteration 230, Cost: 0.00782873160397643\n",
      "Iteration 231, Cost: 0.007773815195908405\n",
      "Iteration 232, Cost: 0.007719586379625663\n",
      "Iteration 233, Cost: 0.007666033159394697\n",
      "Iteration 234, Cost: 0.007613143799266064\n",
      "Iteration 235, Cost: 0.007560906816456516\n",
      "Iteration 236, Cost: 0.007509310974925519\n",
      "Iteration 237, Cost: 0.007458345279139572\n",
      "Iteration 238, Cost: 0.007407998968018026\n",
      "Iteration 239, Cost: 0.007358261509054318\n",
      "Iteration 240, Cost: 0.007309122592606731\n",
      "Iteration 241, Cost: 0.0072605721263531405\n",
      "Iteration 242, Cost: 0.00721260022990427\n",
      "Iteration 243, Cost: 0.0071651972295702785\n",
      "Iteration 244, Cost: 0.007118353653275709\n",
      "Iteration 245, Cost: 0.007072060225617903\n",
      "Iteration 246, Cost: 0.007026307863064344\n",
      "Iteration 247, Cost: 0.006981087669284385\n",
      "Iteration 248, Cost: 0.006936390930611107\n",
      "Iteration 249, Cost: 0.006892209111629144\n",
      "Iteration 250, Cost: 0.006848533850884516\n",
      "Iteration 251, Cost: 0.006805356956712586\n",
      "Iteration 252, Cost: 0.006762670403180477\n",
      "Iteration 253, Cost: 0.006720466326140349\n",
      "Iteration 254, Cost: 0.0066787370193901\n",
      "Iteration 255, Cost: 0.006637474930938189\n",
      "Iteration 256, Cost: 0.006596672659369344\n",
      "Iteration 257, Cost: 0.006556322950308109\n",
      "Iteration 258, Cost: 0.006516418692977177\n",
      "Iteration 259, Cost: 0.006476952916847741\n",
      "Iteration 260, Cost: 0.006437918788378958\n",
      "Iteration 261, Cost: 0.006399309607843943\n",
      "Iteration 262, Cost: 0.006361118806239648\n",
      "Iteration 263, Cost: 0.00632333994227814\n",
      "Iteration 264, Cost: 0.006285966699456869\n",
      "Iteration 265, Cost: 0.006248992883205561\n",
      "Iteration 266, Cost: 0.006212412418107522\n",
      "Iteration 267, Cost: 0.0061762193451931156\n",
      "Iteration 268, Cost: 0.00614040781930336\n",
      "Iteration 269, Cost: 0.00610497210652156\n",
      "Iteration 270, Cost: 0.006069906581671027\n",
      "Iteration 271, Cost: 0.006035205725876959\n",
      "Iteration 272, Cost: 0.00600086412419066\n",
      "Iteration 273, Cost: 0.0059668764632742665\n",
      "Iteration 274, Cost: 0.0059332375291443204\n",
      "Iteration 275, Cost: 0.005899942204972432\n",
      "Iteration 276, Cost: 0.00586698546894149\n",
      "Iteration 277, Cost: 0.005834362392155786\n",
      "Iteration 278, Cost: 0.005802068136603591\n",
      "Iteration 279, Cost: 0.0057700979531706475\n",
      "Iteration 280, Cost: 0.005738447179703229\n",
      "Iteration 281, Cost: 0.005707111239119304\n",
      "Iteration 282, Cost: 0.005676085637566545\n",
      "Iteration 283, Cost: 0.005645365962625826\n",
      "Iteration 284, Cost: 0.005614947881558971\n",
      "Iteration 285, Cost: 0.005584827139599561\n",
      "Iteration 286, Cost: 0.005554999558285574\n",
      "Iteration 287, Cost: 0.005525461033832746\n",
      "Iteration 288, Cost: 0.005496207535547542\n",
      "Iteration 289, Cost: 0.005467235104278632\n",
      "Iteration 290, Cost: 0.0054385398509058614\n",
      "Iteration 291, Cost: 0.005410117954865693\n",
      "Iteration 292, Cost: 0.005381965662712123\n",
      "Iteration 293, Cost: 0.00535407928671213\n",
      "Iteration 294, Cost: 0.005326455203474754\n",
      "Iteration 295, Cost: 0.005299089852612846\n",
      "Iteration 296, Cost: 0.005271979735436705\n",
      "Iteration 297, Cost: 0.005245121413678667\n",
      "Iteration 298, Cost: 0.0052185115082478955\n",
      "Iteration 299, Cost: 0.005192146698014536\n",
      "Iteration 300, Cost: 0.005166023718622481\n",
      "Iteration 301, Cost: 0.005140139361329973\n",
      "Iteration 302, Cost: 0.005114490471877369\n",
      "Iteration 303, Cost: 0.005089073949381258\n",
      "Iteration 304, Cost: 0.0050638867452543765\n",
      "Iteration 305, Cost: 0.005038925862150527\n",
      "Iteration 306, Cost: 0.005014188352933932\n",
      "Iteration 307, Cost: 0.004989671319672379\n",
      "Converged!\n",
      "Iteration 1, Cost: 0.2533148338472445\n",
      "Iteration 2, Cost: 0.23973071815264924\n",
      "Iteration 3, Cost: 0.2311374722300328\n",
      "Iteration 4, Cost: 0.22513644409721448\n",
      "Iteration 5, Cost: 0.22048936660842183\n",
      "Iteration 6, Cost: 0.2166559784803324\n",
      "Iteration 7, Cost: 0.21336745457174902\n",
      "Iteration 8, Cost: 0.21045672553507722\n",
      "Iteration 9, Cost: 0.20780499656054494\n",
      "Iteration 10, Cost: 0.20532538747357804\n",
      "Iteration 11, Cost: 0.2029556532518299\n",
      "Iteration 12, Cost: 0.200652589105975\n",
      "Iteration 13, Cost: 0.19838706900767294\n",
      "Iteration 14, Cost: 0.19613991608834175\n",
      "Iteration 15, Cost: 0.1938987278336646\n",
      "Iteration 16, Cost: 0.19165557436339503\n",
      "Iteration 17, Cost: 0.1894053939064218\n",
      "Iteration 18, Cost: 0.1871449049575099\n",
      "Iteration 19, Cost: 0.18487188761703804\n",
      "Iteration 20, Cost: 0.18258472514285323\n",
      "Iteration 21, Cost: 0.18028212804362553\n",
      "Iteration 22, Cost: 0.1779629850499819\n",
      "Iteration 23, Cost: 0.17562630007461086\n",
      "Iteration 24, Cost: 0.173271184534986\n",
      "Iteration 25, Cost: 0.17089688216587995\n",
      "Iteration 26, Cost: 0.1685028097477377\n",
      "Iteration 27, Cost: 0.16608860238609308\n",
      "Iteration 28, Cost: 0.16365415613565937\n",
      "Iteration 29, Cost: 0.1611996638586744\n",
      "Iteration 30, Cost: 0.15872564231033212\n",
      "Iteration 31, Cost: 0.1562329497238212\n",
      "Iteration 32, Cost: 0.15372279384996457\n",
      "Iteration 33, Cost: 0.1511967307204813\n",
      "Iteration 34, Cost: 0.1486566545397933\n",
      "Iteration 35, Cost: 0.1461047792004912\n",
      "Iteration 36, Cost: 0.1435436120352908\n",
      "Iteration 37, Cost: 0.14097592058742284\n",
      "Iteration 38, Cost: 0.1384046933911076\n",
      "Iteration 39, Cost: 0.13583309597370613\n",
      "Iteration 40, Cost: 0.13326442348453177\n",
      "Iteration 41, Cost: 0.1307020514889272\n",
      "Iteration 42, Cost: 0.1281493865168602\n",
      "Iteration 43, Cost: 0.1256098179128496\n",
      "Iteration 44, Cost: 0.12308667240157452\n",
      "Iteration 45, Cost: 0.12058317257522369\n",
      "Iteration 46, Cost: 0.11810240024630421\n",
      "Iteration 47, Cost: 0.1156472653186885\n",
      "Iteration 48, Cost: 0.1132204805354163\n",
      "Iteration 49, Cost: 0.11082454218628451\n",
      "Iteration 50, Cost: 0.10846171661842849\n",
      "Iteration 51, Cost: 0.10613403219965238\n",
      "Iteration 52, Cost: 0.10384327624180392\n",
      "Iteration 53, Cost: 0.10159099629926809\n",
      "Iteration 54, Cost: 0.09937850521080768\n",
      "Iteration 55, Cost: 0.09720688924396713\n",
      "Iteration 56, Cost: 0.09507701872124562\n",
      "Iteration 57, Cost: 0.09298956054732188\n",
      "Iteration 58, Cost: 0.09094499210870287\n",
      "Iteration 59, Cost: 0.08894361607463606\n",
      "Iteration 60, Cost: 0.08698557568609874\n",
      "Iteration 61, Cost: 0.08507087017508742\n",
      "Iteration 62, Cost: 0.08319937000787928\n",
      "Iteration 63, Cost: 0.08137083169337433\n",
      "Iteration 64, Cost: 0.07958491194195196\n",
      "Iteration 65, Cost: 0.07784118100283606\n",
      "Iteration 66, Cost: 0.07613913505006155\n",
      "Iteration 67, Cost: 0.07447820752954828\n",
      "Iteration 68, Cost: 0.07285777942243427\n",
      "Iteration 69, Cost: 0.07127718842158014\n",
      "Iteration 70, Cost: 0.0697357370569885\n",
      "Iteration 71, Cost: 0.06823269983915994\n",
      "Iteration 72, Cost: 0.06676732951449285\n",
      "Iteration 73, Cost: 0.065338862541701\n",
      "Iteration 74, Cost: 0.06394652390200316\n",
      "Iteration 75, Cost: 0.0625895313491261\n",
      "Iteration 76, Cost: 0.061267099189952906\n",
      "Iteration 77, Cost: 0.0599784416659905\n",
      "Iteration 78, Cost: 0.05872277598322303\n",
      "Iteration 79, Cost: 0.057499325016677864\n",
      "Iteration 80, Cost: 0.05630731969876332\n",
      "Iteration 81, Cost: 0.0551460010887259\n",
      "Iteration 82, Cost: 0.05401462211492863\n",
      "Iteration 83, Cost: 0.05291244898165546\n",
      "Iteration 84, Cost: 0.051838762236737286\n",
      "Iteration 85, Cost: 0.05079285750407369\n",
      "Iteration 86, Cost: 0.04977404589462227\n",
      "Iteration 87, Cost: 0.04878165411930868\n",
      "Iteration 88, Cost: 0.047815024336472126\n",
      "Iteration 89, Cost: 0.046873513774085294\n",
      "Iteration 90, Cost: 0.045956494172528194\n",
      "Iteration 91, Cost: 0.04506335109685149\n",
      "Iteration 92, Cost: 0.044193483168145226\n",
      "Iteration 93, Cost: 0.043346301261914216\n",
      "Iteration 94, Cost: 0.04252122771747884\n",
      "Iteration 95, Cost: 0.041717695596710666\n",
      "Iteration 96, Cost: 0.04093514802331247\n",
      "Iteration 97, Cost: 0.04017303762585565\n",
      "Iteration 98, Cost: 0.039430826099418614\n",
      "Iteration 99, Cost: 0.03870798389244404\n",
      "Iteration 100, Cost: 0.03800399001781698\n",
      "Iteration 101, Cost: 0.03731833198055396\n",
      "Iteration 102, Cost: 0.03665050580916664\n",
      "Iteration 103, Cost: 0.03600001617389004\n",
      "Iteration 104, Cost: 0.035366376572588294\n",
      "Iteration 105, Cost: 0.03474910956420108\n",
      "Iteration 106, Cost: 0.03414774702991254\n",
      "Iteration 107, Cost: 0.03356183044358543\n",
      "Iteration 108, Cost: 0.03299091113514697\n",
      "Iteration 109, Cost: 0.032434550533266444\n",
      "Iteration 110, Cost: 0.03189232037657735\n",
      "Iteration 111, Cost: 0.03136380288564264\n",
      "Iteration 112, Cost: 0.030848590890664777\n",
      "Iteration 113, Cost: 0.030346287912469346\n",
      "Iteration 114, Cost: 0.02985650819645933\n",
      "Iteration 115, Cost: 0.029378876701002872\n",
      "Iteration 116, Cost: 0.02891302904307523\n",
      "Iteration 117, Cost: 0.028458611404942435\n",
      "Iteration 118, Cost: 0.0280152804062908\n",
      "Iteration 119, Cost: 0.027582702946519492\n",
      "Iteration 120, Cost: 0.027160556021978807\n",
      "Iteration 121, Cost: 0.026748526522807733\n",
      "Iteration 122, Cost: 0.02634631101375291\n",
      "Iteration 123, Cost: 0.025953615502981767\n",
      "Iteration 124, Cost: 0.02557015520247481\n",
      "Iteration 125, Cost: 0.025195654283127755\n",
      "Iteration 126, Cost: 0.024829845627237928\n",
      "Iteration 127, Cost: 0.024472470580609283\n",
      "Iteration 128, Cost: 0.02412327870610096\n",
      "Iteration 129, Cost: 0.023782027540071762\n",
      "Iteration 130, Cost: 0.023448482352843424\n",
      "Iteration 131, Cost: 0.023122415914019798\n",
      "Iteration 132, Cost: 0.02280360826325635\n",
      "Iteration 133, Cost: 0.02249184648687284\n",
      "Iteration 134, Cost: 0.02218692450053785\n",
      "Iteration 135, Cost: 0.021888642838123397\n",
      "Iteration 136, Cost: 0.021596808446727126\n",
      "Iteration 137, Cost: 0.021311234487783767\n",
      "Iteration 138, Cost: 0.021031740144133112\n",
      "Iteration 139, Cost: 0.02075815043287519\n",
      "Iteration 140, Cost: 0.020490296023820177\n",
      "Iteration 141, Cost: 0.020228013063329234\n",
      "Iteration 142, Cost: 0.01997114300333905\n",
      "Iteration 143, Cost: 0.019719532435366203\n",
      "Iteration 144, Cost: 0.01947303292929495\n",
      "Iteration 145, Cost: 0.01923150087676262\n",
      "Iteration 146, Cost: 0.018994797338969124\n",
      "Iteration 147, Cost: 0.018762787898750247\n",
      "Iteration 148, Cost: 0.01853534251676756\n",
      "Iteration 149, Cost: 0.01831233539168075\n",
      "Iteration 150, Cost: 0.018093644824180116\n",
      "Iteration 151, Cost: 0.017879153084768024\n",
      "Iteration 152, Cost: 0.01766874628518813\n",
      "Iteration 153, Cost: 0.01746231425340971\n",
      "Iteration 154, Cost: 0.017259750412082036\n",
      "Iteration 155, Cost: 0.017060951660380012\n",
      "Iteration 156, Cost: 0.01686581825916759\n",
      "Iteration 157, Cost: 0.016674253719409563\n",
      "Iteration 158, Cost: 0.016486164693766084\n",
      "Iteration 159, Cost: 0.01630146087130638\n",
      "Iteration 160, Cost: 0.01612005487528056\n",
      "Iteration 161, Cost: 0.015941862163889563\n",
      "Iteration 162, Cost: 0.015766800933994216\n",
      "Iteration 163, Cost: 0.015594792027705284\n",
      "Iteration 164, Cost: 0.015425758841796304\n",
      "Iteration 165, Cost: 0.015259627239881574\n",
      "Iteration 166, Cost: 0.015096325467301212\n",
      "Iteration 167, Cost: 0.014935784068655592\n",
      "Iteration 168, Cost: 0.014777935807930933\n",
      "Iteration 169, Cost: 0.014622715591157909\n",
      "Iteration 170, Cost: 0.014470060391545084\n",
      "Iteration 171, Cost: 0.01431990917702879\n",
      "Iteration 172, Cost: 0.014172202840181175\n",
      "Iteration 173, Cost: 0.014026884130418353\n",
      "Iteration 174, Cost: 0.01388389758845057\n",
      "Iteration 175, Cost: 0.0137431894829168\n",
      "Iteration 176, Cost: 0.013604707749146402\n",
      "Iteration 177, Cost: 0.013468401929991098\n",
      "Iteration 178, Cost: 0.01333422311867097\n",
      "Iteration 179, Cost: 0.013202123903578748\n",
      "Iteration 180, Cost: 0.01307205831498769\n",
      "Iteration 181, Cost: 0.012943981773608667\n",
      "Iteration 182, Cost: 0.012817851040943322\n",
      "Iteration 183, Cost: 0.012693624171380806\n",
      "Iteration 184, Cost: 0.012571260465986582\n",
      "Iteration 185, Cost: 0.012450720427932865\n",
      "Iteration 186, Cost: 0.012331965719521061\n",
      "Iteration 187, Cost: 0.012214959120747746\n",
      "Iteration 188, Cost: 0.012099664489366689\n",
      "Iteration 189, Cost: 0.011986046722400497\n",
      "Iteration 190, Cost: 0.011874071719056403\n",
      "Iteration 191, Cost: 0.011763706345001936\n",
      "Iteration 192, Cost: 0.011654918397957074\n",
      "Iteration 193, Cost: 0.011547676574560628\n",
      "Iteration 194, Cost: 0.011441950438469564\n",
      "Iteration 195, Cost: 0.011337710389651015\n",
      "Iteration 196, Cost: 0.011234927634827708\n",
      "Iteration 197, Cost: 0.011133574159038479\n",
      "Iteration 198, Cost: 0.011033622698276607\n",
      "Iteration 199, Cost: 0.010935046713169479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 200, Cost: 0.01083782036366423\n",
      "Iteration 201, Cost: 0.010741918484684676\n",
      "Iteration 202, Cost: 0.010647316562725983\n",
      "Iteration 203, Cost: 0.010553990713354156\n",
      "Iteration 204, Cost: 0.01046191765957855\n",
      "Iteration 205, Cost: 0.01037107471106616\n",
      "Iteration 206, Cost: 0.010281439744167483\n",
      "Iteration 207, Cost: 0.010192991182724546\n",
      "Iteration 208, Cost: 0.010105707979632299\n",
      "Iteration 209, Cost: 0.010019569599125651\n",
      "Iteration 210, Cost: 0.009934555999764925\n",
      "Iteration 211, Cost: 0.009850647618093466\n",
      "Iteration 212, Cost: 0.009767825352941805\n",
      "Iteration 213, Cost: 0.009686070550353483\n",
      "Iteration 214, Cost: 0.009605364989108477\n",
      "Iteration 215, Cost: 0.009525690866820837\n",
      "Iteration 216, Cost: 0.009447030786587794\n",
      "Iteration 217, Cost: 0.009369367744168406\n",
      "Iteration 218, Cost: 0.009292685115670456\n",
      "Iteration 219, Cost: 0.009216966645724977\n",
      "Iteration 220, Cost: 0.009142196436128479\n",
      "Iteration 221, Cost: 0.009068358934933619\n",
      "Iteration 222, Cost: 0.008995438925969688\n",
      "Iteration 223, Cost: 0.008923421518774961\n",
      "Iteration 224, Cost: 0.008852292138923561\n",
      "Iteration 225, Cost: 0.008782036518730085\n",
      "Iteration 226, Cost: 0.008712640688315976\n",
      "Iteration 227, Cost: 0.00864409096702203\n",
      "Iteration 228, Cost: 0.008576373955152155\n",
      "Iteration 229, Cost: 0.008509476526034072\n",
      "Iteration 230, Cost: 0.00844338581838308\n",
      "Iteration 231, Cost: 0.00837808922895571\n",
      "Iteration 232, Cost: 0.008313574405480563\n",
      "Iteration 233, Cost: 0.00824982923985406\n",
      "Iteration 234, Cost: 0.00818684186158953\n",
      "Iteration 235, Cost: 0.008124600631508362\n",
      "Iteration 236, Cost: 0.008063094135662552\n",
      "Iteration 237, Cost: 0.008002311179478349\n",
      "Iteration 238, Cost: 0.007942240782111247\n",
      "Iteration 239, Cost: 0.007882872171002923\n",
      "Iteration 240, Cost: 0.00782419477663118\n",
      "Iteration 241, Cost: 0.007766198227444306\n",
      "Iteration 242, Cost: 0.007708872344971779\n",
      "Iteration 243, Cost: 0.007652207139103455\n",
      "Iteration 244, Cost: 0.007596192803529846\n",
      "Iteration 245, Cost: 0.007540819711336433\n",
      "Iteration 246, Cost: 0.0074860784107452305\n",
      "Iteration 247, Cost: 0.007431959620997231\n",
      "Iteration 248, Cost: 0.007378454228369586\n",
      "Iteration 249, Cost: 0.00732555328232168\n",
      "Iteration 250, Cost: 0.007273247991764626\n",
      "Iteration 251, Cost: 0.007221529721448818\n",
      "Iteration 252, Cost: 0.007170389988464614\n",
      "Iteration 253, Cost: 0.007119820458851299\n",
      "Iteration 254, Cost: 0.007069812944309836\n",
      "Iteration 255, Cost: 0.007020359399015084\n",
      "Iteration 256, Cost: 0.006971451916523332\n",
      "Iteration 257, Cost: 0.006923082726771308\n",
      "Iteration 258, Cost: 0.0068752441931628945\n",
      "Iteration 259, Cost: 0.006827928809740052\n",
      "Iteration 260, Cost: 0.006781129198434569\n",
      "Iteration 261, Cost: 0.006734838106397443\n",
      "Iteration 262, Cost: 0.006689048403402887\n",
      "Iteration 263, Cost: 0.006643753079324004\n",
      "Iteration 264, Cost: 0.0065989452416774395\n",
      "Iteration 265, Cost: 0.006554618113234369\n",
      "Iteration 266, Cost: 0.0065107650296953\n",
      "Iteration 267, Cost: 0.006467379437426366\n",
      "Iteration 268, Cost: 0.006424454891254803\n",
      "Iteration 269, Cost: 0.006381985052321493\n",
      "Iteration 270, Cost: 0.0063399636859884905\n",
      "Iteration 271, Cost: 0.006298384659799595\n",
      "Iteration 272, Cost: 0.006257241941492098\n",
      "Iteration 273, Cost: 0.006216529597057919\n",
      "Iteration 274, Cost: 0.006176241788852457\n",
      "Iteration 275, Cost: 0.006136372773749493\n",
      "Iteration 276, Cost: 0.0060969169013406444\n",
      "Iteration 277, Cost: 0.00605786861217786\n",
      "Iteration 278, Cost: 0.006019222436057557\n",
      "Iteration 279, Cost: 0.005980972990345055\n",
      "Iteration 280, Cost: 0.005943114978338001\n",
      "Iteration 281, Cost: 0.005905643187667554\n",
      "Iteration 282, Cost: 0.005868552488736173\n",
      "Iteration 283, Cost: 0.0058318378331908205\n",
      "Iteration 284, Cost: 0.00579549425243057\n",
      "Iteration 285, Cost: 0.005759516856147501\n",
      "Iteration 286, Cost: 0.005723900830899968\n",
      "Iteration 287, Cost: 0.005688641438717196\n",
      "Iteration 288, Cost: 0.005653734015734355\n",
      "Iteration 289, Cost: 0.005619173970857238\n",
      "Iteration 290, Cost: 0.0055849567844556075\n",
      "Iteration 291, Cost: 0.005551078007084516\n",
      "Iteration 292, Cost: 0.005517533258232742\n",
      "Iteration 293, Cost: 0.005484318225097608\n",
      "Iteration 294, Cost: 0.005451428661385471\n",
      "Iteration 295, Cost: 0.005418860386137164\n",
      "Iteration 296, Cost: 0.005386609282577766\n",
      "Iteration 297, Cost: 0.005354671296989989\n",
      "Iteration 298, Cost: 0.00532304243761062\n",
      "Iteration 299, Cost: 0.00529171877354939\n",
      "Iteration 300, Cost: 0.005260696433729687\n",
      "Iteration 301, Cost: 0.005229971605850564\n",
      "Iteration 302, Cost: 0.005199540535369528\n",
      "Iteration 303, Cost: 0.005169399524505532\n",
      "Iteration 304, Cost: 0.00513954493126172\n",
      "Iteration 305, Cost: 0.005109973168467422\n",
      "Iteration 306, Cost: 0.005080680702838921\n",
      "Iteration 307, Cost: 0.00505166405405855\n",
      "Iteration 308, Cost: 0.005022919793871677\n",
      "Iteration 309, Cost: 0.004994444545201152\n",
      "Converged!\n",
      "Iteration 1, Cost: 0.36112749046280823\n",
      "Iteration 2, Cost: 0.3160803701238174\n",
      "Iteration 3, Cost: 0.28133259140901123\n",
      "Iteration 4, Cost: 0.2560785130319208\n",
      "Iteration 5, Cost: 0.2386826081044682\n",
      "Iteration 6, Cost: 0.22695541611821765\n",
      "Iteration 7, Cost: 0.21864533652951543\n",
      "Iteration 8, Cost: 0.21220915408118823\n",
      "Iteration 9, Cost: 0.20679909189607237\n",
      "Iteration 10, Cost: 0.20198173103385925\n",
      "Iteration 11, Cost: 0.19753661456027846\n",
      "Iteration 12, Cost: 0.1933497333216344\n",
      "Iteration 13, Cost: 0.18936053774844547\n",
      "Iteration 14, Cost: 0.1855354450288419\n",
      "Iteration 15, Cost: 0.18185439393583144\n",
      "Iteration 16, Cost: 0.17830399124500187\n",
      "Iteration 17, Cost: 0.17487406333710911\n",
      "Iteration 18, Cost: 0.1715559765271867\n",
      "Iteration 19, Cost: 0.1683418623095153\n",
      "Iteration 20, Cost: 0.16522428970748582\n",
      "Iteration 21, Cost: 0.1621961470164085\n",
      "Iteration 22, Cost: 0.15925061453603856\n",
      "Iteration 23, Cost: 0.15638117272798546\n",
      "Iteration 24, Cost: 0.1535816216757341\n",
      "Iteration 25, Cost: 0.1508461023792867\n",
      "Iteration 26, Cost: 0.1481691166188879\n",
      "Iteration 27, Cost: 0.1455455443937708\n",
      "Iteration 28, Cost: 0.14297065857338437\n",
      "Iteration 29, Cost: 0.14044013646064904\n",
      "Iteration 30, Cost: 0.137950067910345\n",
      "Iteration 31, Cost: 0.1354969596314385\n",
      "Iteration 32, Cost: 0.1330777353661114\n",
      "Iteration 33, Cost: 0.1306897317685101\n",
      "Iteration 34, Cost: 0.1283306899754436\n",
      "Iteration 35, Cost: 0.1259987430401415\n",
      "Iteration 36, Cost: 0.12369239956328808\n",
      "Iteration 37, Cost: 0.12141052398406375\n",
      "Iteration 38, Cost: 0.11915231407691482\n",
      "Iteration 39, Cost: 0.11691727623418038\n",
      "Iteration 40, Cost: 0.11470519910425975\n",
      "Iteration 41, Cost: 0.11251612610843349\n",
      "Iteration 42, Cost: 0.11035032728840202\n",
      "Iteration 43, Cost: 0.10820827085371064\n",
      "Iteration 44, Cost: 0.10609059471554741\n",
      "Iteration 45, Cost: 0.10399807822129412\n",
      "Iteration 46, Cost: 0.10193161425058518\n",
      "Iteration 47, Cost: 0.0998921818034577\n",
      "Iteration 48, Cost: 0.09788081920619378\n",
      "Iteration 49, Cost: 0.09589859807906762\n",
      "Iteration 50, Cost: 0.09394659824759083\n",
      "Iteration 51, Cost: 0.09202588382734476\n",
      "Iteration 52, Cost: 0.0901374807623794\n",
      "Iteration 53, Cost: 0.08828235613781237\n",
      "Iteration 54, Cost: 0.08646139960862081\n",
      "Iteration 55, Cost: 0.08467540728081796\n",
      "Iteration 56, Cost: 0.08292506834403017\n",
      "Iteration 57, Cost: 0.08121095468629329\n",
      "Iteration 58, Cost: 0.07953351362770841\n",
      "Iteration 59, Cost: 0.07789306379837559\n",
      "Iteration 60, Cost: 0.07628979406902993\n",
      "Iteration 61, Cost: 0.07472376533170791\n",
      "Iteration 62, Cost: 0.07319491483273408\n",
      "Iteration 63, Cost: 0.07170306268860943\n",
      "Iteration 64, Cost: 0.0702479201706717\n",
      "Iteration 65, Cost: 0.06882909932683087\n",
      "Iteration 66, Cost: 0.067446123515527\n",
      "Iteration 67, Cost: 0.06609843845367185\n",
      "Iteration 68, Cost: 0.0647854234212111\n",
      "Iteration 69, Cost: 0.06350640231458742\n",
      "Iteration 70, Cost: 0.062260654294941466\n",
      "Iteration 71, Cost: 0.06104742383049933\n",
      "Iteration 72, Cost: 0.059865929983483654\n",
      "Iteration 73, Cost: 0.05871537483829662\n",
      "Iteration 74, Cost: 0.057594951008733936\n",
      "Iteration 75, Cost: 0.05650384819730387\n",
      "Iteration 76, Cost: 0.05544125880945372\n",
      "Iteration 77, Cost: 0.05440638264999061\n",
      "Iteration 78, Cost: 0.053398430748658327\n",
      "Iteration 79, Cost: 0.05241662837713812\n",
      "Iteration 80, Cost: 0.051460217331079555\n",
      "Iteration 81, Cost: 0.05052845755848367\n",
      "Iteration 82, Cost: 0.04962062822016188\n",
      "Iteration 83, Cost: 0.04873602826936705\n",
      "Iteration 84, Cost: 0.047873976636335455\n",
      "Iteration 85, Cost: 0.04703381209972002\n",
      "Iteration 86, Cost: 0.046214892921108155\n",
      "Iteration 87, Cost: 0.04541659631142145\n",
      "Iteration 88, Cost: 0.044638317789440725\n",
      "Iteration 89, Cost: 0.043879470483454544\n",
      "Iteration 90, Cost: 0.04313948441754893\n",
      "Iteration 91, Cost: 0.04241780581476051\n",
      "Iteration 92, Cost: 0.04171389644056657\n",
      "Iteration 93, Cost: 0.0410272330022727\n",
      "Iteration 94, Cost: 0.04035730661298415\n",
      "Iteration 95, Cost: 0.039703622323130655\n",
      "Iteration 96, Cost: 0.03906569871799324\n",
      "Iteration 97, Cost: 0.03844306757632231\n",
      "Iteration 98, Cost: 0.037835273582851915\n",
      "Iteration 99, Cost: 0.03724187408617406\n",
      "Iteration 100, Cost: 0.036662438892888005\n",
      "Iteration 101, Cost: 0.036096550089018464\n",
      "Iteration 102, Cost: 0.03554380188024339\n",
      "Iteration 103, Cost: 0.035003800443338264\n",
      "Iteration 104, Cost: 0.034476163782300964\n",
      "Iteration 105, Cost: 0.03396052158376087\n",
      "Iteration 106, Cost: 0.0334565150674149\n",
      "Iteration 107, Cost: 0.03296379682830954\n",
      "Iteration 108, Cost: 0.03248203066876126\n",
      "Iteration 109, Cost: 0.032010891418553046\n",
      "Iteration 110, Cost: 0.03155006474275325\n",
      "Iteration 111, Cost: 0.031099246937074276\n",
      "Iteration 112, Cost: 0.03065814471113209\n",
      "Iteration 113, Cost: 0.030226474960295173\n",
      "Iteration 114, Cost: 0.02980396452703992\n",
      "Iteration 115, Cost: 0.02939034995287531\n",
      "Iteration 116, Cost: 0.02898537722197955\n",
      "Iteration 117, Cost: 0.028588801497720886\n",
      "Iteration 118, Cost: 0.028200386853227722\n",
      "Iteration 119, Cost: 0.027819905997141955\n",
      "Iteration 120, Cost: 0.027447139995643584\n",
      "Iteration 121, Cost: 0.027081877991782174\n",
      "Iteration 122, Cost: 0.026723916923097824\n",
      "Iteration 123, Cost: 0.026373061238464966\n",
      "Iteration 124, Cost: 0.026029122615048982\n",
      "Iteration 125, Cost: 0.025691919676230034\n",
      "Iteration 126, Cost: 0.025361277711319907\n",
      "Iteration 127, Cost: 0.025037028397876027\n",
      "Iteration 128, Cost: 0.02471900952739959\n",
      "Iteration 129, Cost: 0.024407064735190616\n",
      "Iteration 130, Cost: 0.024101043235118434\n",
      "Iteration 131, Cost: 0.023800799560050056\n",
      "Iteration 132, Cost: 0.02350619330865733\n",
      "Iteration 133, Cost: 0.023217088899296266\n",
      "Iteration 134, Cost: 0.022933355331614703\n",
      "Iteration 135, Cost: 0.02265486595649834\n",
      "Iteration 136, Cost: 0.022381498254907952\n",
      "Iteration 137, Cost: 0.02211313362609307\n",
      "Iteration 138, Cost: 0.021849657185590256\n",
      "Iteration 139, Cost: 0.02159095757332848\n",
      "Iteration 140, Cost: 0.021336926772071548\n",
      "Iteration 141, Cost: 0.021087459936330787\n",
      "Iteration 142, Cost: 0.02084245523178243\n",
      "Iteration 143, Cost: 0.02060181368512565\n",
      "Iteration 144, Cost: 0.020365439044222538\n",
      "Iteration 145, Cost: 0.02013323764827152\n",
      "Iteration 146, Cost: 0.019905118307684527\n",
      "Iteration 147, Cost: 0.019680992193265894\n",
      "Iteration 148, Cost: 0.019460772734230393\n",
      "Iteration 149, Cost: 0.019244375524548305\n",
      "Iteration 150, Cost: 0.01903171823706911\n",
      "Iteration 151, Cost: 0.018822720544850548\n",
      "Iteration 152, Cost: 0.018617304049107462\n",
      "Iteration 153, Cost: 0.01841539221319323\n",
      "Iteration 154, Cost: 0.018216910302034883\n",
      "Iteration 155, Cost: 0.01802178532646055\n",
      "Iteration 156, Cost: 0.017829945991881865\n",
      "Iteration 157, Cost: 0.017641322650824683\n",
      "Iteration 158, Cost: 0.017455847258835977\n",
      "Iteration 159, Cost: 0.01727345333333247\n",
      "Iteration 160, Cost: 0.017094075914996155\n",
      "Iteration 161, Cost: 0.01691765153136173\n",
      "Iteration 162, Cost: 0.016744118162280754\n",
      "Iteration 163, Cost: 0.016573415206985583\n",
      "Iteration 164, Cost: 0.016405483452513067\n",
      "Iteration 165, Cost: 0.01624026504328176\n",
      "Iteration 166, Cost: 0.01607770345164839\n",
      "Iteration 167, Cost: 0.01591774344929774\n",
      "Iteration 168, Cost: 0.015760331079345914\n",
      "Iteration 169, Cost: 0.015605413629059654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 170, Cost: 0.01545293960311419\n",
      "Iteration 171, Cost: 0.015302858697329188\n",
      "Iteration 172, Cost: 0.015155121772836927\n",
      "Iteration 173, Cost: 0.015009680830648905\n",
      "Iteration 174, Cost: 0.014866488986597082\n",
      "Iteration 175, Cost: 0.014725500446634216\n",
      "Iteration 176, Cost: 0.014586670482483969\n",
      "Iteration 177, Cost: 0.01444995540763668\n",
      "Iteration 178, Cost: 0.014315312553690295\n",
      "Iteration 179, Cost: 0.014182700247038781\n",
      "Iteration 180, Cost: 0.014052077785912087\n",
      "Iteration 181, Cost: 0.01392340541777298\n",
      "Iteration 182, Cost: 0.01379664431707655\n",
      "Iteration 183, Cost: 0.013671756563398455\n",
      "Iteration 184, Cost: 0.013548705119937525\n",
      "Iteration 185, Cost: 0.013427453812398167\n",
      "Iteration 186, Cost: 0.013307967308257156\n",
      "Iteration 187, Cost: 0.013190211096418724\n",
      "Iteration 188, Cost: 0.013074151467260956\n",
      "Iteration 189, Cost: 0.012959755493075773\n",
      "Iteration 190, Cost: 0.012846991008903673\n",
      "Iteration 191, Cost: 0.012735826593763861\n",
      "Iteration 192, Cost: 0.012626231552279278\n",
      "Iteration 193, Cost: 0.012518175896695473\n",
      "Iteration 194, Cost: 0.012411630329291561\n",
      "Iteration 195, Cost: 0.01230656622518071\n",
      "Iteration 196, Cost: 0.01220295561549722\n",
      "Iteration 197, Cost: 0.012100771170966548\n",
      "Iteration 198, Cost: 0.011999986185854373\n",
      "Iteration 199, Cost: 0.011900574562290224\n",
      "Iteration 200, Cost: 0.01180251079496097\n",
      "Iteration 201, Cost: 0.011705769956169091\n",
      "Iteration 202, Cost: 0.011610327681250478\n",
      "Iteration 203, Cost: 0.011516160154346276\n",
      "Iteration 204, Cost: 0.011423244094523053\n",
      "Iteration 205, Cost: 0.011331556742235576\n",
      "Iteration 206, Cost: 0.011241075846126247\n",
      "Iteration 207, Cost: 0.011151779650155184\n",
      "Iteration 208, Cost: 0.011063646881054977\n",
      "Iteration 209, Cost: 0.010976656736103978\n",
      "Iteration 210, Cost: 0.010890788871212016\n",
      "Iteration 211, Cost: 0.010806023389312469\n",
      "Iteration 212, Cost: 0.010722340829054459\n",
      "Iteration 213, Cost: 0.010639722153789223\n",
      "Iteration 214, Cost: 0.010558148740844406\n",
      "Iteration 215, Cost: 0.010477602371080315\n",
      "Iteration 216, Cost: 0.010398065218722078\n",
      "Iteration 217, Cost: 0.010319519841461677\n",
      "Iteration 218, Cost: 0.010241949170823968\n",
      "Iteration 219, Cost: 0.010165336502790683\n",
      "Iteration 220, Cost: 0.010089665488676656\n",
      "Iteration 221, Cost: 0.010014920126252406\n",
      "Iteration 222, Cost: 0.00994108475110731\n",
      "Iteration 223, Cost: 0.009868144028247686\n",
      "Iteration 224, Cost: 0.009796082943924181\n",
      "Iteration 225, Cost: 0.009724886797682788\n",
      "Iteration 226, Cost: 0.009654541194634077\n",
      "Iteration 227, Cost: 0.009585032037935105\n",
      "Iteration 228, Cost: 0.009516345521478677\n",
      "Iteration 229, Cost: 0.009448468122784584\n",
      "Iteration 230, Cost: 0.00938138659608762\n",
      "Iteration 231, Cost: 0.009315087965617178\n",
      "Iteration 232, Cost: 0.009249559519063304\n",
      "Iteration 233, Cost: 0.00918478880122425\n",
      "Iteration 234, Cost: 0.009120763607830507\n",
      "Iteration 235, Cost: 0.009057471979540483\n",
      "Iteration 236, Cost: 0.008994902196103076\n",
      "Iteration 237, Cost: 0.008933042770682385\n",
      "Iteration 238, Cost: 0.008871882444339998\n",
      "Iteration 239, Cost: 0.008811410180670304\n",
      "Iteration 240, Cost: 0.008751615160584397\n",
      "Iteration 241, Cost: 0.00869248677723825\n",
      "Iteration 242, Cost: 0.00863401463110087\n",
      "Iteration 243, Cost: 0.008576188525158297\n",
      "Iteration 244, Cost: 0.008518998460249394\n",
      "Iteration 245, Cost: 0.008462434630529392\n",
      "Iteration 246, Cost: 0.008406487419057425\n",
      "Iteration 247, Cost: 0.00835114739350414\n",
      "Iteration 248, Cost: 0.008296405301975803\n",
      "Iteration 249, Cost: 0.008242252068951232\n",
      "Iteration 250, Cost: 0.008188678791328145\n",
      "Iteration 251, Cost: 0.008135676734575444\n",
      "Iteration 252, Cost: 0.008083237328988154\n",
      "Iteration 253, Cost: 0.008031352166041845\n",
      "Iteration 254, Cost: 0.007980012994843374\n",
      "Iteration 255, Cost: 0.007929211718674924\n",
      "Iteration 256, Cost: 0.007878940391628467\n",
      "Iteration 257, Cost: 0.00782919121532771\n",
      "Iteration 258, Cost: 0.007779956535734876\n",
      "Iteration 259, Cost: 0.007731228840039572\n",
      "Iteration 260, Cost: 0.007683000753627217\n",
      "Iteration 261, Cost: 0.007635265037124524\n",
      "Iteration 262, Cost: 0.007588014583519611\n",
      "Iteration 263, Cost: 0.007541242415354437\n",
      "Iteration 264, Cost: 0.007494941681987279\n",
      "Iteration 265, Cost: 0.00744910565692314\n",
      "Iteration 266, Cost: 0.007403727735209909\n",
      "Iteration 267, Cost: 0.007358801430898327\n",
      "Iteration 268, Cost: 0.007314320374563762\n",
      "Iteration 269, Cost: 0.007270278310887932\n",
      "Iteration 270, Cost: 0.007226669096298753\n",
      "Iteration 271, Cost: 0.007183486696666584\n",
      "Iteration 272, Cost: 0.007140725185055151\n",
      "Iteration 273, Cost: 0.007098378739525562\n",
      "Iteration 274, Cost: 0.0070564416409918266\n",
      "Iteration 275, Cost: 0.007014908271126384\n",
      "Iteration 276, Cost: 0.006973773110314186\n",
      "Iteration 277, Cost: 0.0069330307356539386\n",
      "Iteration 278, Cost: 0.006892675819005156\n",
      "Iteration 279, Cost: 0.006852703125079721\n",
      "Iteration 280, Cost: 0.006813107509576719\n",
      "Iteration 281, Cost: 0.006773883917359328\n",
      "Iteration 282, Cost: 0.006735027380672611\n",
      "Iteration 283, Cost: 0.006696533017401085\n",
      "Iteration 284, Cost: 0.006658396029364999\n",
      "Iteration 285, Cost: 0.006620611700654283\n",
      "Iteration 286, Cost: 0.006583175395999145\n",
      "Iteration 287, Cost: 0.006546082559176382\n",
      "Iteration 288, Cost: 0.00650932871145044\n",
      "Iteration 289, Cost: 0.006472909450048351\n",
      "Iteration 290, Cost: 0.006436820446667655\n",
      "Iteration 291, Cost: 0.006401057446016495\n",
      "Iteration 292, Cost: 0.006365616264385038\n",
      "Iteration 293, Cost: 0.00633049278824747\n",
      "Iteration 294, Cost: 0.00629568297289381\n",
      "Iteration 295, Cost: 0.006261182841090775\n",
      "Iteration 296, Cost: 0.006226988481771025\n",
      "Iteration 297, Cost: 0.006193096048750092\n",
      "Iteration 298, Cost: 0.006159501759470324\n",
      "Iteration 299, Cost: 0.006126201893771197\n",
      "Iteration 300, Cost: 0.006093192792685396\n",
      "Iteration 301, Cost: 0.006060470857260027\n",
      "Iteration 302, Cost: 0.006028032547402417\n",
      "Iteration 303, Cost: 0.005995874380749879\n",
      "Iteration 304, Cost: 0.005963992931562959\n",
      "Iteration 305, Cost: 0.005932384829641564\n",
      "Iteration 306, Cost: 0.005901046759263506\n",
      "Iteration 307, Cost: 0.005869975458144913\n",
      "Iteration 308, Cost: 0.0058391677164220486\n",
      "Iteration 309, Cost: 0.0058086203756540535\n",
      "Iteration 310, Cost: 0.0057783303278461295\n",
      "Iteration 311, Cost: 0.005748294514492755\n",
      "Iteration 312, Cost: 0.005718509925640423\n",
      "Iteration 313, Cost: 0.005688973598969562\n",
      "Iteration 314, Cost: 0.0056596826188951505\n",
      "Iteration 315, Cost: 0.005630634115685659\n",
      "Iteration 316, Cost: 0.005601825264599899\n",
      "Iteration 317, Cost: 0.005573253285041413\n",
      "Iteration 318, Cost: 0.005544915439730033\n",
      "Iteration 319, Cost: 0.005516809033890211\n",
      "Iteration 320, Cost: 0.005488931414455787\n",
      "Iteration 321, Cost: 0.005461279969290855\n",
      "Iteration 322, Cost: 0.005433852126426351\n",
      "Iteration 323, Cost: 0.0054066453533120635\n",
      "Iteration 324, Cost: 0.005379657156083729\n",
      "Iteration 325, Cost: 0.005352885078844892\n",
      "Iteration 326, Cost: 0.005326326702963213\n",
      "Iteration 327, Cost: 0.005299979646380953\n",
      "Iteration 328, Cost: 0.005273841562939289\n",
      "Iteration 329, Cost: 0.005247910141716226\n",
      "Iteration 330, Cost: 0.00522218310637776\n",
      "Iteration 331, Cost: 0.005196658214542097\n",
      "Iteration 332, Cost: 0.005171333257156559\n",
      "Iteration 333, Cost: 0.005146206057887002\n",
      "Iteration 334, Cost: 0.005121274472519438\n",
      "Iteration 335, Cost: 0.005096536388373611\n",
      "Iteration 336, Cost: 0.005071989723728297\n",
      "Iteration 337, Cost: 0.005047632427258083\n",
      "Iteration 338, Cost: 0.005023462477481365\n",
      "Iteration 339, Cost: 0.004999477882219363\n",
      "Converged!\n",
      "Iteration 1, Cost: 0.35470375710711927\n",
      "Iteration 2, Cost: 0.30966661905669973\n",
      "Iteration 3, Cost: 0.2753786301027292\n",
      "Iteration 4, Cost: 0.25056494632876675\n",
      "Iteration 5, Cost: 0.2341942497709664\n",
      "Iteration 6, Cost: 0.22364252914724536\n",
      "Iteration 7, Cost: 0.2163398064635911\n",
      "Iteration 8, Cost: 0.21069038269171045\n",
      "Iteration 9, Cost: 0.20589109602252983\n",
      "Iteration 10, Cost: 0.20156801044594988\n",
      "Iteration 11, Cost: 0.19754870767938038\n",
      "Iteration 12, Cost: 0.1937503765090337\n",
      "Iteration 13, Cost: 0.19012906991193343\n",
      "Iteration 14, Cost: 0.1866577128055647\n",
      "Iteration 15, Cost: 0.18331702184799853\n",
      "Iteration 16, Cost: 0.18009195142553996\n",
      "Iteration 17, Cost: 0.17697033689743455\n",
      "Iteration 18, Cost: 0.17394231865983695\n",
      "Iteration 19, Cost: 0.17099999641504884\n",
      "Iteration 20, Cost: 0.1681371290365574\n",
      "Iteration 21, Cost: 0.16534883648794654\n",
      "Iteration 22, Cost: 0.16263130762617964\n",
      "Iteration 23, Cost: 0.15998152902048843\n",
      "Iteration 24, Cost: 0.15739704840655985\n",
      "Iteration 25, Cost: 0.15487578095627294\n",
      "Iteration 26, Cost: 0.15241586061281698\n",
      "Iteration 27, Cost: 0.1500155337758256\n",
      "Iteration 28, Cost: 0.14767308924797065\n",
      "Iteration 29, Cost: 0.14538681670620282\n",
      "Iteration 30, Cost: 0.14315498581501765\n",
      "Iteration 31, Cost: 0.1409758390165936\n",
      "Iteration 32, Cost: 0.13884759251142267\n",
      "Iteration 33, Cost: 0.13676844154666892\n",
      "Iteration 34, Cost: 0.13473656755818303\n",
      "Iteration 35, Cost: 0.1327501458146848\n",
      "Iteration 36, Cost: 0.13080735295805054\n",
      "Iteration 37, Cost: 0.12890637426676982\n",
      "Iteration 38, Cost: 0.1270454106695384\n",
      "Iteration 39, Cost: 0.12522268558585467\n",
      "Iteration 40, Cost: 0.12343645164067675\n",
      "Iteration 41, Cost: 0.121684997241469\n",
      "Iteration 42, Cost: 0.1199666529497714\n",
      "Iteration 43, Cost: 0.11827979754144365\n",
      "Iteration 44, Cost: 0.11662286363453961\n",
      "Iteration 45, Cost: 0.11499434276919406\n",
      "Iteration 46, Cost: 0.11339278984453269\n",
      "Iteration 47, Cost: 0.11181682684716278\n",
      "Iteration 48, Cost: 0.1102651458385733\n",
      "Iteration 49, Cost: 0.10873651120034637\n",
      "Iteration 50, Cost: 0.10722976116349206\n",
      "Iteration 51, Cost: 0.10574380866981749\n",
      "Iteration 52, Cost: 0.10427764162842382\n",
      "Iteration 53, Cost: 0.10283032263930784\n",
      "Iteration 54, Cost: 0.10140098825918775\n",
      "Iteration 55, Cost: 0.09998884788284736\n",
      "Iteration 56, Cost: 0.0985931823073571\n",
      "Iteration 57, Cost: 0.09721334203733163\n",
      "Iteration 58, Cost: 0.09584874537776922\n",
      "Iteration 59, Cost: 0.09449887634783467\n",
      "Iteration 60, Cost: 0.0931632824350975\n",
      "Iteration 61, Cost: 0.09184157219620186\n",
      "Iteration 62, Cost: 0.09053341269780377\n",
      "Iteration 63, Cost: 0.08923852678200421\n",
      "Iteration 64, Cost: 0.08795669013455591\n",
      "Iteration 65, Cost: 0.08668772813280258\n",
      "Iteration 66, Cost: 0.08543151245427874\n",
      "Iteration 67, Cost: 0.08418795743632783\n",
      "Iteration 68, Cost: 0.08295701619151916\n",
      "Iteration 69, Cost: 0.08173867650190021\n",
      "Iteration 70, Cost: 0.08053295653538296\n",
      "Iteration 71, Cost: 0.07933990044752277\n",
      "Iteration 72, Cost: 0.0781595739491069\n",
      "Iteration 73, Cost: 0.0769920599319976\n",
      "Iteration 74, Cost: 0.07583745425080202\n",
      "Iteration 75, Cost: 0.07469586175524663\n",
      "Iteration 76, Cost: 0.0735673926577168\n",
      "Iteration 77, Cost: 0.07245215930337029\n",
      "Iteration 78, Cost: 0.07135027338843944\n",
      "Iteration 79, Cost: 0.07026184364816493\n",
      "Iteration 80, Cost: 0.06918697401171402\n",
      "Iteration 81, Cost: 0.06812576219963723\n",
      "Iteration 82, Cost: 0.0670782987215884\n",
      "Iteration 83, Cost: 0.06604466621919851\n",
      "Iteration 84, Cost: 0.06502493909149763\n",
      "Iteration 85, Cost: 0.06401918333787975\n",
      "Iteration 86, Cost: 0.06302745655562274\n",
      "Iteration 87, Cost: 0.06204980803447\n",
      "Iteration 88, Cost: 0.061086278898708414\n",
      "Iteration 89, Cost: 0.06013690225653204\n",
      "Iteration 90, Cost: 0.05920170332636794\n",
      "Iteration 91, Cost: 0.058280699519537085\n",
      "Iteration 92, Cost: 0.05737390046758625\n",
      "Iteration 93, Cost: 0.056481307990486035\n",
      "Iteration 94, Cost: 0.05560291600842816\n",
      "Iteration 95, Cost: 0.05473871040507905\n",
      "Iteration 96, Cost: 0.05388866885385767\n",
      "Iteration 97, Cost: 0.053052760621180375\n",
      "Iteration 98, Cost: 0.0522309463617764\n",
      "Iteration 99, Cost: 0.051423177921285285\n",
      "Iteration 100, Cost: 0.05062939816057814\n",
      "Iteration 101, Cost: 0.04984954081478435\n",
      "Iteration 102, Cost: 0.04908353039804078\n",
      "Iteration 103, Cost: 0.04833128216268696\n",
      "Iteration 104, Cost: 0.04759270211917196\n",
      "Iteration 105, Cost: 0.04686768712045866\n",
      "Iteration 106, Cost: 0.04615612501232978\n",
      "Iteration 107, Cost: 0.04545789484881428\n",
      "Iteration 108, Cost: 0.044772867170031684\n",
      "Iteration 109, Cost: 0.044100904338141914\n",
      "Iteration 110, Cost: 0.04344186092581264\n",
      "Iteration 111, Cost: 0.0427955841506774\n",
      "Iteration 112, Cost: 0.04216191434864281\n",
      "Iteration 113, Cost: 0.04154068547858652\n",
      "Iteration 114, Cost: 0.040931725650932976\n",
      "Iteration 115, Cost: 0.040334857672763194\n",
      "Iteration 116, Cost: 0.039749899602463265\n",
      "Iteration 117, Cost: 0.03917666530740405\n",
      "Iteration 118, Cost: 0.03861496501872974\n",
      "Iteration 119, Cost: 0.03806460587798214\n",
      "Iteration 120, Cost: 0.03752539247096749\n",
      "Iteration 121, Cost: 0.03699712734495846\n",
      "Iteration 122, Cost: 0.03647961150599401\n",
      "Iteration 123, Cost: 0.03597264489367892\n",
      "Iteration 124, Cost: 0.03547602683147992\n",
      "Iteration 125, Cost: 0.034989556451060624\n",
      "Iteration 126, Cost: 0.03451303308968642\n",
      "Iteration 127, Cost: 0.03404625666016373\n",
      "Iteration 128, Cost: 0.033589027993153726\n",
      "Iteration 129, Cost: 0.03314114915202244\n",
      "Iteration 130, Cost: 0.032702423720659755\n",
      "Iteration 131, Cost: 0.032272657064922675\n",
      "Iteration 132, Cost: 0.031851656568538174\n",
      "Iteration 133, Cost: 0.0314392318444427\n",
      "Iteration 134, Cost: 0.03103519492264268\n",
      "Iteration 135, Cost: 0.030639360415758085\n",
      "Iteration 136, Cost: 0.03025154566346379\n",
      "Iteration 137, Cost: 0.029871570857073336\n",
      "Iteration 138, Cost: 0.029499259145522466\n",
      "Iteration 139, Cost: 0.02913443672400607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 140, Cost: 0.028776932906507128\n",
      "Iteration 141, Cost: 0.02842658018342974\n",
      "Iteration 142, Cost: 0.028083214265515116\n",
      "Iteration 143, Cost: 0.027746674115178623\n",
      "Iteration 144, Cost: 0.027416801966361197\n",
      "Iteration 145, Cost: 0.0270934433339398\n",
      "Iteration 146, Cost: 0.026776447013690547\n",
      "Iteration 147, Cost: 0.02646566507374557\n",
      "Iteration 148, Cost: 0.02616095283843106\n",
      "Iteration 149, Cost: 0.025862168865320598\n",
      "Iteration 150, Cost: 0.025569174916284194\n",
      "Iteration 151, Cost: 0.025281835923261133\n",
      "Iteration 152, Cost: 0.02500001994943288\n",
      "Iteration 153, Cost: 0.02472359814642222\n",
      "Iteration 154, Cost: 0.024452444708096155\n",
      "Iteration 155, Cost: 0.024186436821502847\n",
      "Iteration 156, Cost: 0.023925454615428263\n",
      "Iteration 157, Cost: 0.023669381107014657\n",
      "Iteration 158, Cost: 0.023418102146842337\n",
      "Iteration 159, Cost: 0.02317150636283712\n",
      "Iteration 160, Cost: 0.022929485103329268\n",
      "Iteration 161, Cost: 0.022691932379555243\n",
      "Iteration 162, Cost: 0.022458744807861124\n",
      "Iteration 163, Cost: 0.022229821551836716\n",
      "Iteration 164, Cost: 0.02200506426458134\n",
      "Iteration 165, Cost: 0.02178437703127624\n",
      "Iteration 166, Cost: 0.021567666312215295\n",
      "Iteration 167, Cost: 0.02135484088642355\n",
      "Iteration 168, Cost: 0.02114581179597356\n",
      "Iteration 169, Cost: 0.020940492291091548\n",
      "Iteration 170, Cost: 0.0207387977761292\n",
      "Iteration 171, Cost: 0.02054064575646252\n",
      "Iteration 172, Cost: 0.02034595578636622\n",
      "Iteration 173, Cost: 0.02015464941790082\n",
      "Iteration 174, Cost: 0.019966650150839427\n",
      "Iteration 175, Cost: 0.019781883383652714\n",
      "Iteration 176, Cost: 0.019600276365562698\n",
      "Iteration 177, Cost: 0.01942175814966971\n",
      "Iteration 178, Cost: 0.01924625954715136\n",
      "Iteration 179, Cost: 0.01907371308252741\n",
      "Iteration 180, Cost: 0.018904052949981084\n",
      "Iteration 181, Cost: 0.018737214970723712\n",
      "Iteration 182, Cost: 0.01857313655138741\n",
      "Iteration 183, Cost: 0.01841175664342824\n",
      "Iteration 184, Cost: 0.018253015703521048\n",
      "Iteration 185, Cost: 0.01809685565492584\n",
      "Iteration 186, Cost: 0.017943219849804758\n",
      "Iteration 187, Cost: 0.01779205303246859\n",
      "Iteration 188, Cost: 0.017643301303530944\n",
      "Iteration 189, Cost: 0.017496912084948888\n",
      "Iteration 190, Cost: 0.01735283408592815\n",
      "Iteration 191, Cost: 0.01721101726967216\n",
      "Iteration 192, Cost: 0.017071412820953716\n",
      "Iteration 193, Cost: 0.016933973114489187\n",
      "Iteration 194, Cost: 0.0167986516840951\n",
      "Iteration 195, Cost: 0.01666540319260799\n",
      "Iteration 196, Cost: 0.016534183402548478\n",
      "Iteration 197, Cost: 0.016404949147511495\n",
      "Iteration 198, Cost: 0.016277658304264784\n",
      "Iteration 199, Cost: 0.016152269765538754\n",
      "Iteration 200, Cost: 0.016028743413490833\n",
      "Iteration 201, Cost: 0.015907040093828383\n",
      "Iteration 202, Cost: 0.015787121590574563\n",
      "Iteration 203, Cost: 0.015668950601461944\n",
      "Iteration 204, Cost: 0.015552490713939278\n",
      "Iteration 205, Cost: 0.015437706381777098\n",
      "Iteration 206, Cost: 0.015324562902258339\n",
      "Iteration 207, Cost: 0.015213026393940456\n",
      "Iteration 208, Cost: 0.015103063774975898\n",
      "Iteration 209, Cost: 0.01499464274197815\n",
      "Iteration 210, Cost: 0.01488773174942079\n",
      "Iteration 211, Cost: 0.014782299989557405\n",
      "Iteration 212, Cost: 0.014678317372850276\n",
      "Iteration 213, Cost: 0.014575754508896331\n",
      "Iteration 214, Cost: 0.014474582687838678\n",
      "Iteration 215, Cost: 0.014374773862252669\n",
      "Iteration 216, Cost: 0.014276300629495363\n",
      "Iteration 217, Cost: 0.014179136214507598\n",
      "Iteration 218, Cost: 0.014083254453058061\n",
      "Iteration 219, Cost: 0.013988629775418948\n",
      "Iteration 220, Cost: 0.013895237190462893\n",
      "Iteration 221, Cost: 0.01380305227017123\n",
      "Iteration 222, Cost: 0.013712051134543568\n",
      "Iteration 223, Cost: 0.013622210436899047\n",
      "Iteration 224, Cost: 0.013533507349559711\n",
      "Iteration 225, Cost: 0.013445919549906619\n",
      "Iteration 226, Cost: 0.013359425206799459\n",
      "Iteration 227, Cost: 0.013274002967350701\n",
      "Iteration 228, Cost: 0.013189631944045292\n",
      "Iteration 229, Cost: 0.013106291702197318\n",
      "Iteration 230, Cost: 0.01302396224773497\n",
      "Iteration 231, Cost: 0.01294262401530553\n",
      "Iteration 232, Cost: 0.01286225785669207\n",
      "Iteration 233, Cost: 0.012782845029533946\n",
      "Iteration 234, Cost: 0.012704367186343069\n",
      "Iteration 235, Cost: 0.012626806363808382\n",
      "Iteration 236, Cost: 0.012550144972380885\n",
      "Iteration 237, Cost: 0.012474365786131989\n",
      "Iteration 238, Cost: 0.012399451932877847\n",
      "Iteration 239, Cost: 0.012325386884562793\n",
      "Iteration 240, Cost: 0.012252154447894946\n",
      "Iteration 241, Cost: 0.012179738755227388\n",
      "Iteration 242, Cost: 0.012108124255678396\n",
      "Iteration 243, Cost: 0.012037295706484414\n",
      "Iteration 244, Cost: 0.011967238164579683\n",
      "Iteration 245, Cost: 0.011897936978396572\n",
      "Iteration 246, Cost: 0.011829377779880816\n",
      "Iteration 247, Cost: 0.011761546476716194\n",
      "Iteration 248, Cost: 0.01169442924475315\n",
      "Iteration 249, Cost: 0.011628012520636258\n",
      "Iteration 250, Cost: 0.011562282994625517\n",
      "Iteration 251, Cost: 0.011497227603606653\n",
      "Iteration 252, Cost: 0.011432833524285868\n",
      "Iteration 253, Cost: 0.01136908816656463\n",
      "Iteration 254, Cost: 0.011305979167090304\n",
      "Iteration 255, Cost: 0.011243494382978659\n",
      "Iteration 256, Cost: 0.011181621885704507\n",
      "Iteration 257, Cost: 0.011120349955156861\n",
      "Iteration 258, Cost: 0.011059667073855391\n",
      "Iteration 259, Cost: 0.010999561921324978\n",
      "Iteration 260, Cost: 0.010940023368625576\n",
      "Iteration 261, Cost: 0.010881040473034734\n",
      "Iteration 262, Cost: 0.010822602472880409\n",
      "Iteration 263, Cost: 0.010764698782521927\n",
      "Iteration 264, Cost: 0.010707318987477265\n",
      "Iteration 265, Cost: 0.010650452839695023\n",
      "Iteration 266, Cost: 0.01059409025296979\n",
      "Iteration 267, Cost: 0.01053822129849985\n",
      "Iteration 268, Cost: 0.010482836200586524\n",
      "Iteration 269, Cost: 0.010427925332474648\n",
      "Iteration 270, Cost: 0.010373479212334141\n",
      "Iteration 271, Cost: 0.010319488499382785\n",
      "Iteration 272, Cost: 0.01026594399015082\n",
      "Iteration 273, Cost: 0.01021283661488818\n",
      "Iteration 274, Cost: 0.010160157434115635\n",
      "Iteration 275, Cost: 0.010107897635321442\n",
      "Iteration 276, Cost: 0.010056048529805442\n",
      "Iteration 277, Cost: 0.01000460154967306\n",
      "Iteration 278, Cost: 0.009953548244981857\n",
      "Iteration 279, Cost: 0.009902880281043936\n",
      "Iteration 280, Cost: 0.009852589435887685\n",
      "Iteration 281, Cost: 0.009802667597882949\n",
      "Iteration 282, Cost: 0.00975310676353407\n",
      "Iteration 283, Cost: 0.009703899035445643\n",
      "Iteration 284, Cost: 0.009655036620466368\n",
      "Iteration 285, Cost: 0.009606511828016744\n",
      "Iteration 286, Cost: 0.009558317068606813\n",
      "Iteration 287, Cost: 0.009510444852550577\n",
      "Iteration 288, Cost: 0.009462887788884166\n",
      "Iteration 289, Cost: 0.009415638584495218\n",
      "Iteration 290, Cost: 0.009368690043471298\n",
      "Iteration 291, Cost: 0.009322035066675586\n",
      "Iteration 292, Cost: 0.009275666651558339\n",
      "Iteration 293, Cost: 0.009229577892212962\n",
      "Iteration 294, Cost: 0.009183761979685711\n",
      "Iteration 295, Cost: 0.009138212202548252\n",
      "Iteration 296, Cost: 0.009092921947742366\n",
      "Iteration 297, Cost: 0.00904788470170616\n",
      "Iteration 298, Cost: 0.009003094051790976\n",
      "Iteration 299, Cost: 0.008958543687978104\n",
      "Iteration 300, Cost: 0.008914227404903973\n",
      "Iteration 301, Cost: 0.008870139104202101\n",
      "Iteration 302, Cost: 0.008826272797169462\n",
      "Iteration 303, Cost: 0.008782622607764063\n",
      "Iteration 304, Cost: 0.008739182775939608\n",
      "Iteration 305, Cost: 0.008695947661321859\n",
      "Iteration 306, Cost: 0.008652911747229819\n",
      "Iteration 307, Cost: 0.008610069645043255\n",
      "Iteration 308, Cost: 0.00856741609891603\n",
      "Iteration 309, Cost: 0.008524945990832415\n",
      "Iteration 310, Cost: 0.008482654346001046\n",
      "Iteration 311, Cost: 0.00844053633857828\n",
      "Iteration 312, Cost: 0.00839858729770941\n",
      "Iteration 313, Cost: 0.00835680271387275\n",
      "Iteration 314, Cost: 0.008315178245507579\n",
      "Iteration 315, Cost: 0.00827370972590282\n",
      "Iteration 316, Cost: 0.008232393170318682\n",
      "Iteration 317, Cost: 0.008191224783308693\n",
      "Iteration 318, Cost: 0.008150200966204413\n",
      "Iteration 319, Cost: 0.008109318324719733\n",
      "Iteration 320, Cost: 0.008068573676626157\n",
      "Iteration 321, Cost: 0.00802796405944473\n",
      "Iteration 322, Cost: 0.007987486738094636\n",
      "Iteration 323, Cost: 0.007947139212432702\n",
      "Iteration 324, Cost: 0.007906919224612601\n",
      "Iteration 325, Cost: 0.007866824766187117\n",
      "Iteration 326, Cost: 0.007826854084872064\n",
      "Iteration 327, Cost: 0.007787005690885836\n",
      "Iteration 328, Cost: 0.007747278362774991\n",
      "Iteration 329, Cost: 0.00770767115263314\n",
      "Iteration 330, Cost: 0.007668183390618514\n",
      "Iteration 331, Cost: 0.00762881468867463\n",
      "Iteration 332, Cost: 0.007589564943358834\n",
      "Iteration 333, Cost: 0.007550434337685311\n",
      "Iteration 334, Cost: 0.007511423341892352\n",
      "Iteration 335, Cost: 0.007472532713048693\n",
      "Iteration 336, Cost: 0.007433763493420248\n",
      "Iteration 337, Cost: 0.007395117007527003\n",
      "Iteration 338, Cost: 0.007356594857829956\n",
      "Iteration 339, Cost: 0.0073181989189997805\n",
      "Iteration 340, Cost: 0.007279931330732456\n",
      "Iteration 341, Cost: 0.0072417944890920665\n",
      "Iteration 342, Cost: 0.007203791036377264\n",
      "Iteration 343, Cost: 0.007165923849525367\n",
      "Iteration 344, Cost: 0.0071281960270862445\n",
      "Iteration 345, Cost: 0.007090610874816933\n",
      "Iteration 346, Cost: 0.007053171889966677\n",
      "Iteration 347, Cost: 0.007015882744340848\n",
      "Iteration 348, Cost: 0.006978747266250085\n",
      "Iteration 349, Cost: 0.006941769421467869\n",
      "Iteration 350, Cost: 0.006904953293335341\n",
      "Iteration 351, Cost: 0.006868303062165646\n",
      "Iteration 352, Cost: 0.00683182298411166\n",
      "Iteration 353, Cost: 0.00679551736966984\n",
      "Iteration 354, Cost: 0.0067593905619992645\n",
      "Iteration 355, Cost: 0.006723446915238265\n",
      "Iteration 356, Cost: 0.0066876907730015\n",
      "Iteration 357, Cost: 0.006652126447237659\n",
      "Iteration 358, Cost: 0.006616758197622448\n",
      "Iteration 359, Cost: 0.006581590211653262\n",
      "Iteration 360, Cost: 0.006546626585600809\n",
      "Iteration 361, Cost: 0.0065118713064599544\n",
      "Iteration 362, Cost: 0.0064773282350265434\n",
      "Iteration 363, Cost: 0.006443001090210373\n",
      "Iteration 364, Cost: 0.0064088934346761845\n",
      "Iteration 365, Cost: 0.006375008661885908\n",
      "Iteration 366, Cost: 0.006341349984596026\n",
      "Iteration 367, Cost: 0.00630792042484478\n",
      "Iteration 368, Cost: 0.0062747228054452564\n",
      "Iteration 369, Cost: 0.006241759742982317\n",
      "Iteration 370, Cost: 0.006209033642294444\n",
      "Iteration 371, Cost: 0.006176546692406108\n",
      "Iteration 372, Cost: 0.0061443008638621495\n",
      "Iteration 373, Cost: 0.00611229790740341\n",
      "Iteration 374, Cost: 0.006080539353912345\n",
      "Iteration 375, Cost: 0.006049026515548685\n",
      "Iteration 376, Cost: 0.006017760487988399\n",
      "Iteration 377, Cost: 0.00598674215367423\n",
      "Iteration 378, Cost: 0.00595597218598272\n",
      "Iteration 379, Cost: 0.00592545105421106\n",
      "Iteration 380, Cost: 0.0058951790292868355\n",
      "Iteration 381, Cost: 0.005865156190104849\n",
      "Iteration 382, Cost: 0.005835382430397562\n",
      "Iteration 383, Cost: 0.005805857466048872\n",
      "Iteration 384, Cost: 0.005776580842765108\n",
      "Iteration 385, Cost: 0.0057475519440218815\n",
      "Iteration 386, Cost: 0.005718769999210652\n",
      "Iteration 387, Cost: 0.005690234091914476\n",
      "Iteration 388, Cost: 0.005661943168248352\n",
      "Iteration 389, Cost: 0.005633896045205378\n",
      "Iteration 390, Cost: 0.005606091418955982\n",
      "Iteration 391, Cost: 0.0055785278730533395\n",
      "Iteration 392, Cost: 0.005551203886503741\n",
      "Iteration 393, Cost: 0.005524117841666196\n",
      "Iteration 394, Cost: 0.005497268031950736\n",
      "Iteration 395, Cost: 0.005470652669289817\n",
      "Iteration 396, Cost: 0.005444269891361686\n",
      "Iteration 397, Cost: 0.0054181177685489424\n",
      "Iteration 398, Cost: 0.005392194310619245\n",
      "Iteration 399, Cost: 0.005366497473118698\n",
      "Iteration 400, Cost: 0.00534102516347161\n",
      "Iteration 401, Cost: 0.005315775246783074\n",
      "Iteration 402, Cost: 0.0052907455513434105\n",
      "Iteration 403, Cost: 0.005265933873835568\n",
      "Iteration 404, Cost: 0.005241337984248602\n",
      "Iteration 405, Cost: 0.005216955630501878\n",
      "Iteration 406, Cost: 0.0051927845427860485\n",
      "Iteration 407, Cost: 0.005168822437628024\n",
      "Iteration 408, Cost: 0.005145067021688022\n",
      "Iteration 409, Cost: 0.005121515995297638\n",
      "Iteration 410, Cost: 0.005098167055748321\n",
      "Iteration 411, Cost: 0.005075017900340171\n",
      "Iteration 412, Cost: 0.005052066229201254\n",
      "Iteration 413, Cost: 0.005029309747887733\n",
      "Iteration 414, Cost: 0.005006746169775311\n",
      "Iteration 415, Cost: 0.004984373218252358\n",
      "Converged!\n",
      "Iteration 1, Cost: 0.25195909561247204\n",
      "Iteration 2, Cost: 0.228581515196285\n",
      "Iteration 3, Cost: 0.21298217481000403\n",
      "Iteration 4, Cost: 0.203194678447239\n",
      "Iteration 5, Cost: 0.1962760987678183\n",
      "Iteration 6, Cost: 0.19064005935380676\n",
      "Iteration 7, Cost: 0.18564468043197532\n",
      "Iteration 8, Cost: 0.1810366144865922\n",
      "Iteration 9, Cost: 0.17670808117894102\n",
      "Iteration 10, Cost: 0.17260641902715482\n",
      "Iteration 11, Cost: 0.16870049176768392\n",
      "Iteration 12, Cost: 0.16496811679239562\n",
      "Iteration 13, Cost: 0.16139143098495257\n",
      "Iteration 14, Cost: 0.15795521617998165\n",
      "Iteration 15, Cost: 0.15464626537380294\n",
      "Iteration 16, Cost: 0.15145307889848755\n",
      "Iteration 17, Cost: 0.14836565228012547\n",
      "Iteration 18, Cost: 0.14537528999599586\n",
      "Iteration 19, Cost: 0.14247443422244285\n",
      "Iteration 20, Cost: 0.13965651063147805\n",
      "Iteration 21, Cost: 0.13691579365322235\n",
      "Iteration 22, Cost: 0.134247291461266\n",
      "Iteration 23, Cost: 0.13164664911654783\n",
      "Iteration 24, Cost: 0.12911006730467947\n",
      "Iteration 25, Cost: 0.1266342337776038\n",
      "Iteration 26, Cost: 0.12421626473106405\n",
      "Iteration 27, Cost: 0.1218536537222767\n",
      "Iteration 28, Cost: 0.11954422621784681\n",
      "Iteration 29, Cost: 0.1172860983644411\n",
      "Iteration 30, Cost: 0.11507763903279798\n",
      "Iteration 31, Cost: 0.11291743456541817\n",
      "Iteration 32, Cost: 0.11080425594667916\n",
      "Iteration 33, Cost: 0.10873702831263206\n",
      "Iteration 34, Cost: 0.10671480283677681\n",
      "Iteration 35, Cost: 0.10473673108254528\n",
      "Iteration 36, Cost: 0.10280204191913608\n",
      "Iteration 37, Cost: 0.10091002107008457\n",
      "Iteration 38, Cost: 0.09905999331700238\n",
      "Iteration 39, Cost: 0.09725130732547456\n",
      "Iteration 40, Cost: 0.09548332300490708\n",
      "Iteration 41, Cost: 0.09375540126556343\n",
      "Iteration 42, Cost: 0.09206689599824056\n",
      "Iteration 43, Cost: 0.09041714807709633\n",
      "Iteration 44, Cost: 0.08880548117427928\n",
      "Iteration 45, Cost: 0.08723119917492624\n",
      "Iteration 46, Cost: 0.08569358499027493\n",
      "Iteration 47, Cost: 0.08419190058179878\n",
      "Iteration 48, Cost: 0.08272538802678706\n",
      "Iteration 49, Cost: 0.08129327147216274\n",
      "Iteration 50, Cost: 0.0798947598356137\n",
      "Iteration 51, Cost: 0.07852905011930945\n",
      "Iteration 52, Cost: 0.07719533120079898\n",
      "Iteration 53, Cost: 0.07589278795872963\n",
      "Iteration 54, Cost: 0.07462060557977405\n",
      "Iteration 55, Cost: 0.07337797388080708\n",
      "Iteration 56, Cost: 0.07216409147101262\n",
      "Iteration 57, Cost: 0.07097816957662449\n",
      "Iteration 58, Cost: 0.06981943536040394\n",
      "Iteration 59, Cost: 0.06868713459150076\n",
      "Iteration 60, Cost: 0.06758053355977411\n",
      "Iteration 61, Cost: 0.06649892018012492\n",
      "Iteration 62, Cost: 0.06544160429237075\n",
      "Iteration 63, Cost: 0.06440791722390506\n",
      "Iteration 64, Cost: 0.06339721073791915\n",
      "Iteration 65, Cost: 0.062408855531795905\n",
      "Iteration 66, Cost: 0.06144223947283921\n",
      "Iteration 67, Cost: 0.0604967657593861\n",
      "Iteration 68, Cost: 0.059571851175813983\n",
      "Iteration 69, Cost: 0.05866692457451482\n",
      "Iteration 70, Cost: 0.057781425673199646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 71, Cost: 0.056914804209169306\n",
      "Iteration 72, Cost: 0.056066519449851714\n",
      "Iteration 73, Cost: 0.05523604002550539\n",
      "Iteration 74, Cost: 0.0544228440277626\n",
      "Iteration 75, Cost: 0.05362641930670709\n",
      "Iteration 76, Cost: 0.05284626389789759\n",
      "Iteration 77, Cost: 0.052081886516670756\n",
      "Iteration 78, Cost: 0.051332807067440624\n",
      "Iteration 79, Cost: 0.05059855712805009\n",
      "Iteration 80, Cost: 0.04987868038156226\n",
      "Iteration 81, Cost: 0.049172732978884956\n",
      "Iteration 82, Cost: 0.048480283824592796\n",
      "Iteration 83, Cost: 0.04780091478502716\n",
      "Iteration 84, Cost: 0.04713422082234138\n",
      "Iteration 85, Cost: 0.04647981006093693\n",
      "Iteration 86, Cost: 0.04583730379411719\n",
      "Iteration 87, Cost: 0.04520633643917496\n",
      "Iteration 88, Cost: 0.04458655544888817\n",
      "Iteration 89, Cost: 0.0439776211868094\n",
      "Iteration 90, Cost: 0.043379206773003005\n",
      "Iteration 91, Cost: 0.042790997906140354\n",
      "Iteration 92, Cost: 0.04221269266718115\n",
      "Iteration 93, Cost: 0.041644001309271234\n",
      "Iteration 94, Cost: 0.041084646037973745\n",
      "Iteration 95, Cost: 0.04053436078549701\n",
      "Iteration 96, Cost: 0.03999289098216162\n",
      "Iteration 97, Cost: 0.03945999332792923\n",
      "Iteration 98, Cost: 0.03893543556637275\n",
      "Iteration 99, Cost: 0.03841899626297973\n",
      "Iteration 100, Cost: 0.037910464589138826\n",
      "Iteration 101, Cost: 0.03740964011255906\n",
      "Iteration 102, Cost: 0.03691633259421746\n",
      "Iteration 103, Cost: 0.036430361791236474\n",
      "Iteration 104, Cost: 0.03595155726437371\n",
      "Iteration 105, Cost: 0.03547975818808992\n",
      "Iteration 106, Cost: 0.03501481316047137\n",
      "Iteration 107, Cost: 0.03455658000965329\n",
      "Iteration 108, Cost: 0.03410492559285229\n",
      "Iteration 109, Cost: 0.033659725583702296\n",
      "Iteration 110, Cost: 0.03322086424333045\n",
      "Iteration 111, Cost: 0.032788234170537306\n",
      "Iteration 112, Cost: 0.03236173602658085\n",
      "Iteration 113, Cost: 0.03194127823042494\n",
      "Iteration 114, Cost: 0.03152677662090623\n",
      "Iteration 115, Cost: 0.03111815408309615\n",
      "Iteration 116, Cost: 0.030715340137170796\n",
      "Iteration 117, Cost: 0.030318270489322157\n",
      "Iteration 118, Cost: 0.029926886545606966\n",
      "Iteration 119, Cost: 0.029541134891079936\n",
      "Iteration 120, Cost: 0.029160966738031816\n",
      "Iteration 121, Cost: 0.028786337348577627\n",
      "Iteration 122, Cost: 0.028417205438142946\n",
      "Iteration 123, Cost: 0.028053532567503998\n",
      "Iteration 124, Cost: 0.02769528253188662\n",
      "Iteration 125, Cost: 0.02734242075616771\n",
      "Iteration 126, Cost: 0.02699491370541678\n",
      "Iteration 127, Cost: 0.026652728319848926\n",
      "Iteration 128, Cost: 0.02631583148274153\n",
      "Iteration 129, Cost: 0.0259841895290228\n",
      "Iteration 130, Cost: 0.025657767801117972\n",
      "Iteration 131, Cost: 0.025336530257300264\n",
      "Iteration 132, Cost: 0.025020439136311454\n",
      "Iteration 133, Cost: 0.02470945468046726\n",
      "Iteration 134, Cost: 0.02440353491792212\n",
      "Iteration 135, Cost: 0.024102635503305027\n",
      "Iteration 136, Cost: 0.023806709614612894\n",
      "Iteration 137, Cost: 0.023515707903105077\n",
      "Iteration 138, Cost: 0.02322957849201303\n",
      "Iteration 139, Cost: 0.02294826701917885\n",
      "Iteration 140, Cost: 0.022671716718265747\n",
      "Iteration 141, Cost: 0.022399868532933076\n",
      "Iteration 142, Cost: 0.02213266125831716\n",
      "Iteration 143, Cost: 0.02187003170428039\n",
      "Iteration 144, Cost: 0.02161191487515397\n",
      "Iteration 145, Cost: 0.021358244161071883\n",
      "Iteration 146, Cost: 0.021108951536443543\n",
      "Iteration 147, Cost: 0.02086396776161099\n",
      "Iteration 148, Cost: 0.020623222584257566\n",
      "Iteration 149, Cost: 0.02038664493765701\n",
      "Iteration 150, Cost: 0.02015416313335756\n",
      "Iteration 151, Cost: 0.019925705046371922\n",
      "Iteration 152, Cost: 0.01970119829138129\n",
      "Iteration 153, Cost: 0.01948057038885539\n",
      "Iteration 154, Cost: 0.01926374892033624\n",
      "Iteration 155, Cost: 0.019050661672432603\n",
      "Iteration 156, Cost: 0.01884123676932496\n",
      "Iteration 157, Cost: 0.01863540279379007\n",
      "Iteration 158, Cost: 0.018433088896924358\n",
      "Iteration 159, Cost: 0.018234224896879106\n",
      "Iteration 160, Cost: 0.018038741367023256\n",
      "Iteration 161, Cost: 0.017846569714024756\n",
      "Iteration 162, Cost: 0.0176576422463939\n",
      "Iteration 163, Cost: 0.017471892234064614\n",
      "Iteration 164, Cost: 0.017289253959606912\n",
      "Iteration 165, Cost: 0.017109662761667636\n",
      "Iteration 166, Cost: 0.01693305507123077\n",
      "Iteration 167, Cost: 0.01675936844127448\n",
      "Iteration 168, Cost: 0.016588541570382635\n",
      "Iteration 169, Cost: 0.016420514320844248\n",
      "Iteration 170, Cost: 0.016255227731747764\n",
      "Iteration 171, Cost: 0.016092624027548193\n",
      "Iteration 172, Cost: 0.01593264662255572\n",
      "Iteration 173, Cost: 0.01577524012176463\n",
      "Iteration 174, Cost: 0.015620350318411762\n",
      "Iteration 175, Cost: 0.015467924188625205\n",
      "Iteration 176, Cost: 0.015317909883495958\n",
      "Iteration 177, Cost: 0.01517025671887872\n",
      "Iteration 178, Cost: 0.015024915163202975\n",
      "Iteration 179, Cost: 0.014881836823551294\n",
      "Iteration 180, Cost: 0.014740974430239891\n",
      "Iteration 181, Cost: 0.014602281820115009\n",
      "Iteration 182, Cost: 0.01446571391875971\n",
      "Iteration 183, Cost: 0.01433122672178709\n",
      "Iteration 184, Cost: 0.014198777275379524\n",
      "Iteration 185, Cost: 0.014068323656217735\n",
      "Iteration 186, Cost: 0.01393982495092941\n",
      "Iteration 187, Cost: 0.013813241235173775\n",
      "Iteration 188, Cost: 0.013688533552466532\n",
      "Iteration 189, Cost: 0.013565663892838475\n",
      "Iteration 190, Cost: 0.013444595171410867\n",
      "Iteration 191, Cost: 0.013325291206961477\n",
      "Iteration 192, Cost: 0.01320771670054665\n",
      "Iteration 193, Cost: 0.013091837214236993\n",
      "Iteration 194, Cost: 0.012977619150017368\n",
      "Iteration 195, Cost: 0.01286502972889526\n",
      "Iteration 196, Cost: 0.01275403697025602\n",
      "Iteration 197, Cost: 0.012644609671497978\n",
      "Iteration 198, Cost: 0.012536717387975797\n",
      "Iteration 199, Cost: 0.012430330413276075\n",
      "Iteration 200, Cost: 0.01232541975984525\n",
      "Iteration 201, Cost: 0.01222195713998645\n",
      "Iteration 202, Cost: 0.012119914947238708\n",
      "Iteration 203, Cost: 0.012019266238149114\n",
      "Iteration 204, Cost: 0.011919984714446016\n",
      "Iteration 205, Cost: 0.011822044705619017\n",
      "Iteration 206, Cost: 0.011725421151909594\n",
      "Iteration 207, Cost: 0.011630089587714283\n",
      "Iteration 208, Cost: 0.01153602612540086\n",
      "Iteration 209, Cost: 0.011443207439536519\n",
      "Iteration 210, Cost: 0.011351610751525769\n",
      "Iteration 211, Cost: 0.011261213814654843\n",
      "Iteration 212, Cost: 0.01117199489953827\n",
      "Iteration 213, Cost: 0.011083932779962639\n",
      "Iteration 214, Cost: 0.010997006719121787\n",
      "Iteration 215, Cost: 0.010911196456237105\n",
      "Iteration 216, Cost: 0.010826482193556093\n",
      "Iteration 217, Cost: 0.010742844583721984\n",
      "Iteration 218, Cost: 0.0106602647175068\n",
      "Iteration 219, Cost: 0.010578724111900062\n",
      "Iteration 220, Cost: 0.010498204698545071\n",
      "Iteration 221, Cost: 0.010418688812514525\n",
      "Iteration 222, Cost: 0.010340159181417206\n",
      "Iteration 223, Cost: 0.01026259891482736\n",
      "Iteration 224, Cost: 0.010185991494028287\n",
      "Iteration 225, Cost: 0.010110320762061843\n",
      "Iteration 226, Cost: 0.01003557091407544\n",
      "Iteration 227, Cost: 0.009961726487958187\n",
      "Iteration 228, Cost: 0.009888772355258072\n",
      "Iteration 229, Cost: 0.009816693712371932\n",
      "Iteration 230, Cost: 0.009745476072000285\n",
      "Iteration 231, Cost: 0.009675105254859128\n",
      "Iteration 232, Cost: 0.009605567381640959\n",
      "Iteration 233, Cost: 0.009536848865217433\n",
      "Iteration 234, Cost: 0.009468936403076271\n",
      "Iteration 235, Cost: 0.00940181696998511\n",
      "Iteration 236, Cost: 0.009335477810875268\n",
      "Iteration 237, Cost: 0.009269906433938458\n",
      "Iteration 238, Cost: 0.009205090603929749\n",
      "Iteration 239, Cost: 0.009141018335670218\n",
      "Iteration 240, Cost: 0.009077677887742901\n",
      "Iteration 241, Cost: 0.009015057756375868\n",
      "Iteration 242, Cost: 0.008953146669506357\n",
      "Iteration 243, Cost: 0.008891933581020214\n",
      "Iteration 244, Cost: 0.008831407665160839\n",
      "Iteration 245, Cost: 0.00877155831110229\n",
      "Iteration 246, Cost: 0.008712375117681085\n",
      "Iteration 247, Cost: 0.008653847888281668\n",
      "Iteration 248, Cost: 0.008595966625870418\n",
      "Iteration 249, Cost: 0.008538721528173492\n",
      "Iteration 250, Cost: 0.00848210298299373\n",
      "Iteration 251, Cost: 0.008426101563662159\n",
      "Iteration 252, Cost: 0.00837070802461968\n",
      "Iteration 253, Cost: 0.008315913297124753\n",
      "Iteration 254, Cost: 0.008261708485082953\n",
      "Iteration 255, Cost: 0.008208084860994468\n",
      "Iteration 256, Cost: 0.00815503386201574\n",
      "Iteration 257, Cost: 0.008102547086131538\n",
      "Iteration 258, Cost: 0.008050616288433902\n",
      "Iteration 259, Cost: 0.00799923337750455\n",
      "Iteration 260, Cost: 0.007948390411897413\n",
      "Iteration 261, Cost: 0.007898079596718089\n",
      "Iteration 262, Cost: 0.007848293280297127\n",
      "Iteration 263, Cost: 0.007799023950954152\n",
      "Iteration 264, Cost: 0.00775026423384997\n",
      "Iteration 265, Cost: 0.007702006887923831\n",
      "Iteration 266, Cost: 0.007654244802913209\n",
      "Iteration 267, Cost: 0.007606970996453448\n",
      "Iteration 268, Cost: 0.007560178611254841\n",
      "Iteration 269, Cost: 0.007513860912354675\n",
      "Iteration 270, Cost: 0.007468011284441923\n",
      "Iteration 271, Cost: 0.007422623229252343\n",
      "Iteration 272, Cost: 0.00737769036303179\n",
      "Iteration 273, Cost: 0.0073332064140656595\n",
      "Iteration 274, Cost: 0.0072891652202724515\n",
      "Iteration 275, Cost: 0.0072455607268594405\n",
      "Iteration 276, Cost: 0.00720238698403863\n",
      "Iteration 277, Cost: 0.007159638144801139\n",
      "Iteration 278, Cost: 0.0071173084627482435\n",
      "Iteration 279, Cost: 0.007075392289977409\n",
      "Iteration 280, Cost: 0.007033884075021623\n",
      "Iteration 281, Cost: 0.0069927783608405\n",
      "Iteration 282, Cost: 0.006952069782861577\n",
      "Iteration 283, Cost: 0.006911753067070322\n",
      "Iteration 284, Cost: 0.006871823028147469\n",
      "Iteration 285, Cost: 0.006832274567652235\n",
      "Iteration 286, Cost: 0.006793102672250131\n",
      "Iteration 287, Cost: 0.006754302411984044\n",
      "Iteration 288, Cost: 0.00671586893858736\n",
      "Iteration 289, Cost: 0.006677797483837909\n",
      "Iteration 290, Cost: 0.006640083357951571\n",
      "Iteration 291, Cost: 0.0066027219480144115\n",
      "Iteration 292, Cost: 0.006565708716452257\n",
      "Iteration 293, Cost: 0.0065290391995366515\n",
      "Iteration 294, Cost: 0.006492709005926163\n",
      "Iteration 295, Cost: 0.006456713815242081\n",
      "Iteration 296, Cost: 0.006421049376677509\n",
      "Iteration 297, Cost: 0.006385711507638971\n",
      "Iteration 298, Cost: 0.006350696092419576\n",
      "Iteration 299, Cost: 0.006315999080902948\n",
      "Iteration 300, Cost: 0.0062816164872970095\n",
      "Iteration 301, Cost: 0.006247544388896869\n",
      "Iteration 302, Cost: 0.006213778924875962\n",
      "Iteration 303, Cost: 0.006180316295104746\n",
      "Iteration 304, Cost: 0.006147152758996166\n",
      "Iteration 305, Cost: 0.006114284634377187\n",
      "Iteration 306, Cost: 0.006081708296385714\n",
      "Iteration 307, Cost: 0.006049420176392209\n",
      "Iteration 308, Cost: 0.006017416760945358\n",
      "Iteration 309, Cost: 0.005985694590741182\n",
      "Iteration 310, Cost: 0.0059542502596149184\n",
      "Iteration 311, Cost: 0.005923080413555167\n",
      "Iteration 312, Cost: 0.0058921817497396434\n",
      "Iteration 313, Cost: 0.0058615510155920255\n",
      "Iteration 314, Cost: 0.005831185007859359\n",
      "Iteration 315, Cost: 0.005801080571709449\n",
      "Iteration 316, Cost: 0.0057712345998477904\n",
      "Iteration 317, Cost: 0.005741644031653498\n",
      "Iteration 318, Cost: 0.00571230585233377\n",
      "Iteration 319, Cost: 0.005683217092096434\n",
      "Iteration 320, Cost: 0.00565437482534008\n",
      "Iteration 321, Cost: 0.005625776169861417\n",
      "Iteration 322, Cost: 0.005597418286079336\n",
      "Iteration 323, Cost: 0.005569298376275336\n",
      "Iteration 324, Cost: 0.005541413683849885\n",
      "Iteration 325, Cost: 0.005513761492594321\n",
      "Iteration 326, Cost: 0.0054863391259779215\n",
      "Iteration 327, Cost: 0.00545914394644977\n",
      "Iteration 328, Cost: 0.00543217335475506\n",
      "Iteration 329, Cost: 0.005405424789265503\n",
      "Iteration 330, Cost: 0.005378895725323468\n",
      "Iteration 331, Cost: 0.005352583674599574\n",
      "Iteration 332, Cost: 0.005326486184463356\n",
      "Iteration 333, Cost: 0.005300600837366752\n",
      "Iteration 334, Cost: 0.005274925250240059\n",
      "Iteration 335, Cost: 0.005249457073900108\n",
      "Iteration 336, Cost: 0.005224193992470331\n",
      "Iteration 337, Cost: 0.005199133722812472\n",
      "Iteration 338, Cost: 0.005174274013969661\n",
      "Iteration 339, Cost: 0.0051496126466205735\n",
      "Iteration 340, Cost: 0.005125147432544454\n",
      "Iteration 341, Cost: 0.005100876214096713\n",
      "Iteration 342, Cost: 0.005076796863694882\n",
      "Iteration 343, Cost: 0.005052907283314678\n",
      "Iteration 344, Cost: 0.005029205403995964\n",
      "Iteration 345, Cost: 0.005005689185358348\n",
      "Iteration 346, Cost: 0.004982356615126234\n",
      "Converged!\n",
      "Iteration 1, Cost: 0.3017944791619989\n",
      "Iteration 2, Cost: 0.27458494298234065\n",
      "Iteration 3, Cost: 0.2448035763673259\n",
      "Iteration 4, Cost: 0.2186104751414209\n",
      "Iteration 5, Cost: 0.2000583586760999\n",
      "Iteration 6, Cost: 0.1875761871515051\n",
      "Iteration 7, Cost: 0.1783027982760069\n",
      "Iteration 8, Cost: 0.17060974156796255\n",
      "Iteration 9, Cost: 0.16377558264673492\n",
      "Iteration 10, Cost: 0.15747958759600328\n",
      "Iteration 11, Cost: 0.1515681136152493\n",
      "Iteration 12, Cost: 0.14596210810933724\n",
      "Iteration 13, Cost: 0.14061902799047077\n",
      "Iteration 14, Cost: 0.13551546829047664\n",
      "Iteration 15, Cost: 0.13063827027478872\n",
      "Iteration 16, Cost: 0.12597952253380756\n",
      "Iteration 17, Cost: 0.12153360541402598\n",
      "Iteration 18, Cost: 0.11729545210305097\n",
      "Iteration 19, Cost: 0.113259599848306\n",
      "Iteration 20, Cost: 0.10941976940246656\n",
      "Iteration 21, Cost: 0.10576878689166569\n",
      "Iteration 22, Cost: 0.10229870790480665\n",
      "Iteration 23, Cost: 0.09900103985772352\n",
      "Iteration 24, Cost: 0.09586699087840439\n",
      "Iteration 25, Cost: 0.09288770104152771\n",
      "Iteration 26, Cost: 0.09005443330534232\n",
      "Iteration 27, Cost: 0.08735871652703571\n",
      "Iteration 28, Cost: 0.08479244218349931\n",
      "Iteration 29, Cost: 0.08234792127422018\n",
      "Iteration 30, Cost: 0.08001790977384413\n",
      "Iteration 31, Cost: 0.07779561110897755\n",
      "Iteration 32, Cost: 0.07567466327242596\n",
      "Iteration 33, Cost: 0.07364911686801724\n",
      "Iteration 34, Cost: 0.07171340890619042\n",
      "Iteration 35, Cost: 0.06986233573415082\n",
      "Iteration 36, Cost: 0.06809102720926544\n",
      "Iteration 37, Cost: 0.06639492318974985\n",
      "Iteration 38, Cost: 0.06476975265938725\n",
      "Iteration 39, Cost: 0.06321151531869428\n",
      "Iteration 40, Cost: 0.06171646522552196\n",
      "Iteration 41, Cost: 0.06028109599522159\n",
      "Iteration 42, Cost: 0.05890212711089311\n",
      "Iteration 43, Cost: 0.05757649099191244\n",
      "Iteration 44, Cost: 0.05630132058219049\n",
      "Iteration 45, Cost: 0.05507393732247379\n",
      "Iteration 46, Cost: 0.05389183945097683\n",
      "Iteration 47, Cost: 0.05275269063085685\n",
      "Iteration 48, Cost: 0.05165430893425103\n",
      "Iteration 49, Cost: 0.05059465622602158\n",
      "Iteration 50, Cost: 0.04957182799162145\n",
      "Iteration 51, Cost: 0.04858404364750311\n",
      "Iteration 52, Cost: 0.04762963736297761\n",
      "Iteration 53, Cost: 0.046707049411936166\n",
      "Iteration 54, Cost: 0.045814818062904196\n",
      "Iteration 55, Cost: 0.04495157200728686\n",
      "Iteration 56, Cost: 0.044116023318645246\n",
      "Iteration 57, Cost: 0.04330696093036175\n",
      "Iteration 58, Cost: 0.04252324461489956\n",
      "Iteration 59, Cost: 0.04176379944476676\n",
      "Iteration 60, Cost: 0.04102761071300236\n",
      "Iteration 61, Cost: 0.04031371928930105\n",
      "Iteration 62, Cost: 0.039621217386633316\n",
      "Iteration 63, Cost: 0.03894924471230708\n",
      "Iteration 64, Cost: 0.0382969849768129\n",
      "Iteration 65, Cost: 0.03766366273348516\n",
      "Iteration 66, Cost: 0.0370485405220043\n",
      "Iteration 67, Cost: 0.03645091628906956\n",
      "Iteration 68, Cost: 0.03587012106019058\n",
      "Iteration 69, Cost: 0.035305516837470546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 70, Cost: 0.034756494699456436\n",
      "Iteration 71, Cost: 0.03422247308057444\n",
      "Iteration 72, Cost: 0.03370289620929824\n",
      "Iteration 73, Cost: 0.03319723268595824\n",
      "Iteration 74, Cost: 0.03270497418292941\n",
      "Iteration 75, Cost: 0.03222563425177912\n",
      "Iteration 76, Cost: 0.031758747223761735\n",
      "Iteration 77, Cost: 0.0313038671917727\n",
      "Iteration 78, Cost: 0.03086056706348778\n",
      "Iteration 79, Cost: 0.03042843767689135\n",
      "Iteration 80, Cost: 0.030007086970726887\n",
      "Iteration 81, Cost: 0.029596139203578464\n",
      "Iteration 82, Cost: 0.029195234216315445\n",
      "Iteration 83, Cost: 0.028804026733510034\n",
      "Iteration 84, Cost: 0.028422185700179767\n",
      "Iteration 85, Cost: 0.028049393650826555\n",
      "Iteration 86, Cost: 0.027685346108254304\n",
      "Iteration 87, Cost: 0.027329751010063054\n",
      "Iteration 88, Cost: 0.026982328161051897\n",
      "Iteration 89, Cost: 0.02664280871002926\n",
      "Iteration 90, Cost: 0.026310934649738528\n",
      "Iteration 91, Cost: 0.0259864583387704\n",
      "Iteration 92, Cost: 0.02566914204445932\n",
      "Iteration 93, Cost: 0.025358757505858094\n",
      "Iteration 94, Cost: 0.025055085515958253\n",
      "Iteration 95, Cost: 0.02475791552238018\n",
      "Iteration 96, Cost: 0.024467045245799405\n",
      "Iteration 97, Cost: 0.024182280315409234\n",
      "Iteration 98, Cost: 0.023903433920745662\n",
      "Iteration 99, Cost: 0.02363032647922265\n",
      "Iteration 100, Cost: 0.023362785318743726\n",
      "Iteration 101, Cost: 0.023100644374773018\n",
      "Iteration 102, Cost: 0.02284374390126411\n",
      "Iteration 103, Cost: 0.02259193019486053\n",
      "Iteration 104, Cost: 0.02234505533179699\n",
      "Iteration 105, Cost: 0.022102976916946196\n",
      "Iteration 106, Cost: 0.021865557844472062\n",
      "Iteration 107, Cost: 0.021632666069566833\n",
      "Iteration 108, Cost: 0.02140417439076673\n",
      "Iteration 109, Cost: 0.021179960242358062\n",
      "Iteration 110, Cost: 0.020959905496403697\n",
      "Iteration 111, Cost: 0.020743896273937794\n",
      "Iteration 112, Cost: 0.020531822764894608\n",
      "Iteration 113, Cost: 0.02032357905635552\n",
      "Iteration 114, Cost: 0.02011906296871626\n",
      "Iteration 115, Cost: 0.01991817589939397\n",
      "Iteration 116, Cost: 0.019720822673711245\n",
      "Iteration 117, Cost: 0.019526911402611317\n",
      "Iteration 118, Cost: 0.0193363533468749\n",
      "Iteration 119, Cost: 0.019149062787525418\n",
      "Iteration 120, Cost: 0.01896495690212459\n",
      "Iteration 121, Cost: 0.018783955646675395\n",
      "Iteration 122, Cost: 0.018605981642863385\n",
      "Iteration 123, Cost: 0.018430960070381135\n",
      "Iteration 124, Cost: 0.01825881856409332\n",
      "Iteration 125, Cost: 0.018089487115812486\n",
      "Iteration 126, Cost: 0.01792289798046697\n",
      "Iteration 127, Cost: 0.017758985586453654\n",
      "Iteration 128, Cost: 0.017597686449978604\n",
      "Iteration 129, Cost: 0.01743893909319872\n",
      "Iteration 130, Cost: 0.017282683965986482\n",
      "Iteration 131, Cost: 0.017128863371149192\n",
      "Iteration 132, Cost: 0.016977421392941894\n",
      "Iteration 133, Cost: 0.01682830382872155\n",
      "Iteration 134, Cost: 0.01668145812359694\n",
      "Iteration 135, Cost: 0.01653683330793612\n",
      "Iteration 136, Cost: 0.016394379937599676\n",
      "Iteration 137, Cost: 0.01625405003677439\n",
      "Iteration 138, Cost: 0.016115797043287645\n",
      "Iteration 139, Cost: 0.015979575756288677\n",
      "Iteration 140, Cost: 0.015845342286188023\n",
      "Iteration 141, Cost: 0.01571305400675143\n",
      "Iteration 142, Cost: 0.015582669509249403\n",
      "Iteration 143, Cost: 0.015454148558568003\n",
      "Iteration 144, Cost: 0.015327452051190899\n",
      "Iteration 145, Cost: 0.01520254197496665\n",
      "Iteration 146, Cost: 0.015079381370579311\n",
      "Iteration 147, Cost: 0.014957934294643956\n",
      "Iteration 148, Cost: 0.014838165784352483\n",
      "Iteration 149, Cost: 0.014720041823598328\n",
      "Iteration 150, Cost: 0.014603529310512049\n",
      "Iteration 151, Cost: 0.014488596026342788\n",
      "Iteration 152, Cost: 0.014375210605623715\n",
      "Iteration 153, Cost: 0.014263342507562237\n",
      "Iteration 154, Cost: 0.014152961988598662\n",
      "Iteration 155, Cost: 0.014044040076079485\n",
      "Iteration 156, Cost: 0.013936548542994035\n",
      "Iteration 157, Cost: 0.013830459883725613\n",
      "Iteration 158, Cost: 0.013725747290770533\n",
      "Iteration 159, Cost: 0.013622384632380613\n",
      "Iteration 160, Cost: 0.01352034643108692\n",
      "Iteration 161, Cost: 0.01341960784306443\n",
      "Iteration 162, Cost: 0.013320144638299227\n",
      "Iteration 163, Cost: 0.013221933181521721\n",
      "Iteration 164, Cost: 0.013124950413871182\n",
      "Iteration 165, Cost: 0.013029173835258425\n",
      "Iteration 166, Cost: 0.012934581487395135\n",
      "Iteration 167, Cost: 0.012841151937459995\n",
      "Iteration 168, Cost: 0.012748864262372957\n",
      "Iteration 169, Cost: 0.012657698033650666\n",
      "Iteration 170, Cost: 0.01256763330281719\n",
      "Iteration 171, Cost: 0.012478650587345538\n",
      "Iteration 172, Cost: 0.012390730857106657\n",
      "Iteration 173, Cost: 0.012303855521303742\n",
      "Iteration 174, Cost: 0.012218006415870708\n",
      "Iteration 175, Cost: 0.012133165791314873\n",
      "Iteration 176, Cost: 0.012049316300984688\n",
      "Iteration 177, Cost: 0.011966440989744446\n",
      "Iteration 178, Cost: 0.01188452328303869\n",
      "Iteration 179, Cost: 0.011803546976329961\n",
      "Iteration 180, Cost: 0.011723496224894239\n",
      "Iteration 181, Cost: 0.011644355533959276\n",
      "Iteration 182, Cost: 0.011566109749171726\n",
      "Iteration 183, Cost: 0.011488744047379576\n",
      "Iteration 184, Cost: 0.011412243927717185\n",
      "Iteration 185, Cost: 0.011336595202980691\n",
      "Iteration 186, Cost: 0.011261783991282252\n",
      "Iteration 187, Cost: 0.011187796707972129\n",
      "Iteration 188, Cost: 0.01111462005781799\n",
      "Iteration 189, Cost: 0.011042241027431586\n",
      "Iteration 190, Cost: 0.01097064687793315\n",
      "Iteration 191, Cost: 0.010899825137844517\n",
      "Iteration 192, Cost: 0.010829763596202224\n",
      "Iteration 193, Cost: 0.010760450295882408\n",
      "Iteration 194, Cost: 0.010691873527129586\n",
      "Iteration 195, Cost: 0.010624021821281771\n",
      "Iteration 196, Cost: 0.010556883944684813\n",
      "Iteration 197, Cost: 0.01049044889278905\n",
      "Iteration 198, Cost: 0.010424705884421747\n",
      "Iteration 199, Cost: 0.010359644356229051\n",
      "Iteration 200, Cost: 0.010295253957281505\n",
      "Iteration 201, Cost: 0.010231524543837377\n",
      "Iteration 202, Cost: 0.010168446174258343\n",
      "Iteration 203, Cost: 0.010106009104072252\n",
      "Iteration 204, Cost: 0.010044203781178016\n",
      "Iteration 205, Cost: 0.009983020841187764\n",
      "Iteration 206, Cost: 0.009922451102901688\n",
      "Iteration 207, Cost: 0.009862485563911129\n",
      "Iteration 208, Cost: 0.009803115396325742\n",
      "Iteration 209, Cost: 0.00974433194262058\n",
      "Iteration 210, Cost: 0.009686126711599262\n",
      "Iteration 211, Cost: 0.00962849137446948\n",
      "Iteration 212, Cost: 0.009571417761027197\n",
      "Iteration 213, Cost: 0.009514897855946136\n",
      "Iteration 214, Cost: 0.00945892379516918\n",
      "Iteration 215, Cost: 0.00940348786239855\n",
      "Iteration 216, Cost: 0.009348582485681598\n",
      "Iteration 217, Cost: 0.009294200234089323\n",
      "Iteration 218, Cost: 0.009240333814484714\n",
      "Iteration 219, Cost: 0.00918697606837818\n",
      "Iteration 220, Cost: 0.009134119968867439\n",
      "Iteration 221, Cost: 0.009081758617659233\n",
      "Iteration 222, Cost: 0.009029885242170506\n",
      "Iteration 223, Cost: 0.00897849319270658\n",
      "Iteration 224, Cost: 0.008927575939714072\n",
      "Iteration 225, Cost: 0.008877127071106305\n",
      "Iteration 226, Cost: 0.008827140289659113\n",
      "Iteration 227, Cost: 0.008777609410474885\n",
      "Iteration 228, Cost: 0.008728528358512944\n",
      "Iteration 229, Cost: 0.008679891166184258\n",
      "Iteration 230, Cost: 0.008631691971008581\n",
      "Iteration 231, Cost: 0.00858392501333232\n",
      "Iteration 232, Cost: 0.008536584634105233\n",
      "Iteration 233, Cost: 0.008489665272714376\n",
      "Iteration 234, Cost: 0.008443161464873554\n",
      "Iteration 235, Cost: 0.008397067840566774\n",
      "Iteration 236, Cost: 0.008351379122044065\n",
      "Iteration 237, Cost: 0.008306090121868212\n",
      "Iteration 238, Cost: 0.008261195741010982\n",
      "Iteration 239, Cost: 0.008216690966997335\n",
      "Iteration 240, Cost: 0.00817257087209637\n",
      "Iteration 241, Cost: 0.008128830611557596\n",
      "Iteration 242, Cost: 0.008085465421891282\n",
      "Iteration 243, Cost: 0.00804247061919163\n",
      "Iteration 244, Cost: 0.00799984159750155\n",
      "Iteration 245, Cost: 0.007957573827217903\n",
      "Iteration 246, Cost: 0.00791566285353599\n",
      "Iteration 247, Cost: 0.007874104294932294\n",
      "Iteration 248, Cost: 0.007832893841684284\n",
      "Iteration 249, Cost: 0.0077920272544263446\n",
      "Iteration 250, Cost: 0.007751500362740723\n",
      "Iteration 251, Cost: 0.007711309063782598\n",
      "Iteration 252, Cost: 0.007671449320938234\n",
      "Iteration 253, Cost: 0.0076319171625153755\n",
      "Iteration 254, Cost: 0.007592708680464915\n",
      "Iteration 255, Cost: 0.007553820029132995\n",
      "Iteration 256, Cost: 0.007515247424042682\n",
      "Iteration 257, Cost: 0.007476987140704417\n",
      "Iteration 258, Cost: 0.007439035513454399\n",
      "Iteration 259, Cost: 0.0074013889343201395\n",
      "Iteration 260, Cost: 0.00736404385191249\n",
      "Iteration 261, Cost: 0.00732699677034329\n",
      "Iteration 262, Cost: 0.007290244248168061\n",
      "Iteration 263, Cost: 0.007253782897352954\n",
      "Iteration 264, Cost: 0.007217609382265339\n",
      "Iteration 265, Cost: 0.007181720418687336\n",
      "Iteration 266, Cost: 0.007146112772851737\n",
      "Iteration 267, Cost: 0.007110783260499603\n",
      "Iteration 268, Cost: 0.00707572874595902\n",
      "Iteration 269, Cost: 0.007040946141244414\n",
      "Iteration 270, Cost: 0.007006432405175839\n",
      "Iteration 271, Cost: 0.006972184542517744\n",
      "Iteration 272, Cost: 0.006938199603136638\n",
      "Iteration 273, Cost: 0.0069044746811771805\n",
      "Iteration 274, Cost: 0.006871006914256201\n",
      "Iteration 275, Cost: 0.006837793482674116\n",
      "Iteration 276, Cost: 0.006804831608643346\n",
      "Iteration 277, Cost: 0.006772118555533228\n",
      "Iteration 278, Cost: 0.006739651627131014\n",
      "Iteration 279, Cost: 0.006707428166918504\n",
      "Iteration 280, Cost: 0.006675445557363922\n",
      "Iteration 281, Cost: 0.006643701219228626\n",
      "Iteration 282, Cost: 0.006612192610888246\n",
      "Iteration 283, Cost: 0.006580917227667912\n",
      "Iteration 284, Cost: 0.006549872601191178\n",
      "Iteration 285, Cost: 0.006519056298742278\n",
      "Iteration 286, Cost: 0.006488465922641395\n",
      "Iteration 287, Cost: 0.0064580991096326226\n",
      "Iteration 288, Cost: 0.006427953530284244\n",
      "Iteration 289, Cost: 0.006398026888401052\n",
      "Iteration 290, Cost: 0.006368316920448428\n",
      "Iteration 291, Cost: 0.006338821394987825\n",
      "Iteration 292, Cost: 0.006309538112123435\n",
      "Iteration 293, Cost: 0.0062804649029597065\n",
      "Iteration 294, Cost: 0.0062515996290695075\n",
      "Iteration 295, Cost: 0.006222940181972608\n",
      "Iteration 296, Cost: 0.006194484482624274\n",
      "Iteration 297, Cost: 0.0061662304809137385\n",
      "Iteration 298, Cost: 0.006138176155172264\n",
      "Iteration 299, Cost: 0.006110319511690617\n",
      "Iteration 300, Cost: 0.006082658584245715\n",
      "Iteration 301, Cost: 0.006055191433636218\n",
      "Iteration 302, Cost: 0.006027916147226891\n",
      "Iteration 303, Cost: 0.006000830838501484\n",
      "Iteration 304, Cost: 0.005973933646623997\n",
      "Iteration 305, Cost: 0.0059472227360080916\n",
      "Iteration 306, Cost: 0.005920696295894482\n",
      "Iteration 307, Cost: 0.005894352539936123\n",
      "Iteration 308, Cost: 0.005868189705791046\n",
      "Iteration 309, Cost: 0.005842206054722646\n",
      "Iteration 310, Cost: 0.005816399871207254\n",
      "Iteration 311, Cost: 0.0057907694625488814\n",
      "Iteration 312, Cost: 0.00576531315850094\n",
      "Iteration 313, Cost: 0.0057400293108947845\n",
      "Iteration 314, Cost: 0.005714916293274986\n",
      "Iteration 315, Cost: 0.005689972500541153\n",
      "Iteration 316, Cost: 0.005665196348596161\n",
      "Iteration 317, Cost: 0.005640586274000696\n",
      "Iteration 318, Cost: 0.005616140733633942\n",
      "Iteration 319, Cost: 0.005591858204360334\n",
      "Iteration 320, Cost: 0.005567737182702205\n",
      "Iteration 321, Cost: 0.005543776184518257\n",
      "Iteration 322, Cost: 0.005519973744687707\n",
      "Iteration 323, Cost: 0.005496328416800032\n",
      "Iteration 324, Cost: 0.005472838772850154\n",
      "Iteration 325, Cost: 0.005449503402939034\n",
      "Iteration 326, Cost: 0.005426320914979493\n",
      "Iteration 327, Cost: 0.005403289934407229\n",
      "Iteration 328, Cost: 0.00538040910389689\n",
      "Iteration 329, Cost: 0.005357677083083121\n",
      "Iteration 330, Cost: 0.005335092548286521\n",
      "Iteration 331, Cost: 0.005312654192244356\n",
      "Iteration 332, Cost: 0.0052903607238460205\n",
      "Iteration 333, Cost: 0.005268210867873116\n",
      "Iteration 334, Cost: 0.005246203364744041\n",
      "Iteration 335, Cost: 0.0052243369702630925\n",
      "Iteration 336, Cost: 0.005202610455373915\n",
      "Iteration 337, Cost: 0.005181022605917273\n",
      "Iteration 338, Cost: 0.005159572222393047\n",
      "Iteration 339, Cost: 0.005138258119726396\n",
      "Iteration 340, Cost: 0.005117079127038001\n",
      "Iteration 341, Cost: 0.005096034087418327\n",
      "Iteration 342, Cost: 0.005075121857705848\n",
      "Iteration 343, Cost: 0.005054341308269109\n",
      "Iteration 344, Cost: 0.005033691322792674\n",
      "Iteration 345, Cost: 0.005013170798066753\n",
      "Iteration 346, Cost: 0.004992778643780573\n",
      "Converged!\n",
      "Iteration 1, Cost: 0.3893521931986895\n",
      "Iteration 2, Cost: 0.3590079870234394\n",
      "Iteration 3, Cost: 0.32865125371431564\n",
      "Iteration 4, Cost: 0.2955138777167679\n",
      "Iteration 5, Cost: 0.2628474233051528\n",
      "Iteration 6, Cost: 0.23991779241152564\n",
      "Iteration 7, Cost: 0.22760838270819436\n",
      "Iteration 8, Cost: 0.22040923413462613\n",
      "Iteration 9, Cost: 0.21516506246749204\n",
      "Iteration 10, Cost: 0.2107351363594581\n",
      "Iteration 11, Cost: 0.20670031010583476\n",
      "Iteration 12, Cost: 0.20288794775986163\n",
      "Iteration 13, Cost: 0.19921758225423863\n",
      "Iteration 14, Cost: 0.1956468701174784\n",
      "Iteration 15, Cost: 0.1921510633862266\n",
      "Iteration 16, Cost: 0.1887146954194599\n",
      "Iteration 17, Cost: 0.18532796484497477\n",
      "Iteration 18, Cost: 0.1819849581106184\n",
      "Iteration 19, Cost: 0.1786825876091432\n",
      "Iteration 20, Cost: 0.17541981229710626\n",
      "Iteration 21, Cost: 0.17219698868151814\n",
      "Iteration 22, Cost: 0.16901531260106545\n",
      "Iteration 23, Cost: 0.16587635296227432\n",
      "Iteration 24, Cost: 0.16278168766799522\n",
      "Iteration 25, Cost: 0.15973264715931965\n",
      "Iteration 26, Cost: 0.15673016126945818\n",
      "Iteration 27, Cost: 0.15377469566671254\n",
      "Iteration 28, Cost: 0.15086625793720643\n",
      "Iteration 29, Cost: 0.148004451310111\n",
      "Iteration 30, Cost: 0.14518855564815347\n",
      "Iteration 31, Cost: 0.14241761926207178\n",
      "Iteration 32, Cost: 0.1396905498616418\n",
      "Iteration 33, Cost: 0.13700619737685077\n",
      "Iteration 34, Cost: 0.13436342485259728\n",
      "Iteration 35, Cost: 0.13176116599306562\n",
      "Iteration 36, Cost: 0.12919846935739362\n",
      "Iteration 37, Cost: 0.12667452995396292\n",
      "Iteration 38, Cost: 0.12418870931039867\n",
      "Iteration 39, Cost: 0.12174054521267887\n",
      "Iteration 40, Cost: 0.11932975233888664\n",
      "Iteration 41, Cost: 0.11695621502864864\n",
      "Iteration 42, Cost: 0.1146199734527081\n",
      "Iteration 43, Cost: 0.1123212044774248\n",
      "Iteration 44, Cost: 0.1100601985432685\n",
      "Iteration 45, Cost: 0.10783733387876146\n",
      "Iteration 46, Cost: 0.10565304933885121\n",
      "Iteration 47, Cost: 0.10350781708212639\n",
      "Iteration 48, Cost: 0.1014021161839303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 49, Cost: 0.09933640812739693\n",
      "Iteration 50, Cost: 0.09731111493107954\n",
      "Iteration 51, Cost: 0.09532660047181549\n",
      "Iteration 52, Cost: 0.09338315535697793\n",
      "Iteration 53, Cost: 0.09148098550270632\n",
      "Iteration 54, Cost: 0.08962020439381403\n",
      "Iteration 55, Cost: 0.08780082884444061\n",
      "Iteration 56, Cost: 0.08602277795144056\n",
      "Iteration 57, Cost: 0.08428587483795165\n",
      "Iteration 58, Cost: 0.08258985072333133\n",
      "Iteration 59, Cost: 0.08093435082643238\n",
      "Iteration 60, Cost: 0.07931894160904453\n",
      "Iteration 61, Cost: 0.07774311889094214\n",
      "Iteration 62, Cost: 0.07620631641214558\n",
      "Iteration 63, Cost: 0.07470791447608442\n",
      "Iteration 64, Cost: 0.0732472483737004\n",
      "Iteration 65, Cost: 0.0718236163578827\n",
      "Iteration 66, Cost: 0.07043628700538378\n",
      "Iteration 67, Cost: 0.06908450586579207\n",
      "Iteration 68, Cost: 0.06776750135149437\n",
      "Iteration 69, Cost: 0.06648448986713199\n",
      "Iteration 70, Cost: 0.06523468021111874\n",
      "Iteration 71, Cost: 0.06401727730551016\n",
      "Iteration 72, Cost: 0.06283148532478938\n",
      "Iteration 73, Cost: 0.06167651030038619\n",
      "Iteration 74, Cost: 0.06055156227769928\n",
      "Iteration 75, Cost: 0.05945585709785548\n",
      "Iteration 76, Cost: 0.058388617869117705\n",
      "Iteration 77, Cost: 0.05734907618420279\n",
      "Iteration 78, Cost: 0.05633647313091204\n",
      "Iteration 79, Cost: 0.05535006013518309\n",
      "Iteration 80, Cost: 0.05438909966837547\n",
      "Iteration 81, Cost: 0.053452865844468546\n",
      "Iteration 82, Cost: 0.052540644927848004\n",
      "Iteration 83, Cost: 0.051651735768333935\n",
      "Iteration 84, Cost: 0.05078545017686038\n",
      "Iteration 85, Cost: 0.049941113252552474\n",
      "Iteration 86, Cost: 0.04911806366969718\n",
      "Iteration 87, Cost: 0.04831565393114772\n",
      "Iteration 88, Cost: 0.04753325059297038\n",
      "Iteration 89, Cost: 0.04677023446360677\n",
      "Iteration 90, Cost: 0.04602600077948351\n",
      "Iteration 91, Cost: 0.04529995935786767\n",
      "Iteration 92, Cost: 0.04459153472685996\n",
      "Iteration 93, Cost: 0.04390016623174571\n",
      "Iteration 94, Cost: 0.04322530811649086\n",
      "Iteration 95, Cost: 0.04256642957896289\n",
      "Iteration 96, Cost: 0.04192301479845148\n",
      "Iteration 97, Cost: 0.041294562934230505\n",
      "Iteration 98, Cost: 0.04068058809420078\n",
      "Iteration 99, Cost: 0.040080619273046264\n",
      "Iteration 100, Cost: 0.03949420025978157\n",
      "Iteration 101, Cost: 0.03892088951503582\n",
      "Iteration 102, Cost: 0.03836026001887286\n",
      "Iteration 103, Cost: 0.03781189909037144\n",
      "Iteration 104, Cost: 0.037275408180561166\n",
      "Iteration 105, Cost: 0.036750402640622765\n",
      "Iteration 106, Cost: 0.03623651146750768\n",
      "Iteration 107, Cost: 0.035733377029312036\n",
      "Iteration 108, Cost: 0.03524065477285749\n",
      "Iteration 109, Cost: 0.03475801291598884\n",
      "Iteration 110, Cost: 0.034285132127106085\n",
      "Iteration 111, Cost: 0.033821705194410405\n",
      "Iteration 112, Cost: 0.033367436687269963\n",
      "Iteration 113, Cost: 0.03292204261200807\n",
      "Iteration 114, Cost: 0.0324852500642896\n",
      "Iteration 115, Cost: 0.03205679688013885\n",
      "Iteration 116, Cost: 0.031636431287464975\n",
      "Iteration 117, Cost: 0.031223911559807138\n",
      "Iteration 118, Cost: 0.03081900567383944\n",
      "Iteration 119, Cost: 0.03042149097200096\n",
      "Iteration 120, Cost: 0.03003115383143699\n",
      "Iteration 121, Cost: 0.029647789340257136\n",
      "Iteration 122, Cost: 0.029271200981932914\n",
      "Iteration 123, Cost: 0.028901200328473087\n",
      "Iteration 124, Cost: 0.028537606742829236\n",
      "Iteration 125, Cost: 0.028180247090797965\n",
      "Iteration 126, Cost: 0.027828955462499843\n",
      "Iteration 127, Cost: 0.02748357290333181\n",
      "Iteration 128, Cost: 0.027143947154110108\n",
      "Iteration 129, Cost: 0.026809932399948905\n",
      "Iteration 130, Cost: 0.02648138902725971\n",
      "Iteration 131, Cost: 0.02615818338811186\n",
      "Iteration 132, Cost: 0.02584018757107217\n",
      "Iteration 133, Cost: 0.025527279177545832\n",
      "Iteration 134, Cost: 0.025219341102578327\n",
      "Iteration 135, Cost: 0.024916261319054615\n",
      "Iteration 136, Cost: 0.02461793266425194\n",
      "Iteration 137, Cost: 0.024324252627770344\n",
      "Iteration 138, Cost: 0.024035123139983162\n",
      "Iteration 139, Cost: 0.023750450360316498\n",
      "Iteration 140, Cost: 0.023470144464882176\n",
      "Iteration 141, Cost: 0.02319411943324423\n",
      "Iteration 142, Cost: 0.02292229283438863\n",
      "Iteration 143, Cost: 0.022654585612276668\n",
      "Iteration 144, Cost: 0.022390921871680496\n",
      "Iteration 145, Cost: 0.022131228665309286\n",
      "Iteration 146, Cost: 0.021875435783518197\n",
      "Iteration 147, Cost: 0.021623475548134114\n",
      "Iteration 148, Cost: 0.02137528261211456\n",
      "Iteration 149, Cost: 0.021130793766867457\n",
      "Iteration 150, Cost: 0.020889947759087632\n",
      "Iteration 151, Cost: 0.020652685118907256\n",
      "Iteration 152, Cost: 0.020418948001009875\n",
      "Iteration 153, Cost: 0.020188680040126018\n",
      "Iteration 154, Cost: 0.0199618262220224\n",
      "Iteration 155, Cost: 0.01973833277072977\n",
      "Iteration 156, Cost: 0.019518147052343903\n",
      "Iteration 157, Cost: 0.019301217495299818\n",
      "Iteration 158, Cost: 0.019087493526581345\n",
      "Iteration 159, Cost: 0.018876925522907748\n",
      "Iteration 160, Cost: 0.01866946477555375\n",
      "Iteration 161, Cost: 0.01846506346712746\n",
      "Iteration 162, Cost: 0.01826367465836377\n",
      "Iteration 163, Cost: 0.01806525228279975\n",
      "Iteration 164, Cost: 0.01786975114708839\n",
      "Iteration 165, Cost: 0.017677126934680366\n",
      "Iteration 166, Cost: 0.017487336210657668\n",
      "Iteration 167, Cost: 0.017300336425634395\n",
      "Iteration 168, Cost: 0.017116085916839996\n",
      "Iteration 169, Cost: 0.01693454390475944\n",
      "Iteration 170, Cost: 0.016755670484011602\n",
      "Iteration 171, Cost: 0.01657942660748839\n",
      "Iteration 172, Cost: 0.016405774063139533\n",
      "Iteration 173, Cost: 0.0162346754431572\n",
      "Iteration 174, Cost: 0.016066094105677324\n",
      "Iteration 175, Cost: 0.015899994129457005\n",
      "Iteration 176, Cost: 0.015736340262298143\n",
      "Iteration 177, Cost: 0.015575097864255179\n",
      "Iteration 178, Cost: 0.01541623284688134\n",
      "Iteration 179, Cost: 0.015259711609926339\n",
      "Iteration 180, Cost: 0.015105500976995027\n",
      "Iteration 181, Cost: 0.014953568131710148\n",
      "Iteration 182, Cost: 0.014803880555894208\n",
      "Iteration 183, Cost: 0.014656405971199934\n",
      "Iteration 184, Cost: 0.014511112285482406\n",
      "Iteration 185, Cost: 0.014367967545026904\n",
      "Iteration 186, Cost: 0.014226939893535299\n",
      "Iteration 187, Cost: 0.014087997538540675\n",
      "Iteration 188, Cost: 0.01395110872567677\n",
      "Iteration 189, Cost: 0.013816241720985346\n",
      "Iteration 190, Cost: 0.013683364801212164\n",
      "Iteration 191, Cost: 0.013552446251827898\n",
      "Iteration 192, Cost: 0.013423454372322069\n",
      "Iteration 193, Cost: 0.01329635748816027\n",
      "Iteration 194, Cost: 0.013171123968670735\n",
      "Iteration 195, Cost: 0.013047722250037206\n",
      "Iteration 196, Cost: 0.012926120862519893\n",
      "Iteration 197, Cost: 0.012806288461003812\n",
      "Iteration 198, Cost: 0.012688193857980153\n",
      "Iteration 199, Cost: 0.01257180605809814\n",
      "Iteration 200, Cost: 0.012457094293477328\n",
      "Iteration 201, Cost: 0.012344028059038874\n",
      "Iteration 202, Cost: 0.012232577147194452\n",
      "Iteration 203, Cost: 0.01212271168131895\n",
      "Iteration 204, Cost: 0.012014402147523532\n",
      "Iteration 205, Cost: 0.011907619424336475\n",
      "Iteration 206, Cost: 0.01180233480998681\n",
      "Iteration 207, Cost: 0.011698520047068504\n",
      "Iteration 208, Cost: 0.011596147344439036\n",
      "Iteration 209, Cost: 0.0114951893962744\n",
      "Iteration 210, Cost: 0.011395619398262554\n",
      "Iteration 211, Cost: 0.011297411060968566\n",
      "Iteration 212, Cost: 0.011200538620447622\n",
      "Iteration 213, Cost: 0.011104976846216585\n",
      "Iteration 214, Cost: 0.011010701046722011\n",
      "Iteration 215, Cost: 0.010917687072462604\n",
      "Iteration 216, Cost: 0.010825911316938107\n",
      "Iteration 217, Cost: 0.010735350715605147\n",
      "Iteration 218, Cost: 0.010645982743024618\n",
      "Iteration 219, Cost: 0.010557785408385026\n",
      "Iteration 220, Cost: 0.010470737249583483\n",
      "Iteration 221, Cost: 0.010384817326039967\n",
      "Iteration 222, Cost: 0.010300005210413276\n",
      "Iteration 223, Cost: 0.01021628097937776\n",
      "Iteration 224, Cost: 0.010133625203610132\n",
      "Iteration 225, Cost: 0.010052018937124796\n",
      "Iteration 226, Cost: 0.009971443706085303\n",
      "Iteration 227, Cost: 0.009891881497208436\n",
      "Iteration 228, Cost: 0.009813314745866605\n",
      "Iteration 229, Cost: 0.009735726323983522\n",
      "Iteration 230, Cost: 0.009659099527808222\n",
      "Iteration 231, Cost: 0.009583418065642606\n",
      "Iteration 232, Cost: 0.00950866604558881\n",
      "Iteration 233, Cost: 0.009434827963374193\n",
      "Iteration 234, Cost: 0.00936188869030392\n",
      "Iteration 235, Cost: 0.00928983346138401\n",
      "Iteration 236, Cost: 0.009218647863651076\n",
      "Iteration 237, Cost: 0.009148317824739098\n",
      "Iteration 238, Cost: 0.009078829601708163\n",
      "Iteration 239, Cost: 0.00901016977015533\n",
      "Iteration 240, Cost: 0.008942325213623427\n",
      "Iteration 241, Cost: 0.008875283113319754\n",
      "Iteration 242, Cost: 0.008809030938153339\n",
      "Iteration 243, Cost: 0.008743556435096396\n",
      "Iteration 244, Cost: 0.008678847619872946\n",
      "Iteration 245, Cost: 0.008614892767975535\n",
      "Iteration 246, Cost: 0.0085516804060087\n",
      "Iteration 247, Cost: 0.00848919930335651\n",
      "Iteration 248, Cost: 0.008427438464169852\n",
      "Iteration 249, Cost: 0.008366387119667983\n",
      "Iteration 250, Cost: 0.008306034720748005\n",
      "Iteration 251, Cost: 0.008246370930894907\n",
      "Iteration 252, Cost: 0.008187385619384301\n",
      "Iteration 253, Cost: 0.008129068854769416\n",
      "Iteration 254, Cost: 0.0080714108986435\n",
      "Iteration 255, Cost: 0.00801440219966851\n",
      "Iteration 256, Cost: 0.007958033387860771\n",
      "Iteration 257, Cost: 0.007902295269124174\n",
      "Iteration 258, Cost: 0.007847178820021398\n",
      "Iteration 259, Cost: 0.007792675182773747\n",
      "Iteration 260, Cost: 0.007738775660480141\n",
      "Iteration 261, Cost: 0.007685471712546015\n",
      "Iteration 262, Cost: 0.007632754950312973\n",
      "Iteration 263, Cost: 0.007580617132880234\n",
      "Iteration 264, Cost: 0.007529050163109109\n",
      "Iteration 265, Cost: 0.007478046083801996\n",
      "Iteration 266, Cost: 0.007427597074047587\n",
      "Iteration 267, Cost: 0.0073776954457242605\n",
      "Iteration 268, Cost: 0.007328333640153893\n",
      "Iteration 269, Cost: 0.007279504224898588\n",
      "Iteration 270, Cost: 0.00723119989069311\n",
      "Iteration 271, Cost: 0.00718341344850607\n",
      "Iteration 272, Cost: 0.007136137826723208\n",
      "Iteration 273, Cost: 0.007089366068446375\n",
      "Iteration 274, Cost: 0.007043091328902139\n",
      "Iteration 275, Cost: 0.006997306872954121\n",
      "Iteration 276, Cost: 0.006952006072713552\n",
      "Iteration 277, Cost: 0.006907182405242702\n",
      "Iteration 278, Cost: 0.00686282945034614\n",
      "Iteration 279, Cost: 0.006818940888445024\n",
      "Iteration 280, Cost: 0.0067755104985298764\n",
      "Iteration 281, Cost: 0.006732532156187483\n",
      "Iteration 282, Cost: 0.006689999831697864\n",
      "Iteration 283, Cost: 0.006647907588197386\n",
      "Iteration 284, Cost: 0.006606249579904401\n",
      "Iteration 285, Cost: 0.006565020050403921\n",
      "Iteration 286, Cost: 0.0065242133309880435\n",
      "Iteration 287, Cost: 0.006483823839049081\n",
      "Iteration 288, Cost: 0.0064438460765224755\n",
      "Iteration 289, Cost: 0.006404274628376755\n",
      "Iteration 290, Cost: 0.006365104161147983\n",
      "Iteration 291, Cost: 0.006326329421516271\n",
      "Iteration 292, Cost: 0.006287945234922097\n",
      "Iteration 293, Cost: 0.006249946504220302\n",
      "Iteration 294, Cost: 0.006212328208369773\n",
      "Iteration 295, Cost: 0.0061750854011569534\n",
      "Iteration 296, Cost: 0.006138213209951408\n",
      "Iteration 297, Cost: 0.0061017068344918456\n",
      "Iteration 298, Cost: 0.006065561545701051\n",
      "Iteration 299, Cost: 0.006029772684528308\n",
      "Iteration 300, Cost: 0.005994335660817965\n",
      "Iteration 301, Cost: 0.005959245952202944\n",
      "Iteration 302, Cost: 0.005924499103021975\n",
      "Iteration 303, Cost: 0.005890090723259532\n",
      "Iteration 304, Cost: 0.005856016487507428\n",
      "Iteration 305, Cost: 0.0058222721339471605\n",
      "Iteration 306, Cost: 0.0057888534633521145\n",
      "Iteration 307, Cost: 0.005755756338108812\n",
      "Iteration 308, Cost: 0.005722976681256473\n",
      "Iteration 309, Cost: 0.005690510475544184\n",
      "Iteration 310, Cost: 0.005658353762504982\n",
      "Iteration 311, Cost: 0.00562650264154631\n",
      "Iteration 312, Cost: 0.005594953269056233\n",
      "Iteration 313, Cost: 0.005563701857524889\n",
      "Iteration 314, Cost: 0.005532744674680739\n",
      "Iteration 315, Cost: 0.005502078042641079\n",
      "Iteration 316, Cost: 0.005471698337076466\n",
      "Iteration 317, Cost: 0.005441601986388619\n",
      "Iteration 318, Cost: 0.0054117854709014495\n",
      "Iteration 319, Cost: 0.005382245322064854\n",
      "Iteration 320, Cost: 0.005352978121670991\n",
      "Iteration 321, Cost: 0.005323980501082681\n",
      "Iteration 322, Cost: 0.0052952491404737265\n",
      "Iteration 323, Cost: 0.005266780768080804\n",
      "Iteration 324, Cost: 0.005238572159466765\n",
      "Iteration 325, Cost: 0.005210620136795047\n",
      "Iteration 326, Cost: 0.0051829215681150195\n",
      "Iteration 327, Cost: 0.00515547336665804\n",
      "Iteration 328, Cost: 0.005128272490144026\n",
      "Iteration 329, Cost: 0.00510131594009837\n",
      "Iteration 330, Cost: 0.005074600761179015\n",
      "Iteration 331, Cost: 0.005048124040513515\n",
      "Iteration 332, Cost: 0.00502188290704595\n",
      "Iteration 333, Cost: 0.004995874530893504\n",
      "Converged!\n",
      "Iteration 1, Cost: 0.45896349708252066\n",
      "Iteration 2, Cost: 0.4267254363501281\n",
      "Iteration 3, Cost: 0.38403195039794535\n",
      "Iteration 4, Cost: 0.34201715952825307\n",
      "Iteration 5, Cost: 0.3077582561268357\n",
      "Iteration 6, Cost: 0.27310757102013045\n",
      "Iteration 7, Cost: 0.23565683898083925\n",
      "Iteration 8, Cost: 0.20808298772073816\n",
      "Iteration 9, Cost: 0.19509719048726262\n",
      "Iteration 10, Cost: 0.18822807152292514\n",
      "Iteration 11, Cost: 0.18309537194553455\n",
      "Iteration 12, Cost: 0.17853519114422792\n",
      "Iteration 13, Cost: 0.17422535365224362\n",
      "Iteration 14, Cost: 0.17006340816132282\n",
      "Iteration 15, Cost: 0.16601145095852393\n",
      "Iteration 16, Cost: 0.16205226351900676\n",
      "Iteration 17, Cost: 0.15817589289313042\n",
      "Iteration 18, Cost: 0.15437547228475942\n",
      "Iteration 19, Cost: 0.15064595515351123\n",
      "Iteration 20, Cost: 0.14698373228401412\n",
      "Iteration 21, Cost: 0.14338646262817575\n",
      "Iteration 22, Cost: 0.1398529091968106\n",
      "Iteration 23, Cost: 0.1363827452705859\n",
      "Iteration 24, Cost: 0.13297636416637912\n",
      "Iteration 25, Cost: 0.1296347322543634\n",
      "Iteration 26, Cost: 0.12635929337107765\n",
      "Iteration 27, Cost: 0.12315189658104995\n",
      "Iteration 28, Cost: 0.12001470660310068\n",
      "Iteration 29, Cost: 0.11695007194358047\n",
      "Iteration 30, Cost: 0.11396035427402333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 31, Cost: 0.11104774469473516\n",
      "Iteration 32, Cost: 0.10821409860381745\n",
      "Iteration 33, Cost: 0.10546081290243257\n",
      "Iteration 34, Cost: 0.10278875526859955\n",
      "Iteration 35, Cost: 0.10019824279127704\n",
      "Iteration 36, Cost: 0.09768906000369779\n",
      "Iteration 37, Cost: 0.0952605041824576\n",
      "Iteration 38, Cost: 0.09291144680536423\n",
      "Iteration 39, Cost: 0.09064040237834708\n",
      "Iteration 40, Cost: 0.08844559826543184\n",
      "Iteration 41, Cost: 0.0863250412042394\n",
      "Iteration 42, Cost: 0.08427657778563544\n",
      "Iteration 43, Cost: 0.08229794738889996\n",
      "Iteration 44, Cost: 0.08038682697521492\n",
      "Iteration 45, Cost: 0.07854086780994302\n",
      "Iteration 46, Cost: 0.07675772464571387\n",
      "Iteration 47, Cost: 0.07503507818431217\n",
      "Iteration 48, Cost: 0.07337065177699985\n",
      "Iteration 49, Cost: 0.0717622233534148\n",
      "Iteration 50, Cost: 0.0702076335215009\n",
      "Iteration 51, Cost: 0.0687047906846896\n",
      "Iteration 52, Cost: 0.0672516739020195\n",
      "Iteration 53, Cost: 0.06584633409006153\n",
      "Iteration 54, Cost: 0.0644868940444812\n",
      "Iteration 55, Cost: 0.06317154765087409\n",
      "Iteration 56, Cost: 0.061898558562415246\n",
      "Iteration 57, Cost: 0.06066625854649219\n",
      "Iteration 58, Cost: 0.05947304564284105\n",
      "Iteration 59, Cost: 0.05831738222989626\n",
      "Iteration 60, Cost: 0.05719779306187123\n",
      "Iteration 61, Cost: 0.05611286331428789\n",
      "Iteration 62, Cost: 0.05506123665823233\n",
      "Iteration 63, Cost: 0.054041613371785\n",
      "Iteration 64, Cost: 0.05305274848942318\n",
      "Iteration 65, Cost: 0.0520934499855828\n",
      "Iteration 66, Cost: 0.051162576986129124\n",
      "Iteration 67, Cost: 0.050259038000569994\n",
      "Iteration 68, Cost: 0.049381789167985635\n",
      "Iteration 69, Cost: 0.04852983251050771\n",
      "Iteration 70, Cost: 0.0477022141895286\n",
      "Iteration 71, Cost: 0.04689802276148906\n",
      "Iteration 72, Cost: 0.04611638743196034\n",
      "Iteration 73, Cost: 0.045356476308703174\n",
      "Iteration 74, Cost: 0.044617494656363776\n",
      "Iteration 75, Cost: 0.04389868315736682\n",
      "Iteration 76, Cost: 0.043199316185300804\n",
      "Iteration 77, Cost: 0.042518700098570644\n",
      "Iteration 78, Cost: 0.04185617156323396\n",
      "Iteration 79, Cost: 0.041211095914666686\n",
      "Iteration 80, Cost: 0.04058286556796958\n",
      "Iteration 81, Cost: 0.03997089848680133\n",
      "Iteration 82, Cost: 0.039374636719614914\n",
      "Iteration 83, Cost: 0.038793545011118834\n",
      "Iteration 84, Cost: 0.038227109495257025\n",
      "Iteration 85, Cost: 0.03767483647419746\n",
      "Iteration 86, Cost: 0.03713625128585442\n",
      "Iteration 87, Cost: 0.03661089726046615\n",
      "Iteration 88, Cost: 0.036098334764823756\n",
      "Iteration 89, Cost: 0.03559814033100357\n",
      "Iteration 90, Cost: 0.03510990586497452\n",
      "Iteration 91, Cost: 0.03463323792928881\n",
      "Iteration 92, Cost: 0.034167757093244676\n",
      "Iteration 93, Cost: 0.03371309734343527\n",
      "Iteration 94, Cost: 0.03326890554744258\n",
      "Iteration 95, Cost: 0.03283484096356245\n",
      "Iteration 96, Cost: 0.032410574789802526\n",
      "Iteration 97, Cost: 0.03199578974592439\n",
      "Iteration 98, Cost: 0.03159017968294777\n",
      "Iteration 99, Cost: 0.031193449215245906\n",
      "Iteration 100, Cost: 0.030805313371091905\n",
      "Iteration 101, Cost: 0.030425497258230804\n",
      "Iteration 102, Cost: 0.030053735741721325\n",
      "Iteration 103, Cost: 0.02968977313189832\n",
      "Iteration 104, Cost: 0.029333362880839434\n",
      "Iteration 105, Cost: 0.02898426728617242\n",
      "Iteration 106, Cost: 0.02864225720143539\n",
      "Iteration 107, Cost: 0.028307111752502126\n",
      "Iteration 108, Cost: 0.02797861805981821\n",
      "Iteration 109, Cost: 0.027656570966366985\n",
      "Iteration 110, Cost: 0.027340772771406927\n",
      "Iteration 111, Cost: 0.02703103297010231\n",
      "Iteration 112, Cost: 0.02672716799921515\n",
      "Iteration 113, Cost: 0.026429000989045878\n",
      "Iteration 114, Cost: 0.026136361521809263\n",
      "Iteration 115, Cost: 0.02584908539661671\n",
      "Iteration 116, Cost: 0.02556701440121018\n",
      "Iteration 117, Cost: 0.025289996090561404\n",
      "Iteration 118, Cost: 0.02501788357241413\n",
      "Iteration 119, Cost: 0.02475053529981106\n",
      "Iteration 120, Cost: 0.024487814870610646\n",
      "Iteration 121, Cost: 0.024229590833964882\n",
      "Iteration 122, Cost: 0.023975736503697116\n",
      "Iteration 123, Cost: 0.023726129778490045\n",
      "Iteration 124, Cost: 0.023480652968768743\n",
      "Iteration 125, Cost: 0.02323919263014092\n",
      "Iteration 126, Cost: 0.023001639403237985\n",
      "Iteration 127, Cost: 0.022767887859784643\n",
      "Iteration 128, Cost: 0.022537836354711953\n",
      "Iteration 129, Cost: 0.0223113868841188\n",
      "Iteration 130, Cost: 0.022088444948879436\n",
      "Iteration 131, Cost: 0.02186891942368922\n",
      "Iteration 132, Cost: 0.021652722431337885\n",
      "Iteration 133, Cost: 0.02143976922199809\n",
      "Iteration 134, Cost: 0.021229978057317377\n",
      "Iteration 135, Cost: 0.02102327009910326\n",
      "Iteration 136, Cost: 0.020819569302394036\n",
      "Iteration 137, Cost: 0.020618802312711855\n",
      "Iteration 138, Cost: 0.02042089836729936\n",
      "Iteration 139, Cost: 0.02022578920014688\n",
      "Iteration 140, Cost: 0.020033408950623333\n",
      "Iteration 141, Cost: 0.019843694075530976\n",
      "Iteration 142, Cost: 0.019656583264411134\n",
      "Iteration 143, Cost: 0.019472017357936133\n",
      "Iteration 144, Cost: 0.019289939269230096\n",
      "Iteration 145, Cost: 0.019110293907969953\n",
      "Iteration 146, Cost: 0.018933028107125972\n",
      "Iteration 147, Cost: 0.018758090552209724\n",
      "Iteration 148, Cost: 0.018585431712905862\n",
      "Iteration 149, Cost: 0.01841500377697238\n",
      "Iteration 150, Cost: 0.018246760586302594\n",
      "Iteration 151, Cost: 0.018080657575049966\n",
      "Iteration 152, Cost: 0.017916651709725277\n",
      "Iteration 153, Cost: 0.01775470143118298\n",
      "Iteration 154, Cost: 0.017594766598421366\n",
      "Iteration 155, Cost: 0.01743680843412815\n",
      "Iteration 156, Cost: 0.01728078947190973\n",
      "Iteration 157, Cost: 0.017126673505148796\n",
      "Iteration 158, Cost: 0.016974425537440795\n",
      "Iteration 159, Cost: 0.016824011734565006\n",
      "Iteration 160, Cost: 0.016675399377951032\n",
      "Iteration 161, Cost: 0.016528556819605526\n",
      "Iteration 162, Cost: 0.016383453438468223\n",
      "Iteration 163, Cost: 0.016240059598169102\n",
      "Iteration 164, Cost: 0.016098346606161648\n",
      "Iteration 165, Cost: 0.015958286674209125\n",
      "Iteration 166, Cost: 0.015819852880202705\n",
      "Iteration 167, Cost: 0.01568301913129131\n",
      "Iteration 168, Cost: 0.01554776012830403\n",
      "Iteration 169, Cost: 0.015414051331446195\n",
      "Iteration 170, Cost: 0.015281868927250393\n",
      "Iteration 171, Cost: 0.01515118979676318\n",
      "Iteration 172, Cost: 0.015021991484947865\n",
      "Iteration 173, Cost: 0.014894252171282867\n",
      "Iteration 174, Cost: 0.01476795064153408\n",
      "Iteration 175, Cost: 0.014643066260678643\n",
      "Iteration 176, Cost: 0.01451957894695635\n",
      "Iteration 177, Cost: 0.014397469147023488\n",
      "Iteration 178, Cost: 0.014276717812182926\n",
      "Iteration 179, Cost: 0.01415730637566279\n",
      "Iteration 180, Cost: 0.014039216730915148\n",
      "Iteration 181, Cost: 0.013922431210904963\n",
      "Iteration 182, Cost: 0.013806932568358715\n",
      "Iteration 183, Cost: 0.013692703956941379\n",
      "Iteration 184, Cost: 0.01357972891332975\n",
      "Iteration 185, Cost: 0.013467991340149787\n",
      "Iteration 186, Cost: 0.013357475489745389\n",
      "Iteration 187, Cost: 0.013248165948745819\n",
      "Iteration 188, Cost: 0.013140047623399357\n",
      "Iteration 189, Cost: 0.013033105725640723\n",
      "Iteration 190, Cost: 0.012927325759860532\n",
      "Iteration 191, Cost: 0.012822693510345392\n",
      "Iteration 192, Cost: 0.01271919502935811\n",
      "Iteration 193, Cost: 0.012616816625828238\n",
      "Iteration 194, Cost: 0.012515544854624054\n",
      "Iteration 195, Cost: 0.012415366506378245\n",
      "Iteration 196, Cost: 0.012316268597840347\n",
      "Iteration 197, Cost: 0.012218238362730434\n",
      "Iteration 198, Cost: 0.012121263243069419\n",
      "Iteration 199, Cost: 0.012025330880962604\n",
      "Iteration 200, Cost: 0.011930429110814343\n",
      "Iteration 201, Cost: 0.01183654595195266\n",
      "Iteration 202, Cost: 0.01174366960164395\n",
      "Iteration 203, Cost: 0.011651788428478938\n",
      "Iteration 204, Cost: 0.01156089096611204\n",
      "Iteration 205, Cost: 0.011470965907337467\n",
      "Iteration 206, Cost: 0.011382002098486199\n",
      "Iteration 207, Cost: 0.011293988534128923\n",
      "Iteration 208, Cost: 0.011206914352070932\n",
      "Iteration 209, Cost: 0.011120768828625656\n",
      "Iteration 210, Cost: 0.011035541374154363\n",
      "Iteration 211, Cost: 0.010951221528860055\n",
      "Iteration 212, Cost: 0.010867798958824415\n",
      "Iteration 213, Cost: 0.010785263452277009\n",
      "Iteration 214, Cost: 0.010703604916086603\n",
      "Iteration 215, Cost: 0.010622813372464813\n",
      "Iteration 216, Cost: 0.010542878955872739\n",
      "Iteration 217, Cost: 0.010463791910121664\n",
      "Iteration 218, Cost: 0.01038554258565907\n",
      "Iteration 219, Cost: 0.010308121437031694\n",
      "Iteration 220, Cost: 0.010231519020517436\n",
      "Iteration 221, Cost: 0.010155725991918328\n",
      "Iteration 222, Cost: 0.01008073310450682\n",
      "Iteration 223, Cost: 0.01000653120711793\n",
      "Iteration 224, Cost: 0.009933111242379966\n",
      "Iteration 225, Cost: 0.009860464245076623\n",
      "Iteration 226, Cost: 0.009788581340633503\n",
      "Iteration 227, Cost: 0.009717453743722139\n",
      "Iteration 228, Cost: 0.009647072756974844\n",
      "Iteration 229, Cost: 0.009577429769803749\n",
      "Iteration 230, Cost: 0.009508516257317598\n",
      "Iteration 231, Cost: 0.00944032377932996\n",
      "Iteration 232, Cost: 0.00937284397945267\n",
      "Iteration 233, Cost: 0.009306068584268416\n",
      "Iteration 234, Cost: 0.009239989402576664\n",
      "Iteration 235, Cost: 0.009174598324707014\n",
      "Iteration 236, Cost: 0.009109887321894546\n",
      "Iteration 237, Cost: 0.009045848445711624\n",
      "Iteration 238, Cost: 0.00898247382755093\n",
      "Iteration 239, Cost: 0.008919755678154607\n",
      "Iteration 240, Cost: 0.008857686287184707\n",
      "Iteration 241, Cost: 0.008796258022830064\n",
      "Iteration 242, Cost: 0.00873546333144524\n",
      "Iteration 243, Cost: 0.008675294737217103\n",
      "Iteration 244, Cost: 0.008615744841854964\n",
      "Iteration 245, Cost: 0.00855680632430036\n",
      "Iteration 246, Cost: 0.008498471940452753\n",
      "Iteration 247, Cost: 0.008440734522907683\n",
      "Iteration 248, Cost: 0.00838358698070406\n",
      "Iteration 249, Cost: 0.00832702229907753\n",
      "Iteration 250, Cost: 0.008271033539217069\n",
      "Iteration 251, Cost: 0.008215613838022122\n",
      "Iteration 252, Cost: 0.008160756407857878\n",
      "Iteration 253, Cost: 0.008106454536306384\n",
      "Iteration 254, Cost: 0.008052701585911514\n",
      "Iteration 255, Cost: 0.007999490993915899\n",
      "Iteration 256, Cost: 0.007946816271988169\n",
      "Iteration 257, Cost: 0.007894671005939047\n",
      "Iteration 258, Cost: 0.00784304885542498\n",
      "Iteration 259, Cost: 0.007791943553638183\n",
      "Iteration 260, Cost: 0.007741348906982149\n",
      "Iteration 261, Cost: 0.007691258794731796\n",
      "Iteration 262, Cost: 0.007641667168677607\n",
      "Iteration 263, Cost: 0.00759256805275326\n",
      "Iteration 264, Cost: 0.007543955542646352\n",
      "Iteration 265, Cost: 0.007495823805391962\n",
      "Iteration 266, Cost: 0.0074481670789489345\n",
      "Iteration 267, Cost: 0.007400979671758819\n",
      "Iteration 268, Cost: 0.007354255962287613\n",
      "Iteration 269, Cost: 0.007307990398550374\n",
      "Iteration 270, Cost: 0.007262177497619056\n",
      "Iteration 271, Cost: 0.007216811845113851\n",
      "Iteration 272, Cost: 0.0071718880946784426\n",
      "Iteration 273, Cost: 0.00712740096743967\n",
      "Iteration 274, Cost: 0.007083345251452109\n",
      "Iteration 275, Cost: 0.007039715801128178\n",
      "Iteration 276, Cost: 0.006996507536654337\n",
      "Iteration 277, Cost: 0.006953715443394118\n",
      "Iteration 278, Cost: 0.006911334571278632\n",
      "Iteration 279, Cost: 0.006869360034185302\n",
      "Iteration 280, Cost: 0.006827787009305558\n",
      "Iteration 281, Cost: 0.006786610736502276\n",
      "Iteration 282, Cost: 0.006745826517657721\n",
      "Iteration 283, Cost: 0.006705429716012796\n",
      "Iteration 284, Cost: 0.006665415755498379\n",
      "Iteration 285, Cost: 0.006625780120059579\n",
      "Iteration 286, Cost: 0.006586518352973634\n",
      "Iteration 287, Cost: 0.006547626056162316\n",
      "Iteration 288, Cost: 0.006509098889499573\n",
      "Iteration 289, Cost: 0.006470932570115198\n",
      "Iteration 290, Cost: 0.0064331228716952675\n",
      "Iteration 291, Cost: 0.006395665623780122\n",
      "Iteration 292, Cost: 0.006358556711060557\n",
      "Iteration 293, Cost: 0.006321792072672996\n",
      "Iteration 294, Cost: 0.006285367701494271\n",
      "Iteration 295, Cost: 0.006249279643436739\n",
      "Iteration 296, Cost: 0.006213523996744284\n",
      "Iteration 297, Cost: 0.006178096911289934\n",
      "Iteration 298, Cost: 0.006142994587875572\n",
      "Iteration 299, Cost: 0.0061082132775344055\n",
      "Iteration 300, Cost: 0.006073749280836669\n",
      "Iteration 301, Cost: 0.006039598947199099\n",
      "Iteration 302, Cost: 0.00600575867419868\n",
      "Iteration 303, Cost: 0.005972224906891099\n",
      "Iteration 304, Cost: 0.005938994137134359\n",
      "Iteration 305, Cost: 0.005906062902917962\n",
      "Iteration 306, Cost: 0.005873427787698026\n",
      "Iteration 307, Cost: 0.005841085419738716\n",
      "Iteration 308, Cost: 0.005809032471460283\n",
      "Iteration 309, Cost: 0.005777265658794051\n",
      "Iteration 310, Cost: 0.005745781740544608\n",
      "Iteration 311, Cost: 0.005714577517759454\n",
      "Iteration 312, Cost: 0.005683649833106344\n",
      "Iteration 313, Cost: 0.0056529955702585245\n",
      "Iteration 314, Cost: 0.005622611653288031\n",
      "Iteration 315, Cost: 0.0055924950460672605\n",
      "Iteration 316, Cost: 0.005562642751678865\n",
      "Iteration 317, Cost: 0.005533051811834174\n",
      "Iteration 318, Cost: 0.005503719306300181\n",
      "Iteration 319, Cost: 0.005474642352335213\n",
      "Iteration 320, Cost: 0.005445818104133292\n",
      "Iteration 321, Cost: 0.005417243752277295\n",
      "Iteration 322, Cost: 0.005388916523200861\n",
      "Iteration 323, Cost: 0.00536083367865913\n",
      "Iteration 324, Cost: 0.005332992515208251\n",
      "Iteration 325, Cost: 0.005305390363693635\n",
      "Iteration 326, Cost: 0.0052780245887469835\n",
      "Iteration 327, Cost: 0.00525089258829196\n",
      "Iteration 328, Cost: 0.005223991793058496\n",
      "Iteration 329, Cost: 0.005197319666105662\n",
      "Iteration 330, Cost: 0.005170873702352983\n",
      "Iteration 331, Cost: 0.00514465142812016\n",
      "Iteration 332, Cost: 0.005118650400675051\n",
      "Iteration 333, Cost: 0.005092868207789851\n",
      "Iteration 334, Cost: 0.005067302467305302\n",
      "Iteration 335, Cost: 0.005041950826702872\n",
      "Iteration 336, Cost: 0.0050168109626847245\n",
      "Iteration 337, Cost: 0.004991880580761397\n",
      "Converged!\n",
      "Iteration 1, Cost: 0.39114540408153914\n",
      "Iteration 2, Cost: 0.37002022336864526\n",
      "Iteration 3, Cost: 0.3489951167423398\n",
      "Iteration 4, Cost: 0.3271571303081192\n",
      "Iteration 5, Cost: 0.3033883826245269\n",
      "Iteration 6, Cost: 0.27716947623773514\n",
      "Iteration 7, Cost: 0.24999424489052594\n",
      "Iteration 8, Cost: 0.22631056898177426\n",
      "Iteration 9, Cost: 0.20944446113610937\n",
      "Iteration 10, Cost: 0.19796725446020502\n",
      "Iteration 11, Cost: 0.18928468582556246\n",
      "Iteration 12, Cost: 0.1819917655482895\n",
      "Iteration 13, Cost: 0.17550580366515706\n",
      "Iteration 14, Cost: 0.16957961793544205\n",
      "Iteration 15, Cost: 0.164093006244213\n",
      "Iteration 16, Cost: 0.15897574157154276\n",
      "Iteration 17, Cost: 0.1541789579616216\n",
      "Iteration 18, Cost: 0.14966418842749288\n",
      "Iteration 19, Cost: 0.14539903087588904\n",
      "Iteration 20, Cost: 0.14135534941292313\n",
      "Iteration 21, Cost: 0.1375084503147794\n",
      "Iteration 22, Cost: 0.13383663442387556\n",
      "Iteration 23, Cost: 0.13032089764149035\n",
      "Iteration 24, Cost: 0.12694469171807302\n",
      "Iteration 25, Cost: 0.12369371047011099\n",
      "Iteration 26, Cost: 0.12055568704807638\n",
      "Iteration 27, Cost: 0.1175201965421798\n",
      "Iteration 28, Cost: 0.11457846235043206\n",
      "Iteration 29, Cost: 0.11172316688268367\n",
      "Iteration 30, Cost: 0.10894826826968165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 31, Cost: 0.10624882519890907\n",
      "Iteration 32, Cost: 0.10362083203273885\n",
      "Iteration 33, Cost: 0.10106106611978306\n",
      "Iteration 34, Cost: 0.09856694878492538\n",
      "Iteration 35, Cost: 0.09613642095374558\n",
      "Iteration 36, Cost: 0.09376783379936808\n",
      "Iteration 37, Cost: 0.09145985425366152\n",
      "Iteration 38, Cost: 0.08921138474956308\n",
      "Iteration 39, Cost: 0.08702149619147276\n",
      "Iteration 40, Cost: 0.08488937290237594\n",
      "Iteration 41, Cost: 0.08281426816813105\n",
      "Iteration 42, Cost: 0.08079546897576764\n",
      "Iteration 43, Cost: 0.07883226859987748\n",
      "Iteration 44, Cost: 0.07692394580266144\n",
      "Iteration 45, Cost: 0.07506974955402305\n",
      "Iteration 46, Cost: 0.07326888832781511\n",
      "Iteration 47, Cost: 0.07152052317410272\n",
      "Iteration 48, Cost: 0.0698237638958459\n",
      "Iteration 49, Cost: 0.06817766776728336\n",
      "Iteration 50, Cost: 0.0665812403197822\n",
      "Iteration 51, Cost: 0.0650334377907897\n",
      "Iteration 52, Cost: 0.06353317088600749\n",
      "Iteration 53, Cost: 0.06207930954777882\n",
      "Iteration 54, Cost: 0.06067068845758938\n",
      "Iteration 55, Cost: 0.0593061130306416\n",
      "Iteration 56, Cost: 0.057984365688007064\n",
      "Iteration 57, Cost: 0.05670421221840489\n",
      "Iteration 58, Cost: 0.05546440806795507\n",
      "Iteration 59, Cost: 0.05426370442247124\n",
      "Iteration 60, Cost: 0.053100853972721754\n",
      "Iteration 61, Cost: 0.051974616278097956\n",
      "Iteration 62, Cost: 0.0508837626676971\n",
      "Iteration 63, Cost: 0.04982708063940446\n",
      "Iteration 64, Cost: 0.04880337773671195\n",
      "Iteration 65, Cost: 0.04781148489945422\n",
      "Iteration 66, Cost: 0.04685025929826202\n",
      "Iteration 67, Cost: 0.04591858667335155\n",
      "Iteration 68, Cost: 0.045015383206440045\n",
      "Iteration 69, Cost: 0.04413959696033816\n",
      "Iteration 70, Cost: 0.04329020892441367\n",
      "Iteration 71, Cost: 0.04246623370596928\n",
      "Iteration 72, Cost: 0.04166671990795259\n",
      "Iteration 73, Cost: 0.04089075023262848\n",
      "Iteration 74, Cost: 0.04013744134917481\n",
      "Iteration 75, Cost: 0.03940594356086048\n",
      "Iteration 76, Cost: 0.03869544030474277\n",
      "Iteration 77, Cost: 0.03800514751385506\n",
      "Iteration 78, Cost: 0.03733431286878795\n",
      "Iteration 79, Cost: 0.03668221496250721\n",
      "Iteration 80, Cost: 0.03604816239928531\n",
      "Iteration 81, Cost: 0.035431492845808626\n",
      "Iteration 82, Cost: 0.034831572049902386\n",
      "Iteration 83, Cost: 0.03424779283991424\n",
      "Iteration 84, Cost: 0.03367957411562951\n",
      "Iteration 85, Cost: 0.03312635983965813\n",
      "Iteration 86, Cost: 0.03258761803653313\n",
      "Iteration 87, Cost: 0.0320628398052824\n",
      "Iteration 88, Cost: 0.03155153834996651\n",
      "Iteration 89, Cost: 0.03105324803160089\n",
      "Iteration 90, Cost: 0.030567523443982125\n",
      "Iteration 91, Cost: 0.030093938515198666\n",
      "Iteration 92, Cost: 0.02963208563600821\n",
      "Iteration 93, Cost: 0.029181574815788848\n",
      "Iteration 94, Cost: 0.02874203286640322\n",
      "Iteration 95, Cost: 0.028313102614037803\n",
      "Iteration 96, Cost: 0.027894442138878428\n",
      "Iteration 97, Cost: 0.027485724042345127\n",
      "Iteration 98, Cost: 0.027086634741520874\n",
      "Iteration 99, Cost: 0.02669687379036034\n",
      "Iteration 100, Cost: 0.026316153227244767\n",
      "Iteration 101, Cost: 0.025944196948450016\n",
      "Iteration 102, Cost: 0.025580740107108847\n",
      "Iteration 103, Cost: 0.025225528537268983\n",
      "Iteration 104, Cost: 0.024878318202670998\n",
      "Iteration 105, Cost: 0.024538874669889264\n",
      "Iteration 106, Cost: 0.024206972605493166\n",
      "Iteration 107, Cost: 0.023882395296891228\n",
      "Iteration 108, Cost: 0.023564934196517076\n",
      "Iteration 109, Cost: 0.02325438848900241\n",
      "Iteration 110, Cost: 0.0229505646809581\n",
      "Iteration 111, Cost: 0.022653276212951944\n",
      "Iteration 112, Cost: 0.022362343093230264\n",
      "Iteration 113, Cost: 0.02207759155268326\n",
      "Iteration 114, Cost: 0.02179885372050184\n",
      "Iteration 115, Cost: 0.021525967319918807\n",
      "Iteration 116, Cost: 0.021258775383372234\n",
      "Iteration 117, Cost: 0.02099712598637451\n",
      "Iteration 118, Cost: 0.02074087199932039\n",
      "Iteration 119, Cost: 0.020489870856421375\n",
      "Iteration 120, Cost: 0.02024398434091473\n",
      "Iteration 121, Cost: 0.02000307838566385\n",
      "Iteration 122, Cost: 0.01976702288824324\n",
      "Iteration 123, Cost: 0.019535691539587355\n",
      "Iteration 124, Cost: 0.01930896166527701\n",
      "Iteration 125, Cost: 0.019086714078541265\n",
      "Iteration 126, Cost: 0.01886883294406485\n",
      "Iteration 127, Cost: 0.018655205651712095\n",
      "Iteration 128, Cost: 0.018445722699306222\n",
      "Iteration 129, Cost: 0.018240277583637252\n",
      "Iteration 130, Cost: 0.018038766698911846\n",
      "Iteration 131, Cost: 0.017841089241902693\n",
      "Iteration 132, Cost: 0.01764714712310284\n",
      "Iteration 133, Cost: 0.01745684488324056\n",
      "Iteration 134, Cost: 0.017270089614561893\n",
      "Iteration 135, Cost: 0.01708679088633991\n",
      "Iteration 136, Cost: 0.0169068606741216\n",
      "Iteration 137, Cost: 0.016730213292273664\n",
      "Iteration 138, Cost: 0.016556765329437766\n",
      "Iteration 139, Cost: 0.01638643558655214\n",
      "Iteration 140, Cost: 0.016219145017141123\n",
      "Iteration 141, Cost: 0.016054816669614922\n",
      "Iteration 142, Cost: 0.015893375631360764\n",
      "Iteration 143, Cost: 0.015734748974440983\n",
      "Iteration 144, Cost: 0.015578865702745826\n",
      "Iteration 145, Cost: 0.01542565670047693\n",
      "Iteration 146, Cost: 0.015275054681863057\n",
      "Iteration 147, Cost: 0.01512699414203167\n",
      "Iteration 148, Cost: 0.014981411308979587\n",
      "Iteration 149, Cost: 0.014838244096602428\n",
      "Iteration 150, Cost: 0.01469743205875681\n",
      "Iteration 151, Cost: 0.014558916344340962\n",
      "Iteration 152, Cost: 0.014422639653389104\n",
      "Iteration 153, Cost: 0.014288546194182749\n",
      "Iteration 154, Cost: 0.014156581641388062\n",
      "Iteration 155, Cost: 0.014026693095232929\n",
      "Iteration 156, Cost: 0.013898829041740638\n",
      "Iteration 157, Cost: 0.01377293931403906\n",
      "Iteration 158, Cost: 0.013648975054765285\n",
      "Iteration 159, Cost: 0.01352688867958584\n",
      "Iteration 160, Cost: 0.013406633841852198\n",
      "Iteration 161, Cost: 0.013288165398410044\n",
      "Iteration 162, Cost: 0.01317143937657925\n",
      "Iteration 163, Cost: 0.01305641294231954\n",
      "Iteration 164, Cost: 0.012943044369594425\n",
      "Iteration 165, Cost: 0.012831293010943497\n",
      "Iteration 166, Cost: 0.012721119269270312\n",
      "Iteration 167, Cost: 0.012612484570850215\n",
      "Iteration 168, Cost: 0.012505351339559304\n",
      "Iteration 169, Cost: 0.012399682972322627\n",
      "Iteration 170, Cost: 0.01229544381577627\n",
      "Iteration 171, Cost: 0.012192599144134812\n",
      "Iteration 172, Cost: 0.012091115138251955\n",
      "Iteration 173, Cost: 0.01199095886585866\n",
      "Iteration 174, Cost: 0.011892098262959524\n",
      "Iteration 175, Cost: 0.011794502116364212\n",
      "Iteration 176, Cost: 0.011698140047327006\n",
      "Iteration 177, Cost: 0.01160298249626344\n",
      "Iteration 178, Cost: 0.011509000708508906\n",
      "Iteration 179, Cost: 0.011416166721079776\n",
      "Iteration 180, Cost: 0.011324453350393178\n",
      "Iteration 181, Cost: 0.01123383418089704\n",
      "Iteration 182, Cost: 0.011144283554557266\n",
      "Iteration 183, Cost: 0.011055776561144145\n",
      "Iteration 184, Cost: 0.010968289029255217\n",
      "Iteration 185, Cost: 0.010881797518006909\n",
      "Iteration 186, Cost: 0.01079627930932227\n",
      "Iteration 187, Cost: 0.01071171240073734\n",
      "Iteration 188, Cost: 0.01062807549864393\n",
      "Iteration 189, Cost: 0.010545348011882\n",
      "Iteration 190, Cost: 0.01046351004559065\n",
      "Iteration 191, Cost: 0.010382542395222873\n",
      "Iteration 192, Cost: 0.010302426540625853\n",
      "Iteration 193, Cost: 0.010223144640085858\n",
      "Iteration 194, Cost: 0.010144679524234868\n",
      "Iteration 195, Cost: 0.01006701468971483\n",
      "Iteration 196, Cost: 0.009990134292495423\n",
      "Iteration 197, Cost: 0.009914023140742023\n",
      "Iteration 198, Cost: 0.0098386666871329\n",
      "Iteration 199, Cost: 0.009764051020527987\n",
      "Iteration 200, Cost: 0.009690162856896493\n",
      "Iteration 201, Cost: 0.009616989529416985\n",
      "Iteration 202, Cost: 0.00954451897767117\n",
      "Iteration 203, Cost: 0.009472739735862011\n",
      "Iteration 204, Cost: 0.009401640919997427\n",
      "Iteration 205, Cost: 0.009331212213992853\n",
      "Iteration 206, Cost: 0.009261443854659298\n",
      "Iteration 207, Cost: 0.009192326615557968\n",
      "Iteration 208, Cost: 0.009123851789717911\n",
      "Iteration 209, Cost: 0.009056011171229174\n",
      "Iteration 210, Cost: 0.008988797035740603\n",
      "Iteration 211, Cost: 0.008922202119908062\n",
      "Iteration 212, Cost: 0.008856219599855556\n",
      "Iteration 213, Cost: 0.00879084306872783\n",
      "Iteration 214, Cost: 0.008726066513428428\n",
      "Iteration 215, Cost: 0.008661884290651652\n",
      "Iteration 216, Cost: 0.008598291102329576\n",
      "Iteration 217, Cost: 0.008535281970626734\n",
      "Iteration 218, Cost: 0.008472852212624294\n",
      "Iteration 219, Cost: 0.008410997414842776\n",
      "Iteration 220, Cost: 0.008349713407757234\n",
      "Iteration 221, Cost: 0.008288996240461295\n",
      "Iteration 222, Cost: 0.008228842155636417\n",
      "Iteration 223, Cost: 0.008169247564980237\n",
      "Iteration 224, Cost: 0.008110209025243066\n",
      "Iteration 225, Cost: 0.008051723215014347\n",
      "Iteration 226, Cost: 0.007993786912391674\n",
      "Iteration 227, Cost: 0.007936396973653833\n",
      "Iteration 228, Cost: 0.007879550313046537\n",
      "Iteration 229, Cost: 0.00782324388377546\n",
      "Iteration 230, Cost: 0.0077674746602860905\n",
      "Iteration 231, Cost: 0.00771223962189413\n",
      "Iteration 232, Cost: 0.007657535737814045\n",
      "Iteration 233, Cost: 0.007603359953617208\n",
      "Iteration 234, Cost: 0.007549709179135211\n",
      "Iteration 235, Cost: 0.007496580277808506\n",
      "Iteration 236, Cost: 0.007443970057466152\n",
      "Iteration 237, Cost: 0.007391875262508718\n",
      "Iteration 238, Cost: 0.007340292567454321\n",
      "Iteration 239, Cost: 0.00728921857179657\n",
      "Iteration 240, Cost: 0.007238649796113788\n",
      "Iteration 241, Cost: 0.0071885826793607705\n",
      "Iteration 242, Cost: 0.007139013577267769\n",
      "Iteration 243, Cost: 0.007089938761766391\n",
      "Iteration 244, Cost: 0.007041354421358449\n",
      "Iteration 245, Cost: 0.0069932566623416145\n",
      "Iteration 246, Cost: 0.006945641510804777\n",
      "Iteration 247, Cost: 0.00689850491530626\n",
      "Iteration 248, Cost: 0.0068518427501493935\n",
      "Iteration 249, Cost: 0.006805650819172143\n",
      "Iteration 250, Cost: 0.006759924859970531\n",
      "Iteration 251, Cost: 0.0067146605484793365\n",
      "Iteration 252, Cost: 0.006669853503837751\n",
      "Iteration 253, Cost: 0.006625499293472368\n",
      "Iteration 254, Cost: 0.006581593438334738\n",
      "Iteration 255, Cost: 0.006538131418236004\n",
      "Iteration 256, Cost: 0.006495108677226169\n",
      "Iteration 257, Cost: 0.006452520628970938\n",
      "Iteration 258, Cost: 0.006410362662084124\n",
      "Iteration 259, Cost: 0.006368630145378658\n",
      "Iteration 260, Cost: 0.006327318433004128\n",
      "Iteration 261, Cost: 0.006286422869443248\n",
      "Iteration 262, Cost: 0.006245938794344179\n",
      "Iteration 263, Cost: 0.006205861547169516\n",
      "Iteration 264, Cost: 0.006166186471646625\n",
      "Iteration 265, Cost: 0.006126908920007418\n",
      "Iteration 266, Cost: 0.006088024257008804\n",
      "Iteration 267, Cost: 0.006049527863727912\n",
      "Iteration 268, Cost: 0.006011415141128635\n",
      "Iteration 269, Cost: 0.005973681513398414\n",
      "Iteration 270, Cost: 0.00593632243105601\n",
      "Iteration 271, Cost: 0.005899333373832832\n",
      "Iteration 272, Cost: 0.005862709853331734\n",
      "Iteration 273, Cost: 0.005826447415468556\n",
      "Iteration 274, Cost: 0.00579054164270256\n",
      "Iteration 275, Cost: 0.00575498815606288\n",
      "Iteration 276, Cost: 0.005719782616978661\n",
      "Iteration 277, Cost: 0.005684920728921108\n",
      "Iteration 278, Cost: 0.005650398238866059\n",
      "Iteration 279, Cost: 0.005616210938585858\n",
      "Iteration 280, Cost: 0.005582354665779519\n",
      "Iteration 281, Cost: 0.005548825305050173\n",
      "Iteration 282, Cost: 0.005515618788738746\n",
      "Iteration 283, Cost: 0.005482731097622749\n",
      "Iteration 284, Cost: 0.005450158261488843\n",
      "Iteration 285, Cost: 0.005417896359587678\n",
      "Iteration 286, Cost: 0.005385941520979225\n",
      "Iteration 287, Cost: 0.005354289924776545\n",
      "Iteration 288, Cost: 0.005322937800295671\n",
      "Iteration 289, Cost: 0.0052918814271188715\n",
      "Iteration 290, Cost: 0.005261117135078362\n",
      "Iteration 291, Cost: 0.00523064130416703\n",
      "Iteration 292, Cost: 0.0052004503643825715\n",
      "Iteration 293, Cost: 0.005170540795510914\n",
      "Iteration 294, Cost: 0.005140909126854641\n",
      "Iteration 295, Cost: 0.005111551936911635\n",
      "Iteration 296, Cost: 0.00508246585300894\n",
      "Iteration 297, Cost: 0.0050536475508965025\n",
      "Iteration 298, Cost: 0.005025093754305097\n",
      "Iteration 299, Cost: 0.004996801234472505\n",
      "Converged!\n",
      "Iteration 1, Cost: 0.30257006332148356\n",
      "Iteration 2, Cost: 0.2792230561127538\n",
      "Iteration 3, Cost: 0.2576284904771391\n",
      "Iteration 4, Cost: 0.24097997540692798\n",
      "Iteration 5, Cost: 0.229456637751594\n",
      "Iteration 6, Cost: 0.2212572740492567\n",
      "Iteration 7, Cost: 0.21479274676749036\n",
      "Iteration 8, Cost: 0.2092092753588274\n",
      "Iteration 9, Cost: 0.20412098808132742\n",
      "Iteration 10, Cost: 0.19936210586085804\n",
      "Iteration 11, Cost: 0.19485938484734594\n",
      "Iteration 12, Cost: 0.19057488372542727\n",
      "Iteration 13, Cost: 0.18648214301056748\n",
      "Iteration 14, Cost: 0.1825576671852981\n",
      "Iteration 15, Cost: 0.1787790408444757\n",
      "Iteration 16, Cost: 0.17512541298999235\n",
      "Iteration 17, Cost: 0.1715783615912362\n",
      "Iteration 18, Cost: 0.16812240393407446\n",
      "Iteration 19, Cost: 0.16474505064364864\n",
      "Iteration 20, Cost: 0.16143654398619575\n",
      "Iteration 21, Cost: 0.15818945453638256\n",
      "Iteration 22, Cost: 0.15499826127439867\n",
      "Iteration 23, Cost: 0.15185897878940582\n",
      "Iteration 24, Cost: 0.148768849634514\n",
      "Iteration 25, Cost: 0.14572609502950876\n",
      "Iteration 26, Cost: 0.14272970874376747\n",
      "Iteration 27, Cost: 0.13977928067571213\n",
      "Iteration 28, Cost: 0.13687484260071645\n",
      "Iteration 29, Cost: 0.1340167345077301\n",
      "Iteration 30, Cost: 0.13120549336316983\n",
      "Iteration 31, Cost: 0.12844176631467816\n",
      "Iteration 32, Cost: 0.12572624808410574\n",
      "Iteration 33, Cost: 0.12305963922414778\n",
      "Iteration 34, Cost: 0.12044261959529019\n",
      "Iteration 35, Cost: 0.1178758307278707\n",
      "Iteration 36, Cost: 0.11535986167287475\n",
      "Iteration 37, Cost: 0.11289523490966828\n",
      "Iteration 38, Cost: 0.11048239107235518\n",
      "Iteration 39, Cost: 0.1081216730319169\n",
      "Iteration 40, Cost: 0.10581331089267687\n",
      "Iteration 41, Cost: 0.10355740970092363\n",
      "Iteration 42, Cost: 0.10135394130481519\n",
      "Iteration 43, Cost: 0.0992027411233176\n",
      "Iteration 44, Cost: 0.09710350984274069\n",
      "Iteration 45, Cost: 0.09505581945385948\n",
      "Iteration 46, Cost: 0.09305912266658822\n",
      "Iteration 47, Cost: 0.0911127646030852\n",
      "Iteration 48, Cost: 0.08921599572676726\n",
      "Iteration 49, Cost: 0.08736798514139998\n",
      "Iteration 50, Cost: 0.08556783361967917\n",
      "Iteration 51, Cost: 0.08381458594003903\n",
      "Iteration 52, Cost: 0.08210724229174943\n",
      "Iteration 53, Cost: 0.0804447686398448\n",
      "Iteration 54, Cost: 0.07882610602539003\n",
      "Iteration 55, Cost: 0.07725017882291123\n",
      "Iteration 56, Cost: 0.07571590199761236\n",
      "Iteration 57, Cost: 0.07422218741127624\n",
      "Iteration 58, Cost: 0.0727679492258873\n",
      "Iteration 59, Cost: 0.07135210845346357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 60, Cost: 0.06997359670223148\n",
      "Iteration 61, Cost: 0.06863135917404452\n",
      "Iteration 62, Cost: 0.06732435697543043\n",
      "Iteration 63, Cost: 0.0660515688136403\n",
      "Iteration 64, Cost: 0.06481199215799946\n",
      "Iteration 65, Cost: 0.06360464395414128\n",
      "Iteration 66, Cost: 0.062428560983009494\n",
      "Iteration 67, Cost: 0.061282799956945765\n",
      "Iteration 68, Cost: 0.06016643744136834\n",
      "Iteration 69, Cost: 0.059078569682666365\n",
      "Iteration 70, Cost: 0.05801831241162489\n",
      "Iteration 71, Cost: 0.056984800677944154\n",
      "Iteration 72, Cost: 0.055977188756389344\n",
      "Iteration 73, Cost: 0.05499465014998983\n",
      "Iteration 74, Cost: 0.05403637770155598\n",
      "Iteration 75, Cost: 0.053101583812424455\n",
      "Iteration 76, Cost: 0.05218950075732075\n",
      "Iteration 77, Cost: 0.05129938107679107\n",
      "Iteration 78, Cost: 0.0504304980237928\n",
      "Iteration 79, Cost: 0.04958214603852874\n",
      "Iteration 80, Cost: 0.04875364122511518\n",
      "Iteration 81, Cost: 0.047944321804764885\n",
      "Iteration 82, Cost: 0.047153548522414886\n",
      "Iteration 83, Cost: 0.04638070498673075\n",
      "Iteration 84, Cost: 0.045625197926836406\n",
      "Iteration 85, Cost: 0.04488645735267891\n",
      "Iteration 86, Cost: 0.044163936609448236\n",
      "Iteration 87, Cost: 0.04345711231980445\n",
      "Iteration 88, Cost: 0.04276548421074948\n",
      "Iteration 89, Cost: 0.04208857482478875\n",
      "Iteration 90, Cost: 0.04142592911756216\n",
      "Iteration 91, Cost: 0.040777113946399726\n",
      "Iteration 92, Cost: 0.0401417174562964\n",
      "Iteration 93, Cost: 0.039519348371622326\n",
      "Iteration 94, Cost: 0.038909635203495\n",
      "Iteration 95, Cost: 0.038312225384135506\n",
      "Iteration 96, Cost: 0.03772678434069315\n",
      "Iteration 97, Cost: 0.03715299452192358\n",
      "Iteration 98, Cost: 0.03659055439170987\n",
      "Iteration 99, Cost: 0.03603917740368776\n",
      "Iteration 100, Cost: 0.03549859097114862\n",
      "Iteration 101, Cost: 0.03496853544592713\n",
      "Iteration 102, Cost: 0.03444876311914057\n",
      "Iteration 103, Cost: 0.033939037255453774\n",
      "Iteration 104, Cost: 0.033439131171043815\n",
      "Iteration 105, Cost: 0.03294882736369808\n",
      "Iteration 106, Cost: 0.03246791670157719\n",
      "Iteration 107, Cost: 0.03199619767520124\n",
      "Iteration 108, Cost: 0.03153347571526536\n",
      "Iteration 109, Cost: 0.03107956257704341\n",
      "Iteration 110, Cost: 0.03063427579047044\n",
      "Iteration 111, Cost: 0.030197438173561332\n",
      "Iteration 112, Cost: 0.029768877405661042\n",
      "Iteration 113, Cost: 0.02934842565614726\n",
      "Iteration 114, Cost: 0.02893591926361608\n",
      "Iteration 115, Cost: 0.02853119846025732\n",
      "Iteration 116, Cost: 0.028134107136036993\n",
      "Iteration 117, Cost: 0.027744492637412584\n",
      "Iteration 118, Cost: 0.027362205595569514\n",
      "Iteration 119, Cost: 0.02698709977954247\n",
      "Iteration 120, Cost: 0.026619031970033968\n",
      "Iteration 121, Cost: 0.026257861850229512\n",
      "Iteration 122, Cost: 0.025903451910405103\n",
      "Iteration 123, Cost: 0.025555667363606156\n",
      "Iteration 124, Cost: 0.02521437607013071\n",
      "Iteration 125, Cost: 0.024879448468962267\n",
      "Iteration 126, Cost: 0.024550757514663773\n",
      "Iteration 127, Cost: 0.024228178618560726\n",
      "Iteration 128, Cost: 0.023911589593309204\n",
      "Iteration 129, Cost: 0.023600870600166437\n",
      "Iteration 130, Cost: 0.023295904098461325\n",
      "Iteration 131, Cost: 0.022996574796904957\n",
      "Iteration 132, Cost: 0.022702769606491816\n",
      "Iteration 133, Cost: 0.02241437759482574\n",
      "Iteration 134, Cost: 0.02213128994176621\n",
      "Iteration 135, Cost: 0.021853399896333572\n",
      "Iteration 136, Cost: 0.02158060273484103\n",
      "Iteration 137, Cost: 0.021312795720239173\n",
      "Iteration 138, Cost: 0.021049878062668764\n",
      "Iteration 139, Cost: 0.020791750881221073\n",
      "Iteration 140, Cost: 0.020538317166904643\n",
      "Iteration 141, Cost: 0.020289481746813838\n",
      "Iteration 142, Cost: 0.0200451512494894\n",
      "Iteration 143, Cost: 0.01980523407145501\n",
      "Iteration 144, Cost: 0.01956964034490733\n",
      "Iteration 145, Cost: 0.01933828190653052\n",
      "Iteration 146, Cost: 0.01911107226740011\n",
      "Iteration 147, Cost: 0.01888792658393575\n",
      "Iteration 148, Cost: 0.01866876162985741\n",
      "Iteration 149, Cost: 0.018453495769095708\n",
      "Iteration 150, Cost: 0.01824204892960394\n",
      "Iteration 151, Cost: 0.01803434257801688\n",
      "Iteration 152, Cost: 0.017830299695099985\n",
      "Iteration 153, Cost: 0.017629844751931766\n",
      "Iteration 154, Cost: 0.017432903686761987\n",
      "Iteration 155, Cost: 0.017239403882488877\n",
      "Iteration 156, Cost: 0.01704927414469966\n",
      "Iteration 157, Cost: 0.01686244468022032\n",
      "Iteration 158, Cost: 0.016678847076122553\n",
      "Iteration 159, Cost: 0.016498414279138413\n",
      "Iteration 160, Cost: 0.016321080575435794\n",
      "Iteration 161, Cost: 0.016146781570711047\n",
      "Iteration 162, Cost: 0.015975454170558182\n",
      "Iteration 163, Cost: 0.01580703656107742\n",
      "Iteration 164, Cost: 0.015641468189689537\n",
      "Iteration 165, Cost: 0.015478689746125533\n",
      "Iteration 166, Cost: 0.015318643143565151\n",
      "Iteration 167, Cost: 0.01516127149990063\n",
      "Iteration 168, Cost: 0.015006519119105976\n",
      "Iteration 169, Cost: 0.01485433147269481\n",
      "Iteration 170, Cost: 0.014704655181253252\n",
      "Iteration 171, Cost: 0.014557437996037089\n",
      "Iteration 172, Cost: 0.014412628780625178\n",
      "Iteration 173, Cost: 0.014270177492623763\n",
      "Iteration 174, Cost: 0.014130035165418537\n",
      "Iteration 175, Cost: 0.013992153889973548\n",
      "Iteration 176, Cost: 0.013856486796677918\n",
      "Iteration 177, Cost: 0.013722988037243092\n",
      "Iteration 178, Cost: 0.013591612766654888\n",
      "Iteration 179, Cost: 0.013462317125185736\n",
      "Iteration 180, Cost: 0.013335058220473924\n",
      "Iteration 181, Cost: 0.013209794109677297\n",
      "Iteration 182, Cost: 0.013086483781709797\n",
      "Iteration 183, Cost: 0.012965087139569738\n",
      "Iteration 184, Cost: 0.012845564982769167\n",
      "Iteration 185, Cost: 0.012727878989873933\n",
      "Iteration 186, Cost: 0.012611991701164341\n",
      "Iteration 187, Cost: 0.012497866501426124\n",
      "Iteration 188, Cost: 0.01238546760288164\n",
      "Iteration 189, Cost: 0.012274760028270897\n",
      "Iteration 190, Cost: 0.012165709594091778\n",
      "Iteration 191, Cost: 0.012058282894008683\n",
      "Iteration 192, Cost: 0.011952447282438184\n",
      "Iteration 193, Cost: 0.011848170858320213\n",
      "Iteration 194, Cost: 0.011745422449082494\n",
      "Iteration 195, Cost: 0.01164417159480572\n",
      "Iteration 196, Cost: 0.011544388532596341\n",
      "Iteration 197, Cost: 0.0114460441811733\n",
      "Iteration 198, Cost: 0.011349110125674529\n",
      "Iteration 199, Cost: 0.01125355860268848\n",
      "Iteration 200, Cost: 0.011159362485515434\n",
      "Iteration 201, Cost: 0.011066495269662706\n",
      "Iteration 202, Cost: 0.010974931058577412\n",
      "Iteration 203, Cost: 0.010884644549619899\n",
      "Iteration 204, Cost: 0.010795611020280463\n",
      "Iteration 205, Cost: 0.010707806314641386\n",
      "Iteration 206, Cost: 0.010621206830085942\n",
      "Iteration 207, Cost: 0.01053578950425555\n",
      "Iteration 208, Cost: 0.010451531802255777\n",
      "Iteration 209, Cost: 0.01036841170411142\n",
      "Iteration 210, Cost: 0.010286407692470705\n",
      "Iteration 211, Cost: 0.010205498740558002\n",
      "Iteration 212, Cost: 0.010125664300374315\n",
      "Iteration 213, Cost: 0.01004688429114434\n",
      "Iteration 214, Cost: 0.00996913908800869\n",
      "Iteration 215, Cost: 0.009892409510959512\n",
      "Iteration 216, Cost: 0.009816676814017494\n",
      "Iteration 217, Cost: 0.009741922674648068\n",
      "Iteration 218, Cost: 0.009668129183414248\n",
      "Iteration 219, Cost: 0.009595278833863585\n",
      "Iteration 220, Cost: 0.00952335451264625\n",
      "Iteration 221, Cost: 0.009452339489861353\n",
      "Iteration 222, Cost: 0.009382217409628272\n",
      "Iteration 223, Cost: 0.009312972280879768\n",
      "Iteration 224, Cost: 0.009244588468373442\n",
      "Iteration 225, Cost: 0.009177050683918091\n",
      "Iteration 226, Cost: 0.009110343977811337\n",
      "Iteration 227, Cost: 0.009044453730484925\n",
      "Iteration 228, Cost: 0.008979365644353969\n",
      "Iteration 229, Cost: 0.00891506573586636\n",
      "Iteration 230, Cost: 0.00885154032774862\n",
      "Iteration 231, Cost: 0.008788776041444275\n",
      "Iteration 232, Cost: 0.008726759789741042\n",
      "Iteration 233, Cost: 0.008665478769582841\n",
      "Iteration 234, Cost: 0.00860492045506294\n",
      "Iteration 235, Cost: 0.008545072590594276\n",
      "Iteration 236, Cost: 0.008485923184253135\n",
      "Iteration 237, Cost: 0.008427460501292504\n",
      "Iteration 238, Cost: 0.008369673057821136\n",
      "Iteration 239, Cost: 0.00831254961464471\n",
      "Iteration 240, Cost: 0.008256079171265324\n",
      "Iteration 241, Cost: 0.008200250960035604\n",
      "Iteration 242, Cost: 0.008145054440463867\n",
      "Iteration 243, Cost: 0.008090479293666667\n",
      "Iteration 244, Cost: 0.008036515416965257\n",
      "Iteration 245, Cost: 0.007983152918622383\n",
      "Iteration 246, Cost: 0.00793038211271606\n",
      "Iteration 247, Cost: 0.007878193514146887\n",
      "Iteration 248, Cost: 0.007826577833775581\n",
      "Iteration 249, Cost: 0.007775525973687479\n",
      "Iteration 250, Cost: 0.007725029022580781\n",
      "Iteration 251, Cost: 0.007675078251275369\n",
      "Iteration 252, Cost: 0.007625665108339143\n",
      "Iteration 253, Cost: 0.0075767812158288195\n",
      "Iteration 254, Cost: 0.007528418365142225\n",
      "Iteration 255, Cost: 0.0074805685129792025\n",
      "Iteration 256, Cost: 0.007433223777408228\n",
      "Iteration 257, Cost: 0.007386376434036066\n",
      "Iteration 258, Cost: 0.007340018912277583\n",
      "Iteration 259, Cost: 0.007294143791723221\n",
      "Iteration 260, Cost: 0.007248743798601415\n",
      "Iteration 261, Cost: 0.007203811802333491\n",
      "Iteration 262, Cost: 0.007159340812178496\n",
      "Iteration 263, Cost: 0.007115323973965629\n",
      "Iteration 264, Cost: 0.007071754566911834\n",
      "Iteration 265, Cost: 0.007028626000522275\n",
      "Iteration 266, Cost: 0.00698593181157149\n",
      "Iteration 267, Cost: 0.00694366566116296\n",
      "Iteration 268, Cost: 0.0069018213318650275\n",
      "Iteration 269, Cost: 0.006860392724921046\n",
      "Iteration 270, Cost: 0.006819373857531735\n",
      "Iteration 271, Cost: 0.006778758860207796\n",
      "Iteration 272, Cost: 0.006738541974190834\n",
      "Iteration 273, Cost: 0.006698717548940722\n",
      "Iteration 274, Cost: 0.006659280039687608\n",
      "Iteration 275, Cost: 0.006620224005046771\n",
      "Iteration 276, Cost: 0.006581544104694566\n",
      "Iteration 277, Cost: 0.006543235097103888\n",
      "Iteration 278, Cost: 0.006505291837337371\n",
      "Iteration 279, Cost: 0.006467709274896874\n",
      "Iteration 280, Cost: 0.006430482451627596\n",
      "Iteration 281, Cost: 0.0063936064996753935\n",
      "Iteration 282, Cost: 0.006357076639495802\n",
      "Iteration 283, Cost: 0.006320888177913343\n",
      "Iteration 284, Cost: 0.006285036506229739\n",
      "Iteration 285, Cost: 0.006249517098379697\n",
      "Iteration 286, Cost: 0.006214325509132949\n",
      "Iteration 287, Cost: 0.006179457372341279\n",
      "Iteration 288, Cost: 0.006144908399229295\n",
      "Iteration 289, Cost: 0.0061106743767277835\n",
      "Iteration 290, Cost: 0.006076751165848425\n",
      "Iteration 291, Cost: 0.0060431347000987805\n",
      "Iteration 292, Cost: 0.0060098209839364375\n",
      "Iteration 293, Cost: 0.005976806091261248\n",
      "Iteration 294, Cost: 0.005944086163944606\n",
      "Iteration 295, Cost: 0.005911657410394783\n",
      "Iteration 296, Cost: 0.005879516104157311\n",
      "Iteration 297, Cost: 0.005847658582549494\n",
      "Iteration 298, Cost: 0.005816081245328087\n",
      "Iteration 299, Cost: 0.005784780553389273\n",
      "Iteration 300, Cost: 0.005753753027500066\n",
      "Iteration 301, Cost: 0.005722995247060257\n",
      "Iteration 302, Cost: 0.005692503848894133\n",
      "Iteration 303, Cost: 0.005662275526071121\n",
      "Iteration 304, Cost: 0.005632307026754623\n",
      "Iteration 305, Cost: 0.005602595153078244\n",
      "Iteration 306, Cost: 0.005573136760048719\n",
      "Iteration 307, Cost: 0.005543928754474813\n",
      "Iteration 308, Cost: 0.005514968093921492\n",
      "Iteration 309, Cost: 0.005486251785688725\n",
      "Iteration 310, Cost: 0.00545777688581421\n",
      "Iteration 311, Cost: 0.0054295404980994715\n",
      "Iteration 312, Cost: 0.005401539773158623\n",
      "Iteration 313, Cost: 0.005373771907489289\n",
      "Iteration 314, Cost: 0.005346234142565022\n",
      "Iteration 315, Cost: 0.0053189237639487165\n",
      "Iteration 316, Cost: 0.005291838100426432\n",
      "Iteration 317, Cost: 0.005264974523161119\n",
      "Iteration 318, Cost: 0.005238330444865713\n",
      "Iteration 319, Cost: 0.0052119033189951\n",
      "Iteration 320, Cost: 0.005185690638956479\n",
      "Iteration 321, Cost: 0.005159689937337629\n",
      "Iteration 322, Cost: 0.005133898785152627\n",
      "Iteration 323, Cost: 0.005108314791104588\n",
      "Iteration 324, Cost: 0.0050829356008649405\n",
      "Iteration 325, Cost: 0.0050577588963688965\n",
      "Iteration 326, Cost: 0.005032782395126628\n",
      "Iteration 327, Cost: 0.0050080038495497895\n",
      "Iteration 328, Cost: 0.004983421046293002\n",
      "Converged!\n",
      "Iteration 1, Cost: 0.3245260871758589\n",
      "Iteration 2, Cost: 0.2831911288831198\n",
      "Iteration 3, Cost: 0.25996657498217457\n",
      "Iteration 4, Cost: 0.2468963750299178\n",
      "Iteration 5, Cost: 0.23814171663253822\n",
      "Iteration 6, Cost: 0.23130808679575374\n",
      "Iteration 7, Cost: 0.22548156307700898\n",
      "Iteration 8, Cost: 0.22026941380656773\n",
      "Iteration 9, Cost: 0.21546482036603384\n",
      "Iteration 10, Cost: 0.21093889634305338\n",
      "Iteration 11, Cost: 0.2066066861421207\n",
      "Iteration 12, Cost: 0.20241368742070448\n",
      "Iteration 13, Cost: 0.19832709821027175\n",
      "Iteration 14, Cost: 0.19432877641939494\n",
      "Iteration 15, Cost: 0.1904098565696009\n",
      "Iteration 16, Cost: 0.18656700865660356\n",
      "Iteration 17, Cost: 0.1828000240621955\n",
      "Iteration 18, Cost: 0.17911032457411918\n",
      "Iteration 19, Cost: 0.17550005522779183\n",
      "Iteration 20, Cost: 0.17197152708084112\n",
      "Iteration 21, Cost: 0.16852686676252232\n",
      "Iteration 22, Cost: 0.16516779235482681\n",
      "Iteration 23, Cost: 0.16189547324093276\n",
      "Iteration 24, Cost: 0.15871045207600307\n",
      "Iteration 25, Cost: 0.15561261643668953\n",
      "Iteration 26, Cost: 0.15260121092800916\n",
      "Iteration 27, Cost: 0.14967488101683338\n",
      "Iteration 28, Cost: 0.14683173972965863\n",
      "Iteration 29, Cost: 0.1440694486110335\n",
      "Iteration 30, Cost: 0.14138530523914142\n",
      "Iteration 31, Cost: 0.13877633098298206\n",
      "Iteration 32, Cost: 0.13623935428359304\n",
      "Iteration 33, Cost: 0.13377108630666498\n",
      "Iteration 34, Cost: 0.13136818719023471\n",
      "Iteration 35, Cost: 0.12902732222424\n",
      "Iteration 36, Cost: 0.1267452081292473\n",
      "Iteration 37, Cost: 0.12451865016319838\n",
      "Iteration 38, Cost: 0.12234457111015345\n",
      "Iteration 39, Cost: 0.12022003333719751\n",
      "Iteration 40, Cost: 0.11814225509351631\n",
      "Iteration 41, Cost: 0.11610862211764127\n",
      "Iteration 42, Cost: 0.11411669545877663\n",
      "Iteration 43, Cost: 0.11216421624150517\n",
      "Iteration 44, Cost: 0.1102491079355766\n",
      "Iteration 45, Cost: 0.108369476549439\n",
      "Iteration 46, Cost: 0.10652360905448605\n",
      "Iteration 47, Cost: 0.10470997026686861\n",
      "Iteration 48, Cost: 0.10292719836106784\n",
      "Iteration 49, Cost: 0.10117409915797645\n",
      "Iteration 50, Cost: 0.09944963931317008\n",
      "Iteration 51, Cost: 0.0977529385221522\n",
      "Iteration 52, Cost: 0.09608326085366704\n",
      "Iteration 53, Cost: 0.09444000531630456\n",
      "Iteration 54, Cost: 0.09282269575581324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 55, Cost: 0.09123097017056812\n",
      "Iteration 56, Cost: 0.08966456952161125\n",
      "Iteration 57, Cost: 0.08812332610370505\n",
      "Iteration 58, Cost: 0.08660715153761979\n",
      "Iteration 59, Cost: 0.08511602444418141\n",
      "Iteration 60, Cost: 0.08364997786965518\n",
      "Iteration 61, Cost: 0.08220908655087511\n",
      "Iteration 62, Cost: 0.08079345413649425\n",
      "Iteration 63, Cost: 0.07940320051519781\n",
      "Iteration 64, Cost: 0.0780384494381846\n",
      "Iteration 65, Cost: 0.07669931665582945\n",
      "Iteration 66, Cost: 0.07538589881092621\n",
      "Iteration 67, Cost: 0.07409826333774913\n",
      "Iteration 68, Cost: 0.07283643960370413\n",
      "Iteration 69, Cost: 0.07160041149760733\n",
      "Iteration 70, Cost: 0.07039011161764927\n",
      "Iteration 71, Cost: 0.06920541714757437\n",
      "Iteration 72, Cost: 0.06804614743805529\n",
      "Iteration 73, Cost: 0.0669120632388794\n",
      "Iteration 74, Cost: 0.06580286746310082\n",
      "Iteration 75, Cost: 0.06471820731200308\n",
      "Iteration 76, Cost: 0.06365767755280678\n",
      "Iteration 77, Cost: 0.0626208247206337\n",
      "Iteration 78, Cost: 0.06160715201148794\n",
      "Iteration 79, Cost: 0.06061612464169655\n",
      "Iteration 80, Cost: 0.05964717546827876\n",
      "Iteration 81, Cost: 0.058699710690714176\n",
      "Iteration 82, Cost: 0.05777311548433652\n",
      "Iteration 83, Cost: 0.0568667594463295\n",
      "Iteration 84, Cost: 0.055980001764896224\n",
      "Iteration 85, Cost: 0.055112196049110676\n",
      "Iteration 86, Cost: 0.0542626947803345\n",
      "Iteration 87, Cost: 0.05343085336549214\n",
      "Iteration 88, Cost: 0.05261603378792284\n",
      "Iteration 89, Cost: 0.05181760786321306\n",
      "Iteration 90, Cost: 0.05103496011577003\n",
      "Iteration 91, Cost: 0.0502674902974167\n",
      "Iteration 92, Cost: 0.04951461557248098\n",
      "Iteration 93, Cost: 0.04877577239520941\n",
      "Iteration 94, Cost: 0.04805041810529724\n",
      "Iteration 95, Cost: 0.04733803226627613\n",
      "Iteration 96, Cost: 0.046638117769753976\n",
      "Iteration 97, Cost: 0.04595020172631286\n",
      "Iteration 98, Cost: 0.045273836161441205\n",
      "Iteration 99, Cost: 0.044608598532354236\n",
      "Iteration 100, Cost: 0.043954092079056845\n",
      "Iteration 101, Cost: 0.0433099460206073\n",
      "Iteration 102, Cost: 0.04267581560531106\n",
      "Iteration 103, Cost: 0.042051382021557684\n",
      "Iteration 104, Cost: 0.04143635217425171\n",
      "Iteration 105, Cost: 0.04083045833031095\n",
      "Iteration 106, Cost: 0.040233457635547976\n",
      "Iteration 107, Cost: 0.03964513150443946\n",
      "Iteration 108, Cost: 0.03906528488385218\n",
      "Iteration 109, Cost: 0.03849374539175724\n",
      "Iteration 110, Cost: 0.03793036233234253\n",
      "Iteration 111, Cost: 0.03737500558973794\n",
      "Iteration 112, Cost: 0.036827564403799\n",
      "Iteration 113, Cost: 0.036287946033040766\n",
      "Iteration 114, Cost: 0.03575607431185129\n",
      "Iteration 115, Cost: 0.035231888111503226\n",
      "Iteration 116, Cost: 0.03471533971716648\n",
      "Iteration 117, Cost: 0.034206393136024256\n",
      "Iteration 118, Cost: 0.03370502235460656\n",
      "Iteration 119, Cost: 0.0332112095664415\n",
      "Iteration 120, Cost: 0.03272494339392076\n",
      "Iteration 121, Cost: 0.03224621713067965\n",
      "Iteration 122, Cost: 0.03177502703258096\n",
      "Iteration 123, Cost: 0.031311370686331504\n",
      "Iteration 124, Cost: 0.03085524548462549\n",
      "Iteration 125, Cost: 0.030406647235317774\n",
      "Iteration 126, Cost: 0.029965568929365024\n",
      "Iteration 127, Cost: 0.02953199968811817\n",
      "Iteration 128, Cost: 0.029105923905101628\n",
      "Iteration 129, Cost: 0.028687320590895706\n",
      "Iteration 130, Cost: 0.02827616292248254\n",
      "Iteration 131, Cost: 0.027872417990845753\n",
      "Iteration 132, Cost: 0.02747604673320443\n",
      "Iteration 133, Cost: 0.027087004029494495\n",
      "Iteration 134, Cost: 0.02670523893703092\n",
      "Iteration 135, Cost: 0.026330695033062648\n",
      "Iteration 136, Cost: 0.02596331083243529\n",
      "Iteration 137, Cost: 0.025603020246950728\n",
      "Iteration 138, Cost: 0.02524975305427659\n",
      "Iteration 139, Cost: 0.024903435347305714\n",
      "Iteration 140, Cost: 0.024563989939474833\n",
      "Iteration 141, Cost: 0.02423133670739943\n",
      "Iteration 142, Cost: 0.023905392858861795\n",
      "Iteration 143, Cost: 0.02358607312123595\n",
      "Iteration 144, Cost: 0.023273289852352212\n",
      "Iteration 145, Cost: 0.02296695308210502\n",
      "Iteration 146, Cost: 0.022666970498349766\n",
      "Iteration 147, Cost: 0.022373247394459984\n",
      "Iteration 148, Cost: 0.022085686598095518\n",
      "Iteration 149, Cost: 0.021804188401181597\n",
      "Iteration 150, Cost: 0.021528650509894758\n",
      "Iteration 151, Cost: 0.02125896803081917\n",
      "Iteration 152, Cost: 0.020995033505720097\n",
      "Iteration 153, Cost: 0.02073673700300026\n",
      "Iteration 154, Cost: 0.02048396626930348\n",
      "Iteration 155, Cost: 0.020236606940324153\n",
      "Iteration 156, Cost: 0.019994542806017295\n",
      "Iteration 157, Cost: 0.01975765612232807\n",
      "Iteration 158, Cost: 0.019525827959407552\n",
      "Iteration 159, Cost: 0.019298938575080058\n",
      "Iteration 160, Cost: 0.01907686780201339\n",
      "Iteration 161, Cost: 0.018859495437485795\n",
      "Iteration 162, Cost: 0.01864670162567271\n",
      "Iteration 163, Cost: 0.018438367223807966\n",
      "Iteration 164, Cost: 0.018234374145232067\n",
      "Iteration 165, Cost: 0.018034605674068876\n",
      "Iteration 166, Cost: 0.017838946747948826\n",
      "Iteration 167, Cost: 0.017647284206731437\n",
      "Iteration 168, Cost: 0.01745950700651473\n",
      "Iteration 169, Cost: 0.017275506399326238\n",
      "Iteration 170, Cost: 0.01709517607976232\n",
      "Iteration 171, Cost: 0.01691841230049116\n",
      "Iteration 172, Cost: 0.01674511395898093\n",
      "Iteration 173, Cost: 0.01657518265808642\n",
      "Iteration 174, Cost: 0.016408522743254945\n",
      "Iteration 175, Cost: 0.01624504131912583\n",
      "Iteration 176, Cost: 0.01608464824822439\n",
      "Iteration 177, Cost: 0.01592725613431565\n",
      "Iteration 178, Cost: 0.015772780292806036\n",
      "Iteration 179, Cost: 0.015621138710379002\n",
      "Iteration 180, Cost: 0.015472251995837082\n",
      "Iteration 181, Cost: 0.01532604332390752\n",
      "Iteration 182, Cost: 0.015182438373559237\n",
      "Iteration 183, Cost: 0.015041365262180055\n",
      "Iteration 184, Cost: 0.014902754476778341\n",
      "Iteration 185, Cost: 0.014766538803204266\n",
      "Iteration 186, Cost: 0.014632653254233187\n",
      "Iteration 187, Cost: 0.014501034997217844\n",
      "Iteration 188, Cost: 0.014371623281895602\n",
      "Iteration 189, Cost: 0.014244359368832001\n",
      "Iteration 190, Cost: 0.014119186458890435\n",
      "Iteration 191, Cost: 0.013996049624038973\n",
      "Iteration 192, Cost: 0.0138748957397379\n",
      "Iteration 193, Cost: 0.013755673419094212\n",
      "Iteration 194, Cost: 0.013638332948920747\n",
      "Iteration 195, Cost: 0.01352282622779687\n",
      "Iteration 196, Cost: 0.0134091067061937\n",
      "Iteration 197, Cost: 0.013297129328698773\n",
      "Iteration 198, Cost: 0.01318685047835164\n",
      "Iteration 199, Cost: 0.013078227923083474\n",
      "Iteration 200, Cost: 0.012971220764238138\n",
      "Iteration 201, Cost: 0.012865789387140355\n",
      "Iteration 202, Cost: 0.012761895413667016\n",
      "Iteration 203, Cost: 0.012659501656770338\n",
      "Iteration 204, Cost: 0.012558572076896151\n",
      "Iteration 205, Cost: 0.012459071740236673\n",
      "Iteration 206, Cost: 0.012360966778754305\n",
      "Iteration 207, Cost: 0.012264224351911508\n",
      "Iteration 208, Cost: 0.012168812610040877\n",
      "Iteration 209, Cost: 0.012074700659289473\n",
      "Iteration 210, Cost: 0.01198185852807188\n",
      "Iteration 211, Cost: 0.011890257134967386\n",
      "Iteration 212, Cost: 0.011799868257997805\n",
      "Iteration 213, Cost: 0.011710664505224119\n",
      "Iteration 214, Cost: 0.01162261928660157\n",
      "Iteration 215, Cost: 0.011535706787034979\n",
      "Iteration 216, Cost: 0.011449901940577704\n",
      "Iteration 217, Cost: 0.01136518040571993\n",
      "Iteration 218, Cost: 0.011281518541713867\n",
      "Iteration 219, Cost: 0.011198893385885552\n",
      "Iteration 220, Cost: 0.011117282631885078\n",
      "Iteration 221, Cost: 0.011036664608828997\n",
      "Iteration 222, Cost: 0.010957018261290783\n",
      "Iteration 223, Cost: 0.010878323130097057\n",
      "Iteration 224, Cost: 0.010800559333889449\n",
      "Iteration 225, Cost: 0.010723707551413516\n",
      "Iteration 226, Cost: 0.010647749004498254\n",
      "Iteration 227, Cost: 0.01057266544169128\n",
      "Iteration 228, Cost: 0.010498439122516594\n",
      "Iteration 229, Cost: 0.010425052802323368\n",
      "Iteration 230, Cost: 0.010352489717695874\n",
      "Iteration 231, Cost: 0.010280733572396044\n",
      "Iteration 232, Cost: 0.010209768523811778\n",
      "Iteration 233, Cost: 0.01013957916988533\n",
      "Iteration 234, Cost: 0.01007015053649759\n",
      "Iteration 235, Cost: 0.010001468065285235\n",
      "Iteration 236, Cost: 0.009933517601869061\n",
      "Iteration 237, Cost: 0.009866285384472824\n",
      "Iteration 238, Cost: 0.009799758032913256\n",
      "Iteration 239, Cost: 0.009733922537942724\n",
      "Iteration 240, Cost: 0.009668766250927264\n",
      "Iteration 241, Cost: 0.00960427687384353\n",
      "Iteration 242, Cost: 0.009540442449579209\n",
      "Iteration 243, Cost: 0.009477251352522227\n",
      "Iteration 244, Cost: 0.009414692279425133\n",
      "Iteration 245, Cost: 0.009352754240531518\n",
      "Iteration 246, Cost: 0.00929142655095237\n",
      "Iteration 247, Cost: 0.009230698822280885\n",
      "Iteration 248, Cost: 0.009170560954434784\n",
      "Iteration 249, Cost: 0.009111003127716132\n",
      "Iteration 250, Cost: 0.009052015795078973\n",
      "Iteration 251, Cost: 0.008993589674595829\n",
      "Iteration 252, Cost: 0.008935715742114632\n",
      "Iteration 253, Cost: 0.008878385224098095\n",
      "Iteration 254, Cost: 0.008821589590638146\n",
      "Iteration 255, Cost: 0.008765320548638359\n",
      "Iteration 256, Cost: 0.008709570035157842\n",
      "Iteration 257, Cost: 0.008654330210910395\n",
      "Iteration 258, Cost: 0.008599593453913208\n",
      "Iteration 259, Cost: 0.00854535235327958\n",
      "Iteration 260, Cost: 0.00849159970315065\n",
      "Iteration 261, Cost: 0.008438328496761372\n",
      "Iteration 262, Cost: 0.008385531920636196\n",
      "Iteration 263, Cost: 0.008333203348910353\n",
      "Iteration 264, Cost: 0.008281336337772826\n",
      "Iteration 265, Cost: 0.00822992462002731\n",
      "Iteration 266, Cost: 0.00817896209976781\n",
      "Iteration 267, Cost: 0.008128442847165688\n",
      "Iteration 268, Cost: 0.008078361093365188\n",
      "Iteration 269, Cost: 0.008028711225484762\n",
      "Iteration 270, Cost: 0.007979487781721566\n",
      "Iteration 271, Cost: 0.007930685446556932\n",
      "Iteration 272, Cost: 0.007882299046060524\n",
      "Iteration 273, Cost: 0.007834323543291293\n",
      "Iteration 274, Cost: 0.0077867540337934246\n",
      "Iteration 275, Cost: 0.007739585741185636\n",
      "Iteration 276, Cost: 0.007692814012842321\n",
      "Iteration 277, Cost: 0.007646434315665322\n",
      "Iteration 278, Cost: 0.007600442231945019\n",
      "Iteration 279, Cost: 0.007554833455309823\n",
      "Iteration 280, Cost: 0.007509603786763081\n",
      "Iteration 281, Cost: 0.007464749130806659\n",
      "Iteration 282, Cost: 0.0074202654916504995\n",
      "Iteration 283, Cost: 0.007376148969507649\n",
      "Iteration 284, Cost: 0.007332395756974197\n",
      "Iteration 285, Cost: 0.00728900213549387\n",
      "Iteration 286, Cost: 0.00724596447190687\n",
      "Iteration 287, Cost: 0.007203279215082844\n",
      "Iteration 288, Cost: 0.007160942892637719\n",
      "Iteration 289, Cost: 0.007118952107734376\n",
      "Iteration 290, Cost: 0.0070773035359670295\n",
      "Iteration 291, Cost: 0.0070359939223293\n",
      "Iteration 292, Cost: 0.006995020078265966\n",
      "Iteration 293, Cost: 0.0069543788788083375\n",
      "Iteration 294, Cost: 0.0069140672597932835\n",
      "Iteration 295, Cost: 0.0068740822151658575\n",
      "Iteration 296, Cost: 0.006834420794365482\n",
      "Iteration 297, Cost: 0.006795080099795596\n",
      "Iteration 298, Cost: 0.0067560572843766865\n",
      "Iteration 299, Cost: 0.006717349549182487\n",
      "Iteration 300, Cost: 0.0066789541411591295\n",
      "Iteration 301, Cost: 0.006640868350926994\n",
      "Iteration 302, Cost: 0.006603089510664845\n",
      "Iteration 303, Cost: 0.006565614992075847\n",
      "Iteration 304, Cost: 0.006528442204434936\n",
      "Iteration 305, Cost: 0.006491568592716929\n",
      "Iteration 306, Cost: 0.006454991635804674\n",
      "Iteration 307, Cost: 0.006418708844776472\n",
      "Iteration 308, Cost: 0.006382717761271858\n",
      "Iteration 309, Cost: 0.006347015955934753\n",
      "Iteration 310, Cost: 0.006311601026932972\n",
      "Iteration 311, Cost: 0.006276470598552821\n",
      "Iteration 312, Cost: 0.006241622319867592\n",
      "Iteration 313, Cost: 0.006207053863478557\n",
      "Iteration 314, Cost: 0.006172762924327025\n",
      "Iteration 315, Cost: 0.006138747218575922\n",
      "Iteration 316, Cost: 0.006105004482559299\n",
      "Iteration 317, Cost: 0.00607153247179802\n",
      "Iteration 318, Cost: 0.006038328960079936\n",
      "Iteration 319, Cost: 0.006005391738602673\n",
      "Iteration 320, Cost: 0.0059727186151771324\n",
      "Iteration 321, Cost: 0.005940307413489802\n",
      "Iteration 322, Cost: 0.005908155972421844\n",
      "Iteration 323, Cost: 0.005876262145422932\n",
      "Iteration 324, Cost: 0.0058446237999378115\n",
      "Iteration 325, Cost: 0.005813238816883439\n",
      "Iteration 326, Cost: 0.005782105090174596\n",
      "Iteration 327, Cost: 0.0057512205262958875\n",
      "Iteration 328, Cost: 0.0057205830439179435\n",
      "Iteration 329, Cost: 0.0056901905735557146\n",
      "Iteration 330, Cost: 0.005660041057266732\n",
      "Iteration 331, Cost: 0.00563013244838721\n",
      "Iteration 332, Cost: 0.005600462711303906\n",
      "Iteration 333, Cost: 0.005571029821259647\n",
      "Iteration 334, Cost: 0.005541831764190495\n",
      "Iteration 335, Cost: 0.005512866536592541\n",
      "Iteration 336, Cost: 0.005484132145416344\n",
      "Iteration 337, Cost: 0.0054556266079870885\n",
      "Iteration 338, Cost: 0.005427347951948617\n",
      "Iteration 339, Cost: 0.005399294215229443\n",
      "Iteration 340, Cost: 0.005371463446029016\n",
      "Iteration 341, Cost: 0.005343853702822504\n",
      "Iteration 342, Cost: 0.0053164630543824235\n",
      "Iteration 343, Cost: 0.005289289579815528\n",
      "Iteration 344, Cost: 0.005262331368613399\n",
      "Iteration 345, Cost: 0.005235586520715276\n",
      "Iteration 346, Cost: 0.005209053146581705\n",
      "Iteration 347, Cost: 0.0051827293672776645\n",
      "Iteration 348, Cost: 0.005156613314563876\n",
      "Iteration 349, Cost: 0.005130703130995091\n",
      "Iteration 350, Cost: 0.005104996970024199\n",
      "Iteration 351, Cost: 0.0050794929961110605\n",
      "Iteration 352, Cost: 0.005054189384835056\n",
      "Iteration 353, Cost: 0.005029084323010384\n",
      "Iteration 354, Cost: 0.005004176008803185\n",
      "Iteration 355, Cost: 0.0049794626518496945\n",
      "Converged!\n",
      "Iteration 1, Cost: 0.2793143012324127\n",
      "Iteration 2, Cost: 0.24725449366833346\n",
      "Iteration 3, Cost: 0.2313900869198969\n",
      "Iteration 4, Cost: 0.22247989342077493\n",
      "Iteration 5, Cost: 0.2160794359288752\n",
      "Iteration 6, Cost: 0.2107129183550812\n",
      "Iteration 7, Cost: 0.20586791367687413\n",
      "Iteration 8, Cost: 0.20135118979224467\n",
      "Iteration 9, Cost: 0.19708315407407062\n",
      "Iteration 10, Cost: 0.19302562099947168\n",
      "Iteration 11, Cost: 0.18915503964050237\n",
      "Iteration 12, Cost: 0.18545280339161774\n",
      "Iteration 13, Cost: 0.1819022337885772\n",
      "Iteration 14, Cost: 0.17848807084306878\n",
      "Iteration 15, Cost: 0.17519675185232392\n",
      "Iteration 16, Cost: 0.17201679073415901\n",
      "Iteration 17, Cost: 0.16893900459032105\n",
      "Iteration 18, Cost: 0.1659565107432865\n",
      "Iteration 19, Cost: 0.16306449044757748\n",
      "Iteration 20, Cost: 0.1602597555030492\n",
      "Iteration 21, Cost: 0.15754018641980058\n",
      "Iteration 22, Cost: 0.15490413517902818\n",
      "Iteration 23, Cost: 0.15234989109000632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 24, Cost: 0.1498752876551085\n",
      "Iteration 25, Cost: 0.14747748726246337\n",
      "Iteration 26, Cost: 0.14515293524509382\n",
      "Iteration 27, Cost: 0.1428974421600714\n",
      "Iteration 28, Cost: 0.14070634083292904\n",
      "Iteration 29, Cost: 0.1385746699494998\n",
      "Iteration 30, Cost: 0.13649735038256844\n",
      "Iteration 31, Cost: 0.13446933605965786\n",
      "Iteration 32, Cost: 0.1324857334417698\n",
      "Iteration 33, Cost: 0.1305418913128491\n",
      "Iteration 34, Cost: 0.12863346619344868\n",
      "Iteration 35, Cost: 0.12675646951575273\n",
      "Iteration 36, Cost: 0.1249073018637751\n",
      "Iteration 37, Cost: 0.12308277791642631\n",
      "Iteration 38, Cost: 0.12128014378227776\n",
      "Iteration 39, Cost: 0.1194970865681735\n",
      "Iteration 40, Cost: 0.11773173459261092\n",
      "Iteration 41, Cost: 0.11598264592033178\n",
      "Iteration 42, Cost: 0.11424878307947223\n",
      "Iteration 43, Cost: 0.11252947300709255\n",
      "Iteration 44, Cost: 0.11082435328972598\n",
      "Iteration 45, Cost: 0.10913330816780956\n",
      "Iteration 46, Cost: 0.10745639988317572\n",
      "Iteration 47, Cost: 0.10579380208565545\n",
      "Iteration 48, Cost: 0.10414574175931261\n",
      "Iteration 49, Cost: 0.10251245451472091\n",
      "Iteration 50, Cost: 0.10089415560071203\n",
      "Iteration 51, Cost: 0.0992910263352478\n",
      "Iteration 52, Cost: 0.0977032135053783\n",
      "Iteration 53, Cost: 0.09613083803032017\n",
      "Iteration 54, Cost: 0.09457400887073454\n",
      "Iteration 55, Cost: 0.09303283859545146\n",
      "Iteration 56, Cost: 0.09150745786709026\n",
      "Iteration 57, Cost: 0.08999802708104003\n",
      "Iteration 58, Cost: 0.08850474427785757\n",
      "Iteration 59, Cost: 0.08702784913988847\n",
      "Iteration 60, Cost: 0.08556762335302744\n",
      "Iteration 61, Cost: 0.08412438788756156\n",
      "Iteration 62, Cost: 0.08269849787256409\n",
      "Iteration 63, Cost: 0.08129033575330616\n",
      "Iteration 64, Cost: 0.07990030337065972\n",
      "Iteration 65, Cost: 0.07852881351562689\n",
      "Iteration 66, Cost: 0.07717628141180824\n",
      "Iteration 67, Cost: 0.07584311647730975\n",
      "Iteration 68, Cost: 0.07452971462348454\n",
      "Iteration 69, Cost: 0.07323645126559389\n",
      "Iteration 70, Cost: 0.07196367515210358\n",
      "Iteration 71, Cost: 0.07071170306538116\n",
      "Iteration 72, Cost: 0.06948081540631727\n",
      "Iteration 73, Cost: 0.06827125264730462\n",
      "Iteration 74, Cost: 0.06708321261995297\n",
      "Iteration 75, Cost: 0.06591684859349117\n",
      "Iteration 76, Cost: 0.06477226809457767\n",
      "Iteration 77, Cost: 0.06364953241700083\n",
      "Iteration 78, Cost: 0.06254865676871042\n",
      "Iteration 79, Cost: 0.06146961100253819\n",
      "Iteration 80, Cost: 0.060412320875176166\n",
      "Iteration 81, Cost: 0.05937666977636534\n",
      "Iteration 82, Cost: 0.058362500867102825\n",
      "Iteration 83, Cost: 0.05736961956256895\n",
      "Iteration 84, Cost: 0.056397796293065836\n",
      "Iteration 85, Cost: 0.055446769475146734\n",
      "Iteration 86, Cost: 0.05451624862573443\n",
      "Iteration 87, Cost: 0.0536059175545744\n",
      "Iteration 88, Cost: 0.052715437574805984\n",
      "Iteration 89, Cost: 0.05184445067750965\n",
      "Iteration 90, Cost: 0.05099258262340082\n",
      "Iteration 91, Cost: 0.05015944591291186\n",
      "Iteration 92, Cost: 0.04934464260424255\n",
      "Iteration 93, Cost: 0.048547766957119695\n",
      "Iteration 94, Cost: 0.04776840788762545\n",
      "Iteration 95, Cost: 0.04700615122626622\n",
      "Iteration 96, Cost: 0.04626058177729789\n",
      "Iteration 97, Cost: 0.04553128518213122\n",
      "Iteration 98, Cost: 0.044817849593417135\n",
      "Iteration 99, Cost: 0.04411986716921778\n",
      "Iteration 100, Cost: 0.04343693539859905\n",
      "Iteration 101, Cost: 0.04276865827114573\n",
      "Iteration 102, Cost: 0.04211464730341489\n",
      "Iteration 103, Cost: 0.041474522435313056\n",
      "Iteration 104, Cost: 0.04084791280890117\n",
      "Iteration 105, Cost: 0.0402344574412753\n",
      "Iteration 106, Cost: 0.039633805802004894\n",
      "Iteration 107, Cost: 0.0390456183041821\n",
      "Iteration 108, Cost: 0.03846956671649247\n",
      "Iteration 109, Cost: 0.03790533450189771\n",
      "Iteration 110, Cost: 0.03735261708657596\n",
      "Iteration 111, Cost: 0.03681112206074929\n",
      "Iteration 112, Cost: 0.03628056931101988\n",
      "Iteration 113, Cost: 0.03576069108192794\n",
      "Iteration 114, Cost: 0.03525123196275558\n",
      "Iteration 115, Cost: 0.03475194879426391\n",
      "Iteration 116, Cost: 0.03426261048921553\n",
      "Iteration 117, Cost: 0.03378299776034689\n",
      "Iteration 118, Cost: 0.03331290275004681\n",
      "Iteration 119, Cost: 0.03285212855746269\n",
      "Iteration 120, Cost: 0.03240048866113214\n",
      "Iteration 121, Cost: 0.031957806238483225\n",
      "Iteration 122, Cost: 0.0315239133875296\n",
      "Iteration 123, Cost: 0.03109865026057793\n",
      "Iteration 124, Cost: 0.030681864124446897\n",
      "Iteration 125, Cost: 0.030273408366187155\n",
      "Iteration 126, Cost: 0.029873141467177344\n",
      "Iteration 127, Cost: 0.029480925971356788\n",
      "Iteration 128, Cost: 0.029096627474912866\n",
      "Iteration 129, Cost: 0.028720113664750913\n",
      "Iteration 130, Cost: 0.028351253431466574\n",
      "Iteration 131, Cost: 0.027989916079398054\n",
      "Iteration 132, Cost: 0.027635970651896997\n",
      "Iteration 133, Cost: 0.027289285384578362\n",
      "Iteration 134, Cost: 0.026949727293426998\n",
      "Iteration 135, Cost: 0.026617161898709437\n",
      "Iteration 136, Cost: 0.02629145308009236\n",
      "Iteration 137, Cost: 0.025972463053560334\n",
      "Iteration 138, Cost: 0.0256600524569065\n",
      "Iteration 139, Cost: 0.025354080527878305\n",
      "Iteration 140, Cost: 0.025054405357519852\n",
      "Iteration 141, Cost: 0.024760884200789753\n",
      "Iteration 142, Cost: 0.024473373827004784\n",
      "Iteration 143, Cost: 0.024191730893874185\n",
      "Iteration 144, Cost: 0.02391581233064012\n",
      "Iteration 145, Cost: 0.023645475717922693\n",
      "Iteration 146, Cost: 0.023380579654100165\n",
      "Iteration 147, Cost: 0.023120984100284677\n",
      "Iteration 148, Cost: 0.02286655069806479\n",
      "Iteration 149, Cost: 0.022617143056097525\n",
      "Iteration 150, Cost: 0.02237262700329627\n",
      "Iteration 151, Cost: 0.022132870807754744\n",
      "Iteration 152, Cost: 0.02189774536167193\n",
      "Iteration 153, Cost: 0.02166712433341228\n",
      "Iteration 154, Cost: 0.02144088428847509\n",
      "Iteration 155, Cost: 0.021218904781586296\n",
      "Iteration 156, Cost: 0.021001068422397372\n",
      "Iteration 157, Cost: 0.020787260917410892\n",
      "Iteration 158, Cost: 0.02057737109077997\n",
      "Iteration 159, Cost: 0.020371290886575025\n",
      "Iteration 160, Cost: 0.0201689153549991\n",
      "Iteration 161, Cost: 0.019970142624880352\n",
      "Iteration 162, Cost: 0.019774873864592882\n",
      "Iteration 163, Cost: 0.01958301323336655\n",
      "Iteration 164, Cost: 0.019394467824752123\n",
      "Iteration 165, Cost: 0.01920914760381631\n",
      "Iteration 166, Cost: 0.019026965339457507\n",
      "Iteration 167, Cost: 0.018847836533059676\n",
      "Iteration 168, Cost: 0.018671679344541416\n",
      "Iteration 169, Cost: 0.018498414516710495\n",
      "Iteration 170, Cost: 0.018327965298701707\n",
      "Iteration 171, Cost: 0.018160257369156694\n",
      "Iteration 172, Cost: 0.01799521875969925\n",
      "Iteration 173, Cost: 0.017832779779166266\n",
      "Iteration 174, Cost: 0.017672872938973027\n",
      "Iteration 175, Cost: 0.017515432879920724\n",
      "Iteration 176, Cost: 0.017360396300692305\n",
      "Iteration 177, Cost: 0.01720770188823021\n",
      "Iteration 178, Cost: 0.017057290250143835\n",
      "Iteration 179, Cost: 0.016909103849256257\n",
      "Iteration 180, Cost: 0.016763086940366566\n",
      "Iteration 181, Cost: 0.016619185509276893\n",
      "Iteration 182, Cost: 0.016477347214109445\n",
      "Iteration 183, Cost: 0.016337521328919993\n",
      "Iteration 184, Cost: 0.01619965868959741\n",
      "Iteration 185, Cost: 0.016063711642026125\n",
      "Iteration 186, Cost: 0.01592963399247676\n",
      "Iteration 187, Cost: 0.01579738096018159\n",
      "Iteration 188, Cost: 0.015666909132043886\n",
      "Iteration 189, Cost: 0.015538176419424248\n",
      "Iteration 190, Cost: 0.015411142016942292\n",
      "Iteration 191, Cost: 0.015285766363228015\n",
      "Iteration 192, Cost: 0.015162011103554165\n",
      "Iteration 193, Cost: 0.015039839054278192\n",
      "Iteration 194, Cost: 0.014919214169020489\n",
      "Iteration 195, Cost: 0.014800101506503741\n",
      "Iteration 196, Cost: 0.014682467199976844\n",
      "Iteration 197, Cost: 0.014566278428145523\n",
      "Iteration 198, Cost: 0.014451503387530769\n",
      "Iteration 199, Cost: 0.01433811126617512\n",
      "Iteration 200, Cost: 0.01422607221861591\n",
      "Iteration 201, Cost: 0.014115357342043715\n",
      "Iteration 202, Cost: 0.014005938653563395\n",
      "Iteration 203, Cost: 0.013897789068474359\n",
      "Iteration 204, Cost: 0.013790882379485891\n",
      "Iteration 205, Cost: 0.013685193236782835\n",
      "Iteration 206, Cost: 0.013580697128856295\n",
      "Iteration 207, Cost: 0.013477370364013785\n",
      "Iteration 208, Cost: 0.013375190052482907\n",
      "Iteration 209, Cost: 0.013274134089022848\n",
      "Iteration 210, Cost: 0.013174181135958344\n",
      "Iteration 211, Cost: 0.013075310606551309\n",
      "Iteration 212, Cost: 0.012977502648626744\n",
      "Iteration 213, Cost: 0.012880738128370935\n",
      "Iteration 214, Cost: 0.012784998614222064\n",
      "Iteration 215, Cost: 0.012690266360776362\n",
      "Iteration 216, Cost: 0.012596524292635858\n",
      "Iteration 217, Cost: 0.01250375598812824\n",
      "Iteration 218, Cost: 0.012411945662833747\n",
      "Iteration 219, Cost: 0.012321078152859651\n",
      "Iteration 220, Cost: 0.012231138897809099\n",
      "Iteration 221, Cost: 0.01214211392339787\n",
      "Iteration 222, Cost: 0.012053989823680451\n",
      "Iteration 223, Cost: 0.011966753742854967\n",
      "Iteration 224, Cost: 0.011880393356625477\n",
      "Iteration 225, Cost: 0.011794896853109603\n",
      "Iteration 226, Cost: 0.011710252913289244\n",
      "Iteration 227, Cost: 0.011626450691012336\n",
      "Iteration 228, Cost: 0.011543479792563953\n",
      "Iteration 229, Cost: 0.011461330255835308\n",
      "Iteration 230, Cost: 0.011379992529129516\n",
      "Iteration 231, Cost: 0.01129945744965286\n",
      "Iteration 232, Cost: 0.01121971622174962\n",
      "Iteration 233, Cost: 0.01114076039494742\n",
      "Iteration 234, Cost: 0.011062581841887785\n",
      "Iteration 235, Cost: 0.01098517273622373\n",
      "Iteration 236, Cost: 0.010908525530571788\n",
      "Iteration 237, Cost: 0.010832632934610568\n",
      "Iteration 238, Cost: 0.010757487893420955\n",
      "Iteration 239, Cost: 0.010683083566164911\n",
      "Iteration 240, Cost: 0.010609413305199896\n",
      "Iteration 241, Cost: 0.010536470635724767\n",
      "Iteration 242, Cost: 0.010464249236050186\n",
      "Iteration 243, Cost: 0.010392742918582314\n",
      "Iteration 244, Cost: 0.010321945611603071\n",
      "Iteration 245, Cost: 0.01025185134192338\n",
      "Iteration 246, Cost: 0.010182454218477927\n",
      "Iteration 247, Cost: 0.010113748416921142\n",
      "Iteration 248, Cost: 0.010045728165274426\n",
      "Iteration 249, Cost: 0.009978387730664588\n",
      "Iteration 250, Cost: 0.00991172140718282\n",
      "Iteration 251, Cost: 0.009845723504882912\n",
      "Iteration 252, Cost: 0.00978038833992672\n",
      "Iteration 253, Cost: 0.009715710225874415\n",
      "Iteration 254, Cost: 0.009651683466107118\n",
      "Iteration 255, Cost: 0.009588302347359901\n",
      "Iteration 256, Cost: 0.009525561134334412\n",
      "Iteration 257, Cost: 0.009463454065352468\n",
      "Iteration 258, Cost: 0.009401975349004731\n",
      "Iteration 259, Cost: 0.009341119161742592\n",
      "Iteration 260, Cost: 0.009280879646356181\n",
      "Iteration 261, Cost: 0.00922125091127746\n",
      "Iteration 262, Cost: 0.009162227030644088\n",
      "Iteration 263, Cost: 0.00910380204505786\n",
      "Iteration 264, Cost: 0.009045969962970227\n",
      "Iteration 265, Cost: 0.008988724762627265\n",
      "Iteration 266, Cost: 0.0089320603945069\n",
      "Iteration 267, Cost: 0.008875970784182634\n",
      "Iteration 268, Cost: 0.008820449835549813\n",
      "Iteration 269, Cost: 0.008765491434353146\n",
      "Iteration 270, Cost: 0.008711089451956942\n",
      "Iteration 271, Cost: 0.00865723774930313\n",
      "Iteration 272, Cost: 0.008603930181005552\n",
      "Iteration 273, Cost: 0.008551160599532984\n",
      "Iteration 274, Cost: 0.00849892285943729\n",
      "Iteration 275, Cost: 0.008447210821587111\n",
      "Iteration 276, Cost: 0.008396018357371646\n",
      "Iteration 277, Cost: 0.008345339352842877\n",
      "Iteration 278, Cost: 0.00829516771276866\n",
      "Iteration 279, Cost: 0.008245497364572692\n",
      "Iteration 280, Cost: 0.008196322262141046\n",
      "Iteration 281, Cost: 0.008147636389478168\n",
      "Iteration 282, Cost: 0.008099433764198584\n",
      "Iteration 283, Cost: 0.008051708440843264\n",
      "Iteration 284, Cost: 0.008004454514012415\n",
      "Iteration 285, Cost: 0.007957666121308845\n",
      "Iteration 286, Cost: 0.007911337446088233\n",
      "Iteration 287, Cost: 0.007865462720014671\n",
      "Iteration 288, Cost: 0.007820036225421427\n",
      "Iteration 289, Cost: 0.007775052297478633\n",
      "Iteration 290, Cost: 0.007730505326170661\n",
      "Iteration 291, Cost: 0.007686389758087303\n",
      "Iteration 292, Cost: 0.007642700098033624\n",
      "Iteration 293, Cost: 0.007599430910464302\n",
      "Iteration 294, Cost: 0.007556576820748754\n",
      "Iteration 295, Cost: 0.00751413251627398\n",
      "Iteration 296, Cost: 0.007472092747392311\n",
      "Iteration 297, Cost: 0.007430452328221564\n",
      "Iteration 298, Cost: 0.007389206137305295\n",
      "Iteration 299, Cost: 0.007348349118140839\n",
      "Iteration 300, Cost: 0.007307876279582944\n",
      "Iteration 301, Cost: 0.007267782696130634\n",
      "Iteration 302, Cost: 0.007228063508104896\n",
      "Iteration 303, Cost: 0.007188713921724646\n",
      "Iteration 304, Cost: 0.007149729209088132\n",
      "Iteration 305, Cost: 0.007111104708066835\n",
      "Iteration 306, Cost: 0.007072835822118585\n",
      "Iteration 307, Cost: 0.007034918020026397\n",
      "Iteration 308, Cost: 0.006997346835569217\n",
      "Iteration 309, Cost: 0.006960117867130512\n",
      "Iteration 310, Cost: 0.006923226777250325\n",
      "Iteration 311, Cost: 0.00688666929212609\n",
      "Iteration 312, Cost: 0.006850441201067299\n",
      "Iteration 313, Cost: 0.006814538355908702\n",
      "Iteration 314, Cost: 0.006778956670386549\n",
      "Iteration 315, Cost: 0.006743692119481992\n",
      "Iteration 316, Cost: 0.006708740738735614\n",
      "Iteration 317, Cost: 0.006674098623536665\n",
      "Iteration 318, Cost: 0.006639761928390447\n",
      "Iteration 319, Cost: 0.00660572686616697\n",
      "Iteration 320, Cost: 0.00657198970733378\n",
      "Iteration 321, Cost: 0.006538546779175701\n",
      "Iteration 322, Cost: 0.006505394465003935\n",
      "Iteration 323, Cost: 0.006472529203356827\n",
      "Iteration 324, Cost: 0.006439947487194423\n",
      "Iteration 325, Cost: 0.006407645863088681\n",
      "Iteration 326, Cost: 0.006375620930411181\n",
      "Iteration 327, Cost: 0.006343869340519876\n",
      "Iteration 328, Cost: 0.006312387795946389\n",
      "Iteration 329, Cost: 0.006281173049585129\n",
      "Iteration 330, Cost: 0.006250221903885497\n",
      "Iteration 331, Cost: 0.006219531210048215\n",
      "Iteration 332, Cost: 0.006189097867226734\n",
      "Iteration 333, Cost: 0.0061589188217346606\n",
      "Iteration 334, Cost: 0.006128991066259896\n",
      "Iteration 335, Cost: 0.00609931163908626\n",
      "Iteration 336, Cost: 0.006069877623323112\n",
      "Iteration 337, Cost: 0.00604068614614361\n",
      "Iteration 338, Cost: 0.006011734378031989\n",
      "Iteration 339, Cost: 0.005983019532040299\n",
      "Iteration 340, Cost: 0.005954538863054956\n",
      "Iteration 341, Cost: 0.005926289667073383\n",
      "Iteration 342, Cost: 0.005898269280490975\n",
      "Iteration 343, Cost: 0.005870475079398633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 344, Cost: 0.005842904478890971\n",
      "Iteration 345, Cost: 0.005815554932385369\n",
      "Iteration 346, Cost: 0.005788423930951934\n",
      "Iteration 347, Cost: 0.005761509002654425\n",
      "Iteration 348, Cost: 0.005734807711902216\n",
      "Iteration 349, Cost: 0.005708317658813249\n",
      "Iteration 350, Cost: 0.005682036478588012\n",
      "Iteration 351, Cost: 0.005655961840894472\n",
      "Iteration 352, Cost: 0.005630091449263947\n",
      "Iteration 353, Cost: 0.00560442304049781\n",
      "Iteration 354, Cost: 0.005578954384084987\n",
      "Iteration 355, Cost: 0.005553683281630121\n",
      "Iteration 356, Cost: 0.005528607566292291\n",
      "Iteration 357, Cost: 0.005503725102234238\n",
      "Iteration 358, Cost: 0.005479033784081879\n",
      "Iteration 359, Cost: 0.0054545315363940415\n",
      "Iteration 360, Cost: 0.005430216313142281\n",
      "Iteration 361, Cost: 0.005406086097200595\n",
      "Iteration 362, Cost: 0.005382138899844952\n",
      "Iteration 363, Cost: 0.005358372760262412\n",
      "Iteration 364, Cost: 0.005334785745069751\n",
      "Iteration 365, Cost: 0.0053113759478413975\n",
      "Iteration 366, Cost: 0.005288141488646518\n",
      "Iteration 367, Cost: 0.00526508051359512\n",
      "Iteration 368, Cost: 0.005242191194393005\n",
      "Iteration 369, Cost: 0.005219471727905383\n",
      "Iteration 370, Cost: 0.0051969203357290295\n",
      "Iteration 371, Cost: 0.005174535263772788\n",
      "Iteration 372, Cost: 0.005152314781846306\n",
      "Iteration 373, Cost: 0.005130257183256785\n",
      "Iteration 374, Cost: 0.005108360784413641\n",
      "Iteration 375, Cost: 0.0050866239244408895\n",
      "Iteration 376, Cost: 0.005065044964797121\n",
      "Iteration 377, Cost: 0.005043622288902881\n",
      "Iteration 378, Cost: 0.0050223543017753535\n",
      "Iteration 379, Cost: 0.005001239429670159\n",
      "Iteration 380, Cost: 0.004980276119730126\n",
      "Converged!\n",
      "Iteration 1, Cost: 0.25074716624125254\n",
      "Iteration 2, Cost: 0.2347306400889607\n",
      "Iteration 3, Cost: 0.22547325321541292\n",
      "Iteration 4, Cost: 0.2196785379829671\n",
      "Iteration 5, Cost: 0.2155252977727125\n",
      "Iteration 6, Cost: 0.21216071031512598\n",
      "Iteration 7, Cost: 0.20919490660688958\n",
      "Iteration 8, Cost: 0.20644238181571326\n",
      "Iteration 9, Cost: 0.20380902894433275\n",
      "Iteration 10, Cost: 0.2012436729483791\n",
      "Iteration 11, Cost: 0.1987165524837847\n",
      "Iteration 12, Cost: 0.19620919463578612\n",
      "Iteration 13, Cost: 0.1937093395380899\n",
      "Iteration 14, Cost: 0.19120827521725867\n",
      "Iteration 15, Cost: 0.18869942342177898\n",
      "Iteration 16, Cost: 0.18617761857758666\n",
      "Iteration 17, Cost: 0.1836387804671255\n",
      "Iteration 18, Cost: 0.18107980481025596\n",
      "Iteration 19, Cost: 0.17849856267811973\n",
      "Iteration 20, Cost: 0.17589393885286744\n",
      "Iteration 21, Cost: 0.17326586327855387\n",
      "Iteration 22, Cost: 0.17061530581576995\n",
      "Iteration 23, Cost: 0.1679442179977474\n",
      "Iteration 24, Cost: 0.16525541988153952\n",
      "Iteration 25, Cost: 0.16255244548812986\n",
      "Iteration 26, Cost: 0.159839372726825\n",
      "Iteration 27, Cost: 0.15712066722852866\n",
      "Iteration 28, Cost: 0.15440106114081917\n",
      "Iteration 29, Cost: 0.1516854709366048\n",
      "Iteration 30, Cost: 0.14897894096343867\n",
      "Iteration 31, Cost: 0.14628659008047687\n",
      "Iteration 32, Cost: 0.14361354085688316\n",
      "Iteration 33, Cost: 0.14096482204465507\n",
      "Iteration 34, Cost: 0.13834524912261015\n",
      "Iteration 35, Cost: 0.1357592981651603\n",
      "Iteration 36, Cost: 0.13321099165000808\n",
      "Iteration 37, Cost: 0.13070381162927103\n",
      "Iteration 38, Cost: 0.1282406490054493\n",
      "Iteration 39, Cost: 0.12582379059668597\n",
      "Iteration 40, Cost: 0.1234549400209721\n",
      "Iteration 41, Cost: 0.12113526472815873\n",
      "Iteration 42, Cost: 0.11886545977740286\n",
      "Iteration 43, Cost: 0.11664581902887286\n",
      "Iteration 44, Cost: 0.11447630591941912\n",
      "Iteration 45, Cost: 0.11235661832417507\n",
      "Iteration 46, Cost: 0.11028624450093164\n",
      "Iteration 47, Cost: 0.10826450924453214\n",
      "Iteration 48, Cost: 0.10629061087311596\n",
      "Iteration 49, Cost: 0.1043636504938469\n",
      "Iteration 50, Cost: 0.10248265526966084\n",
      "Iteration 51, Cost: 0.10064659730531357\n",
      "Iteration 52, Cost: 0.09885440946098895\n",
      "Iteration 53, Cost: 0.09710499902046715\n",
      "Iteration 54, Cost: 0.09539725978015884\n",
      "Iteration 55, Cost: 0.0937300828361663\n",
      "Iteration 56, Cost: 0.0921023661474104\n",
      "Iteration 57, Cost: 0.09051302283997434\n",
      "Iteration 58, Cost: 0.08896098817500112\n",
      "Iteration 59, Cost: 0.08744522510925919\n",
      "Iteration 60, Cost: 0.08596472841447188\n",
      "Iteration 61, Cost: 0.08451852737315965\n",
      "Iteration 62, Cost: 0.0831056871240243\n",
      "Iteration 63, Cost: 0.0817253087816409\n",
      "Iteration 64, Cost: 0.08037652849895426\n",
      "Iteration 65, Cost: 0.07905851567403281\n",
      "Iteration 66, Cost: 0.07777047052293536\n",
      "Iteration 67, Cost: 0.07651162124739493\n",
      "Iteration 68, Cost: 0.07528122101915254\n",
      "Iteration 69, Cost: 0.07407854498305093\n",
      "Iteration 70, Cost: 0.07290288745042985\n",
      "Iteration 71, Cost: 0.07175355941599106\n",
      "Iteration 72, Cost: 0.07062988648887254\n",
      "Iteration 73, Cost: 0.06953120728612823\n",
      "Iteration 74, Cost: 0.06845687229774845\n",
      "Iteration 75, Cost: 0.06740624319955615\n",
      "Iteration 76, Cost: 0.06637869256546927\n",
      "Iteration 77, Cost: 0.06537360391427924\n",
      "Iteration 78, Cost: 0.06439037201780867\n",
      "Iteration 79, Cost: 0.06342840339589859\n",
      "Iteration 80, Cost: 0.06248711692755229\n",
      "Iteration 81, Cost: 0.0615659445150435\n",
      "Iteration 82, Cost: 0.06066433174732489\n",
      "Iteration 83, Cost: 0.059781738519370485\n",
      "Iteration 84, Cost: 0.05891763957421513\n",
      "Iteration 85, Cost: 0.05807152494380906\n",
      "Iteration 86, Cost: 0.057242900273067325\n",
      "Iteration 87, Cost: 0.056431287018557846\n",
      "Iteration 88, Cost: 0.05563622251917584\n",
      "Iteration 89, Cost: 0.054857259941009213\n",
      "Iteration 90, Cost: 0.054093968102550335\n",
      "Iteration 91, Cost: 0.053345931189582786\n",
      "Iteration 92, Cost: 0.0526127483715714\n",
      "Iteration 93, Cost: 0.05189403333327709\n",
      "Iteration 94, Cost: 0.05118941373664388\n",
      "Iteration 95, Cost: 0.05049853062878134\n",
      "Iteration 96, Cost: 0.04982103781210039\n",
      "Iteration 97, Cost: 0.04915660119236809\n",
      "Iteration 98, Cost: 0.04850489811965478\n",
      "Iteration 99, Cost: 0.04786561673590231\n",
      "Iteration 100, Cost: 0.047238455341215305\n",
      "Iteration 101, Cost: 0.04662312178905761\n",
      "Iteration 102, Cost: 0.046019332918428225\n",
      "Iteration 103, Cost: 0.045426814028904146\n",
      "Iteration 104, Cost: 0.04484529840228018\n",
      "Iteration 105, Cost: 0.04427452687250642\n",
      "Iteration 106, Cost: 0.04371424744380111\n",
      "Iteration 107, Cost: 0.043164214955261036\n",
      "Iteration 108, Cost: 0.04262419078903938\n",
      "Iteration 109, Cost: 0.04209394261822349\n",
      "Iteration 110, Cost: 0.04157324418991861\n",
      "Iteration 111, Cost: 0.041061875138702526\n",
      "Iteration 112, Cost: 0.040559620825526006\n",
      "Iteration 113, Cost: 0.04006627219725442\n",
      "Iteration 114, Cost: 0.039581625662327945\n",
      "Iteration 115, Cost: 0.03910548297842091\n",
      "Iteration 116, Cost: 0.03863765114845992\n",
      "Iteration 117, Cost: 0.038177942321881174\n",
      "Iteration 118, Cost: 0.03772617369853885\n",
      "Iteration 119, Cost: 0.037282167433193465\n",
      "Iteration 120, Cost: 0.03684575053899642\n",
      "Iteration 121, Cost: 0.0364167547888287\n",
      "Iteration 122, Cost: 0.03599501661374478\n",
      "Iteration 123, Cost: 0.035580376998109085\n",
      "Iteration 124, Cost: 0.03517268137129658\n",
      "Iteration 125, Cost: 0.0347717794960584\n",
      "Iteration 126, Cost: 0.03437752535383554\n",
      "Iteration 127, Cost: 0.033989777027440264\n",
      "Iteration 128, Cost: 0.03360839658162254\n",
      "Iteration 129, Cost: 0.033233249942102944\n",
      "Iteration 130, Cost: 0.03286420677368769\n",
      "Iteration 131, Cost: 0.032501140358092866\n",
      "Iteration 132, Cost: 0.032143927472096226\n",
      "Iteration 133, Cost: 0.03179244826661078\n",
      "Iteration 134, Cost: 0.031446586147238956\n",
      "Iteration 135, Cost: 0.031106227656821025\n",
      "Iteration 136, Cost: 0.030771262360441563\n",
      "Iteration 137, Cost: 0.030441582733302455\n",
      "Iteration 138, Cost: 0.030117084051815173\n",
      "Iteration 139, Cost: 0.029797664288207553\n",
      "Iteration 140, Cost: 0.029483224008884715\n",
      "Iteration 141, Cost: 0.029173666276729085\n",
      "Iteration 142, Cost: 0.028868896557472854\n",
      "Iteration 143, Cost: 0.028568822630227766\n",
      "Iteration 144, Cost: 0.0282733545022117\n",
      "Iteration 145, Cost: 0.027982404327670737\n",
      "Iteration 146, Cost: 0.027695886330957892\n",
      "Iteration 147, Cost: 0.027413716733696896\n",
      "Iteration 148, Cost: 0.027135813685930106\n",
      "Iteration 149, Cost: 0.02686209720112494\n",
      "Iteration 150, Cost: 0.026592489094892025\n",
      "Iteration 151, Cost: 0.026326912927250846\n",
      "Iteration 152, Cost: 0.026065293948265282\n",
      "Iteration 153, Cost: 0.02580755904686034\n",
      "Iteration 154, Cost: 0.025553636702624897\n",
      "Iteration 155, Cost: 0.025303456940400097\n",
      "Iteration 156, Cost: 0.025056951287451657\n",
      "Iteration 157, Cost: 0.02481405273302478\n",
      "Iteration 158, Cost: 0.024574695690083064\n",
      "Iteration 159, Cost: 0.02433881595903738\n",
      "Iteration 160, Cost: 0.024106350693276993\n",
      "Iteration 161, Cost: 0.023877238366322884\n",
      "Iteration 162, Cost: 0.02365141874043211\n",
      "Iteration 163, Cost: 0.023428832836492094\n",
      "Iteration 164, Cost: 0.023209422905054313\n",
      "Iteration 165, Cost: 0.02299313239836847\n",
      "Iteration 166, Cost: 0.022779905943289967\n",
      "Iteration 167, Cost: 0.022569689314945667\n",
      "Iteration 168, Cost: 0.02236242941105538\n",
      "Iteration 169, Cost: 0.022158074226818637\n",
      "Iteration 170, Cost: 0.021956572830288703\n",
      "Iteration 171, Cost: 0.02175787533816761\n",
      "Iteration 172, Cost: 0.021561932891967586\n",
      "Iteration 173, Cost: 0.021368697634495427\n",
      "Iteration 174, Cost: 0.021178122686626746\n",
      "Iteration 175, Cost: 0.020990162124347195\n",
      "Iteration 176, Cost: 0.020804770956046506\n",
      "Iteration 177, Cost: 0.02062190510006011\n",
      "Iteration 178, Cost: 0.02044152136246007\n",
      "Iteration 179, Cost: 0.020263577415104213\n",
      "Iteration 180, Cost: 0.020088031773957727\n",
      "Iteration 181, Cost: 0.0199148437777065\n",
      "Iteration 182, Cost: 0.019743973566685395\n",
      "Iteration 183, Cost: 0.019575382062147463\n",
      "Iteration 184, Cost: 0.019409030945902326\n",
      "Iteration 185, Cost: 0.019244882640353008\n",
      "Iteration 186, Cost: 0.019082900288961084\n",
      "Iteration 187, Cost: 0.01892304773716924\n",
      "Iteration 188, Cost: 0.018765289513809624\n",
      "Iteration 189, Cost: 0.018609590813024268\n",
      "Iteration 190, Cost: 0.0184559174767217\n",
      "Iteration 191, Cost: 0.018304235977590913\n",
      "Iteration 192, Cost: 0.018154513402690798\n",
      "Iteration 193, Cost: 0.018006717437629324\n",
      "Iteration 194, Cost: 0.017860816351343328\n",
      "Iteration 195, Cost: 0.017716778981485472\n",
      "Iteration 196, Cost: 0.017574574720421295\n",
      "Iteration 197, Cost: 0.01743417350183511\n",
      "Iteration 198, Cost: 0.01729554578793963\n",
      "Iteration 199, Cost: 0.01715866255728075\n",
      "Iteration 200, Cost: 0.017023495293124958\n",
      "Iteration 201, Cost: 0.016890015972414133\n",
      "Iteration 202, Cost: 0.016758197055269184\n",
      "Iteration 203, Cost: 0.01662801147502153\n",
      "Iteration 204, Cost: 0.01649943262874927\n",
      "Iteration 205, Cost: 0.016372434368292853\n",
      "Iteration 206, Cost: 0.016246990991723772\n",
      "Iteration 207, Cost: 0.016123077235238547\n",
      "Iteration 208, Cost: 0.01600066826544968\n",
      "Iteration 209, Cost: 0.015879739672044698\n",
      "Iteration 210, Cost: 0.015760267460784606\n",
      "Iteration 211, Cost: 0.015642228046812992\n",
      "Iteration 212, Cost: 0.015525598248247974\n",
      "Iteration 213, Cost: 0.015410355280029526\n",
      "Iteration 214, Cost: 0.015296476747996158\n",
      "Iteration 215, Cost: 0.015183940643165679\n",
      "Iteration 216, Cost: 0.015072725336196573\n",
      "Iteration 217, Cost: 0.014962809572007466\n",
      "Iteration 218, Cost: 0.014854172464534228\n",
      "Iteration 219, Cost: 0.01474679349160542\n",
      "Iteration 220, Cost: 0.014640652489918719\n",
      "Iteration 221, Cost: 0.014535729650102509\n",
      "Iteration 222, Cost: 0.014432005511848393\n",
      "Iteration 223, Cost: 0.01432946095910214\n",
      "Iteration 224, Cost: 0.014228077215301975\n",
      "Iteration 225, Cost: 0.014127835838654734\n",
      "Iteration 226, Cost: 0.014028718717441763\n",
      "Iteration 227, Cost: 0.013930708065347734\n",
      "Iteration 228, Cost: 0.01383378641680696\n",
      "Iteration 229, Cost: 0.013737936622362742\n",
      "Iteration 230, Cost: 0.013643141844036446\n",
      "Iteration 231, Cost: 0.013549385550704013\n",
      "Iteration 232, Cost: 0.013456651513478307\n",
      "Iteration 233, Cost: 0.013364923801096691\n",
      "Iteration 234, Cost: 0.013274186775313696\n",
      "Iteration 235, Cost: 0.013184425086299353\n",
      "Iteration 236, Cost: 0.013095623668044265\n",
      "Iteration 237, Cost: 0.01300776773377289\n",
      "Iteration 238, Cost: 0.012920842771366828\n",
      "Iteration 239, Cost: 0.012834834538800322\n",
      "Iteration 240, Cost: 0.012749729059590225\n",
      "Iteration 241, Cost: 0.012665512618263057\n",
      "Iteration 242, Cost: 0.012582171755841717\n",
      "Iteration 243, Cost: 0.012499693265354639\n",
      "Iteration 244, Cost: 0.012418064187370055\n",
      "Iteration 245, Cost: 0.012337271805558235\n",
      "Iteration 246, Cost: 0.012257303642284334\n",
      "Iteration 247, Cost: 0.012178147454234548\n",
      "Iteration 248, Cost: 0.01209979122807821\n",
      "Iteration 249, Cost: 0.012022223176168212\n",
      "Iteration 250, Cost: 0.011945431732282281\n",
      "Iteration 251, Cost: 0.011869405547407228\n",
      "Iteration 252, Cost: 0.011794133485568377\n",
      "Iteration 253, Cost: 0.01171960461970614\n",
      "Iteration 254, Cost: 0.01164580822760156\n",
      "Iteration 255, Cost: 0.011572733787852566\n",
      "Iteration 256, Cost: 0.011500370975902396\n",
      "Iteration 257, Cost: 0.011428709660121697\n",
      "Iteration 258, Cost: 0.011357739897945456\n",
      "Iteration 259, Cost: 0.011287451932065947\n",
      "Iteration 260, Cost: 0.011217836186682598\n",
      "Iteration 261, Cost: 0.011148883263809695\n",
      "Iteration 262, Cost: 0.011080583939642523\n",
      "Iteration 263, Cost: 0.011012929160982602\n",
      "Iteration 264, Cost: 0.010945910041722481\n",
      "Iteration 265, Cost: 0.010879517859390345\n",
      "Iteration 266, Cost: 0.010813744051754792\n",
      "Iteration 267, Cost: 0.010748580213489812\n",
      "Iteration 268, Cost: 0.010684018092900078\n",
      "Iteration 269, Cost: 0.010620049588706477\n",
      "Iteration 270, Cost: 0.010556666746891685\n",
      "Iteration 271, Cost: 0.010493861757605699\n",
      "Iteration 272, Cost: 0.010431626952130919\n",
      "Iteration 273, Cost: 0.010369954799906479\n",
      "Iteration 274, Cost: 0.010308837905611426\n",
      "Iteration 275, Cost: 0.010248269006306241\n",
      "Iteration 276, Cost: 0.010188240968632188\n",
      "Iteration 277, Cost: 0.010128746786067929\n",
      "Iteration 278, Cost: 0.010069779576242784\n",
      "Iteration 279, Cost: 0.01001133257830599\n",
      "Iteration 280, Cost: 0.00995339915035124\n",
      "Iteration 281, Cost: 0.009895972766895845\n",
      "Iteration 282, Cost: 0.009839047016413697\n",
      "Iteration 283, Cost: 0.009782615598921325\n",
      "Iteration 284, Cost: 0.009726672323616233\n",
      "Iteration 285, Cost: 0.00967121110656668\n",
      "Iteration 286, Cost: 0.009616225968452137\n",
      "Iteration 287, Cost: 0.009561711032353532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 288, Cost: 0.00950766052159245\n",
      "Iteration 289, Cost: 0.009454068757618493\n",
      "Iteration 290, Cost: 0.009400930157943792\n",
      "Iteration 291, Cost: 0.00934823923412399\n",
      "Iteration 292, Cost: 0.009295990589784694\n",
      "Iteration 293, Cost: 0.009244178918692562\n",
      "Iteration 294, Cost: 0.009192799002870218\n",
      "Iteration 295, Cost: 0.009141845710754016\n",
      "Iteration 296, Cost: 0.009091313995393903\n",
      "Iteration 297, Cost: 0.00904119889269448\n",
      "Iteration 298, Cost: 0.008991495519696354\n",
      "Iteration 299, Cost: 0.008942199072897058\n",
      "Iteration 300, Cost: 0.008893304826610588\n",
      "Iteration 301, Cost: 0.008844808131364793\n",
      "Iteration 302, Cost: 0.008796704412335762\n",
      "Iteration 303, Cost: 0.008748989167818442\n",
      "Iteration 304, Cost: 0.00870165796773263\n",
      "Iteration 305, Cost: 0.008654706452163603\n",
      "Iteration 306, Cost: 0.008608130329936566\n",
      "Iteration 307, Cost: 0.008561925377224182\n",
      "Iteration 308, Cost: 0.008516087436186405\n",
      "Iteration 309, Cost: 0.008470612413641887\n",
      "Iteration 310, Cost: 0.008425496279770232\n",
      "Iteration 311, Cost: 0.008380735066844348\n",
      "Iteration 312, Cost: 0.008336324867992259\n",
      "Iteration 313, Cost: 0.008292261835987564\n",
      "Iteration 314, Cost: 0.008248542182068037\n",
      "Iteration 315, Cost: 0.008205162174781509\n",
      "Iteration 316, Cost: 0.008162118138858528\n",
      "Iteration 317, Cost: 0.008119406454111074\n",
      "Iteration 318, Cost: 0.008077023554356744\n",
      "Iteration 319, Cost: 0.008034965926367734\n",
      "Iteration 320, Cost: 0.007993230108844115\n",
      "Iteration 321, Cost: 0.00795181269141068\n",
      "Iteration 322, Cost: 0.007910710313636918\n",
      "Iteration 323, Cost: 0.00786991966407946\n",
      "Iteration 324, Cost: 0.007829437479346467\n",
      "Iteration 325, Cost: 0.007789260543183451\n",
      "Iteration 326, Cost: 0.0077493856855799575\n",
      "Iteration 327, Cost: 0.007709809781896631\n",
      "Iteration 328, Cost: 0.007670529752012121\n",
      "Iteration 329, Cost: 0.007631542559489385\n",
      "Iteration 330, Cost: 0.007592845210760856\n",
      "Iteration 331, Cost: 0.007554434754332059\n",
      "Iteration 332, Cost: 0.007516308280003167\n",
      "Iteration 333, Cost: 0.007478462918108083\n",
      "Iteration 334, Cost: 0.007440895838770608\n",
      "Iteration 335, Cost: 0.00740360425117726\n",
      "Iteration 336, Cost: 0.007366585402866338\n",
      "Iteration 337, Cost: 0.007329836579032818\n",
      "Iteration 338, Cost: 0.007293355101848689\n",
      "Iteration 339, Cost: 0.007257138329798361\n",
      "Iteration 340, Cost: 0.007221183657028739\n",
      "Iteration 341, Cost: 0.007185488512713624\n",
      "Iteration 342, Cost: 0.0071500503604320755\n",
      "Iteration 343, Cost: 0.007114866697560383\n",
      "Iteration 344, Cost: 0.007079935054677319\n",
      "Iteration 345, Cost: 0.007045252994982329\n",
      "Iteration 346, Cost: 0.00701081811372636\n",
      "Iteration 347, Cost: 0.006976628037654986\n",
      "Iteration 348, Cost: 0.006942680424463568\n",
      "Iteration 349, Cost: 0.006908972962264086\n",
      "Iteration 350, Cost: 0.0068755033690634185\n",
      "Iteration 351, Cost: 0.006842269392252772\n",
      "Iteration 352, Cost: 0.006809268808107941\n",
      "Iteration 353, Cost: 0.006776499421300208\n",
      "Iteration 354, Cost: 0.006743959064417552\n",
      "Iteration 355, Cost: 0.006711645597495953\n",
      "Iteration 356, Cost: 0.006679556907560546\n",
      "Iteration 357, Cost: 0.006647690908176368\n",
      "Iteration 358, Cost: 0.006616045539008451\n",
      "Iteration 359, Cost: 0.006584618765391094\n",
      "Iteration 360, Cost: 0.006553408577906021\n",
      "Iteration 361, Cost: 0.006522412991969248\n",
      "Iteration 362, Cost: 0.006491630047426442\n",
      "Iteration 363, Cost: 0.006461057808156579\n",
      "Iteration 364, Cost: 0.006430694361683662\n",
      "Iteration 365, Cost: 0.006400537818796363\n",
      "Iteration 366, Cost: 0.00637058631317535\n",
      "Iteration 367, Cost: 0.00634083800102815\n",
      "Iteration 368, Cost: 0.006311291060731343\n",
      "Iteration 369, Cost: 0.006281943692479937\n",
      "Iteration 370, Cost: 0.006252794117943728\n",
      "Iteration 371, Cost: 0.006223840579930509\n",
      "Iteration 372, Cost: 0.00619508134205594\n",
      "Iteration 373, Cost: 0.006166514688419948\n",
      "Iteration 374, Cost: 0.006138138923289475\n",
      "Iteration 375, Cost: 0.00610995237078746\n",
      "Iteration 376, Cost: 0.006081953374587872\n",
      "Iteration 377, Cost: 0.006054140297616701\n",
      "Iteration 378, Cost: 0.006026511521758714\n",
      "Iteration 379, Cost: 0.005999065447569876\n",
      "Iteration 380, Cost: 0.005971800493995327\n",
      "Iteration 381, Cost: 0.005944715098092726\n",
      "Iteration 382, Cost: 0.005917807714760899\n",
      "Iteration 383, Cost: 0.005891076816473624\n",
      "Iteration 384, Cost: 0.005864520893018482\n",
      "Iteration 385, Cost: 0.0058381384512406174\n",
      "Iteration 386, Cost: 0.005811928014791314\n",
      "Iteration 387, Cost: 0.005785888123881289\n",
      "Iteration 388, Cost: 0.00576001733503858\n",
      "Iteration 389, Cost: 0.005734314220870926\n",
      "Iteration 390, Cost: 0.005708777369832557\n",
      "Iteration 391, Cost: 0.005683405385995275\n",
      "Iteration 392, Cost: 0.005658196888823733\n",
      "Iteration 393, Cost: 0.005633150512954846\n",
      "Iteration 394, Cost: 0.005608264907981188\n",
      "Iteration 395, Cost: 0.0055835387382383505\n",
      "Iteration 396, Cost: 0.005558970682596133\n",
      "Iteration 397, Cost: 0.005534559434253484\n",
      "Iteration 398, Cost: 0.0055103037005371355\n",
      "Iteration 399, Cost: 0.005486202202703827\n",
      "Iteration 400, Cost: 0.005462253675746047\n",
      "Iteration 401, Cost: 0.005438456868201221\n",
      "Iteration 402, Cost: 0.005414810541964262\n",
      "Iteration 403, Cost: 0.005391313472103427\n",
      "Iteration 404, Cost: 0.005367964446679383\n",
      "Iteration 405, Cost: 0.005344762266567439\n",
      "Iteration 406, Cost: 0.005321705745282868\n",
      "Iteration 407, Cost: 0.005298793708809226\n",
      "Iteration 408, Cost: 0.005276024995429666\n",
      "Iteration 409, Cost: 0.0052533984555611125\n",
      "Iteration 410, Cost: 0.0052309129515912935\n",
      "Iteration 411, Cost: 0.0052085673577185205\n",
      "Iteration 412, Cost: 0.005186360559794202\n",
      "Iteration 413, Cost: 0.005164291455168013\n",
      "Iteration 414, Cost: 0.005142358952535644\n",
      "Iteration 415, Cost: 0.005120561971789131\n",
      "Iteration 416, Cost: 0.005098899443869649\n",
      "Iteration 417, Cost: 0.005077370310622763\n",
      "Iteration 418, Cost: 0.00505597352465606\n",
      "Iteration 419, Cost: 0.005034708049199129\n",
      "Iteration 420, Cost: 0.005013572857965841\n",
      "Iteration 421, Cost: 0.004992566935018848\n",
      "Converged!\n",
      "Iteration 1, Cost: 0.3421696854445059\n",
      "Iteration 2, Cost: 0.3334529135465053\n",
      "Iteration 3, Cost: 0.32868885294907185\n",
      "Iteration 4, Cost: 0.3249690365454325\n",
      "Iteration 5, Cost: 0.3212685777244049\n",
      "Iteration 6, Cost: 0.317100624247468\n",
      "Iteration 7, Cost: 0.31208841379223384\n",
      "Iteration 8, Cost: 0.30580942398761146\n",
      "Iteration 9, Cost: 0.2977200359910778\n",
      "Iteration 10, Cost: 0.287143744377837\n",
      "Iteration 11, Cost: 0.2734188861839328\n",
      "Iteration 12, Cost: 0.25641393465293405\n",
      "Iteration 13, Cost: 0.23745351000084491\n",
      "Iteration 14, Cost: 0.2196207458454325\n",
      "Iteration 15, Cost: 0.2057413460633032\n",
      "Iteration 16, Cost: 0.19604725259504605\n",
      "Iteration 17, Cost: 0.1890773341379826\n",
      "Iteration 18, Cost: 0.18350562921774075\n",
      "Iteration 19, Cost: 0.1785911016540183\n",
      "Iteration 20, Cost: 0.17398603087914008\n",
      "Iteration 21, Cost: 0.16954444769244753\n",
      "Iteration 22, Cost: 0.1652158238595088\n",
      "Iteration 23, Cost: 0.16099095755141238\n",
      "Iteration 24, Cost: 0.15687471516633408\n",
      "Iteration 25, Cost: 0.15287329515769918\n",
      "Iteration 26, Cost: 0.14898961025762206\n",
      "Iteration 27, Cost: 0.14522280073403565\n",
      "Iteration 28, Cost: 0.14156934246704195\n",
      "Iteration 29, Cost: 0.1380243802776296\n",
      "Iteration 30, Cost: 0.13458274650275281\n",
      "Iteration 31, Cost: 0.13123957444255124\n",
      "Iteration 32, Cost: 0.1279905873195953\n",
      "Iteration 33, Cost: 0.12483217238129043\n",
      "Iteration 34, Cost: 0.1217613296019134\n",
      "Iteration 35, Cost: 0.11877555737519269\n",
      "Iteration 36, Cost: 0.1158727161718368\n",
      "Iteration 37, Cost: 0.11305089614149424\n",
      "Iteration 38, Cost: 0.11030830407883954\n",
      "Iteration 39, Cost: 0.10764317741400149\n",
      "Iteration 40, Cost: 0.10505372714832142\n",
      "Iteration 41, Cost: 0.1025381076821383\n",
      "Iteration 42, Cost: 0.1000944091345453\n",
      "Iteration 43, Cost: 0.097720666792155\n",
      "Iteration 44, Cost: 0.09541488236938635\n",
      "Iteration 45, Cost: 0.09317505239407593\n",
      "Iteration 46, Cost: 0.09099919987814069\n",
      "Iteration 47, Cost: 0.08888540623307607\n",
      "Iteration 48, Cost: 0.08683184100730787\n",
      "Iteration 49, Cost: 0.08483678742679454\n",
      "Iteration 50, Cost: 0.08289866196161047\n",
      "Iteration 51, Cost: 0.08101602632200675\n",
      "Iteration 52, Cost: 0.07918759053643723\n",
      "Iteration 53, Cost: 0.07741220620638994\n",
      "Iteration 54, Cost: 0.07568884975286617\n",
      "Iteration 55, Cost: 0.07401659646918352\n",
      "Iteration 56, Cost: 0.0723945873630182\n",
      "Iteration 57, Cost: 0.07082199188221812\n",
      "Iteration 58, Cost: 0.06929797038509579\n",
      "Iteration 59, Cost: 0.06782164038058419\n",
      "Iteration 60, Cost: 0.06639205001230021\n",
      "Iteration 61, Cost: 0.06500816108627627\n",
      "Iteration 62, Cost: 0.0636688424264974\n",
      "Iteration 63, Cost: 0.06237287285398159\n",
      "Iteration 64, Cost: 0.06111895194592694\n",
      "Iteration 65, Cost: 0.05990571611730413\n",
      "Iteration 66, Cost: 0.058731757482318785\n",
      "Iteration 67, Cost: 0.05759564327238386\n",
      "Iteration 68, Cost: 0.05649593413076247\n",
      "Iteration 69, Cost: 0.055431200203963416\n",
      "Iteration 70, Cost: 0.054400034488254236\n",
      "Iteration 71, Cost: 0.05340106330614257\n",
      "Iteration 72, Cost: 0.05243295406894809\n",
      "Iteration 73, Cost: 0.051494420643074655\n",
      "Iteration 74, Cost: 0.050584226707309186\n",
      "Iteration 75, Cost: 0.04970118749591369\n",
      "Iteration 76, Cost: 0.04884417029255288\n",
      "Iteration 77, Cost: 0.048012093991929516\n",
      "Iteration 78, Cost: 0.04720392799191996\n",
      "Iteration 79, Cost: 0.04641869062655353\n",
      "Iteration 80, Cost: 0.045655447303304494\n",
      "Iteration 81, Cost: 0.04491330846844779\n",
      "Iteration 82, Cost: 0.0441914274918351\n",
      "Iteration 83, Cost: 0.04348899853678933\n",
      "Iteration 84, Cost: 0.0428052544609692\n",
      "Iteration 85, Cost: 0.042139464779015263\n",
      "Iteration 86, Cost: 0.04149093370660377\n",
      "Iteration 87, Cost: 0.040858998297367076\n",
      "Iteration 88, Cost: 0.04024302667829001\n",
      "Iteration 89, Cost: 0.03964241638509346\n",
      "Iteration 90, Cost: 0.03905659279632176\n",
      "Iteration 91, Cost: 0.03848500766301222\n",
      "Iteration 92, Cost: 0.03792713772967867\n",
      "Iteration 93, Cost: 0.03738248344168727\n",
      "Iteration 94, Cost: 0.036850567733793665\n",
      "Iteration 95, Cost: 0.03633093489453744\n",
      "Iteration 96, Cost: 0.035823149501274215\n",
      "Iteration 97, Cost: 0.03532679542080996\n",
      "Iteration 98, Cost: 0.03484147487084811\n",
      "Iteration 99, Cost: 0.034366807537738346\n",
      "Iteration 100, Cost: 0.033902429746308194\n",
      "Iteration 101, Cost: 0.03344799367785363\n",
      "Iteration 102, Cost: 0.03300316663265174\n",
      "Iteration 103, Cost: 0.03256763033363574\n",
      "Iteration 104, Cost: 0.03214108026813474\n",
      "Iteration 105, Cost: 0.03172322506482803\n",
      "Iteration 106, Cost: 0.0313137859032953\n",
      "Iteration 107, Cost: 0.0309124959537603\n",
      "Iteration 108, Cost: 0.030519099844827084\n",
      "Iteration 109, Cost: 0.030133353157194884\n",
      "Iteration 110, Cost: 0.029755021941511958\n",
      "Iteration 111, Cost: 0.029383882258689587\n",
      "Iteration 112, Cost: 0.029019719741146872\n",
      "Iteration 113, Cost: 0.02866232917359468\n",
      "Iteration 114, Cost: 0.028311514092093943\n",
      "Iteration 115, Cost: 0.02796708640024016\n",
      "Iteration 116, Cost: 0.02762886600143195\n",
      "Iteration 117, Cost: 0.02729668044627863\n",
      "Iteration 118, Cost: 0.026970364594288778\n",
      "Iteration 119, Cost: 0.02664976028906042\n",
      "Iteration 120, Cost: 0.02633471604626318\n",
      "Iteration 121, Cost: 0.026025086753764783\n",
      "Iteration 122, Cost: 0.025720733383308676\n",
      "Iteration 123, Cost: 0.02542152271319688\n",
      "Iteration 124, Cost: 0.025127327061473973\n",
      "Iteration 125, Cost: 0.02483802402914371\n",
      "Iteration 126, Cost: 0.024553496252981477\n",
      "Iteration 127, Cost: 0.02427363116753337\n",
      "Iteration 128, Cost: 0.023998320775917556\n",
      "Iteration 129, Cost: 0.023727461429066506\n",
      "Iteration 130, Cost: 0.023460953613070015\n",
      "Iteration 131, Cost: 0.02319870174430042\n",
      "Iteration 132, Cost: 0.022940613972022215\n",
      "Iteration 133, Cost: 0.022686601988210424\n",
      "Iteration 134, Cost: 0.02243658084432448\n",
      "Iteration 135, Cost: 0.022190468774808184\n",
      "Iteration 136, Cost: 0.021948187027111248\n",
      "Iteration 137, Cost: 0.021709659698053288\n",
      "Iteration 138, Cost: 0.021474813576377514\n",
      "Iteration 139, Cost: 0.02124357799136729\n",
      "Iteration 140, Cost: 0.021015884667424455\n",
      "Iteration 141, Cost: 0.02079166758453245\n",
      "Iteration 142, Cost: 0.02057086284454979\n",
      "Iteration 143, Cost: 0.02035340854329874\n",
      "Iteration 144, Cost: 0.020139244648430796\n",
      "Iteration 145, Cost: 0.019928312883062495\n",
      "Iteration 146, Cost: 0.019720556615183286\n",
      "Iteration 147, Cost: 0.01951592075284053\n",
      "Iteration 148, Cost: 0.019314351645105008\n",
      "Iteration 149, Cost: 0.01911579698881413\n",
      "Iteration 150, Cost: 0.018920205741079065\n",
      "Iteration 151, Cost: 0.018727528037526864\n",
      "Iteration 152, Cost: 0.01853771511622997\n",
      "Iteration 153, Cost: 0.01835071924725367\n",
      "Iteration 154, Cost: 0.018166493667728104\n",
      "Iteration 155, Cost: 0.01798499252232577\n",
      "Iteration 156, Cost: 0.017806170808999815\n",
      "Iteration 157, Cost: 0.017629984329812214\n",
      "Iteration 158, Cost: 0.01745638964665647\n",
      "Iteration 159, Cost: 0.0172853440416564\n",
      "Iteration 160, Cost: 0.01711680548200186\n",
      "Iteration 161, Cost: 0.016950732588964748\n",
      "Iteration 162, Cost: 0.01678708461082404\n",
      "Iteration 163, Cost: 0.016625821399417776\n",
      "Iteration 164, Cost: 0.016466903390032744\n",
      "Iteration 165, Cost: 0.016310291584338922\n",
      "Iteration 166, Cost: 0.01615594753607615\n",
      "Iteration 167, Cost: 0.01600383333920346\n",
      "Iteration 168, Cost: 0.01585391161822863\n",
      "Iteration 169, Cost: 0.015706145520444508\n",
      "Iteration 170, Cost: 0.015560498709810737\n",
      "Iteration 171, Cost: 0.015416935362233212\n",
      "Iteration 172, Cost: 0.015275420162009197\n",
      "Iteration 173, Cost: 0.015135918299222562\n",
      "Iteration 174, Cost: 0.014998395467891201\n",
      "Iteration 175, Cost: 0.0148628178646867\n",
      "Iteration 176, Cost: 0.014729152188064084\n",
      "Iteration 177, Cost: 0.014597365637657804\n",
      "Iteration 178, Cost: 0.014467425913817111\n",
      "Iteration 179, Cost: 0.014339301217171085\n",
      "Iteration 180, Cost: 0.01421296024812935\n",
      "Iteration 181, Cost: 0.014088372206239572\n",
      "Iteration 182, Cost: 0.013965506789336776\n",
      "Iteration 183, Cost: 0.01384433419243224\n",
      "Iteration 184, Cost: 0.013724825106301504\n",
      "Iteration 185, Cost: 0.013606950715741398\n",
      "Iteration 186, Cost: 0.013490682697475354\n",
      "Iteration 187, Cost: 0.013375993217694511\n",
      "Iteration 188, Cost: 0.013262854929229266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 189, Cost: 0.01315124096835196\n",
      "Iteration 190, Cost: 0.013041124951216784\n",
      "Iteration 191, Cost: 0.012932480969947046\n",
      "Iteration 192, Cost: 0.012825283588383755\n",
      "Iteration 193, Cost: 0.012719507837512004\n",
      "Iteration 194, Cost: 0.012615129210584115\n",
      "Iteration 195, Cost: 0.012512123657959786\n",
      "Iteration 196, Cost: 0.012410467581684853\n",
      "Iteration 197, Cost: 0.012310137829830854\n",
      "Iteration 198, Cost: 0.012211111690617785\n",
      "Iteration 199, Cost: 0.012113366886342698\n",
      "Iteration 200, Cost: 0.01201688156713615\n",
      "Iteration 201, Cost: 0.011921634304568492\n",
      "Iteration 202, Cost: 0.011827604085126838\n",
      "Iteration 203, Cost: 0.01173477030358323\n",
      "Iteration 204, Cost: 0.011643112756273265\n",
      "Iteration 205, Cost: 0.011552611634303706\n",
      "Iteration 206, Cost: 0.011463247516706552\n",
      "Iteration 207, Cost: 0.011375001363555888\n",
      "Iteration 208, Cost: 0.011287854509062929\n",
      "Iteration 209, Cost: 0.011201788654663564\n",
      "Iteration 210, Cost: 0.011116785862111586\n",
      "Iteration 211, Cost: 0.01103282854658996\n",
      "Iteration 212, Cost: 0.010949899469851322\n",
      "Iteration 213, Cost: 0.01086798173339801\n",
      "Iteration 214, Cost: 0.010787058771711091\n",
      "Iteration 215, Cost: 0.010707114345536868\n",
      "Iteration 216, Cost: 0.010628132535238576\n",
      "Iteration 217, Cost: 0.010550097734220188\n",
      "Iteration 218, Cost: 0.010472994642428539\n",
      "Iteration 219, Cost: 0.010396808259939242\n",
      "Iteration 220, Cost: 0.010321523880631237\n",
      "Iteration 221, Cost: 0.0102471270859542\n",
      "Iteration 222, Cost: 0.010173603738792496\n",
      "Iteration 223, Cost: 0.010100939977428796\n",
      "Iteration 224, Cost: 0.010029122209610008\n",
      "Iteration 225, Cost: 0.009958137106717668\n",
      "Iteration 226, Cost: 0.009887971598044625\n",
      "Iteration 227, Cost: 0.009818612865179306\n",
      "Iteration 228, Cost: 0.009750048336498634\n",
      "Iteration 229, Cost: 0.009682265681770217\n",
      "Iteration 230, Cost: 0.00961525280686422\n",
      "Iteration 231, Cost: 0.00954899784857496\n",
      "Iteration 232, Cost: 0.00948348916955209\n",
      "Iteration 233, Cost: 0.00941871535334093\n",
      "Iteration 234, Cost: 0.009354665199531343\n",
      "Iteration 235, Cost: 0.009291327719014338\n",
      "Iteration 236, Cost: 0.00922869212934539\n",
      "Iteration 237, Cost: 0.009166747850213377\n",
      "Iteration 238, Cost: 0.009105484499013747\n",
      "Iteration 239, Cost: 0.009044891886524634\n",
      "Iteration 240, Cost: 0.008984960012684303\n",
      "Iteration 241, Cost: 0.008925679062468312\n",
      "Iteration 242, Cost: 0.008867039401864741\n",
      "Iteration 243, Cost: 0.008809031573945676\n",
      "Iteration 244, Cost: 0.008751646295033107\n",
      "Iteration 245, Cost: 0.008694874450957383\n",
      "Iteration 246, Cost: 0.00863870709340631\n",
      "Iteration 247, Cost: 0.008583135436362884\n",
      "Iteration 248, Cost: 0.008528150852629793\n",
      "Iteration 249, Cost: 0.00847374487043858\n",
      "Iteration 250, Cost: 0.008419909170141567\n",
      "Iteration 251, Cost: 0.008366635580984513\n",
      "Iteration 252, Cost: 0.00831391607795804\n",
      "Iteration 253, Cost: 0.008261742778725831\n",
      "Iteration 254, Cost: 0.008210107940627643\n",
      "Iteration 255, Cost: 0.008159003957755253\n",
      "Iteration 256, Cost: 0.008108423358099375\n",
      "Iteration 257, Cost: 0.008058358800765653\n",
      "Iteration 258, Cost: 0.008008803073257973\n",
      "Iteration 259, Cost: 0.00795974908882716\n",
      "Iteration 260, Cost: 0.007911189883883384\n",
      "Iteration 261, Cost: 0.007863118615470462\n",
      "Iteration 262, Cost: 0.007815528558800402\n",
      "Iteration 263, Cost: 0.007768413104846531\n",
      "Iteration 264, Cost: 0.007721765757993513\n",
      "Iteration 265, Cost: 0.007675580133742818\n",
      "Iteration 266, Cost: 0.007629849956471964\n",
      "Iteration 267, Cost: 0.007584569057246176\n",
      "Iteration 268, Cost: 0.007539731371680889\n",
      "Iteration 269, Cost: 0.007495330937853795\n",
      "Iteration 270, Cost: 0.007451361894265024\n",
      "Iteration 271, Cost: 0.007407818477844126\n",
      "Iteration 272, Cost: 0.007364695022002628\n",
      "Iteration 273, Cost: 0.0073219859547308625\n",
      "Iteration 274, Cost: 0.007279685796737929\n",
      "Iteration 275, Cost: 0.007237789159633586\n",
      "Iteration 276, Cost: 0.007196290744150976\n",
      "Iteration 277, Cost: 0.007155185338409074\n",
      "Iteration 278, Cost: 0.00711446781621385\n",
      "Iteration 279, Cost: 0.007074133135397078\n",
      "Iteration 280, Cost: 0.007034176336191847\n",
      "Iteration 281, Cost: 0.006994592539643831\n",
      "Iteration 282, Cost: 0.006955376946057329\n",
      "Iteration 283, Cost: 0.006916524833475307\n",
      "Iteration 284, Cost: 0.006878031556192456\n",
      "Iteration 285, Cost: 0.006839892543300527\n",
      "Iteration 286, Cost: 0.006802103297265077\n",
      "Iteration 287, Cost: 0.006764659392532898\n",
      "Iteration 288, Cost: 0.006727556474169324\n",
      "Iteration 289, Cost: 0.006690790256524719\n",
      "Iteration 290, Cost: 0.0066543565219294295\n",
      "Iteration 291, Cost: 0.0066182511194165\n",
      "Iteration 292, Cost: 0.006582469963471514\n",
      "Iteration 293, Cost: 0.006547009032808882\n",
      "Iteration 294, Cost: 0.006511864369173973\n",
      "Iteration 295, Cost: 0.0064770320761704785\n",
      "Iteration 296, Cost: 0.0064425083181124014\n",
      "Iteration 297, Cost: 0.006408289318900114\n",
      "Iteration 298, Cost: 0.006374371360919926\n",
      "Iteration 299, Cost: 0.006340750783966605\n",
      "Iteration 300, Cost: 0.006307423984188341\n",
      "Iteration 301, Cost: 0.006274387413053631\n",
      "Iteration 302, Cost: 0.006241637576339585\n",
      "Iteration 303, Cost: 0.006209171033141173\n",
      "Iteration 304, Cost: 0.006176984394900927\n",
      "Iteration 305, Cost: 0.006145074324458673\n",
      "Iteration 306, Cost: 0.006113437535120775\n",
      "Iteration 307, Cost: 0.006082070789748534\n",
      "Iteration 308, Cost: 0.006050970899865258\n",
      "Iteration 309, Cost: 0.006020134724781616\n",
      "Iteration 310, Cost: 0.005989559170738829\n",
      "Iteration 311, Cost: 0.005959241190069386\n",
      "Iteration 312, Cost: 0.005929177780374801\n",
      "Iteration 313, Cost: 0.005899365983720086\n",
      "Iteration 314, Cost: 0.005869802885844578\n",
      "Iteration 315, Cost: 0.005840485615388733\n",
      "Iteration 316, Cost: 0.005811411343136551\n",
      "Iteration 317, Cost: 0.00578257728127329\n",
      "Iteration 318, Cost: 0.0057539806826581216\n",
      "Iteration 319, Cost: 0.005725618840111427\n",
      "Iteration 320, Cost: 0.00569748908571637\n",
      "Iteration 321, Cost: 0.005669588790134489\n",
      "Iteration 322, Cost: 0.005641915361934953\n",
      "Iteration 323, Cost: 0.005614466246937222\n",
      "Iteration 324, Cost: 0.005587238927566781\n",
      "Iteration 325, Cost: 0.005560230922223705\n",
      "Iteration 326, Cost: 0.005533439784663752\n",
      "Iteration 327, Cost: 0.005506863103391702\n",
      "Iteration 328, Cost: 0.005480498501066705\n",
      "Iteration 329, Cost: 0.0054543436339193506\n",
      "Iteration 330, Cost: 0.0054283961911802256\n",
      "Iteration 331, Cost: 0.005402653894519688\n",
      "Iteration 332, Cost: 0.005377114497498651\n",
      "Iteration 333, Cost: 0.005351775785030088\n",
      "Iteration 334, Cost: 0.005326635572851078\n",
      "Iteration 335, Cost: 0.005301691707005117\n",
      "Iteration 336, Cost: 0.005276942063334524\n",
      "Iteration 337, Cost: 0.0052523845469826785\n",
      "Iteration 338, Cost: 0.005228017091905901\n",
      "Iteration 339, Cost: 0.005203837660394772\n",
      "Iteration 340, Cost: 0.0051798442426046615\n",
      "Iteration 341, Cost: 0.0051560348560953095\n",
      "Iteration 342, Cost: 0.005132407545379222\n",
      "Iteration 343, Cost: 0.005108960381478749\n",
      "Iteration 344, Cost: 0.00508569146149157\n",
      "Iteration 345, Cost: 0.005062598908164514\n",
      "Iteration 346, Cost: 0.005039680869475453\n",
      "Iteration 347, Cost: 0.005016935518223131\n",
      "Iteration 348, Cost: 0.0049943610516247705\n",
      "Converged!\n",
      "Iteration 1, Cost: 0.26669485897959505\n",
      "Iteration 2, Cost: 0.25492185849360294\n",
      "Iteration 3, Cost: 0.24670314435485516\n",
      "Iteration 4, Cost: 0.24073703060446014\n",
      "Iteration 5, Cost: 0.23593050523874884\n",
      "Iteration 6, Cost: 0.23157760907079356\n",
      "Iteration 7, Cost: 0.22733654130418485\n",
      "Iteration 8, Cost: 0.2230662436534456\n",
      "Iteration 9, Cost: 0.2187126117235503\n",
      "Iteration 10, Cost: 0.21425742124429034\n",
      "Iteration 11, Cost: 0.20969882451867933\n",
      "Iteration 12, Cost: 0.20504442789669983\n",
      "Iteration 13, Cost: 0.2003092272943183\n",
      "Iteration 14, Cost: 0.19551532568080665\n",
      "Iteration 15, Cost: 0.19069180650096973\n",
      "Iteration 16, Cost: 0.18587366487859988\n",
      "Iteration 17, Cost: 0.18109923477890616\n",
      "Iteration 18, Cost: 0.17640631285143285\n",
      "Iteration 19, Cost: 0.1718279745224835\n",
      "Iteration 20, Cost: 0.16738946183306028\n",
      "Iteration 21, Cost: 0.16310712050958215\n",
      "Iteration 22, Cost: 0.15898933883841077\n",
      "Iteration 23, Cost: 0.15503854386661373\n",
      "Iteration 24, Cost: 0.15125317841257577\n",
      "Iteration 25, Cost: 0.14762910224210152\n",
      "Iteration 26, Cost: 0.14416043680920101\n",
      "Iteration 27, Cost: 0.1408401124385928\n",
      "Iteration 28, Cost: 0.13766031783443175\n",
      "Iteration 29, Cost: 0.13461291196314776\n",
      "Iteration 30, Cost: 0.1316897767929645\n",
      "Iteration 31, Cost: 0.1288830799239577\n",
      "Iteration 32, Cost: 0.12618543884120062\n",
      "Iteration 33, Cost: 0.1235900003533869\n",
      "Iteration 34, Cost: 0.12109045860373777\n",
      "Iteration 35, Cost: 0.11868103487381038\n",
      "Iteration 36, Cost: 0.11635643743805375\n",
      "Iteration 37, Cost: 0.11411181382486685\n",
      "Iteration 38, Cost: 0.11194270285291734\n",
      "Iteration 39, Cost: 0.10984499024880863\n",
      "Iteration 40, Cost: 0.1078148693868058\n",
      "Iteration 41, Cost: 0.10584880739622185\n",
      "Iteration 42, Cost: 0.10394351623245238\n",
      "Iteration 43, Cost: 0.10209592804950972\n",
      "Iteration 44, Cost: 0.10030317416959794\n",
      "Iteration 45, Cost: 0.09856256700745239\n",
      "Iteration 46, Cost: 0.09687158440864195\n",
      "Iteration 47, Cost: 0.09522785596697808\n",
      "Iteration 48, Cost: 0.09362915098039247\n",
      "Iteration 49, Cost: 0.0920733677815417\n",
      "Iteration 50, Cost: 0.09055852423886825\n",
      "Iteration 51, Cost: 0.08908274926823505\n",
      "Iteration 52, Cost: 0.08764427522762906\n",
      "Iteration 53, Cost: 0.08624143109077147\n",
      "Iteration 54, Cost: 0.08487263631227836\n",
      "Iteration 55, Cost: 0.08353639530923407\n",
      "Iteration 56, Cost: 0.08223129249309927\n",
      "Iteration 57, Cost: 0.080955987792786\n",
      "Iteration 58, Cost: 0.07970921261517376\n",
      "Iteration 59, Cost: 0.07848976619376893\n",
      "Iteration 60, Cost: 0.07729651227991376\n",
      "Iteration 61, Cost: 0.0761283761341255\n",
      "Iteration 62, Cost: 0.07498434177791863\n",
      "Iteration 63, Cost: 0.07386344946892688\n",
      "Iteration 64, Cost: 0.07276479336437794\n",
      "Iteration 65, Cost: 0.07168751934005176\n",
      "Iteration 66, Cost: 0.07063082293384373\n",
      "Iteration 67, Cost: 0.06959394738503086\n",
      "Iteration 68, Cost: 0.06857618174237017\n",
      "Iteration 69, Cost: 0.06757685901631656\n",
      "Iteration 70, Cost: 0.06659535435299403\n",
      "Iteration 71, Cost: 0.06563108321014596\n",
      "Iteration 72, Cost: 0.06468349951817041\n",
      "Iteration 73, Cost: 0.06375209381254102\n",
      "Iteration 74, Cost: 0.06283639132743284\n",
      "Iteration 75, Cost: 0.06193595004419754\n",
      "Iteration 76, Cost: 0.061050358692425276\n",
      "Iteration 77, Cost: 0.06017923470561766\n",
      "Iteration 78, Cost: 0.05932222213787825\n",
      "Iteration 79, Cost: 0.058478989552366925\n",
      "Iteration 80, Cost: 0.057649227896400056\n",
      "Iteration 81, Cost: 0.05683264838181639\n",
      "Iteration 82, Cost: 0.05602898039236397\n",
      "Iteration 83, Cost: 0.05523796944218511\n",
      "Iteration 84, Cost: 0.05445937521079135\n",
      "Iteration 85, Cost: 0.0536929696800713\n",
      "Iteration 86, Cost: 0.05293853539776654\n",
      "Iteration 87, Cost: 0.052195863889464046\n",
      "Iteration 88, Cost: 0.05146475423756617\n",
      "Iteration 89, Cost: 0.050745011841079775\n",
      "Iteration 90, Cost: 0.05003644736467432\n",
      "Iteration 91, Cost: 0.04933887587962053\n",
      "Iteration 92, Cost: 0.048652116193300274\n",
      "Iteration 93, Cost: 0.04797599035834299\n",
      "Iteration 94, Cost: 0.04731032334742592\n",
      "Iteration 95, Cost: 0.04665494287564701\n",
      "Iteration 96, Cost: 0.04600967934932355\n",
      "Iteration 97, Cost: 0.04537436591817721\n",
      "Iteration 98, Cost: 0.044748838607131185\n",
      "Iteration 99, Cost: 0.04413293650428125\n",
      "Iteration 100, Cost: 0.04352650198285991\n",
      "Iteration 101, Cost: 0.04292938093700256\n",
      "Iteration 102, Cost: 0.0423414230136365\n",
      "Iteration 103, Cost: 0.04176248182564465\n",
      "Iteration 104, Cost: 0.041192415134417536\n",
      "Iteration 105, Cost: 0.040631084992838726\n",
      "Iteration 106, Cost: 0.040078357842526606\n",
      "Iteration 107, Cost: 0.03953410456168583\n",
      "Iteration 108, Cost: 0.03899820046214727\n",
      "Iteration 109, Cost: 0.038470525236065915\n",
      "Iteration 110, Cost: 0.03795096285429532\n",
      "Iteration 111, Cost: 0.03743940141967708\n",
      "Iteration 112, Cost: 0.036935732979401156\n",
      "Iteration 113, Cost: 0.03643985330124073\n",
      "Iteration 114, Cost: 0.03595166161888423\n",
      "Iteration 115, Cost: 0.035471060351818046\n",
      "Iteration 116, Cost: 0.03499795480529621\n",
      "Iteration 117, Cost: 0.03453225285590423\n",
      "Iteration 118, Cost: 0.03407386462811558\n",
      "Iteration 119, Cost: 0.03362270216707493\n",
      "Iteration 120, Cost: 0.03317867911264098\n",
      "Iteration 121, Cost: 0.032741710379494686\n",
      "Iteration 122, Cost: 0.03231171184786835\n",
      "Iteration 123, Cost: 0.031888600069178236\n",
      "Iteration 124, Cost: 0.03147229199054042\n",
      "Iteration 125, Cost: 0.03106270470180984\n",
      "Iteration 126, Cost: 0.030659755208397203\n",
      "Iteration 127, Cost: 0.03026336023268138\n",
      "Iteration 128, Cost: 0.029873436046343235\n",
      "Iteration 129, Cost: 0.02948989833540142\n",
      "Iteration 130, Cost: 0.029112662099138435\n",
      "Iteration 131, Cost: 0.028741641583477605\n",
      "Iteration 132, Cost: 0.028376750248724443\n",
      "Iteration 133, Cost: 0.028017900770937274\n",
      "Iteration 134, Cost: 0.027665005075563503\n",
      "Iteration 135, Cost: 0.02731797440138878\n",
      "Iteration 136, Cost: 0.026976719392315718\n",
      "Iteration 137, Cost: 0.02664115021403197\n",
      "Iteration 138, Cost: 0.0263111766922567\n",
      "Iteration 139, Cost: 0.025986708468975796\n",
      "Iteration 140, Cost: 0.02566765517289387\n",
      "Iteration 141, Cost: 0.02535392660024156\n",
      "Iteration 142, Cost: 0.025045432902076673\n",
      "Iteration 143, Cost: 0.02474208477429714\n",
      "Iteration 144, Cost: 0.024443793646734383\n",
      "Iteration 145, Cost: 0.024150471867904286\n",
      "Iteration 146, Cost: 0.0238620328822503\n",
      "Iteration 147, Cost: 0.02357839139700592\n",
      "Iteration 148, Cost: 0.023299463536123645\n",
      "Iteration 149, Cost: 0.0230251669790545\n",
      "Iteration 150, Cost: 0.022755421082509223\n",
      "Iteration 151, Cost: 0.022490146983683436\n",
      "Iteration 152, Cost: 0.022229267683779627\n",
      "Iteration 153, Cost: 0.02197270811100568\n",
      "Iteration 154, Cost: 0.02172039516257004\n",
      "Iteration 155, Cost: 0.02147225772552615\n",
      "Iteration 156, Cost: 0.02122822667664269\n",
      "Iteration 157, Cost: 0.020988234861788788\n",
      "Iteration 158, Cost: 0.020752217055626066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 159, Cost: 0.02052010990268794\n",
      "Iteration 160, Cost: 0.020291851841201095\n",
      "Iteration 161, Cost: 0.020067383011260674\n",
      "Iteration 162, Cost: 0.019846645149205956\n",
      "Iteration 163, Cost: 0.019629581470253843\n",
      "Iteration 164, Cost: 0.01941613654162758\n",
      "Iteration 165, Cost: 0.01920625614856275\n",
      "Iteration 166, Cost: 0.01899988715567704\n",
      "Iteration 167, Cost: 0.01879697736624836\n",
      "Iteration 168, Cost: 0.018597475381953665\n",
      "Iteration 169, Cost: 0.018401330465575006\n",
      "Iteration 170, Cost: 0.018208492409077463\n",
      "Iteration 171, Cost: 0.018018911409306056\n",
      "Iteration 172, Cost: 0.017832537953338026\n",
      "Iteration 173, Cost: 0.017649322715266556\n",
      "Iteration 174, Cost: 0.017469216465890298\n",
      "Iteration 175, Cost: 0.01729216999644773\n",
      "Iteration 176, Cost: 0.017118134057177874\n",
      "Iteration 177, Cost: 0.01694705931112087\n",
      "Iteration 178, Cost: 0.01677889630320556\n",
      "Iteration 179, Cost: 0.016613595444319625\n",
      "Iteration 180, Cost: 0.01645110700973135\n",
      "Iteration 181, Cost: 0.016291381150942027\n",
      "Iteration 182, Cost: 0.016134367919801267\n",
      "Iteration 183, Cost: 0.015980017303520992\n",
      "Iteration 184, Cost: 0.015828279269079752\n",
      "Iteration 185, Cost: 0.0156791038154192\n",
      "Iteration 186, Cost: 0.015532441031796306\n",
      "Iteration 187, Cost: 0.015388241160665752\n",
      "Iteration 188, Cost: 0.015246454663520457\n",
      "Iteration 189, Cost: 0.015107032288208937\n",
      "Iteration 190, Cost: 0.014969925136367858\n",
      "Iteration 191, Cost: 0.014835084729750496\n",
      "Iteration 192, Cost: 0.014702463074387766\n",
      "Iteration 193, Cost: 0.014572012721682992\n",
      "Iteration 194, Cost: 0.014443686825706825\n",
      "Iteration 195, Cost: 0.014317439196120269\n",
      "Iteration 196, Cost: 0.014193224346307121\n",
      "Iteration 197, Cost: 0.014070997536438824\n",
      "Iteration 198, Cost: 0.013950714811322384\n",
      "Iteration 199, Cost: 0.013832333032994667\n",
      "Iteration 200, Cost: 0.013715809908122501\n",
      "Iteration 201, Cost: 0.01360110401034886\n",
      "Iteration 202, Cost: 0.013488174797789956\n",
      "Iteration 203, Cost: 0.01337698262593903\n",
      "Iteration 204, Cost: 0.013267488756269681\n",
      "Iteration 205, Cost: 0.013159655360857055\n",
      "Iteration 206, Cost: 0.013053445523350611\n",
      "Iteration 207, Cost: 0.012948823236638136\n",
      "Iteration 208, Cost: 0.012845753397540089\n",
      "Iteration 209, Cost: 0.012744201798866046\n",
      "Iteration 210, Cost: 0.012644135119153667\n",
      "Iteration 211, Cost: 0.012545520910395076\n",
      "Iteration 212, Cost: 0.0124483275840381\n",
      "Iteration 213, Cost: 0.012352524395530067\n",
      "Iteration 214, Cost: 0.012258081427651548\n",
      "Iteration 215, Cost: 0.012164969572866526\n",
      "Iteration 216, Cost: 0.01207316051489471\n",
      "Iteration 217, Cost: 0.011982626709691432\n",
      "Iteration 218, Cost: 0.011893341366001049\n",
      "Iteration 219, Cost: 0.011805278425631149\n",
      "Iteration 220, Cost: 0.011718412543577398\n",
      "Iteration 221, Cost: 0.011632719068112718\n",
      "Iteration 222, Cost: 0.011548174020939227\n",
      "Iteration 223, Cost: 0.011464754077487883\n",
      "Iteration 224, Cost: 0.01138243654743799\n",
      "Iteration 225, Cost: 0.011301199355517606\n",
      "Iteration 226, Cost: 0.011221021022635511\n",
      "Iteration 227, Cost: 0.01114188064738649\n",
      "Iteration 228, Cost: 0.011063757887963422\n",
      "Iteration 229, Cost: 0.010986632944502635\n",
      "Iteration 230, Cost: 0.010910486541882668\n",
      "Iteration 231, Cost: 0.010835299912991076\n",
      "Iteration 232, Cost: 0.010761054782469175\n",
      "Iteration 233, Cost: 0.010687733350940457\n",
      "Iteration 234, Cost: 0.010615318279724917\n",
      "Iteration 235, Cost: 0.01054379267603848\n",
      "Iteration 236, Cost: 0.010473140078674182\n",
      "Iteration 237, Cost: 0.010403344444159633\n",
      "Iteration 238, Cost: 0.010334390133383416\n",
      "Iteration 239, Cost: 0.010266261898681855\n",
      "Iteration 240, Cost: 0.010198944871376115\n",
      "Iteration 241, Cost: 0.010132424549748888\n",
      "Iteration 242, Cost: 0.010066686787449134\n",
      "Iteration 243, Cost: 0.01000171778231278\n",
      "Iteration 244, Cost: 0.009937504065586937\n",
      "Iteration 245, Cost: 0.009874032491545002\n",
      "Iteration 246, Cost: 0.009811290227479782\n",
      "Iteration 247, Cost: 0.009749264744061854\n",
      "Iteration 248, Cost: 0.009687943806050396\n",
      "Iteration 249, Cost: 0.009627315463343781\n",
      "Iteration 250, Cost: 0.009567368042357572\n",
      "Iteration 251, Cost: 0.00950809013771761\n",
      "Iteration 252, Cost: 0.009449470604256232\n",
      "Iteration 253, Cost: 0.009391498549299963\n",
      "Iteration 254, Cost: 0.00933416332523734\n",
      "Iteration 255, Cost: 0.009277454522355778\n",
      "Iteration 256, Cost: 0.009221361961936871\n",
      "Iteration 257, Cost: 0.009165875689599763\n",
      "Iteration 258, Cost: 0.009110985968882601\n",
      "Iteration 259, Cost: 0.009056683275052524\n",
      "Iteration 260, Cost: 0.009002958289134843\n",
      "Iteration 261, Cost: 0.008949801892152607\n",
      "Iteration 262, Cost: 0.008897205159567911\n",
      "Iteration 263, Cost: 0.008845159355916803\n",
      "Iteration 264, Cost: 0.008793655929629884\n",
      "Iteration 265, Cost: 0.00874268650803103\n",
      "Iteration 266, Cost: 0.008692242892507052\n",
      "Iteration 267, Cost: 0.008642317053841298\n",
      "Iteration 268, Cost: 0.008592901127704639\n",
      "Iteration 269, Cost: 0.00854398741029741\n",
      "Iteration 270, Cost: 0.008495568354136306\n",
      "Iteration 271, Cost: 0.008447636563980388\n",
      "Iteration 272, Cost: 0.008400184792890673\n",
      "Iteration 273, Cost: 0.008353205938417952\n",
      "Iteration 274, Cost: 0.008306693038913836\n",
      "Iteration 275, Cost: 0.008260639269960143\n",
      "Iteration 276, Cost: 0.008215037940911989\n",
      "Iteration 277, Cost: 0.008169882491550163\n",
      "Iteration 278, Cost: 0.008125166488838603\n",
      "Iteration 279, Cost: 0.008080883623782844\n",
      "Iteration 280, Cost: 0.008037027708385653\n",
      "Iteration 281, Cost: 0.007993592672696132\n",
      "Iteration 282, Cost: 0.00795057256194877\n",
      "Iteration 283, Cost: 0.007907961533789034\n",
      "Iteration 284, Cost: 0.007865753855582356\n",
      "Iteration 285, Cost: 0.007823943901803349\n",
      "Iteration 286, Cost: 0.007782526151502365\n",
      "Iteration 287, Cost: 0.007741495185846528\n",
      "Iteration 288, Cost: 0.00770084568573261\n",
      "Iteration 289, Cost: 0.00766057242946909\n",
      "Iteration 290, Cost: 0.007620670290525004\n",
      "Iteration 291, Cost: 0.007581134235343151\n",
      "Iteration 292, Cost: 0.007541959321215461\n",
      "Iteration 293, Cost: 0.007503140694218291\n",
      "Iteration 294, Cost: 0.007464673587205618\n",
      "Iteration 295, Cost: 0.007426553317858133\n",
      "Iteration 296, Cost: 0.0073887752867862954\n",
      "Iteration 297, Cost: 0.007351334975685569\n",
      "Iteration 298, Cost: 0.007314227945542032\n",
      "Iteration 299, Cost: 0.007277449834886749\n",
      "Iteration 300, Cost: 0.007240996358097189\n",
      "Iteration 301, Cost: 0.007204863303744246\n",
      "Iteration 302, Cost: 0.007169046532983309\n",
      "Iteration 303, Cost: 0.007133541977987964\n",
      "Iteration 304, Cost: 0.007098345640424974\n",
      "Iteration 305, Cost: 0.007063453589969187\n",
      "Iteration 306, Cost: 0.007028861962857138\n",
      "Iteration 307, Cost: 0.006994566960478096\n",
      "Iteration 308, Cost: 0.006960564848001398\n",
      "Iteration 309, Cost: 0.006926851953038943\n",
      "Iteration 310, Cost: 0.006893424664341748\n",
      "Iteration 311, Cost: 0.006860279430529533\n",
      "Iteration 312, Cost: 0.0068274127588523114\n",
      "Iteration 313, Cost: 0.006794821213983052\n",
      "Iteration 314, Cost: 0.0067625014168404135\n",
      "Iteration 315, Cost: 0.006730450043440727\n",
      "Iteration 316, Cost: 0.006698663823778304\n",
      "Iteration 317, Cost: 0.006667139540733236\n",
      "Iteration 318, Cost: 0.006635874029005921\n",
      "Iteration 319, Cost: 0.006604864174077494\n",
      "Iteration 320, Cost: 0.006574106911195399\n",
      "Iteration 321, Cost: 0.006543599224383447\n",
      "Iteration 322, Cost: 0.006513338145475569\n",
      "Iteration 323, Cost: 0.00648332075317265\n",
      "Iteration 324, Cost: 0.006453544172121772\n",
      "Iteration 325, Cost: 0.006424005572017232\n",
      "Iteration 326, Cost: 0.00639470216672269\n",
      "Iteration 327, Cost: 0.006365631213413942\n",
      "Iteration 328, Cost: 0.006336790011741664\n",
      "Iteration 329, Cost: 0.0063081759030135945\n",
      "Iteration 330, Cost: 0.006279786269395654\n",
      "Iteration 331, Cost: 0.006251618533131458\n",
      "Iteration 332, Cost: 0.006223670155779704\n",
      "Iteration 333, Cost: 0.006195938637469006\n",
      "Iteration 334, Cost: 0.006168421516169632\n",
      "Iteration 335, Cost: 0.006141116366981764\n",
      "Iteration 336, Cost: 0.006114020801439783\n",
      "Iteration 337, Cost: 0.00608713246683219\n",
      "Iteration 338, Cost: 0.006060449045536723\n",
      "Iteration 339, Cost: 0.006033968254370292\n",
      "Iteration 340, Cost: 0.006007687843953335\n",
      "Iteration 341, Cost: 0.005981605598088203\n",
      "Iteration 342, Cost: 0.005955719333151245\n",
      "Iteration 343, Cost: 0.005930026897498214\n",
      "Iteration 344, Cost: 0.005904526170882632\n",
      "Iteration 345, Cost: 0.005879215063886836\n",
      "Iteration 346, Cost: 0.005854091517365336\n",
      "Iteration 347, Cost: 0.005829153501900189\n",
      "Iteration 348, Cost: 0.005804399017268074\n",
      "Iteration 349, Cost: 0.005779826091918794\n",
      "Iteration 350, Cost: 0.005755432782464898\n",
      "Iteration 351, Cost: 0.005731217173182149\n",
      "Iteration 352, Cost: 0.005707177375520585\n",
      "Iteration 353, Cost: 0.005683311527625876\n",
      "Iteration 354, Cost: 0.005659617793870755\n",
      "Iteration 355, Cost: 0.005636094364396262\n",
      "Iteration 356, Cost: 0.0056127394546625605\n",
      "Iteration 357, Cost: 0.005589551305009096\n",
      "Iteration 358, Cost: 0.005566528180223856\n",
      "Iteration 359, Cost: 0.005543668369121529\n",
      "Iteration 360, Cost: 0.00552097018413035\n",
      "Iteration 361, Cost: 0.005498431960887393\n",
      "Iteration 362, Cost: 0.005476052057842143\n",
      "Iteration 363, Cost: 0.0054538288558681385\n",
      "Iteration 364, Cost: 0.005431760757882477\n",
      "Iteration 365, Cost: 0.0054098461884730204\n",
      "Iteration 366, Cost: 0.005388083593533097\n",
      "Iteration 367, Cost: 0.005366471439903557\n",
      "Iteration 368, Cost: 0.005345008215021968\n",
      "Iteration 369, Cost: 0.005323692426578794\n",
      "Iteration 370, Cost: 0.005302522602180437\n",
      "Iteration 371, Cost: 0.005281497289018929\n",
      "Iteration 372, Cost: 0.005260615053548125\n",
      "Iteration 373, Cost: 0.005239874481166304\n",
      "Iteration 374, Cost: 0.005219274175904961\n",
      "Iteration 375, Cost: 0.005198812760123675\n",
      "Iteration 376, Cost: 0.0051784888742109495\n",
      "Iteration 377, Cost: 0.005158301176290821\n",
      "Iteration 378, Cost: 0.00513824834193517\n",
      "Iteration 379, Cost: 0.0051183290638815585\n",
      "Iteration 380, Cost: 0.005098542051756511\n",
      "Iteration 381, Cost: 0.005078886031804077\n",
      "Iteration 382, Cost: 0.005059359746619583\n",
      "Iteration 383, Cost: 0.005039961954888466\n",
      "Iteration 384, Cost: 0.005020691431130032\n",
      "Iteration 385, Cost: 0.005001546965446092\n",
      "Iteration 386, Cost: 0.0049825273632743165\n",
      "Converged!\n",
      "Iteration 1, Cost: 0.3020402199719605\n",
      "Iteration 2, Cost: 0.27354246020448536\n",
      "Iteration 3, Cost: 0.253756854421376\n",
      "Iteration 4, Cost: 0.24280684701440858\n",
      "Iteration 5, Cost: 0.23658013663524044\n",
      "Iteration 6, Cost: 0.23223941202819207\n",
      "Iteration 7, Cost: 0.22854343185971832\n",
      "Iteration 8, Cost: 0.22499474093522326\n",
      "Iteration 9, Cost: 0.22138812922062343\n",
      "Iteration 10, Cost: 0.21763459785863495\n",
      "Iteration 11, Cost: 0.21369510214051785\n",
      "Iteration 12, Cost: 0.2095551505705779\n",
      "Iteration 13, Cost: 0.20521492103892713\n",
      "Iteration 14, Cost: 0.200685435813967\n",
      "Iteration 15, Cost: 0.1959871065864073\n",
      "Iteration 16, Cost: 0.1911490992895422\n",
      "Iteration 17, Cost: 0.18620869924364553\n",
      "Iteration 18, Cost: 0.18121003384549003\n",
      "Iteration 19, Cost: 0.17620161188074782\n",
      "Iteration 20, Cost: 0.17123249821249026\n",
      "Iteration 21, Cost: 0.16634762812328957\n",
      "Iteration 22, Cost: 0.16158345576407065\n",
      "Iteration 23, Cost: 0.15696530020087157\n",
      "Iteration 24, Cost: 0.1525071512136412\n",
      "Iteration 25, Cost: 0.14821366545447817\n",
      "Iteration 26, Cost: 0.14408329143573356\n",
      "Iteration 27, Cost: 0.14011132820464542\n",
      "Iteration 28, Cost: 0.13629214317081634\n",
      "Iteration 29, Cost: 0.13262035082642645\n",
      "Iteration 30, Cost: 0.12909116173574306\n",
      "Iteration 31, Cost: 0.1257002567972187\n",
      "Iteration 32, Cost: 0.12244349772433685\n",
      "Iteration 33, Cost: 0.11931666517207715\n",
      "Iteration 34, Cost: 0.11631530245697132\n",
      "Iteration 35, Cost: 0.11343466903112738\n",
      "Iteration 36, Cost: 0.11066977436495114\n",
      "Iteration 37, Cost: 0.10801545651605536\n",
      "Iteration 38, Cost: 0.10546647638651484\n",
      "Iteration 39, Cost: 0.10301760888814895\n",
      "Iteration 40, Cost: 0.100663721171625\n",
      "Iteration 41, Cost: 0.09839983426592418\n",
      "Iteration 42, Cost: 0.09622116810699029\n",
      "Iteration 43, Cost: 0.09412317173043841\n",
      "Iteration 44, Cost: 0.0921015410730776\n",
      "Iteration 45, Cost: 0.09015222689033099\n",
      "Iteration 46, Cost: 0.08827143507461536\n",
      "Iteration 47, Cost: 0.0864556213331506\n",
      "Iteration 48, Cost: 0.0847014818428312\n",
      "Iteration 49, Cost: 0.0830059411849178\n",
      "Iteration 50, Cost: 0.08136613858784561\n",
      "Iteration 51, Cost: 0.07977941327467382\n",
      "Iteration 52, Cost: 0.07824328951973432\n",
      "Iteration 53, Cost: 0.07675546186221809\n",
      "Iteration 54, Cost: 0.07531378079783148\n",
      "Iteration 55, Cost: 0.07391623916864867\n",
      "Iteration 56, Cost: 0.07256095939177984\n",
      "Iteration 57, Cost: 0.07124618160589119\n",
      "Iteration 58, Cost: 0.06997025276784864\n",
      "Iteration 59, Cost: 0.06873161669714062\n",
      "Iteration 60, Cost: 0.06752880504095546\n",
      "Iteration 61, Cost: 0.06636042911586637\n",
      "Iteration 62, Cost: 0.06522517257132567\n",
      "Iteration 63, Cost: 0.06412178481415941\n",
      "Iteration 64, Cost: 0.06304907513080216\n",
      "Iteration 65, Cost: 0.062005907444137025\n",
      "Iteration 66, Cost: 0.060991195643730005\n",
      "Iteration 67, Cost: 0.0600038994313453\n",
      "Iteration 68, Cost: 0.05904302062743088\n",
      "Iteration 69, Cost: 0.05810759988841477\n",
      "Iteration 70, Cost: 0.05719671378890441\n",
      "Iteration 71, Cost: 0.05630947222706269\n",
      "Iteration 72, Cost: 0.05544501611544043\n",
      "Iteration 73, Cost: 0.05460251532331819\n",
      "Iteration 74, Cost: 0.053781166840128194\n",
      "Iteration 75, Cost: 0.05298019313279127\n",
      "Iteration 76, Cost: 0.05219884067282973\n",
      "Iteration 77, Cost: 0.05143637861193016\n",
      "Iteration 78, Cost: 0.050692097587257595\n",
      "Iteration 79, Cost: 0.049965308640293006\n",
      "Iteration 80, Cost: 0.0492553422353043\n",
      "Iteration 81, Cost: 0.04856154736579283\n",
      "Iteration 82, Cost: 0.04788329073940096\n",
      "Iteration 83, Cost: 0.04721995603384087\n",
      "Iteration 84, Cost: 0.046570943218425837\n",
      "Iteration 85, Cost: 0.04593566793776627\n",
      "Iteration 86, Cost: 0.04531356095614542\n",
      "Iteration 87, Cost: 0.04470406766302388\n",
      "Iteration 88, Cost: 0.04410664764204208\n",
      "Iteration 89, Cost: 0.0435207743077981\n",
      "Iteration 90, Cost: 0.042945934616564076\n",
      "Iteration 91, Cost: 0.04238162885894952\n",
      "Iteration 92, Cost: 0.041827370544285994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 93, Cost: 0.04128268638813494\n",
      "Iteration 94, Cost: 0.04074711641572053\n",
      "Iteration 95, Cost: 0.04022021419513597\n",
      "Iteration 96, Cost: 0.03970154721469844\n",
      "Iteration 97, Cost: 0.039190697418619566\n",
      "Iteration 98, Cost: 0.03868726191395102\n",
      "Iteration 99, Cost: 0.03819085385925205\n",
      "Iteration 100, Cost: 0.03770110354126732\n",
      "Iteration 101, Cost: 0.03721765963975772\n",
      "Iteration 102, Cost: 0.036740190672186014\n",
      "Iteration 103, Cost: 0.03626838659902118\n",
      "Iteration 104, Cost: 0.03580196055696485\n",
      "Iteration 105, Cost: 0.03534065067168395\n",
      "Iteration 106, Cost: 0.034884221884301604\n",
      "Iteration 107, Cost: 0.03443246770808486\n",
      "Iteration 108, Cost: 0.033985211815133\n",
      "Iteration 109, Cost: 0.03354230933957914\n",
      "Iteration 110, Cost: 0.03310364777639738\n",
      "Iteration 111, Cost: 0.03266914735595729\n",
      "Iteration 112, Cost: 0.03223876078623967\n",
      "Iteration 113, Cost: 0.031812472278492314\n",
      "Iteration 114, Cost: 0.0313902958080279\n",
      "Iteration 115, Cost: 0.030972272607998706\n",
      "Iteration 116, Cost: 0.030558467946550386\n",
      "Iteration 117, Cost: 0.03014896729133156\n",
      "Iteration 118, Cost: 0.029743872013555876\n",
      "Iteration 119, Cost: 0.02934329482042542\n",
      "Iteration 120, Cost: 0.02894735512475378\n",
      "Iteration 121, Cost: 0.028556174561377903\n",
      "Iteration 122, Cost: 0.028169872841557135\n",
      "Iteration 123, Cost: 0.02778856410195878\n",
      "Iteration 124, Cost: 0.02741235385911864\n",
      "Iteration 125, Cost: 0.027041336629662318\n",
      "Iteration 126, Cost: 0.026675594227255423\n",
      "Iteration 127, Cost: 0.02631519470433841\n",
      "Iteration 128, Cost: 0.02596019187361364\n",
      "Iteration 129, Cost: 0.025610625322498748\n",
      "Iteration 130, Cost: 0.025266520823123216\n",
      "Iteration 131, Cost: 0.024927891039423448\n",
      "Iteration 132, Cost: 0.024594736439240393\n",
      "Iteration 133, Cost: 0.024267046330569975\n",
      "Iteration 134, Cost: 0.023944799954950934\n",
      "Iteration 135, Cost: 0.023627967585496235\n",
      "Iteration 136, Cost: 0.02331651159089503\n",
      "Iteration 137, Cost: 0.02301038743895215\n",
      "Iteration 138, Cost: 0.02270954462344527\n",
      "Iteration 139, Cost: 0.022413927506155562\n",
      "Iteration 140, Cost: 0.022123476071978874\n",
      "Iteration 141, Cost: 0.0218381265992979\n",
      "Iteration 142, Cost: 0.02155781225059802\n",
      "Iteration 143, Cost: 0.02128246358995371\n",
      "Iteration 144, Cost: 0.021012009034790072\n",
      "Iteration 145, Cost: 0.020746375249484268\n",
      "Iteration 146, Cost: 0.02048548748811822\n",
      "Iteration 147, Cost: 0.020229269893184573\n",
      "Iteration 148, Cost: 0.019977645756400434\n",
      "Iteration 149, Cost: 0.01973053774707946\n",
      "Iteration 150, Cost: 0.01948786811281098\n",
      "Iteration 151, Cost: 0.019249558856527293\n",
      "Iteration 152, Cost: 0.019015531893430367\n",
      "Iteration 153, Cost: 0.018785709190703753\n",
      "Iteration 154, Cost: 0.018560012892458972\n",
      "Iteration 155, Cost: 0.01833836543195455\n",
      "Iteration 156, Cost: 0.01812068963277608\n",
      "Iteration 157, Cost: 0.01790690880037104\n",
      "Iteration 158, Cost: 0.017696946805085854\n",
      "Iteration 159, Cost: 0.017490728157648364\n",
      "Iteration 160, Cost: 0.01728817807787103\n",
      "Iteration 161, Cost: 0.017089222557212243\n",
      "Iteration 162, Cost: 0.016893788415721533\n",
      "Iteration 163, Cost: 0.01670180335380407\n",
      "Iteration 164, Cost: 0.016513195999167344\n",
      "Iteration 165, Cost: 0.016327895949255634\n",
      "Iteration 166, Cost: 0.016145833809432363\n",
      "Iteration 167, Cost: 0.015966941227135482\n",
      "Iteration 168, Cost: 0.01579115092220384\n",
      "Iteration 169, Cost: 0.01561839671355237\n",
      "Iteration 170, Cost: 0.015448613542358724\n",
      "Iteration 171, Cost: 0.015281737491913529\n",
      "Iteration 172, Cost: 0.015117705804278736\n",
      "Iteration 173, Cost: 0.014956456893893822\n",
      "Iteration 174, Cost: 0.01479793035826652\n",
      "Iteration 175, Cost: 0.014642066985883202\n",
      "Iteration 176, Cost: 0.01448880876147324\n",
      "Iteration 177, Cost: 0.014338098868761444\n",
      "Iteration 178, Cost: 0.014189881690842957\n",
      "Iteration 179, Cost: 0.014044102808314808\n",
      "Iteration 180, Cost: 0.013900708995298557\n",
      "Iteration 181, Cost: 0.013759648213487933\n",
      "Iteration 182, Cost: 0.013620869604354813\n",
      "Iteration 183, Cost: 0.013484323479645626\n",
      "Iteration 184, Cost: 0.013349961310298775\n",
      "Iteration 185, Cost: 0.013217735713911409\n",
      "Iteration 186, Cost: 0.01308760044088152\n",
      "Iteration 187, Cost: 0.01295951035934804\n",
      "Iteration 188, Cost: 0.012833421439048358\n",
      "Iteration 189, Cost: 0.012709290734208699\n",
      "Iteration 190, Cost: 0.012587076365578658\n",
      "Iteration 191, Cost: 0.012466737501716684\n",
      "Iteration 192, Cost: 0.012348234339628556\n",
      "Iteration 193, Cost: 0.012231528084855973\n",
      "Iteration 194, Cost: 0.012116580931107391\n",
      "Iteration 195, Cost: 0.012003356039517878\n",
      "Iteration 196, Cost: 0.011891817517619882\n",
      "Iteration 197, Cost: 0.011781930398101182\n",
      "Iteration 198, Cost: 0.011673660617421512\n",
      "Iteration 199, Cost: 0.011566974994353942\n",
      "Iteration 200, Cost: 0.0114618412085122\n",
      "Iteration 201, Cost: 0.011358227778920262\n",
      "Iteration 202, Cost: 0.011256104042675701\n",
      "Iteration 203, Cost: 0.011155440133753714\n",
      "Iteration 204, Cost: 0.011056206961994384\n",
      "Iteration 205, Cost: 0.010958376192311528\n",
      "Iteration 206, Cost: 0.010861920224157339\n",
      "Iteration 207, Cost: 0.010766812171273404\n",
      "Iteration 208, Cost: 0.010673025841754918\n",
      "Iteration 209, Cost: 0.010580535718451618\n",
      "Iteration 210, Cost: 0.010489316939725758\n",
      "Iteration 211, Cost: 0.010399345280584432\n",
      "Iteration 212, Cost: 0.01031059713420088\n",
      "Iteration 213, Cost: 0.010223049493836697\n",
      "Iteration 214, Cost: 0.010136679935174642\n",
      "Iteration 215, Cost: 0.0100514665990695\n",
      "Iteration 216, Cost: 0.009967388174722357\n",
      "Iteration 217, Cost: 0.009884423883281942\n",
      "Iteration 218, Cost: 0.009802553461874971\n",
      "Iteration 219, Cost: 0.009721757148065861\n",
      "Iteration 220, Cost: 0.009642015664744864\n",
      "Iteration 221, Cost: 0.009563310205442483\n",
      "Iteration 222, Cost: 0.009485622420066777\n",
      "Iteration 223, Cost: 0.009408934401059383\n",
      "Iteration 224, Cost: 0.00933322866996506\n",
      "Iteration 225, Cost: 0.009258488164408874\n",
      "Iteration 226, Cost: 0.009184696225474472\n",
      "Iteration 227, Cost: 0.009111836585476318\n",
      "Iteration 228, Cost: 0.009039893356118297\n",
      "Iteration 229, Cost: 0.008968851017030606\n",
      "Iteration 230, Cost: 0.00889869440467666\n",
      "Iteration 231, Cost: 0.008829408701621326\n",
      "Iteration 232, Cost: 0.00876097942615164\n",
      "Iteration 233, Cost: 0.00869339242224104\n",
      "Iteration 234, Cost: 0.008626633849847958\n",
      "Iteration 235, Cost: 0.008560690175539601\n",
      "Iteration 236, Cost: 0.008495548163431735\n",
      "Iteration 237, Cost: 0.008431194866435196\n",
      "Iteration 238, Cost: 0.008367617617800043\n",
      "Iteration 239, Cost: 0.008304804022948179\n",
      "Iteration 240, Cost: 0.00824274195158549\n",
      "Iteration 241, Cost: 0.00818141953008453\n",
      "Iteration 242, Cost: 0.008120825134129038\n",
      "Iteration 243, Cost: 0.008060947381611639\n",
      "Iteration 244, Cost: 0.008001775125776284\n",
      "Iteration 245, Cost: 0.00794329744859709\n",
      "Iteration 246, Cost: 0.007885503654385524\n",
      "Iteration 247, Cost: 0.00782838326361798\n",
      "Iteration 248, Cost: 0.007771926006976013\n",
      "Iteration 249, Cost: 0.007716121819591733\n",
      "Iteration 250, Cost: 0.007660960835491016\n",
      "Iteration 251, Cost: 0.0076064333822274165\n",
      "Iteration 252, Cost: 0.007552529975699864\n",
      "Iteration 253, Cost: 0.007499241315147483\n",
      "Iteration 254, Cost: 0.007446558278314969\n",
      "Iteration 255, Cost: 0.007394471916782301\n",
      "Iteration 256, Cost: 0.007342973451452658\n",
      "Iteration 257, Cost: 0.007292054268192686\n",
      "Iteration 258, Cost: 0.007241705913619387\n",
      "Iteration 259, Cost: 0.007191920091028189\n",
      "Iteration 260, Cost: 0.007142688656456864\n",
      "Iteration 261, Cost: 0.007094003614880179\n",
      "Iteration 262, Cost: 0.007045857116530375\n",
      "Iteration 263, Cost: 0.006998241453338692\n",
      "Iteration 264, Cost: 0.0069511490554933775\n",
      "Iteration 265, Cost: 0.006904572488109777\n",
      "Iteration 266, Cost: 0.006858504448008245\n",
      "Iteration 267, Cost: 0.006812937760595803\n",
      "Iteration 268, Cost: 0.006767865376847605\n",
      "Iteration 269, Cost: 0.006723280370384458\n",
      "Iteration 270, Cost: 0.0066791759346427155\n",
      "Iteration 271, Cost: 0.0066355453801330965\n",
      "Iteration 272, Cost: 0.00659238213178505\n",
      "Iteration 273, Cost: 0.006549679726373431\n",
      "Iteration 274, Cost: 0.006507431810024393\n",
      "Iteration 275, Cost: 0.0064656321357975475\n",
      "Iteration 276, Cost: 0.006424274561341456\n",
      "Iteration 277, Cost: 0.006383353046619794\n",
      "Iteration 278, Cost: 0.006342861651705485\n",
      "Iteration 279, Cost: 0.006302794534640272\n",
      "Iteration 280, Cost: 0.006263145949357349\n",
      "Iteration 281, Cost: 0.00622391024366466\n",
      "Iteration 282, Cost: 0.006185081857286615\n",
      "Iteration 283, Cost: 0.0061466553199621375\n",
      "Iteration 284, Cost: 0.006108625249596892\n",
      "Iteration 285, Cost: 0.006070986350467779\n",
      "Iteration 286, Cost: 0.006033733411477751\n",
      "Iteration 287, Cost: 0.00599686130445912\n",
      "Iteration 288, Cost: 0.005960364982523632\n",
      "Iteration 289, Cost: 0.005924239478457593\n",
      "Iteration 290, Cost: 0.00588847990316045\n",
      "Iteration 291, Cost: 0.005853081444125275\n",
      "Iteration 292, Cost: 0.005818039363959626\n",
      "Iteration 293, Cost: 0.0057833489989454236\n",
      "Iteration 294, Cost: 0.005749005757636412\n",
      "Iteration 295, Cost: 0.005715005119491904\n",
      "Iteration 296, Cost: 0.005681342633545568\n",
      "Iteration 297, Cost: 0.005648013917107998\n",
      "Iteration 298, Cost: 0.005615014654501969\n",
      "Iteration 299, Cost: 0.005582340595829178\n",
      "Iteration 300, Cost: 0.005549987555767462\n",
      "Iteration 301, Cost: 0.005517951412397442\n",
      "Iteration 302, Cost: 0.005486228106057585\n",
      "Iteration 303, Cost: 0.005454813638226763\n",
      "Iteration 304, Cost: 0.0054237040704333845\n",
      "Iteration 305, Cost: 0.005392895523190205\n",
      "Iteration 306, Cost: 0.0053623841749540135\n",
      "Iteration 307, Cost: 0.005332166261109367\n",
      "Iteration 308, Cost: 0.0053022380729755665\n",
      "Iteration 309, Cost: 0.005272595956836199\n",
      "Iteration 310, Cost: 0.005243236312990468\n",
      "Iteration 311, Cost: 0.005214155594825655\n",
      "Iteration 312, Cost: 0.005185350307910037\n",
      "Iteration 313, Cost: 0.005156817009105656\n",
      "Iteration 314, Cost: 0.005128552305700271\n",
      "Iteration 315, Cost: 0.00510055285455798\n",
      "Iteration 316, Cost: 0.005072815361287905\n",
      "Iteration 317, Cost: 0.005045336579430376\n",
      "Iteration 318, Cost: 0.005018113309660174\n",
      "Iteration 319, Cost: 0.0049911423990062565\n",
      "Converged!\n",
      "Iteration 1, Cost: 0.22297702036348072\n",
      "Iteration 2, Cost: 0.21619924026909124\n",
      "Iteration 3, Cost: 0.20988354370936482\n",
      "Iteration 4, Cost: 0.20414149388271283\n",
      "Iteration 5, Cost: 0.19898266956143254\n",
      "Iteration 6, Cost: 0.19434355110090978\n",
      "Iteration 7, Cost: 0.19013539918101785\n",
      "Iteration 8, Cost: 0.1862774949291951\n",
      "Iteration 9, Cost: 0.1827085954929308\n",
      "Iteration 10, Cost: 0.1793854938726109\n",
      "Iteration 11, Cost: 0.176277744274617\n",
      "Iteration 12, Cost: 0.17336289218309525\n",
      "Iteration 13, Cost: 0.1706232573959955\n",
      "Iteration 14, Cost: 0.1680440410499626\n",
      "Iteration 15, Cost: 0.1656122903058826\n",
      "Iteration 16, Cost: 0.16331635063242178\n",
      "Iteration 17, Cost: 0.16114557433542606\n",
      "Iteration 18, Cost: 0.15909015676260055\n",
      "Iteration 19, Cost: 0.15714103396415896\n",
      "Iteration 20, Cost: 0.15528980966871747\n",
      "Iteration 21, Cost: 0.1535286967616448\n",
      "Iteration 22, Cost: 0.15185046673434155\n",
      "Iteration 23, Cost: 0.1502484042960845\n",
      "Iteration 24, Cost: 0.14871626591126014\n",
      "Iteration 25, Cost: 0.1472482416488904\n",
      "Iteration 26, Cost: 0.14583891997134132\n",
      "Iteration 27, Cost: 0.1444832551876739\n",
      "Iteration 28, Cost: 0.14317653734845676\n",
      "Iteration 29, Cost: 0.14191436439458507\n",
      "Iteration 30, Cost: 0.14069261640115158\n",
      "Iteration 31, Cost: 0.13950743178034722\n",
      "Iteration 32, Cost: 0.1383551853253153\n",
      "Iteration 33, Cost: 0.1372324679907523\n",
      "Iteration 34, Cost: 0.13613606831702701\n",
      "Iteration 35, Cost: 0.13506295541381352\n",
      "Iteration 36, Cost: 0.1340102634275952\n",
      "Iteration 37, Cost: 0.13297527742545823\n",
      "Iteration 38, Cost: 0.13195542063564886\n",
      "Iteration 39, Cost: 0.1309482429934735\n",
      "Iteration 40, Cost: 0.12995141094920745\n",
      "Iteration 41, Cost: 0.12896269850255923\n",
      "Iteration 42, Cost: 0.12797997943566738\n",
      "Iteration 43, Cost: 0.12700122072328004\n",
      "Iteration 44, Cost: 0.12602447710431758\n",
      "Iteration 45, Cost: 0.12504788680302661\n",
      "Iteration 46, Cost: 0.12406966838992403\n",
      "Iteration 47, Cost: 0.12308811877218274\n",
      "Iteration 48, Cost: 0.12210161229949557\n",
      "Iteration 49, Cost: 0.12110860096426498\n",
      "Iteration 50, Cost: 0.12010761566379566\n",
      "Iteration 51, Cost: 0.1190972684767873\n",
      "Iteration 52, Cost: 0.11807625588689424\n",
      "Iteration 53, Cost: 0.11704336286288368\n",
      "Iteration 54, Cost: 0.11599746767892119\n",
      "Iteration 55, Cost: 0.11493754733121063\n",
      "Iteration 56, Cost: 0.11386268338059935\n",
      "Iteration 57, Cost: 0.11277206802720686\n",
      "Iteration 58, Cost: 0.1116650102051561\n",
      "Iteration 59, Cost: 0.11054094147538279\n",
      "Iteration 60, Cost: 0.10939942149392869\n",
      "Iteration 61, Cost: 0.10824014284272478\n",
      "Iteration 62, Cost: 0.10706293502899479\n",
      "Iteration 63, Cost: 0.10586776748613787\n",
      "Iteration 64, Cost: 0.10465475144038709\n",
      "Iteration 65, Cost: 0.10342414054040303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 66, Cost: 0.10217633017828547\n",
      "Iteration 67, Cost: 0.100911855458296\n",
      "Iteration 68, Cost: 0.09963138779327978\n",
      "Iteration 69, Cost: 0.09833573012913396\n",
      "Iteration 70, Cost: 0.09702581081643916\n",
      "Iteration 71, Cost: 0.09570267616761857\n",
      "Iteration 72, Cost: 0.09436748175939104\n",
      "Iteration 73, Cost: 0.09302148256459582\n",
      "Iteration 74, Cost: 0.09166602202432442\n",
      "Iteration 75, Cost: 0.09030252019931176\n",
      "Iteration 76, Cost: 0.08893246116670045\n",
      "Iteration 77, Cost: 0.0875573798523949\n",
      "Iteration 78, Cost: 0.0861788485082951\n",
      "Iteration 79, Cost: 0.08479846305626602\n",
      "Iteration 80, Cost: 0.08341782952586294\n",
      "Iteration 81, Cost: 0.08203855081025038\n",
      "Iteration 82, Cost: 0.08066221395452372\n",
      "Iteration 83, Cost: 0.0792903781732218\n",
      "Iteration 84, Cost: 0.07792456376996307\n",
      "Iteration 85, Cost: 0.07656624210288919\n",
      "Iteration 86, Cost: 0.07521682670629412\n",
      "Iteration 87, Cost: 0.0738776656430531\n",
      "Iteration 88, Cost: 0.07255003512603442\n",
      "Iteration 89, Cost: 0.07123513441141957\n",
      "Iteration 90, Cost: 0.06993408193450266\n",
      "Iteration 91, Cost: 0.06864791263052661\n",
      "Iteration 92, Cost: 0.06737757636045813\n",
      "Iteration 93, Cost: 0.06612393734482697\n",
      "Iteration 94, Cost: 0.06488777449788674\n",
      "Iteration 95, Cost: 0.06366978254899783\n",
      "Iteration 96, Cost: 0.06247057383757295\n",
      "Iteration 97, Cost: 0.061290680671261935\n",
      "Iteration 98, Cost: 0.06013055814331354\n",
      "Iteration 99, Cost: 0.058990587313303804\n",
      "Iteration 100, Cost: 0.05787107866483101\n",
      "Iteration 101, Cost: 0.05677227576365426\n",
      "Iteration 102, Cost: 0.055694359049568085\n",
      "Iteration 103, Cost: 0.05463744970467364\n",
      "Iteration 104, Cost: 0.05360161354938372\n",
      "Iteration 105, Cost: 0.05258686492534453\n",
      "Iteration 106, Cost: 0.05159317053142165\n",
      "Iteration 107, Cost: 0.050620453184989304\n",
      "Iteration 108, Cost: 0.04966859548603259\n",
      "Iteration 109, Cost: 0.04873744336609736\n",
      "Iteration 110, Cost: 0.0478268095079885\n",
      "Iteration 111, Cost: 0.04693647662541392\n",
      "Iteration 112, Cost: 0.04606620059458282\n",
      "Iteration 113, Cost: 0.04521571343216886\n",
      "Iteration 114, Cost: 0.04438472611610614\n",
      "Iteration 115, Cost: 0.04357293124745398\n",
      "Iteration 116, Cost: 0.042780005553086725\n",
      "Iteration 117, Cost: 0.04200561223027268\n",
      "Iteration 118, Cost: 0.041249403135327274\n",
      "Iteration 119, Cost: 0.04051102081947933\n",
      "Iteration 120, Cost: 0.03979010041589196\n",
      "Iteration 121, Cost: 0.03908627138244358\n",
      "Iteration 122, Cost: 0.038399159105410564\n",
      "Iteration 123, Cost: 0.037728386369611414\n",
      "Iteration 124, Cost: 0.037073574700883474\n",
      "Iteration 125, Cost: 0.03643434558697552\n",
      "Iteration 126, Cost: 0.03581032158306581\n",
      "Iteration 127, Cost: 0.03520112730816414\n",
      "Iteration 128, Cost: 0.03460639033863839\n",
      "Iteration 129, Cost: 0.03402574200503361\n",
      "Iteration 130, Cost: 0.03345881809823218\n",
      "Iteration 131, Cost: 0.03290525949084768\n",
      "Iteration 132, Cost: 0.03236471267956179\n",
      "Iteration 133, Cost: 0.03183683025390927\n",
      "Iteration 134, Cost: 0.03132127129679766\n",
      "Iteration 135, Cost: 0.030817701721822764\n",
      "Iteration 136, Cost: 0.030325794552210183\n",
      "Iteration 137, Cost: 0.02984523014598371\n",
      "Iteration 138, Cost: 0.029375696371733657\n",
      "Iteration 139, Cost: 0.028916888739135546\n",
      "Iteration 140, Cost: 0.028468510488153183\n",
      "Iteration 141, Cost: 0.028030272640650344\n",
      "Iteration 142, Cost: 0.027601894017933358\n",
      "Iteration 143, Cost: 0.027183101227551985\n",
      "Iteration 144, Cost: 0.02677362862249878\n",
      "Iteration 145, Cost: 0.026373218235766882\n",
      "Iteration 146, Cost: 0.025981619693052512\n",
      "Iteration 147, Cost: 0.025598590106220807\n",
      "Iteration 148, Cost: 0.025223893949992048\n",
      "Iteration 149, Cost: 0.024857302924148443\n",
      "Iteration 150, Cost: 0.02449859580340979\n",
      "Iteration 151, Cost: 0.024147558276978964\n",
      "Iteration 152, Cost: 0.023803982779614714\n",
      "Iteration 153, Cost: 0.02346766831595004\n",
      "Iteration 154, Cost: 0.02313842027963877\n",
      "Iteration 155, Cost: 0.02281605026878156\n",
      "Iteration 156, Cost: 0.022500375898954542\n",
      "Iteration 157, Cost: 0.022191220615040537\n",
      "Iteration 158, Cost: 0.021888413502943103\n",
      "Iteration 159, Cost: 0.021591789102148946\n",
      "Iteration 160, Cost: 0.021301187219994046\n",
      "Iteration 161, Cost: 0.02101645274838375\n",
      "Iteration 162, Cost: 0.020737435483617204\n",
      "Iteration 163, Cost: 0.02046398994987245\n",
      "Iteration 164, Cost: 0.020195975226819752\n",
      "Iteration 165, Cost: 0.019933254781748382\n",
      "Iteration 166, Cost: 0.019675696306515618\n",
      "Iteration 167, Cost: 0.01942317155955641\n",
      "Iteration 168, Cost: 0.019175556213128102\n",
      "Iteration 169, Cost: 0.01893272970590661\n",
      "Iteration 170, Cost: 0.018694575100998603\n",
      "Iteration 171, Cost: 0.018460978949388123\n",
      "Iteration 172, Cost: 0.018231831158795706\n",
      "Iteration 173, Cost: 0.018007024867893115\n",
      "Iteration 174, Cost: 0.017786456325786924\n",
      "Iteration 175, Cost: 0.017570024776659046\n",
      "Iteration 176, Cost: 0.017357632349431672\n",
      "Iteration 177, Cost: 0.017149183952307592\n",
      "Iteration 178, Cost: 0.016944587172023802\n",
      "Iteration 179, Cost: 0.016743752177646877\n",
      "Iteration 180, Cost: 0.016546591628731937\n",
      "Iteration 181, Cost: 0.016353020587663008\n",
      "Iteration 182, Cost: 0.01616295643599088\n",
      "Iteration 183, Cost: 0.015976318794584717\n",
      "Iteration 184, Cost: 0.015793029447415504\n",
      "Iteration 185, Cost: 0.015613012268792694\n",
      "Iteration 186, Cost: 0.01543619315387958\n",
      "Iteration 187, Cost: 0.01526249995231829\n",
      "Iteration 188, Cost: 0.015091862404800992\n",
      "Iteration 189, Cost: 0.01492421208243038\n",
      "Iteration 190, Cost: 0.014759482328719209\n",
      "Iteration 191, Cost: 0.0145976082040856\n",
      "Iteration 192, Cost: 0.014438526432707766\n",
      "Iteration 193, Cost: 0.014282175351609028\n",
      "Iteration 194, Cost: 0.01412849486185074\n",
      "Iteration 195, Cost: 0.013977426381717768\n",
      "Iteration 196, Cost: 0.0138289128017877\n",
      "Iteration 197, Cost: 0.013682898441781348\n",
      "Iteration 198, Cost: 0.013539329009098374\n",
      "Iteration 199, Cost: 0.01339815155894772\n",
      "Iteration 200, Cost: 0.01325931445598805\n",
      "Iteration 201, Cost: 0.013122767337398845\n",
      "Iteration 202, Cost: 0.012988461077307707\n",
      "Iteration 203, Cost: 0.012856347752504297\n",
      "Iteration 204, Cost: 0.01272638060937557\n",
      "Iteration 205, Cost: 0.012598514032001344\n",
      "Iteration 206, Cost: 0.012472703511353019\n",
      "Iteration 207, Cost: 0.012348905615541913\n",
      "Iteration 208, Cost: 0.012227077961067175\n",
      "Iteration 209, Cost: 0.012107179185016255\n",
      "Iteration 210, Cost: 0.011989168918173998\n",
      "Iteration 211, Cost: 0.011873007758999077\n",
      "Iteration 212, Cost: 0.011758657248429083\n",
      "Iteration 213, Cost: 0.011646079845477957\n",
      "Iteration 214, Cost: 0.011535238903591575\n",
      "Iteration 215, Cost: 0.011426098647729444\n",
      "Iteration 216, Cost: 0.011318624152142253\n",
      "Iteration 217, Cost: 0.011212781318816884\n",
      "Iteration 218, Cost: 0.011108536856561985\n",
      "Iteration 219, Cost: 0.011005858260708852\n",
      "Iteration 220, Cost: 0.010904713793403638\n",
      "Iteration 221, Cost: 0.010805072464468279\n",
      "Iteration 222, Cost: 0.010706904012808728\n",
      "Iteration 223, Cost: 0.01061017888835015\n",
      "Iteration 224, Cost: 0.010514868234479783\n",
      "Iteration 225, Cost: 0.010420943870979188\n",
      "Iteration 226, Cost: 0.010328378277428392\n",
      "Iteration 227, Cost: 0.010237144577065349\n",
      "Iteration 228, Cost: 0.010147216521084875\n",
      "Iteration 229, Cost: 0.010058568473361906\n",
      "Iteration 230, Cost: 0.009971175395584638\n",
      "Iteration 231, Cost: 0.009885012832783726\n",
      "Iteration 232, Cost: 0.009800056899244255\n",
      "Iteration 233, Cost: 0.009716284264787786\n",
      "Iteration 234, Cost: 0.009633672141412293\n",
      "Iteration 235, Cost: 0.009552198270278244\n",
      "Iteration 236, Cost: 0.009471840909029583\n",
      "Iteration 237, Cost: 0.00939257881943875\n",
      "Iteration 238, Cost: 0.00931439125536532\n",
      "Iteration 239, Cost: 0.009237257951018188\n",
      "Iteration 240, Cost: 0.009161159109511645\n",
      "Iteration 241, Cost: 0.009086075391705929\n",
      "Iteration 242, Cost: 0.009011987905323338\n",
      "Iteration 243, Cost: 0.008938878194331111\n",
      "Iteration 244, Cost: 0.008866728228582754\n",
      "Iteration 245, Cost: 0.008795520393709674\n",
      "Iteration 246, Cost: 0.00872523748125533\n",
      "Iteration 247, Cost: 0.008655862679044323\n",
      "Iteration 248, Cost: 0.008587379561779204\n",
      "Iteration 249, Cost: 0.00851977208185789\n",
      "Iteration 250, Cost: 0.008453024560405021\n",
      "Iteration 251, Cost: 0.008387121678510585\n",
      "Iteration 252, Cost: 0.008322048468669673\n",
      "Iteration 253, Cost: 0.008257790306417144\n",
      "Iteration 254, Cost: 0.008194332902151456\n",
      "Iteration 255, Cost: 0.008131662293141972\n",
      "Iteration 256, Cost: 0.008069764835714355\n",
      "Iteration 257, Cost: 0.008008627197608831\n",
      "Iteration 258, Cost: 0.007948236350506318\n",
      "Iteration 259, Cost: 0.007888579562717577\n",
      "Iteration 260, Cost: 0.007829644392030842\n",
      "Iteration 261, Cost: 0.007771418678713415\n",
      "Iteration 262, Cost: 0.0077138905386630605\n",
      "Iteration 263, Cost: 0.007657048356705077\n",
      "Iteration 264, Cost: 0.0076008807800311915\n",
      "Iteration 265, Cost: 0.007545376711776573\n",
      "Iteration 266, Cost: 0.007490525304731388\n",
      "Iteration 267, Cost: 0.007436315955183545\n",
      "Iteration 268, Cost: 0.007382738296889372\n",
      "Iteration 269, Cost: 0.007329782195169241\n",
      "Iteration 270, Cost: 0.007277437741125109\n",
      "Iteration 271, Cost: 0.0072256952459773096\n",
      "Iteration 272, Cost: 0.007174545235517926\n",
      "Iteration 273, Cost: 0.007123978444678282\n",
      "Iteration 274, Cost: 0.00707398581220818\n",
      "Iteration 275, Cost: 0.007024558475464691\n",
      "Iteration 276, Cost: 0.00697568776530841\n",
      "Iteration 277, Cost: 0.006927365201105168\n",
      "Iteration 278, Cost: 0.006879582485831406\n",
      "Iteration 279, Cost: 0.006832331501281422\n",
      "Iteration 280, Cost: 0.006785604303374885\n",
      "Iteration 281, Cost: 0.006739393117563076\n",
      "Iteration 282, Cost: 0.006693690334332419\n",
      "Iteration 283, Cost: 0.006648488504803958\n",
      "Iteration 284, Cost: 0.0066037803364275465\n",
      "Iteration 285, Cost: 0.006559558688769541\n",
      "Iteration 286, Cost: 0.0065158165693928885\n",
      "Iteration 287, Cost: 0.006472547129828648\n",
      "Iteration 288, Cost: 0.006429743661637873\n",
      "Iteration 289, Cost: 0.00638739959256301\n",
      "Iteration 290, Cost: 0.006345508482767933\n",
      "Iteration 291, Cost: 0.006304064021165795\n",
      "Iteration 292, Cost: 0.006263060021833889\n",
      "Iteration 293, Cost: 0.0062224904205148\n",
      "Iteration 294, Cost: 0.006182349271203123\n",
      "Iteration 295, Cost: 0.006142630742817007\n",
      "Iteration 296, Cost: 0.006103329115953875\n",
      "Iteration 297, Cost: 0.00606443877972959\n",
      "Iteration 298, Cost: 0.00602595422870042\n",
      "Iteration 299, Cost: 0.005987870059867082\n",
      "Iteration 300, Cost: 0.005950180969760185\n",
      "Iteration 301, Cost: 0.005912881751606343\n",
      "Iteration 302, Cost: 0.0058759672925742246\n",
      "Iteration 303, Cost: 0.005839432571099781\n",
      "Iteration 304, Cost: 0.005803272654289859\n",
      "Iteration 305, Cost: 0.005767482695403393\n",
      "Iteration 306, Cost: 0.005732057931409253\n",
      "Iteration 307, Cost: 0.005696993680619957\n",
      "Iteration 308, Cost: 0.005662285340400195\n",
      "Iteration 309, Cost: 0.005627928384949247\n",
      "Iteration 310, Cost: 0.005593918363156247\n",
      "Iteration 311, Cost: 0.005560250896527185\n",
      "Iteration 312, Cost: 0.005526921677182561\n",
      "Iteration 313, Cost: 0.005493926465924487\n",
      "Iteration 314, Cost: 0.005461261090372015\n",
      "Iteration 315, Cost: 0.005428921443163411\n",
      "Iteration 316, Cost: 0.00539690348022408\n",
      "Iteration 317, Cost: 0.005365203219098737\n",
      "Iteration 318, Cost: 0.0053338167373464486\n",
      "Iteration 319, Cost: 0.0053027401709970666\n",
      "Iteration 320, Cost: 0.005271969713067592\n",
      "Iteration 321, Cost: 0.005241501612136903\n",
      "Iteration 322, Cost: 0.005211332170977317\n",
      "Iteration 323, Cost: 0.005181457745241411\n",
      "Iteration 324, Cost: 0.005151874742202414\n",
      "Iteration 325, Cost: 0.005122579619546641\n",
      "Iteration 326, Cost: 0.005093568884216194\n",
      "Iteration 327, Cost: 0.005064839091300375\n",
      "Iteration 328, Cost: 0.005036386842974028\n",
      "Iteration 329, Cost: 0.005008208787481196\n",
      "Iteration 330, Cost: 0.0049803016181623715\n",
      "Converged!\n",
      "Iteration 1, Cost: 0.3646791508039554\n",
      "Iteration 2, Cost: 0.31078472457840917\n",
      "Iteration 3, Cost: 0.2632080637830069\n",
      "Iteration 4, Cost: 0.2322174638739765\n",
      "Iteration 5, Cost: 0.21504436457791598\n",
      "Iteration 6, Cost: 0.2052196082405892\n",
      "Iteration 7, Cost: 0.19887696931457308\n",
      "Iteration 8, Cost: 0.19422289493826456\n",
      "Iteration 9, Cost: 0.1904357334548483\n",
      "Iteration 10, Cost: 0.18712143317307697\n",
      "Iteration 11, Cost: 0.18408095668347374\n",
      "Iteration 12, Cost: 0.1812092177463392\n",
      "Iteration 13, Cost: 0.1784488458238326\n",
      "Iteration 14, Cost: 0.1757678248854938\n",
      "Iteration 15, Cost: 0.17314810470178024\n",
      "Iteration 16, Cost: 0.17057952631107007\n",
      "Iteration 17, Cost: 0.16805644785670212\n",
      "Iteration 18, Cost: 0.16557580416155393\n",
      "Iteration 19, Cost: 0.16313595931499109\n",
      "Iteration 20, Cost: 0.160736015272788\n",
      "Iteration 21, Cost: 0.158375392509522\n",
      "Iteration 22, Cost: 0.1560535784449088\n",
      "Iteration 23, Cost: 0.15376998213615883\n",
      "Iteration 24, Cost: 0.151523857432822\n",
      "Iteration 25, Cost: 0.1493142704325271\n",
      "Iteration 26, Cost: 0.14714009529230332\n",
      "Iteration 27, Cost: 0.1450000276564223\n",
      "Iteration 28, Cost: 0.14289260842462242\n",
      "Iteration 29, Cost: 0.14081625297486056\n",
      "Iteration 30, Cost: 0.13876928263687277\n",
      "Iteration 31, Cost: 0.1367499563964912\n",
      "Iteration 32, Cost: 0.1347565016280793\n",
      "Iteration 33, Cost: 0.13278714319626017\n",
      "Iteration 34, Cost: 0.13084013060898536\n",
      "Iteration 35, Cost: 0.1289137630982042\n",
      "Iteration 36, Cost: 0.12700641259709977\n",
      "Iteration 37, Cost: 0.12511654460963004\n",
      "Iteration 38, Cost: 0.12324273695611619\n",
      "Iteration 39, Cost: 0.12138369634783183\n",
      "Iteration 40, Cost: 0.11953827270807564\n",
      "Iteration 41, Cost: 0.11770547112664952\n",
      "Iteration 42, Cost: 0.1158844613152823\n",
      "Iteration 43, Cost: 0.11407458442721526\n",
      "Iteration 44, Cost: 0.1122753571170038\n",
      "Iteration 45, Cost: 0.11048647274721021\n",
      "Iteration 46, Cost: 0.10870779969628788\n",
      "Iteration 47, Cost: 0.10693937678436218\n",
      "Iteration 48, Cost: 0.10518140590712578\n",
      "Iteration 49, Cost: 0.10343424204770367\n",
      "Iteration 50, Cost: 0.10169838091616246\n",
      "Iteration 51, Cost: 0.09997444454001649\n",
      "Iteration 52, Cost: 0.09826316519062346\n",
      "Iteration 53, Cost: 0.09656536807484922\n",
      "Iteration 54, Cost: 0.09488195324557855\n",
      "Iteration 55, Cost: 0.09321387718737201\n",
      "Iteration 56, Cost: 0.09156213451569593\n",
      "Iteration 57, Cost: 0.08992774019237505\n",
      "Iteration 58, Cost: 0.08831171261018075\n",
      "Iteration 59, Cost: 0.08671505784039066\n",
      "Iteration 60, Cost: 0.08513875527337639\n",
      "Iteration 61, Cost: 0.08358374481799581\n",
      "Iteration 62, Cost: 0.08205091576420795\n",
      "Iteration 63, Cost: 0.08054109735740494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 64, Cost: 0.07905505108407256\n",
      "Iteration 65, Cost: 0.07759346462734994\n",
      "Iteration 66, Cost: 0.07615694741801665\n",
      "Iteration 67, Cost: 0.07474602768106765\n",
      "Iteration 68, Cost: 0.07336115085970413\n",
      "Iteration 69, Cost: 0.07200267928644379\n",
      "Iteration 70, Cost: 0.07067089296423294\n",
      "Iteration 71, Cost: 0.06936599131801931\n",
      "Iteration 72, Cost: 0.06808809577836648\n",
      "Iteration 73, Cost: 0.06683725306259262\n",
      "Iteration 74, Cost: 0.06561343902493166\n",
      "Iteration 75, Cost: 0.06441656295478541\n",
      "Iteration 76, Cost: 0.06324647221079764\n",
      "Iteration 77, Cost: 0.06210295708786035\n",
      "Iteration 78, Cost: 0.060985755823955164\n",
      "Iteration 79, Cost: 0.05989455966369424\n",
      "Iteration 80, Cost: 0.05882901790535636\n",
      "Iteration 81, Cost: 0.05778874286795218\n",
      "Iteration 82, Cost: 0.05677331472426241\n",
      "Iteration 83, Cost: 0.0557822861547668\n",
      "Iteration 84, Cost: 0.05481518678582697\n",
      "Iteration 85, Cost: 0.05387152738333797\n",
      "Iteration 86, Cost: 0.05295080378026849\n",
      "Iteration 87, Cost: 0.0520525005230387\n",
      "Iteration 88, Cost: 0.051176094227523845\n",
      "Iteration 89, Cost: 0.050321056640624\n",
      "Iteration 90, Cost: 0.04948685740782433\n",
      "Iteration 91, Cost: 0.04867296655101519\n",
      "Iteration 92, Cost: 0.04787885666408936\n",
      "Iteration 93, Cost: 0.04710400483652895\n",
      "Iteration 94, Cost: 0.04634789431738975\n",
      "Iteration 95, Cost: 0.045610015933835214\n",
      "Iteration 96, Cost: 0.044889869279716325\n",
      "Iteration 97, Cost: 0.04418696369068166\n",
      "Iteration 98, Cost: 0.04350081902297521\n",
      "Iteration 99, Cost: 0.04283096625347125\n",
      "Iteration 100, Cost: 0.04217694791863427\n",
      "Iteration 101, Cost: 0.041538318410001746\n",
      "Iteration 102, Cost: 0.040914644143484814\n",
      "Iteration 103, Cost: 0.040305503619286705\n",
      "Iteration 104, Cost: 0.039710487388561484\n",
      "Iteration 105, Cost: 0.03912919794209782\n",
      "Iteration 106, Cost: 0.03856124953532656\n",
      "Iteration 107, Cost: 0.03800626796284199\n",
      "Iteration 108, Cost: 0.03746389029441663\n",
      "Iteration 109, Cost: 0.03693376458320638\n",
      "Iteration 110, Cost: 0.03641554955551728\n",
      "Iteration 111, Cost: 0.03590891429016708\n",
      "Iteration 112, Cost: 0.03541353789415808\n",
      "Iteration 113, Cost: 0.03492910918010931\n",
      "Iteration 114, Cost: 0.034455326349705516\n",
      "Iteration 115, Cost: 0.03399189668632839\n",
      "Iteration 116, Cost: 0.03353853625906077\n",
      "Iteration 117, Cost: 0.033094969639407425\n",
      "Iteration 118, Cost: 0.03266092963136312\n",
      "Iteration 119, Cost: 0.0322361570148795\n",
      "Iteration 120, Cost: 0.031820400302332144\n",
      "Iteration 121, Cost: 0.031413415507258104\n",
      "Iteration 122, Cost: 0.03101496592441198\n",
      "Iteration 123, Cost: 0.030624821920058016\n",
      "Iteration 124, Cost: 0.030242760731364095\n",
      "Iteration 125, Cost: 0.029868566273773996\n",
      "Iteration 126, Cost: 0.029502028955292393\n",
      "Iteration 127, Cost: 0.02914294549670886\n",
      "Iteration 128, Cost: 0.028791118756901442\n",
      "Iteration 129, Cost: 0.028446357562485546\n",
      "Iteration 130, Cost: 0.028108476541203017\n",
      "Iteration 131, Cost: 0.027777295958571743\n",
      "Iteration 132, Cost: 0.027452641557433436\n",
      "Iteration 133, Cost: 0.0271343444001438\n",
      "Iteration 134, Cost: 0.026822240713241678\n",
      "Iteration 135, Cost: 0.026516171734512504\n",
      "Iteration 136, Cost: 0.026215983562425615\n",
      "Iteration 137, Cost: 0.02592152700797517\n",
      "Iteration 138, Cost: 0.025632657448992246\n",
      "Iteration 139, Cost: 0.025349234687021757\n",
      "Iteration 140, Cost: 0.02507112280687369\n",
      "Iteration 141, Cost: 0.024798190038966196\n",
      "Iteration 142, Cost: 0.024530308624577963\n",
      "Iteration 143, Cost: 0.02426735468412272\n",
      "Iteration 144, Cost: 0.024009208088548804\n",
      "Iteration 145, Cost: 0.023755752333954275\n",
      "Iteration 146, Cost: 0.023506874419493184\n",
      "Iteration 147, Cost: 0.023262464728632398\n",
      "Iteration 148, Cost: 0.023022416913801684\n",
      "Iteration 149, Cost: 0.02278662778446304\n",
      "Iteration 150, Cost: 0.02255499719860899\n",
      "Iteration 151, Cost: 0.022327427957684172\n",
      "Iteration 152, Cost: 0.022103825704910228\n",
      "Iteration 153, Cost: 0.021884098826980992\n",
      "Iteration 154, Cost: 0.021668158359083346\n",
      "Iteration 155, Cost: 0.02145591789318898\n",
      "Iteration 156, Cost: 0.021247293489553475\n",
      "Iteration 157, Cost: 0.021042203591352025\n",
      "Iteration 158, Cost: 0.020840568942375164\n",
      "Iteration 159, Cost: 0.02064231250770323\n",
      "Iteration 160, Cost: 0.020447359397275013\n",
      "Iteration 161, Cost: 0.02025563679226382\n",
      "Iteration 162, Cost: 0.02006707387417268\n",
      "Iteration 163, Cost: 0.019881601756560455\n",
      "Iteration 164, Cost: 0.01969915341931051\n",
      "Iteration 165, Cost: 0.019519663645355047\n",
      "Iteration 166, Cost: 0.019343068959769463\n",
      "Iteration 167, Cost: 0.01916930757115337\n",
      "Iteration 168, Cost: 0.018998319315217057\n",
      "Iteration 169, Cost: 0.018830045600494935\n",
      "Iteration 170, Cost: 0.018664429356110162\n",
      "Iteration 171, Cost: 0.018501414981517684\n",
      "Iteration 172, Cost: 0.018340948298155652\n",
      "Iteration 173, Cost: 0.018182976502938254\n",
      "Iteration 174, Cost: 0.018027448123525677\n",
      "Iteration 175, Cost: 0.017874312975309704\n",
      "Iteration 176, Cost: 0.01772352212005603\n",
      "Iteration 177, Cost: 0.017575027826146766\n",
      "Iteration 178, Cost: 0.017428783530368766\n",
      "Iteration 179, Cost: 0.01728474380119561\n",
      "Iteration 180, Cost: 0.01714286430351278\n",
      "Iteration 181, Cost: 0.017003101764737204\n",
      "Iteration 182, Cost: 0.01686541394228398\n",
      "Iteration 183, Cost: 0.016729759592334147\n",
      "Iteration 184, Cost: 0.016596098439858643\n",
      "Iteration 185, Cost: 0.016464391149854454\n",
      "Iteration 186, Cost: 0.01633459929975002\n",
      "Iteration 187, Cost: 0.01620668535293729\n",
      "Iteration 188, Cost: 0.016080612633388898\n",
      "Iteration 189, Cost: 0.015956345301319175\n",
      "Iteration 190, Cost: 0.0158338483298484\n",
      "Iteration 191, Cost: 0.01571308748263007\n",
      "Iteration 192, Cost: 0.015594029292401508\n",
      "Iteration 193, Cost: 0.01547664104041856\n",
      "Iteration 194, Cost: 0.015360890736735567\n",
      "Iteration 195, Cost: 0.01524674710129242\n",
      "Iteration 196, Cost: 0.015134179545770909\n",
      "Iteration 197, Cost: 0.015023158156183289\n",
      "Iteration 198, Cost: 0.014913653676156548\n",
      "Iteration 199, Cost: 0.014805637490876638\n",
      "Iteration 200, Cost: 0.014699081611657616\n",
      "Iteration 201, Cost: 0.01459395866110153\n",
      "Iteration 202, Cost: 0.014490241858815728\n",
      "Iteration 203, Cost: 0.014387905007655273\n",
      "Iteration 204, Cost: 0.014286922480459067\n",
      "Iteration 205, Cost: 0.014187269207249383\n",
      "Iteration 206, Cost: 0.01408892066286564\n",
      "Iteration 207, Cost: 0.0139918528550043\n",
      "Iteration 208, Cost: 0.013896042312638027\n",
      "Iteration 209, Cost: 0.013801466074788427\n",
      "Iteration 210, Cost: 0.01370810167962783\n",
      "Iteration 211, Cost: 0.013615927153886997\n",
      "Iteration 212, Cost: 0.013524921002546651\n",
      "Iteration 213, Cost: 0.013435062198792178\n",
      "Iteration 214, Cost: 0.013346330174211987\n",
      "Iteration 215, Cost: 0.013258704809221278\n",
      "Iteration 216, Cost: 0.013172166423694235\n",
      "Iteration 217, Cost: 0.013086695767788816\n",
      "Iteration 218, Cost: 0.013002274012949537\n",
      "Iteration 219, Cost: 0.01291888274307486\n",
      "Iteration 220, Cost: 0.012836503945836823\n",
      "Iteration 221, Cost: 0.012755120004141803\n",
      "Iteration 222, Cost: 0.0126747136877223\n",
      "Iteration 223, Cost: 0.012595268144850728\n",
      "Iteration 224, Cost: 0.012516766894167159\n",
      "Iteration 225, Cost: 0.012439193816614036\n",
      "Iteration 226, Cost: 0.012362533147471806\n",
      "Iteration 227, Cost: 0.012286769468490211\n",
      "Iteration 228, Cost: 0.012211887700111081\n",
      "Iteration 229, Cost: 0.01213787309377905\n",
      "Iteration 230, Cost: 0.012064711224337617\n",
      "Iteration 231, Cost: 0.011992387982508667\n",
      "Iteration 232, Cost: 0.01192088956745414\n",
      "Iteration 233, Cost: 0.011850202479419515\n",
      "Iteration 234, Cost: 0.011780313512459024\n",
      "Iteration 235, Cost: 0.011711209747243401\n",
      "Iteration 236, Cost: 0.011642878543951388\n",
      "Iteration 237, Cost: 0.011575307535246602\n",
      "Iteration 238, Cost: 0.011508484619342039\n",
      "Iteration 239, Cost: 0.011442397953154637\n",
      "Iteration 240, Cost: 0.011377035945552854\n",
      "Iteration 241, Cost: 0.011312387250700344\n",
      "Iteration 242, Cost: 0.011248440761499204\n",
      "Iteration 243, Cost: 0.011185185603136305\n",
      "Iteration 244, Cost: 0.011122611126736435\n",
      "Iteration 245, Cost: 0.0110607069031261\n",
      "Iteration 246, Cost: 0.010999462716711698\n",
      "Iteration 247, Cost: 0.010938868559475966\n",
      "Iteration 248, Cost: 0.010878914625096405\n",
      "Iteration 249, Cost: 0.010819591303189244\n",
      "Iteration 250, Cost: 0.010760889173682531\n",
      "Iteration 251, Cost: 0.010702799001321444\n",
      "Iteration 252, Cost: 0.010645311730309007\n",
      "Iteration 253, Cost: 0.01058841847908487\n",
      "Iteration 254, Cost: 0.010532110535244539\n",
      "Iteration 255, Cost: 0.010476379350601304\n",
      "Iteration 256, Cost: 0.010421216536392507\n",
      "Iteration 257, Cost: 0.010366613858631498\n",
      "Iteration 258, Cost: 0.01031256323360635\n",
      "Iteration 259, Cost: 0.01025905672352579\n",
      "Iteration 260, Cost: 0.01020608653231259\n",
      "Iteration 261, Cost: 0.010153645001544056\n",
      "Iteration 262, Cost: 0.010101724606539082\n",
      "Iteration 263, Cost: 0.010050317952590553\n",
      "Iteration 264, Cost: 0.009999417771341668\n",
      "Iteration 265, Cost: 0.00994901691730441\n",
      "Iteration 266, Cost: 0.009899108364517779\n",
      "Iteration 267, Cost: 0.009849685203343379\n",
      "Iteration 268, Cost: 0.00980074063739534\n",
      "Iteration 269, Cost: 0.009752267980601501\n",
      "Iteration 270, Cost: 0.009704260654392276\n",
      "Iteration 271, Cost: 0.009656712185013594\n",
      "Iteration 272, Cost: 0.009609616200959974\n",
      "Iteration 273, Cost: 0.00956296643052356\n",
      "Iteration 274, Cost: 0.009516756699454992\n",
      "Iteration 275, Cost: 0.009470980928731602\n",
      "Iteration 276, Cost: 0.009425633132428538\n",
      "Iteration 277, Cost: 0.009380707415688286\n",
      "Iteration 278, Cost: 0.009336197972783927\n",
      "Iteration 279, Cost: 0.009292099085271661\n",
      "Iteration 280, Cost: 0.009248405120227924\n",
      "Iteration 281, Cost: 0.009205110528566618\n",
      "Iteration 282, Cost: 0.009162209843431978\n",
      "Iteration 283, Cost: 0.009119697678662667\n",
      "Iteration 284, Cost: 0.009077568727322821\n",
      "Iteration 285, Cost: 0.009035817760295831\n",
      "Iteration 286, Cost: 0.008994439624936897\n",
      "Iteration 287, Cost: 0.008953429243780326\n",
      "Iteration 288, Cost: 0.00891278161329793\n",
      "Iteration 289, Cost: 0.008872491802704848\n",
      "Iteration 290, Cost: 0.008832554952809404\n",
      "Iteration 291, Cost: 0.008792966274903734\n",
      "Iteration 292, Cost: 0.008753721049692086\n",
      "Iteration 293, Cost: 0.008714814626253887\n",
      "Iteration 294, Cost: 0.008676242421038845\n",
      "Iteration 295, Cost: 0.008637999916891547\n",
      "Iteration 296, Cost: 0.008600082662103107\n",
      "Iteration 297, Cost: 0.008562486269487735\n",
      "Iteration 298, Cost: 0.00852520641548209\n",
      "Iteration 299, Cost: 0.00848823883926558\n",
      "Iteration 300, Cost: 0.008451579341899844\n",
      "Iteration 301, Cost: 0.008415223785485844\n",
      "Iteration 302, Cost: 0.00837916809233712\n",
      "Iteration 303, Cost: 0.00834340824416786\n",
      "Iteration 304, Cost: 0.008307940281294666\n",
      "Iteration 305, Cost: 0.008272760301850906\n",
      "Iteration 306, Cost: 0.008237864461012705\n",
      "Iteration 307, Cost: 0.008203248970235739\n",
      "Iteration 308, Cost: 0.008168910096502108\n",
      "Iteration 309, Cost: 0.008134844161576607\n",
      "Iteration 310, Cost: 0.008101047541271812\n",
      "Iteration 311, Cost: 0.00806751666472155\n",
      "Iteration 312, Cost: 0.008034248013662261\n",
      "Iteration 313, Cost: 0.008001238121721958\n",
      "Iteration 314, Cost: 0.007968483573716417\n",
      "Iteration 315, Cost: 0.007935981004952462\n",
      "Iteration 316, Cost: 0.007903727100538015\n",
      "Iteration 317, Cost: 0.007871718594698849\n",
      "Iteration 318, Cost: 0.007839952270101922\n",
      "Iteration 319, Cost: 0.007808424957185153\n",
      "Iteration 320, Cost: 0.007777133533493622\n",
      "Iteration 321, Cost: 0.0077460749230221755\n",
      "Iteration 322, Cost: 0.007715246095564386\n",
      "Iteration 323, Cost: 0.007684644066067914\n",
      "Iteration 324, Cost: 0.007654265893996258\n",
      "Iteration 325, Cost: 0.007624108682696964\n",
      "Iteration 326, Cost: 0.007594169578776293\n",
      "Iteration 327, Cost: 0.007564445771480449\n",
      "Iteration 328, Cost: 0.007534934492083375\n",
      "Iteration 329, Cost: 0.007505633013281202\n",
      "Iteration 330, Cost: 0.007476538648593414\n",
      "Iteration 331, Cost: 0.007447648751770769\n",
      "Iteration 332, Cost: 0.007418960716210062\n",
      "Iteration 333, Cost: 0.007390471974375764\n",
      "Iteration 334, Cost: 0.007362179997228601\n",
      "Iteration 335, Cost: 0.007334082293661128\n",
      "Iteration 336, Cost: 0.007306176409940346\n",
      "Iteration 337, Cost: 0.007278459929157382\n",
      "Iteration 338, Cost: 0.007250930470684304\n",
      "Iteration 339, Cost: 0.007223585689638072\n",
      "Iteration 340, Cost: 0.007196423276351625\n",
      "Iteration 341, Cost: 0.007169440955852222\n",
      "Iteration 342, Cost: 0.007142636487346915\n",
      "Iteration 343, Cost: 0.007116007663715258\n",
      "Iteration 344, Cost: 0.007089552311009193\n",
      "Iteration 345, Cost: 0.007063268287960156\n",
      "Iteration 346, Cost: 0.007037153485493317\n",
      "Iteration 347, Cost: 0.0070112058262490175\n",
      "Iteration 348, Cost: 0.006985423264111303\n",
      "Iteration 349, Cost: 0.006959803783743563\n",
      "Iteration 350, Cost: 0.006934345400131218\n",
      "Iteration 351, Cost: 0.006909046158131431\n",
      "Iteration 352, Cost: 0.00688390413202976\n",
      "Iteration 353, Cost: 0.006858917425103724\n",
      "Iteration 354, Cost: 0.006834084169193219\n",
      "Iteration 355, Cost: 0.0068094025242777054\n",
      "Iteration 356, Cost: 0.00678487067806011\n",
      "Iteration 357, Cost: 0.0067604868455573685\n",
      "Iteration 358, Cost: 0.006736249268697527\n",
      "Iteration 359, Cost: 0.006712156215923317\n",
      "Iteration 360, Cost: 0.006688205981802152\n",
      "Iteration 361, Cost: 0.006664396886642405\n",
      "Iteration 362, Cost: 0.006640727276115924\n",
      "Iteration 363, Cost: 0.006617195520886668\n",
      "Iteration 364, Cost: 0.006593800016245391\n",
      "Iteration 365, Cost: 0.0065705391817502375\n",
      "Iteration 366, Cost: 0.006547411460873205\n",
      "Iteration 367, Cost: 0.006524415320652329\n",
      "Iteration 368, Cost: 0.0065015492513495\n",
      "Iteration 369, Cost: 0.006478811766113825\n",
      "Iteration 370, Cost: 0.006456201400650423\n",
      "Iteration 371, Cost: 0.006433716712894519\n",
      "Iteration 372, Cost: 0.0064113562826908005\n",
      "Iteration 373, Cost: 0.006389118711477832\n",
      "Iteration 374, Cost: 0.006367002621977532\n",
      "Iteration 375, Cost: 0.0063450066578895105\n",
      "Iteration 376, Cost: 0.006323129483590226\n",
      "Iteration 377, Cost: 0.006301369783836807\n",
      "Iteration 378, Cost: 0.00627972626347548\n",
      "Iteration 379, Cost: 0.006258197647154459\n",
      "Iteration 380, Cost: 0.006236782679041201\n",
      "Iteration 381, Cost: 0.0062154801225439565\n",
      "Iteration 382, Cost: 0.006194288760037457\n",
      "Iteration 383, Cost: 0.006173207392592671\n",
      "Iteration 384, Cost: 0.006152234839710532\n",
      "Iteration 385, Cost: 0.006131369939059524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 386, Cost: 0.006110611546217017\n",
      "Iteration 387, Cost: 0.006089958534414284\n",
      "Iteration 388, Cost: 0.006069409794285086\n",
      "Iteration 389, Cost: 0.006048964233617729\n",
      "Iteration 390, Cost: 0.006028620777110497\n",
      "Iteration 391, Cost: 0.006008378366130394\n",
      "Iteration 392, Cost: 0.005988235958475081\n",
      "Iteration 393, Cost: 0.005968192528137933\n",
      "Iteration 394, Cost: 0.005948247065076133\n",
      "Iteration 395, Cost: 0.005928398574981697\n",
      "Iteration 396, Cost: 0.0059086460790553985\n",
      "Iteration 397, Cost: 0.0058889886137834585\n",
      "Iteration 398, Cost: 0.005869425230716967\n",
      "Iteration 399, Cost: 0.005849954996253931\n",
      "Iteration 400, Cost: 0.005830576991423907\n",
      "Iteration 401, Cost: 0.005811290311675114\n",
      "Iteration 402, Cost: 0.0057920940666640035\n",
      "Iteration 403, Cost: 0.005772987380047177\n",
      "Iteration 404, Cost: 0.005753969389275623\n",
      "Iteration 405, Cost: 0.005735039245391196\n",
      "Iteration 406, Cost: 0.005716196112825289\n",
      "Iteration 407, Cost: 0.005697439169199627\n",
      "Iteration 408, Cost: 0.005678767605129178\n",
      "Iteration 409, Cost: 0.005660180624027067\n",
      "Iteration 410, Cost: 0.005641677441911482\n",
      "Iteration 411, Cost: 0.005623257287214552\n",
      "Iteration 412, Cost: 0.005604919400593085\n",
      "Iteration 413, Cost: 0.0055866630347411956\n",
      "Iteration 414, Cost: 0.00556848745420477\n",
      "Iteration 415, Cost: 0.0055503919351976815\n",
      "Iteration 416, Cost: 0.005532375765419822\n",
      "Iteration 417, Cost: 0.005514438243876818\n",
      "Iteration 418, Cost: 0.005496578680701475\n",
      "Iteration 419, Cost: 0.005478796396976907\n",
      "Iteration 420, Cost: 0.00546109072456132\n",
      "Iteration 421, Cost: 0.005443461005914427\n",
      "Iteration 422, Cost: 0.0054259065939255\n",
      "Iteration 423, Cost: 0.005408426851743041\n",
      "Iteration 424, Cost: 0.005391021152606008\n",
      "Iteration 425, Cost: 0.005373688879676685\n",
      "Iteration 426, Cost: 0.005356429425875073\n",
      "Iteration 427, Cost: 0.0053392421937149\n",
      "Iteration 428, Cost: 0.005322126595141171\n",
      "Iteration 429, Cost: 0.005305082051369283\n",
      "Iteration 430, Cost: 0.005288107992725733\n",
      "Iteration 431, Cost: 0.005271203858490356\n",
      "Iteration 432, Cost: 0.005254369096740158\n",
      "Iteration 433, Cost: 0.005237603164194711\n",
      "Iteration 434, Cost: 0.005220905526063145\n",
      "Iteration 435, Cost: 0.005204275655892695\n",
      "Iteration 436, Cost: 0.005187713035418891\n",
      "Iteration 437, Cost: 0.0051712171544173215\n",
      "Iteration 438, Cost: 0.0051547875105570195\n",
      "Iteration 439, Cost: 0.005138423609255515\n",
      "Iteration 440, Cost: 0.0051221249635354845\n",
      "Iteration 441, Cost: 0.005105891093883096\n",
      "Iteration 442, Cost: 0.005089721528108029\n",
      "Iteration 443, Cost: 0.005073615801205156\n",
      "Iteration 444, Cost: 0.005057573455217988\n",
      "Iteration 445, Cost: 0.005041594039103799\n",
      "Iteration 446, Cost: 0.005025677108600516\n",
      "Iteration 447, Cost: 0.0050098222260953765\n",
      "Iteration 448, Cost: 0.004994028960495354\n",
      "Converged!\n",
      "Iteration 1, Cost: 0.2055198062774365\n",
      "Iteration 2, Cost: 0.19628634925008318\n",
      "Iteration 3, Cost: 0.1899244004027923\n",
      "Iteration 4, Cost: 0.184869258619689\n",
      "Iteration 5, Cost: 0.18041479550424794\n",
      "Iteration 6, Cost: 0.17625116637247432\n",
      "Iteration 7, Cost: 0.17223647810746223\n",
      "Iteration 8, Cost: 0.16830130080774908\n",
      "Iteration 9, Cost: 0.16440975664143395\n",
      "Iteration 10, Cost: 0.16054329707551557\n",
      "Iteration 11, Cost: 0.15669356272843565\n",
      "Iteration 12, Cost: 0.15285895839141647\n",
      "Iteration 13, Cost: 0.14904280353760535\n",
      "Iteration 14, Cost: 0.1452521666832186\n",
      "Iteration 15, Cost: 0.141496977095762\n",
      "Iteration 16, Cost: 0.13778921142632541\n",
      "Iteration 17, Cost: 0.13414206066152093\n",
      "Iteration 18, Cost: 0.13056905806540758\n",
      "Iteration 19, Cost: 0.12708320603921522\n",
      "Iteration 20, Cost: 0.12369617556408953\n",
      "Iteration 21, Cost: 0.12041766034547016\n",
      "Iteration 22, Cost: 0.11725494934128132\n",
      "Iteration 23, Cost: 0.11421274477509508\n",
      "Iteration 24, Cost: 0.11129321176832449\n",
      "Iteration 25, Cost: 0.10849621317547938\n",
      "Iteration 26, Cost: 0.10581966622147436\n",
      "Iteration 27, Cost: 0.10325995647516506\n",
      "Iteration 28, Cost: 0.10081235513128037\n",
      "Iteration 29, Cost: 0.09847140149354124\n",
      "Iteration 30, Cost: 0.09623122882492452\n",
      "Iteration 31, Cost: 0.0940858252179135\n",
      "Iteration 32, Cost: 0.09202923064079852\n",
      "Iteration 33, Cost: 0.09005567697265339\n",
      "Iteration 34, Cost: 0.08815968044751618\n",
      "Iteration 35, Cost: 0.08633609644955177\n",
      "Iteration 36, Cost: 0.08458014588638232\n",
      "Iteration 37, Cost: 0.0828874210534399\n",
      "Iteration 38, Cost: 0.08125387740683854\n",
      "Iteration 39, Cost: 0.07967581622894561\n",
      "Iteration 40, Cost: 0.07814986191877461\n",
      "Iteration 41, Cost: 0.07667293660940704\n",
      "Iteration 42, Cost: 0.07524223400337962\n",
      "Iteration 43, Cost: 0.07385519369916377\n",
      "Iteration 44, Cost: 0.07250947682446013\n",
      "Iteration 45, Cost: 0.07120294346175472\n",
      "Iteration 46, Cost: 0.06993363211857566\n",
      "Iteration 47, Cost: 0.06869974133408084\n",
      "Iteration 48, Cost: 0.06749961340480809\n",
      "Iteration 49, Cost: 0.06633172013985687\n",
      "Iteration 50, Cost: 0.06519465050744153\n",
      "Iteration 51, Cost: 0.06408710000179761\n",
      "Iteration 52, Cost: 0.06300786153561787\n",
      "Iteration 53, Cost: 0.06195581764460996\n",
      "Iteration 54, Cost: 0.06092993377553712\n",
      "Iteration 55, Cost: 0.05992925241725262\n",
      "Iteration 56, Cost: 0.05895288782749605\n",
      "Iteration 57, Cost: 0.05800002110966894\n",
      "Iteration 58, Cost: 0.057069895407273814\n",
      "Iteration 59, Cost: 0.05616181101283629\n",
      "Iteration 60, Cost: 0.05527512023520845\n",
      "Iteration 61, Cost: 0.05440922193381173\n",
      "Iteration 62, Cost: 0.0535635557066511\n",
      "Iteration 63, Cost: 0.05273759580299117\n",
      "Iteration 64, Cost: 0.05193084491054888\n",
      "Iteration 65, Cost: 0.05114282802892341\n",
      "Iteration 66, Cost: 0.05037308667530187\n",
      "Iteration 67, Cost: 0.04962117366904179\n",
      "Iteration 68, Cost: 0.04888664870837109\n",
      "Iteration 69, Cost: 0.04816907489129177\n",
      "Iteration 70, Cost: 0.04746801625478054\n",
      "Iteration 71, Cost: 0.046783036324676584\n",
      "Iteration 72, Cost: 0.04611369759550843\n",
      "Iteration 73, Cost: 0.045459561803941693\n",
      "Iteration 74, Cost: 0.044820190826024765\n",
      "Iteration 75, Cost: 0.04419514801700767\n",
      "Iteration 76, Cost: 0.043583999819838716\n",
      "Iteration 77, Cost: 0.0429863174892146\n",
      "Iteration 78, Cost: 0.042401678806522604\n",
      "Iteration 79, Cost: 0.04182966969205117\n",
      "Iteration 80, Cost: 0.04126988565057616\n",
      "Iteration 81, Cost: 0.040721933012372986\n",
      "Iteration 82, Cost: 0.04018542995265014\n",
      "Iteration 83, Cost: 0.039660007288108874\n",
      "Iteration 84, Cost: 0.03914530906021287\n",
      "Iteration 85, Cost: 0.03864099292155927\n",
      "Iteration 86, Cost: 0.0381467303453578\n",
      "Iteration 87, Cost: 0.037662206679313213\n",
      "Iteration 88, Cost: 0.03718712106491849\n",
      "Iteration 89, Cost: 0.03672118624192014\n",
      "Iteration 90, Cost: 0.036264128255977544\n",
      "Iteration 91, Cost: 0.03581568608563943\n",
      "Iteration 92, Cost: 0.03537561120292384\n",
      "Iteration 93, Cost: 0.034943667080141357\n",
      "Iteration 94, Cost: 0.03451962865420804\n",
      "Iteration 95, Cost: 0.034103281758562386\n",
      "Iteration 96, Cost: 0.033694422531908436\n",
      "Iteration 97, Cost: 0.03329285681230639\n",
      "Iteration 98, Cost: 0.03289839952456784\n",
      "Iteration 99, Cost: 0.03251087406842353\n",
      "Iteration 100, Cost: 0.03213011171446165\n",
      "Iteration 101, Cost: 0.031755951014337494\n",
      "Iteration 102, Cost: 0.031388237231193473\n",
      "Iteration 103, Cost: 0.0310268217955837\n",
      "Iteration 104, Cost: 0.030671561791460744\n",
      "Iteration 105, Cost: 0.030322319475962534\n",
      "Iteration 106, Cost: 0.02997896183585284\n",
      "Iteration 107, Cost: 0.029641360182546945\n",
      "Iteration 108, Cost: 0.029309389786727456\n",
      "Iteration 109, Cost: 0.028982929552658767\n",
      "Iteration 110, Cost: 0.0286618617314739\n",
      "Iteration 111, Cost: 0.028346071671963693\n",
      "Iteration 112, Cost: 0.02803544760676705\n",
      "Iteration 113, Cost: 0.02772988047135603\n",
      "Iteration 114, Cost: 0.02742926375283841\n",
      "Iteration 115, Cost: 0.02713349336536114\n",
      "Iteration 116, Cost: 0.02684246754878351\n",
      "Iteration 117, Cost: 0.026556086787287336\n",
      "Iteration 118, Cost: 0.026274253744685117\n",
      "Iteration 119, Cost: 0.025996873213359646\n",
      "Iteration 120, Cost: 0.0257238520740002\n",
      "Iteration 121, Cost: 0.02545509926357324\n",
      "Iteration 122, Cost: 0.025190525749262796\n",
      "Iteration 123, Cost: 0.024930044506423228\n",
      "Iteration 124, Cost: 0.024673570498891635\n",
      "Iteration 125, Cost: 0.024421020660300278\n",
      "Iteration 126, Cost: 0.02417231387530292\n",
      "Iteration 127, Cost: 0.023927370959878583\n",
      "Iteration 128, Cost: 0.02368611464009851\n",
      "Iteration 129, Cost: 0.023448469528936068\n",
      "Iteration 130, Cost: 0.023214362100864937\n",
      "Iteration 131, Cost: 0.022983720664129046\n",
      "Iteration 132, Cost: 0.022756475330680235\n",
      "Iteration 133, Cost: 0.022532557983868877\n",
      "Iteration 134, Cost: 0.022311902244040723\n",
      "Iteration 135, Cost: 0.022094443432243253\n",
      "Iteration 136, Cost: 0.021880118532278878\n",
      "Iteration 137, Cost: 0.02166886615136295\n",
      "Iteration 138, Cost: 0.0214606264796544\n",
      "Iteration 139, Cost: 0.021255341248927598\n",
      "Iteration 140, Cost: 0.02105295369064748\n",
      "Iteration 141, Cost: 0.020853408493698388\n",
      "Iteration 142, Cost: 0.020656651762001245\n",
      "Iteration 143, Cost: 0.02046263097223467\n",
      "Iteration 144, Cost: 0.020271294931855796\n",
      "Iteration 145, Cost: 0.020082593737594252\n",
      "Iteration 146, Cost: 0.019896478734571905\n",
      "Iteration 147, Cost: 0.019712902476178764\n",
      "Iteration 148, Cost: 0.019531818684815183\n",
      "Iteration 149, Cost: 0.01935318221359047\n",
      "Iteration 150, Cost: 0.019176949009049715\n",
      "Iteration 151, Cost: 0.019003076074983544\n",
      "Iteration 152, Cost: 0.018831521437360055\n",
      "Iteration 153, Cost: 0.018662244110404253\n",
      "Iteration 154, Cost: 0.018495204063838113\n",
      "Iteration 155, Cost: 0.018330362191283464\n",
      "Iteration 156, Cost: 0.01816768027982091\n",
      "Iteration 157, Cost: 0.01800712098069001\n",
      "Iteration 158, Cost: 0.0178486477811096\n",
      "Iteration 159, Cost: 0.017692224977191976\n",
      "Iteration 160, Cost: 0.017537817647920426\n",
      "Iteration 161, Cost: 0.017385391630156715\n",
      "Iteration 162, Cost: 0.017234913494642935\n",
      "Iteration 163, Cost: 0.01708635052296067\n",
      "Iteration 164, Cost: 0.01693967068540993\n",
      "Iteration 165, Cost: 0.016794842619770052\n",
      "Iteration 166, Cost: 0.01665183561090527\n",
      "Iteration 167, Cost: 0.016510619571178397\n",
      "Iteration 168, Cost: 0.0163711650216372\n",
      "Iteration 169, Cost: 0.016233443073939306\n",
      "Iteration 170, Cost: 0.016097425412983136\n",
      "Iteration 171, Cost: 0.01596308428021381\n",
      "Iteration 172, Cost: 0.015830392457574876\n",
      "Iteration 173, Cost: 0.015699323252078203\n",
      "Iteration 174, Cost: 0.01556985048096632\n",
      "Iteration 175, Cost: 0.015441948457442834\n",
      "Iteration 176, Cost: 0.015315591976948597\n",
      "Iteration 177, Cost: 0.015190756303962272\n",
      "Iteration 178, Cost: 0.015067417159305834\n",
      "Iteration 179, Cost: 0.014945550707936477\n",
      "Iteration 180, Cost: 0.014825133547207861\n",
      "Iteration 181, Cost: 0.014706142695584524\n",
      "Iteration 182, Cost: 0.014588555581794279\n",
      "Iteration 183, Cost: 0.014472350034404368\n",
      "Iteration 184, Cost: 0.014357504271807734\n",
      "Iteration 185, Cost: 0.01424399689260649\n",
      "Iteration 186, Cost: 0.014131806866380218\n",
      "Iteration 187, Cost: 0.014020913524827107\n",
      "Iteration 188, Cost: 0.013911296553266487\n",
      "Iteration 189, Cost: 0.013802935982491367\n",
      "Iteration 190, Cost: 0.013695812180960104\n",
      "Iteration 191, Cost: 0.01358990584731633\n",
      "Iteration 192, Cost: 0.013485198003226558\n",
      "Iteration 193, Cost: 0.013381669986524881\n",
      "Iteration 194, Cost: 0.013279303444654385\n",
      "Iteration 195, Cost: 0.01317808032839488\n",
      "Iteration 196, Cost: 0.013077982885866686\n",
      "Iteration 197, Cost: 0.012978993656800131\n",
      "Iteration 198, Cost: 0.012881095467060684\n",
      "Iteration 199, Cost: 0.012784271423419402\n",
      "Iteration 200, Cost: 0.012688504908558624\n",
      "Iteration 201, Cost: 0.012593779576302904\n",
      "Iteration 202, Cost: 0.012500079347065034\n",
      "Iteration 203, Cost: 0.012407388403497316\n",
      "Iteration 204, Cost: 0.012315691186338212\n",
      "Iteration 205, Cost: 0.012224972390444615\n",
      "Iteration 206, Cost: 0.012135216961000063\n",
      "Iteration 207, Cost: 0.012046410089889496\n",
      "Iteration 208, Cost: 0.01195853721223104\n",
      "Iteration 209, Cost: 0.011871584003055788\n",
      "Iteration 210, Cost: 0.011785536374126352\n",
      "Iteration 211, Cost: 0.011700380470885553\n",
      "Iteration 212, Cost: 0.011616102669526337\n",
      "Iteration 213, Cost: 0.011532689574174708\n",
      "Iteration 214, Cost: 0.011450128014177274\n",
      "Iteration 215, Cost: 0.011368405041485447\n",
      "Iteration 216, Cost: 0.01128750792812849\n",
      "Iteration 217, Cost: 0.011207424163767883\n",
      "Iteration 218, Cost: 0.01112814145332562\n",
      "Iteration 219, Cost: 0.011049647714679403\n",
      "Iteration 220, Cost: 0.010971931076417868\n",
      "Iteration 221, Cost: 0.010894979875649234\n",
      "Iteration 222, Cost: 0.010818782655857026\n",
      "Iteration 223, Cost: 0.010743328164796718\n",
      "Iteration 224, Cost: 0.010668605352427453\n",
      "Iteration 225, Cost: 0.01059460336887316\n",
      "Iteration 226, Cost: 0.010521311562407683\n",
      "Iteration 227, Cost: 0.0104487194774587\n",
      "Iteration 228, Cost: 0.01037681685262556\n",
      "Iteration 229, Cost: 0.010305593618706239\n",
      "Iteration 230, Cost: 0.010235039896729001\n",
      "Iteration 231, Cost: 0.010165145995984422\n",
      "Iteration 232, Cost: 0.010095902412053806\n",
      "Iteration 233, Cost: 0.010027299824830149\n",
      "Iteration 234, Cost: 0.00995932909652804\n",
      "Iteration 235, Cost: 0.009891981269679115\n",
      "Iteration 236, Cost: 0.009825247565110025\n",
      "Iteration 237, Cost: 0.009759119379899794\n",
      "Iteration 238, Cost: 0.009693588285314064\n",
      "Iteration 239, Cost: 0.00962864602471362\n",
      "Iteration 240, Cost: 0.00956428451143502\n",
      "Iteration 241, Cost: 0.009500495826641283\n",
      "Iteration 242, Cost: 0.00943727221714087\n",
      "Iteration 243, Cost: 0.009374606093173423\n",
      "Iteration 244, Cost: 0.009312490026160946\n",
      "Iteration 245, Cost: 0.00925091674642342\n",
      "Iteration 246, Cost: 0.009189879140857964\n",
      "Iteration 247, Cost: 0.009129370250581107\n",
      "Iteration 248, Cost: 0.009069383268533734\n",
      "Iteration 249, Cost: 0.009009911537048797\n",
      "Iteration 250, Cost: 0.008950948545381893\n",
      "Iteration 251, Cost: 0.008892487927205261\n",
      "Iteration 252, Cost: 0.008834523458065858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 253, Cost: 0.008777049052808572\n",
      "Iteration 254, Cost: 0.008720058762965725\n",
      "Iteration 255, Cost: 0.008663546774114435\n",
      "Iteration 256, Cost: 0.008607507403203525\n",
      "Iteration 257, Cost: 0.008551935095851925\n",
      "Iteration 258, Cost: 0.008496824423620852\n",
      "Iteration 259, Cost: 0.00844217008126211\n",
      "Iteration 260, Cost: 0.008387966883945143\n",
      "Iteration 261, Cost: 0.008334209764465786\n",
      "Iteration 262, Cost: 0.008280893770439527\n",
      "Iteration 263, Cost: 0.008228014061482674\n",
      "Iteration 264, Cost: 0.008175565906384593\n",
      "Iteration 265, Cost: 0.008123544680274606\n",
      "Iteration 266, Cost: 0.008071945861787033\n",
      "Iteration 267, Cost: 0.00802076503022813\n",
      "Iteration 268, Cost: 0.007969997862748634\n",
      "Iteration 269, Cost: 0.007919640131525716\n",
      "Iteration 270, Cost: 0.007869687700958188\n",
      "Iteration 271, Cost: 0.007820136524878807\n",
      "Iteration 272, Cost: 0.007770982643787473\n",
      "Iteration 273, Cost: 0.007722222182109099\n",
      "Iteration 274, Cost: 0.007673851345479967\n",
      "Iteration 275, Cost: 0.00762586641806605\n",
      "Iteration 276, Cost: 0.007578263759917018\n",
      "Iteration 277, Cost: 0.007531039804359199\n",
      "Iteration 278, Cost: 0.007484191055430854\n",
      "Iteration 279, Cost: 0.007437714085362856\n",
      "Iteration 280, Cost: 0.007391605532107711\n",
      "Iteration 281, Cost: 0.007345862096919657\n",
      "Iteration 282, Cost: 0.007300480541988426\n",
      "Iteration 283, Cost: 0.007255457688128939\n",
      "Iteration 284, Cost: 0.0072107904125291175\n",
      "Iteration 285, Cost: 0.007166475646557587\n",
      "Iteration 286, Cost: 0.007122510373633023\n",
      "Iteration 287, Cost: 0.007078891627156453\n",
      "Iteration 288, Cost: 0.007035616488507696\n",
      "Iteration 289, Cost: 0.006992682085106867\n",
      "Iteration 290, Cost: 0.006950085588541622\n",
      "Iteration 291, Cost: 0.006907824212760532\n",
      "Iteration 292, Cost: 0.006865895212332893\n",
      "Iteration 293, Cost: 0.006824295880774858\n",
      "Iteration 294, Cost: 0.006783023548941709\n",
      "Iteration 295, Cost: 0.006742075583485871\n",
      "Iteration 296, Cost: 0.00670144938537995\n",
      "Iteration 297, Cost: 0.006661142388504089\n",
      "Iteration 298, Cost: 0.006621152058296579\n",
      "Iteration 299, Cost: 0.006581475890466646\n",
      "Iteration 300, Cost: 0.006542111409768116\n",
      "Iteration 301, Cost: 0.006503056168832547\n",
      "Iteration 302, Cost: 0.006464307747060325\n",
      "Iteration 303, Cost: 0.0064258637495680955\n",
      "Iteration 304, Cost: 0.006387721806190835\n",
      "Iteration 305, Cost: 0.0063498795705367594\n",
      "Iteration 306, Cost: 0.0063123347190932886\n",
      "Iteration 307, Cost: 0.006275084950382142\n",
      "Iteration 308, Cost: 0.0062381279841616675\n",
      "Iteration 309, Cost: 0.0062014615606744965\n",
      "Iteration 310, Cost: 0.006165083439938562\n",
      "Iteration 311, Cost: 0.006128991401079555\n",
      "Iteration 312, Cost: 0.006093183241702896\n",
      "Iteration 313, Cost: 0.006057656777303329\n",
      "Iteration 314, Cost: 0.006022409840710246\n",
      "Iteration 315, Cost: 0.005987440281566927\n",
      "Iteration 316, Cost: 0.005952745965841893\n",
      "Iteration 317, Cost: 0.005918324775370641\n",
      "Iteration 318, Cost: 0.005884174607426048\n",
      "Iteration 319, Cost: 0.0058502933743158625\n",
      "Iteration 320, Cost: 0.005816679003005648\n",
      "Iteration 321, Cost: 0.005783329434765735\n",
      "Iteration 322, Cost: 0.00575024262484071\n",
      "Iteration 323, Cost: 0.005717416542140085\n",
      "Iteration 324, Cost: 0.005684849168948833\n",
      "Iteration 325, Cost: 0.005652538500656555\n",
      "Iteration 326, Cost: 0.005620482545504132\n",
      "Iteration 327, Cost: 0.005588679324346717\n",
      "Iteration 328, Cost: 0.005557126870432094\n",
      "Iteration 329, Cost: 0.005525823229193364\n",
      "Iteration 330, Cost: 0.005494766458055098\n",
      "Iteration 331, Cost: 0.005463954626252088\n",
      "Iteration 332, Cost: 0.0054333858146598975\n",
      "Iteration 333, Cost: 0.005403058115636495\n",
      "Iteration 334, Cost: 0.005372969632874253\n",
      "Iteration 335, Cost: 0.005343118481261711\n",
      "Iteration 336, Cost: 0.005313502786754479\n",
      "Iteration 337, Cost: 0.005284120686254771\n",
      "Iteration 338, Cost: 0.005254970327499006\n",
      "Iteration 339, Cost: 0.005226049868953097\n",
      "Iteration 340, Cost: 0.005197357479714896\n",
      "Iteration 341, Cost: 0.005168891339423469\n",
      "Iteration 342, Cost: 0.005140649638174793\n",
      "Iteration 343, Cost: 0.005112630576443544\n",
      "Iteration 344, Cost: 0.0050848323650106424\n",
      "Iteration 345, Cost: 0.005057253224896283\n",
      "Iteration 346, Cost: 0.005029891387298162\n",
      "Iteration 347, Cost: 0.005002745093534616\n",
      "Iteration 348, Cost: 0.004975812594992479\n",
      "Converged!\n",
      "Iteration 1, Cost: 0.5061145050064298\n",
      "Iteration 2, Cost: 0.45931241663434635\n",
      "Iteration 3, Cost: 0.4072220498097818\n",
      "Iteration 4, Cost: 0.3546220849032771\n",
      "Iteration 5, Cost: 0.3077516876365266\n",
      "Iteration 6, Cost: 0.26706134284587424\n",
      "Iteration 7, Cost: 0.22872394899319132\n",
      "Iteration 8, Cost: 0.19630068462670056\n",
      "Iteration 9, Cost: 0.1746161619345919\n",
      "Iteration 10, Cost: 0.16154167213527995\n",
      "Iteration 11, Cost: 0.15312744147735635\n",
      "Iteration 12, Cost: 0.14707840974098976\n",
      "Iteration 13, Cost: 0.14231547295418417\n",
      "Iteration 14, Cost: 0.13832276167819135\n",
      "Iteration 15, Cost: 0.13483617126968434\n",
      "Iteration 16, Cost: 0.1317098433586949\n",
      "Iteration 17, Cost: 0.12885697130279145\n",
      "Iteration 18, Cost: 0.12622203623052225\n",
      "Iteration 19, Cost: 0.1237670624840253\n",
      "Iteration 20, Cost: 0.12146447898622494\n",
      "Iteration 21, Cost: 0.11929324672751666\n",
      "Iteration 22, Cost: 0.11723666832686613\n",
      "Iteration 23, Cost: 0.11528109481779907\n",
      "Iteration 24, Cost: 0.11341512650175807\n",
      "Iteration 25, Cost: 0.11162909452996954\n",
      "Iteration 26, Cost: 0.10991470743598898\n",
      "Iteration 27, Cost: 0.10826479831864594\n",
      "Iteration 28, Cost: 0.10667313611195836\n",
      "Iteration 29, Cost: 0.10513427958761297\n",
      "Iteration 30, Cost: 0.10364346121228195\n",
      "Iteration 31, Cost: 0.10219649279005043\n",
      "Iteration 32, Cost: 0.10078968760583464\n",
      "Iteration 33, Cost: 0.09941979544377029\n",
      "Iteration 34, Cost: 0.09808394787566561\n",
      "Iteration 35, Cost: 0.09677961186953396\n",
      "Iteration 36, Cost: 0.09550455020762859\n",
      "Iteration 37, Cost: 0.09425678751210703\n",
      "Iteration 38, Cost: 0.09303458090302168\n",
      "Iteration 39, Cost: 0.09183639448631009\n",
      "Iteration 40, Cost: 0.09066087700620896\n",
      "Iteration 41, Cost: 0.08950684210802874\n",
      "Iteration 42, Cost: 0.08837325075073078\n",
      "Iteration 43, Cost: 0.08725919538909623\n",
      "Iteration 44, Cost: 0.0861638856157244\n",
      "Iteration 45, Cost: 0.08508663501572272\n",
      "Iteration 46, Cost: 0.08402684904293979\n",
      "Iteration 47, Cost: 0.08298401377643637\n",
      "Iteration 48, Cost: 0.08195768545957834\n",
      "Iteration 49, Cost: 0.08094748076133819\n",
      "Iteration 50, Cost: 0.07995306772964564\n",
      "Iteration 51, Cost: 0.07897415742951272\n",
      "Iteration 52, Cost: 0.07801049627395953\n",
      "Iteration 53, Cost: 0.07706185906359028\n",
      "Iteration 54, Cost: 0.07612804275151364\n",
      "Iteration 55, Cost: 0.0752088609450625\n",
      "Iteration 56, Cost: 0.07430413914569355\n",
      "Iteration 57, Cost: 0.0734137107150175\n",
      "Iteration 58, Cost: 0.07253741353971828\n",
      "Iteration 59, Cost: 0.07167508735270983\n",
      "Iteration 60, Cost: 0.07082657165363633\n",
      "Iteration 61, Cost: 0.06999170415986784\n",
      "Iteration 62, Cost: 0.06917031971031153\n",
      "Iteration 63, Cost: 0.06836224953917854\n",
      "Iteration 64, Cost: 0.06756732083558177\n",
      "Iteration 65, Cost: 0.06678535650751699\n",
      "Iteration 66, Cost: 0.0660161750752106\n",
      "Iteration 67, Cost: 0.0652595906286039\n",
      "Iteration 68, Cost: 0.06451541279627333\n",
      "Iteration 69, Cost: 0.06378344668751695\n",
      "Iteration 70, Cost: 0.06306349278463126\n",
      "Iteration 71, Cost: 0.062355346777375434\n",
      "Iteration 72, Cost: 0.061658799345058725\n",
      "Iteration 73, Cost: 0.06097363590249521\n",
      "Iteration 74, Cost: 0.06029963633341972\n",
      "Iteration 75, Cost: 0.059636574738422744\n",
      "Iteration 76, Cost: 0.058984219224078475\n",
      "Iteration 77, Cost: 0.058342331756205026\n",
      "Iteration 78, Cost: 0.05771066809396776\n",
      "Iteration 79, Cost: 0.05708897781389824\n",
      "Iteration 80, Cost: 0.05647700442496982\n",
      "Iteration 81, Cost: 0.05587448556866141\n",
      "Iteration 82, Cost: 0.05528115329222432\n",
      "Iteration 83, Cost: 0.054696734379632114\n",
      "Iteration 84, Cost: 0.054120950723131764\n",
      "Iteration 85, Cost: 0.05355351971887732\n",
      "Iteration 86, Cost: 0.05299415467259194\n",
      "Iteration 87, Cost: 0.05244256520526431\n",
      "Iteration 88, Cost: 0.05189845765421707\n",
      "Iteration 89, Cost: 0.05136153547121188\n",
      "Iteration 90, Cost: 0.05083149962637292\n",
      "Iteration 91, Cost: 0.050308049034503304\n",
      "Iteration 92, Cost: 0.049790881028790056\n",
      "Iteration 93, Cost: 0.04927969191593866\n",
      "Iteration 94, Cost: 0.048774177656429295\n",
      "Iteration 95, Cost: 0.04827403472373447\n",
      "Iteration 96, Cost: 0.047778961206688626\n",
      "Iteration 97, Cost: 0.04728865822914179\n",
      "Iteration 98, Cost: 0.0468028317694764\n",
      "Iteration 99, Cost: 0.04632119496777758\n",
      "Iteration 100, Cost: 0.0458434710078477\n",
      "Iteration 101, Cost: 0.04536939665130106\n",
      "Iteration 102, Cost: 0.044898726477175646\n",
      "Iteration 103, Cost: 0.0444312378377097\n",
      "Iteration 104, Cost: 0.043966736474145084\n",
      "Iteration 105, Cost: 0.04350506264221566\n",
      "Iteration 106, Cost: 0.043046097475784535\n",
      "Iteration 107, Cost: 0.0425897691760962\n",
      "Iteration 108, Cost: 0.04213605847017355\n",
      "Iteration 109, Cost: 0.04168500266322684\n",
      "Iteration 110, Cost: 0.04123669755453873\n",
      "Iteration 111, Cost: 0.040791296535224154\n",
      "Iteration 112, Cost: 0.04034900637117711\n",
      "Iteration 113, Cost: 0.03991007950146342\n",
      "Iteration 114, Cost: 0.039474803118057185\n",
      "Iteration 115, Cost: 0.03904348576133252\n",
      "Iteration 116, Cost: 0.03861644256306128\n",
      "Iteration 117, Cost: 0.0381939804932965\n",
      "Iteration 118, Cost: 0.037776384956899146\n",
      "Iteration 119, Cost: 0.037363908841286125\n",
      "Iteration 120, Cost: 0.03695676470571342\n",
      "Iteration 121, Cost: 0.03655512033013179\n",
      "Iteration 122, Cost: 0.03615909741572862\n",
      "Iteration 123, Cost: 0.035768772925139\n",
      "Iteration 124, Cost: 0.03538418239633804\n",
      "Iteration 125, Cost: 0.03500532454632385\n",
      "Iteration 126, Cost: 0.03463216655922426\n",
      "Iteration 127, Cost: 0.03426464958215308\n",
      "Iteration 128, Cost: 0.033902694091853834\n",
      "Iteration 129, Cost: 0.03354620492002745\n",
      "Iteration 130, Cost: 0.03319507582268249\n",
      "Iteration 131, Cost: 0.03284919354634565\n",
      "Iteration 132, Cost: 0.03250844138497204\n",
      "Iteration 133, Cost: 0.03217270224193087\n",
      "Iteration 134, Cost: 0.03184186121793177\n",
      "Iteration 135, Cost: 0.03151580774388667\n",
      "Iteration 136, Cost: 0.031194437272053968\n",
      "Iteration 137, Cost: 0.03087765253287626\n",
      "Iteration 138, Cost: 0.030565364361244466\n",
      "Iteration 139, Cost: 0.03025749209621283\n",
      "Iteration 140, Cost: 0.029953963563369834\n",
      "Iteration 141, Cost: 0.029654714659231526\n",
      "Iteration 142, Cost: 0.02935968857138814\n",
      "Iteration 143, Cost: 0.029068834685081108\n",
      "Iteration 144, Cost: 0.028782107244107943\n",
      "Iteration 145, Cost: 0.028499463848772055\n",
      "Iteration 146, Cost: 0.02822086388342675\n",
      "Iteration 147, Cost: 0.027946266969007802\n",
      "Iteration 148, Cost: 0.027675631530826427\n",
      "Iteration 149, Cost: 0.02740891355907076\n",
      "Iteration 150, Cost: 0.027146065620425687\n",
      "Iteration 151, Cost: 0.026887036156366614\n",
      "Iteration 152, Cost: 0.026631769079847892\n",
      "Iteration 153, Cost: 0.02638020366001795\n",
      "Iteration 154, Cost: 0.026132274666417314\n",
      "Iteration 155, Cost: 0.025887912731175508\n",
      "Iteration 156, Cost: 0.02564704488043136\n",
      "Iteration 157, Cost: 0.025409595184176893\n",
      "Iteration 158, Cost: 0.025175485476023957\n",
      "Iteration 159, Cost: 0.024944636099786928\n",
      "Iteration 160, Cost: 0.024716966646993625\n",
      "Iteration 161, Cost: 0.02449239665735627\n",
      "Iteration 162, Cost: 0.024270846261977026\n",
      "Iteration 163, Cost: 0.024052236756031447\n",
      "Iteration 164, Cost: 0.02383649109353685\n",
      "Iteration 165, Cost: 0.023623534301450404\n",
      "Iteration 166, Cost: 0.023413293813793055\n",
      "Iteration 167, Cost: 0.023205699728892592\n",
      "Iteration 168, Cost: 0.023000684994367845\n",
      "Iteration 169, Cost: 0.022798185525331933\n",
      "Iteration 170, Cost: 0.0225981402616628\n",
      "Iteration 171, Cost: 0.02240049117023557\n",
      "Iteration 172, Cost: 0.022205183197864246\n",
      "Iteration 173, Cost: 0.022012164180458493\n",
      "Iteration 174, Cost: 0.02182138471363385\n",
      "Iteration 175, Cost: 0.021632797989762986\n",
      "Iteration 176, Cost: 0.02144635960624337\n",
      "Iteration 177, Cost: 0.02126202734958686\n",
      "Iteration 178, Cost: 0.021079760959803597\n",
      "Iteration 179, Cost: 0.020899521879439522\n",
      "Iteration 180, Cost: 0.02072127299151646\n",
      "Iteration 181, Cost: 0.020544978350496106\n",
      "Iteration 182, Cost: 0.020370602910226063\n",
      "Iteration 183, Cost: 0.02019811225261551\n",
      "Iteration 184, Cost: 0.020027472320519137\n",
      "Iteration 185, Cost: 0.01985864915798172\n",
      "Iteration 186, Cost: 0.01969160866061303\n",
      "Iteration 187, Cost: 0.019526316338436135\n",
      "Iteration 188, Cost: 0.01936273709309457\n",
      "Iteration 189, Cost: 0.019200835010835183\n",
      "Iteration 190, Cost: 0.01904057317222372\n",
      "Iteration 191, Cost: 0.018881913479122315\n",
      "Iteration 192, Cost: 0.01872481649908262\n",
      "Iteration 193, Cost: 0.018569241327006887\n",
      "Iteration 194, Cost: 0.018415145463719248\n",
      "Iteration 195, Cost: 0.018262484710987518\n",
      "Iteration 196, Cost: 0.018111213082555196\n",
      "Iteration 197, Cost: 0.017961282730895516\n",
      "Iteration 198, Cost: 0.017812643889694856\n",
      "Iteration 199, Cost: 0.01766524483251964\n",
      "Iteration 200, Cost: 0.017519031848727818\n",
      "Iteration 201, Cost: 0.01737394923846219\n",
      "Iteration 202, Cost: 0.017229939329514946\n",
      "Iteration 203, Cost: 0.01708694251999001\n",
      "Iteration 204, Cost: 0.016944897352016895\n",
      "Iteration 205, Cost: 0.016803740623287763\n",
      "Iteration 206, Cost: 0.016663407544891772\n",
      "Iteration 207, Cost: 0.01652383195578542\n",
      "Iteration 208, Cost: 0.016384946606221626\n",
      "Iteration 209, Cost: 0.016246683524488926\n",
      "Iteration 210, Cost: 0.01610897448326338\n",
      "Iteration 211, Cost: 0.01597175158356894\n",
      "Iteration 212, Cost: 0.01583494797551636\n",
      "Iteration 213, Cost: 0.015698498735289546\n",
      "Iteration 214, Cost: 0.015562341916806548\n",
      "Iteration 215, Cost: 0.015426419793518805\n",
      "Iteration 216, Cost: 0.015290680300246131\n",
      "Iteration 217, Cost: 0.015155078676036302\n",
      "Iteration 218, Cost: 0.015019579296074236\n",
      "Iteration 219, Cost: 0.014884157663103392\n",
      "Iteration 220, Cost: 0.014748802506495\n",
      "Iteration 221, Cost: 0.014613517910488446\n",
      "Iteration 222, Cost: 0.014478325363658433\n",
      "Iteration 223, Cost: 0.014343265591998283\n",
      "Iteration 224, Cost: 0.014208400012178322\n",
      "Iteration 225, Cost: 0.014073811624844204\n",
      "Iteration 226, Cost: 0.013939605166312296\n",
      "Iteration 227, Cost: 0.013805906356489367\n",
      "Iteration 228, Cost: 0.013672860125360047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 229, Cost: 0.013540627770604143\n",
      "Iteration 230, Cost: 0.013409383090628753\n",
      "Iteration 231, Cost: 0.0132793076408613\n",
      "Iteration 232, Cost: 0.013150585362193682\n",
      "Iteration 233, Cost: 0.013023396912313084\n",
      "Iteration 234, Cost: 0.012897914077899171\n",
      "Iteration 235, Cost: 0.012774294648090205\n",
      "Iteration 236, Cost: 0.012652678085126238\n",
      "Iteration 237, Cost: 0.012533182243588912\n",
      "Iteration 238, Cost: 0.012415901279552307\n",
      "Iteration 239, Cost: 0.01230090477351987\n",
      "Iteration 240, Cost: 0.012188237984093304\n",
      "Iteration 241, Cost: 0.012077923066544573\n",
      "Iteration 242, Cost: 0.011969961039128044\n",
      "Iteration 243, Cost: 0.011864334260771369\n",
      "Iteration 244, Cost: 0.011761009192164553\n",
      "Iteration 245, Cost: 0.01165993924058986\n",
      "Iteration 246, Cost: 0.011561067528597058\n",
      "Iteration 247, Cost: 0.011464329470177474\n",
      "Iteration 248, Cost: 0.011369655079608522\n",
      "Iteration 249, Cost: 0.011276970973943147\n",
      "Iteration 250, Cost: 0.01118620205845817\n",
      "Iteration 251, Cost: 0.01109727290498767\n",
      "Iteration 252, Cost: 0.01101010884665259\n",
      "Iteration 253, Cost: 0.010924636820258534\n",
      "Iteration 254, Cost: 0.010840785990928645\n",
      "Iteration 255, Cost: 0.010758488193657524\n",
      "Iteration 256, Cost: 0.010677678224517513\n",
      "Iteration 257, Cost: 0.010598294011089455\n",
      "Iteration 258, Cost: 0.010520276687968204\n",
      "Iteration 259, Cost: 0.0104435705993508\n",
      "Iteration 260, Cost: 0.010368123247035722\n",
      "Iteration 261, Cost: 0.010293885198807546\n",
      "Iteration 262, Cost: 0.010220809969230716\n",
      "Iteration 263, Cost: 0.010148853882349384\n",
      "Iteration 264, Cost: 0.010077975923673028\n",
      "Iteration 265, Cost: 0.01000813758708432\n",
      "Iteration 266, Cost: 0.009939302720892912\n",
      "Iteration 267, Cost: 0.00987143737612879\n",
      "Iteration 268, Cost: 0.009804509659276695\n",
      "Iteration 269, Cost: 0.00973848959095729\n",
      "Iteration 270, Cost: 0.009673348971524779\n",
      "Iteration 271, Cost: 0.00960906125414305\n",
      "Iteration 272, Cost: 0.00954560142559732\n",
      "Iteration 273, Cost: 0.009482945894873585\n",
      "Iteration 274, Cost: 0.009421072389376494\n",
      "Iteration 275, Cost: 0.009359959858543104\n",
      "Iteration 276, Cost: 0.009299588384534275\n",
      "Iteration 277, Cost: 0.009239939099637781\n",
      "Iteration 278, Cost: 0.009180994109990844\n",
      "Iteration 279, Cost: 0.009122736425218986\n",
      "Iteration 280, Cost: 0.009065149893588566\n",
      "Iteration 281, Cost: 0.009008219142279008\n",
      "Iteration 282, Cost: 0.008951929522394712\n",
      "Iteration 283, Cost: 0.008896267058354437\n",
      "Iteration 284, Cost: 0.008841218401315768\n",
      "Iteration 285, Cost: 0.008786770786313298\n",
      "Iteration 286, Cost: 0.008732911992810621\n",
      "Iteration 287, Cost: 0.008679630308387283\n",
      "Iteration 288, Cost: 0.008626914495302672\n",
      "Iteration 289, Cost: 0.008574753759698362\n",
      "Iteration 290, Cost: 0.00852313772321942\n",
      "Iteration 291, Cost: 0.008472056396852775\n",
      "Iteration 292, Cost: 0.008421500156797261\n",
      "Iteration 293, Cost: 0.008371459722195471\n",
      "Iteration 294, Cost: 0.008321926134571684\n",
      "Iteration 295, Cost: 0.008272890738833416\n",
      "Iteration 296, Cost: 0.008224345165706252\n",
      "Iteration 297, Cost: 0.008176281315482757\n",
      "Iteration 298, Cost: 0.008128691342976556\n",
      "Iteration 299, Cost: 0.00808156764358199\n",
      "Iteration 300, Cost: 0.008034902840348397\n",
      "Iteration 301, Cost: 0.007988689771985875\n",
      "Iteration 302, Cost: 0.007942921481726541\n",
      "Iteration 303, Cost: 0.007897591206971886\n",
      "Iteration 304, Cost: 0.007852692369662774\n",
      "Iteration 305, Cost: 0.00780821856731404\n",
      "Iteration 306, Cost: 0.007764163564660681\n",
      "Iteration 307, Cost: 0.007720521285867106\n",
      "Iteration 308, Cost: 0.007677285807255052\n",
      "Iteration 309, Cost: 0.0076344513505095584\n",
      "Iteration 310, Cost: 0.007592012276325814\n",
      "Iteration 311, Cost: 0.0075499630784627994\n",
      "Iteration 312, Cost: 0.007508298378172536\n",
      "Iteration 313, Cost: 0.007467012918976372\n",
      "Iteration 314, Cost: 0.007426101561762018\n",
      "Iteration 315, Cost: 0.007385559280177383\n",
      "Iteration 316, Cost: 0.007345381156299038\n",
      "Iteration 317, Cost: 0.007305562376555138\n",
      "Iteration 318, Cost: 0.00726609822788414\n",
      "Iteration 319, Cost: 0.007226984094112255\n",
      "Iteration 320, Cost: 0.007188215452533895\n",
      "Iteration 321, Cost: 0.007149787870680667\n",
      "Iteration 322, Cost: 0.007111697003265641\n",
      "Iteration 323, Cost: 0.007073938589290617\n",
      "Iteration 324, Cost: 0.007036508449305101\n",
      "Iteration 325, Cost: 0.006999402482806668\n",
      "Iteration 326, Cost: 0.00696261666577305\n",
      "Iteration 327, Cost: 0.0069261470483171625\n",
      "Iteration 328, Cost: 0.0068899897524569076\n",
      "Iteration 329, Cost: 0.006854140969992177\n",
      "Iteration 330, Cost: 0.006818596960482188\n",
      "Iteration 331, Cost: 0.006783354049316598\n",
      "Iteration 332, Cost: 0.006748408625874568\n",
      "Iteration 333, Cost: 0.00671375714176618\n",
      "Iteration 334, Cost: 0.0066793961091511506\n",
      "Iteration 335, Cost: 0.00664532209913011\n",
      "Iteration 336, Cost: 0.006611531740204052\n",
      "Iteration 337, Cost: 0.006578021716797925\n",
      "Iteration 338, Cost: 0.006544788767844539\n",
      "Iteration 339, Cost: 0.0065118296854253565\n",
      "Iteration 340, Cost: 0.00647914131346484\n",
      "Iteration 341, Cost: 0.0064467205464753844\n",
      "Iteration 342, Cost: 0.006414564328349982\n",
      "Iteration 343, Cost: 0.0063826696512000345\n",
      "Iteration 344, Cost: 0.006351033554235812\n",
      "Iteration 345, Cost: 0.006319653122687346\n",
      "Iteration 346, Cost: 0.006288525486763606\n",
      "Iteration 347, Cost: 0.006257647820647919\n",
      "Iteration 348, Cost: 0.006227017341527911\n",
      "Iteration 349, Cost: 0.006196631308658074\n",
      "Iteration 350, Cost: 0.006166487022453495\n",
      "Iteration 351, Cost: 0.0061365818236131\n",
      "Iteration 352, Cost: 0.00610691309227109\n",
      "Iteration 353, Cost: 0.0060774782471751665\n",
      "Iteration 354, Cost: 0.006048274744890331\n",
      "Iteration 355, Cost: 0.006019300079027101\n",
      "Iteration 356, Cost: 0.005990551779492992\n",
      "Iteration 357, Cost: 0.005962027411766272\n",
      "Iteration 358, Cost: 0.005933724576190995\n",
      "Iteration 359, Cost: 0.00590564090729239\n",
      "Iteration 360, Cost: 0.005877774073111767\n",
      "Iteration 361, Cost: 0.005850121774560101\n",
      "Iteration 362, Cost: 0.005822681744789522\n",
      "Iteration 363, Cost: 0.00579545174858201\n",
      "Iteration 364, Cost: 0.005768429581754576\n",
      "Iteration 365, Cost: 0.005741613070580318\n",
      "Iteration 366, Cost: 0.0057150000712247\n",
      "Iteration 367, Cost: 0.005688588469196498\n",
      "Iteration 368, Cost: 0.005662376178812854\n",
      "Iteration 369, Cost: 0.005636361142677913\n",
      "Iteration 370, Cost: 0.005610541331174549\n",
      "Iteration 371, Cost: 0.005584914741968716\n",
      "Iteration 372, Cost: 0.005559479399525959\n",
      "Iteration 373, Cost: 0.005534233354639689\n",
      "Iteration 374, Cost: 0.0055091746839707745\n",
      "Iteration 375, Cost: 0.005484301489598114\n",
      "Iteration 376, Cost: 0.005459611898579778\n",
      "Iteration 377, Cost: 0.005435104062524395\n",
      "Iteration 378, Cost: 0.00541077615717244\n",
      "Iteration 379, Cost: 0.0053866263819871175\n",
      "Iteration 380, Cost: 0.005362652959754494\n",
      "Iteration 381, Cost: 0.005338854136192655\n",
      "Iteration 382, Cost: 0.005315228179569549\n",
      "Iteration 383, Cost: 0.005291773380329277\n",
      "Iteration 384, Cost: 0.005268488050726567\n",
      "Iteration 385, Cost: 0.005245370524469198\n",
      "Iteration 386, Cost: 0.005222419156368109\n",
      "Iteration 387, Cost: 0.005199632321995035\n",
      "Iteration 388, Cost: 0.005177008417347339\n",
      "Iteration 389, Cost: 0.0051545458585199695\n",
      "Iteration 390, Cost: 0.005132243081384217\n",
      "Iteration 391, Cost: 0.0051100985412731925\n",
      "Iteration 392, Cost: 0.005088110712673736\n",
      "Iteration 393, Cost: 0.005066278088924656\n",
      "Iteration 394, Cost: 0.005044599181921092\n",
      "Iteration 395, Cost: 0.00502307252182485\n",
      "Iteration 396, Cost: 0.005001696656780535\n",
      "Iteration 397, Cost: 0.004980470152637357\n",
      "Converged!\n",
      "Iteration 1, Cost: 0.540417032408313\n",
      "Iteration 2, Cost: 0.5068914928186614\n",
      "Iteration 3, Cost: 0.46207169007417026\n",
      "Iteration 4, Cost: 0.406297054358585\n",
      "Iteration 5, Cost: 0.3471937642550883\n",
      "Iteration 6, Cost: 0.29851843289045205\n",
      "Iteration 7, Cost: 0.26706405407442274\n",
      "Iteration 8, Cost: 0.24872309738893883\n",
      "Iteration 9, Cost: 0.23760880649373173\n",
      "Iteration 10, Cost: 0.2301274034522104\n",
      "Iteration 11, Cost: 0.22448312053496178\n",
      "Iteration 12, Cost: 0.2198015091797449\n",
      "Iteration 13, Cost: 0.21564584850005927\n",
      "Iteration 14, Cost: 0.21178996069357695\n",
      "Iteration 15, Cost: 0.20811233879954957\n",
      "Iteration 16, Cost: 0.2045454290950491\n",
      "Iteration 17, Cost: 0.20105049967591054\n",
      "Iteration 18, Cost: 0.1976047630194592\n",
      "Iteration 19, Cost: 0.19419455945673297\n",
      "Iteration 20, Cost: 0.1908116327781946\n",
      "Iteration 21, Cost: 0.1874510321868549\n",
      "Iteration 22, Cost: 0.1841098958555555\n",
      "Iteration 23, Cost: 0.1807867263732801\n",
      "Iteration 24, Cost: 0.17748094779322726\n",
      "Iteration 25, Cost: 0.1741926273195031\n",
      "Iteration 26, Cost: 0.17092229479178617\n",
      "Iteration 27, Cost: 0.167670820971355\n",
      "Iteration 28, Cost: 0.16443933160192092\n",
      "Iteration 29, Cost: 0.1612291436205523\n",
      "Iteration 30, Cost: 0.1580417155220113\n",
      "Iteration 31, Cost: 0.15487860726333086\n",
      "Iteration 32, Cost: 0.15174144710678508\n",
      "Iteration 33, Cost: 0.1486319039571671\n",
      "Iteration 34, Cost: 0.14555166437369768\n",
      "Iteration 35, Cost: 0.1425024137323054\n",
      "Iteration 36, Cost: 0.13948582111435165\n",
      "Iteration 37, Cost: 0.13650352749246045\n",
      "Iteration 38, Cost: 0.13355713673293165\n",
      "Iteration 39, Cost: 0.1306482088769633\n",
      "Iteration 40, Cost: 0.12777825512463675\n",
      "Iteration 41, Cost: 0.12494873394024734\n",
      "Iteration 42, Cost: 0.12216104773037538\n",
      "Iteration 43, Cost: 0.1194165396155786\n",
      "Iteration 44, Cost: 0.11671648991619651\n",
      "Iteration 45, Cost: 0.11406211209253789\n",
      "Iteration 46, Cost: 0.11145454800804731\n",
      "Iteration 47, Cost: 0.10889486250920415\n",
      "Iteration 48, Cost: 0.10638403742753001\n",
      "Iteration 49, Cost: 0.10392296519918398\n",
      "Iteration 50, Cost: 0.10151244236121845\n",
      "Iteration 51, Cost: 0.09915316321876753\n",
      "Iteration 52, Cost: 0.09684571398519029\n",
      "Iteration 53, Cost: 0.09459056768067367\n",
      "Iteration 54, Cost: 0.09238808003871454\n",
      "Iteration 55, Cost: 0.09023848661972206\n",
      "Iteration 56, Cost: 0.08814190127231382\n",
      "Iteration 57, Cost: 0.08609831602094142\n",
      "Iteration 58, Cost: 0.08410760239776426\n",
      "Iteration 59, Cost: 0.08216951418075077\n",
      "Iteration 60, Cost: 0.08028369145138514\n",
      "Iteration 61, Cost: 0.07844966584567704\n",
      "Iteration 62, Cost: 0.0766668668421478\n",
      "Iteration 63, Cost: 0.07493462891011937\n",
      "Iteration 64, Cost: 0.07325219933046055\n",
      "Iteration 65, Cost: 0.07161874649808574\n",
      "Iteration 66, Cost: 0.07003336851987457\n",
      "Iteration 67, Cost: 0.06849510193209944\n",
      "Iteration 68, Cost: 0.06700293037668495\n",
      "Iteration 69, Cost: 0.06555579309446528\n",
      "Iteration 70, Cost: 0.06415259311486284\n",
      "Iteration 71, Cost: 0.06279220504395201\n",
      "Iteration 72, Cost: 0.06147348237563956\n",
      "Iteration 73, Cost: 0.060195264272731516\n",
      "Iteration 74, Cost: 0.05895638178514846\n",
      "Iteration 75, Cost: 0.057755663490849776\n",
      "Iteration 76, Cost: 0.05659194056067473\n",
      "Iteration 77, Cost: 0.05546405126105302\n",
      "Iteration 78, Cost: 0.054370844918328685\n",
      "Iteration 79, Cost: 0.05331118537540235\n",
      "Iteration 80, Cost: 0.05228395397579952\n",
      "Iteration 81, Cost: 0.05128805211248573\n",
      "Iteration 82, Cost: 0.05032240337919804\n",
      "Iteration 83, Cost: 0.04938595536118453\n",
      "Iteration 84, Cost: 0.04847768110044962\n",
      "Iteration 85, Cost: 0.04759658026825704\n",
      "Iteration 86, Cost: 0.046741680075037435\n",
      "Iteration 87, Cost: 0.04591203594520851\n",
      "Iteration 88, Cost: 0.045106731981893834\n",
      "Iteration 89, Cost: 0.04432488124421408\n",
      "Iteration 90, Cost: 0.043565625857762996\n",
      "Iteration 91, Cost: 0.04282813697707365\n",
      "Iteration 92, Cost: 0.042111614617309306\n",
      "Iteration 93, Cost: 0.041415287371042366\n",
      "Iteration 94, Cost: 0.04073841202477676\n",
      "Iteration 95, Cost: 0.040080273088783966\n",
      "Iteration 96, Cost: 0.03944018225282767\n",
      "Iteration 97, Cost: 0.0388174777794194\n",
      "Iteration 98, Cost: 0.03821152384535757\n",
      "Iteration 99, Cost: 0.03762170984144227\n",
      "Iteration 100, Cost: 0.037047449639419625\n",
      "Iteration 101, Cost: 0.03648818083439202\n",
      "Iteration 102, Cost: 0.03594336397013209\n",
      "Iteration 103, Cost: 0.03541248175396478\n",
      "Iteration 104, Cost: 0.03489503826713477\n",
      "Iteration 105, Cost: 0.034390558175863055\n",
      "Iteration 106, Cost: 0.03389858594761879\n",
      "Iteration 107, Cost: 0.033418685076496195\n",
      "Iteration 108, Cost: 0.03295043732099414\n",
      "Iteration 109, Cost: 0.03249344195694931\n",
      "Iteration 110, Cost: 0.03204731504787507\n",
      "Iteration 111, Cost: 0.03161168873450683\n",
      "Iteration 112, Cost: 0.031186210544950466\n",
      "Iteration 113, Cost: 0.030770542726472026\n",
      "Iteration 114, Cost: 0.030364361599652393\n",
      "Iteration 115, Cost: 0.02996735693535749\n",
      "Iteration 116, Cost: 0.029579231354740245\n",
      "Iteration 117, Cost: 0.029199699752291804\n",
      "Iteration 118, Cost: 0.028828488741792772\n",
      "Iteration 119, Cost: 0.028465336124878494\n",
      "Iteration 120, Cost: 0.028109990381820587\n",
      "Iteration 121, Cost: 0.02776221018403903\n",
      "Iteration 122, Cost: 0.027421763927790152\n",
      "Iteration 123, Cost: 0.02708842928842465\n",
      "Iteration 124, Cost: 0.026761992794573083\n",
      "Iteration 125, Cost: 0.026442249421591343\n",
      "Iteration 126, Cost: 0.026129002203584696\n",
      "Iteration 127, Cost: 0.025822061863322573\n",
      "Iteration 128, Cost: 0.02552124645935731\n",
      "Iteration 129, Cost: 0.02522638104966602\n",
      "Iteration 130, Cost: 0.02493729737114503\n",
      "Iteration 131, Cost: 0.024653833534299896\n",
      "Iteration 132, Cost: 0.024375833732489728\n",
      "Iteration 133, Cost: 0.024103147965102125\n",
      "Iteration 134, Cost: 0.02383563177405368\n",
      "Iteration 135, Cost: 0.023573145993030117\n",
      "Iteration 136, Cost: 0.023315556508900152\n",
      "Iteration 137, Cost: 0.023062734034756487\n",
      "Iteration 138, Cost: 0.022814553894057236\n",
      "Iteration 139, Cost: 0.02257089581536027\n",
      "Iteration 140, Cost: 0.022331643737161937\n",
      "Iteration 141, Cost: 0.02209668562237016\n",
      "Iteration 142, Cost: 0.0218659132819599\n",
      "Iteration 143, Cost: 0.021639222207376397\n",
      "Iteration 144, Cost: 0.02141651141126871\n",
      "Iteration 145, Cost: 0.021197683276152464\n",
      "Iteration 146, Cost: 0.020982643410616585\n",
      "Iteration 147, Cost: 0.020771300512704358\n",
      "Iteration 148, Cost: 0.020563566240113962\n",
      "Iteration 149, Cost: 0.02035935508687828\n",
      "Iteration 150, Cost: 0.020158584266197564\n",
      "Iteration 151, Cost: 0.01996117359911238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 152, Cost: 0.019767045408717203\n",
      "Iteration 153, Cost: 0.019576124419627916\n",
      "Iteration 154, Cost: 0.019388337662428718\n",
      "Iteration 155, Cost: 0.01920361438283603\n",
      "Iteration 156, Cost: 0.019021885955328388\n",
      "Iteration 157, Cost: 0.018843085801002808\n",
      "Iteration 158, Cost: 0.01866714930942861\n",
      "Iteration 159, Cost: 0.01849401376428065\n",
      "Iteration 160, Cost: 0.018323618272543585\n",
      "Iteration 161, Cost: 0.018155903697089134\n",
      "Iteration 162, Cost: 0.01799081259243737\n",
      "Iteration 163, Cost: 0.017828289143522674\n",
      "Iteration 164, Cost: 0.01766827910729359\n",
      "Iteration 165, Cost: 0.01751072975698454\n",
      "Iteration 166, Cost: 0.017355589828905733\n",
      "Iteration 167, Cost: 0.017202809471605397\n",
      "Iteration 168, Cost: 0.017052340197266543\n",
      "Iteration 169, Cost: 0.016904134835207713\n",
      "Iteration 170, Cost: 0.016758147487364386\n",
      "Iteration 171, Cost: 0.01661433348563498\n",
      "Iteration 172, Cost: 0.016472649350981763\n",
      "Iteration 173, Cost: 0.016333052754183845\n",
      "Iteration 174, Cost: 0.01619550247814539\n",
      "Iteration 175, Cost: 0.016059958381668623\n",
      "Iteration 176, Cost: 0.015926381364606695\n",
      "Iteration 177, Cost: 0.015794733334317388\n",
      "Iteration 178, Cost: 0.015664977173343954\n",
      "Iteration 179, Cost: 0.015537076708254697\n",
      "Iteration 180, Cost: 0.015410996679577852\n",
      "Iteration 181, Cost: 0.01528670271277319\n",
      "Iteration 182, Cost: 0.015164161290186474\n",
      "Iteration 183, Cost: 0.015043339723937201\n",
      "Iteration 184, Cost: 0.014924206129694301\n",
      "Iteration 185, Cost: 0.014806729401298574\n",
      "Iteration 186, Cost: 0.014690879186194288\n",
      "Iteration 187, Cost: 0.014576625861635984\n",
      "Iteration 188, Cost: 0.014463940511639932\n",
      "Iteration 189, Cost: 0.014352794904652683\n",
      "Iteration 190, Cost: 0.014243161471912173\n",
      "Iteration 191, Cost: 0.014135013286479358\n",
      "Iteration 192, Cost: 0.01402832404292085\n",
      "Iteration 193, Cost: 0.013923068037625212\n",
      "Iteration 194, Cost: 0.013819220149737386\n",
      "Iteration 195, Cost: 0.013716755822697579\n",
      "Iteration 196, Cost: 0.013615651046372291\n",
      "Iteration 197, Cost: 0.013515882339766423\n",
      "Iteration 198, Cost: 0.01341742673430647\n",
      "Iteration 199, Cost: 0.0133202617576855\n",
      "Iteration 200, Cost: 0.013224365418261304\n",
      "Iteration 201, Cost: 0.013129716189999507\n",
      "Iteration 202, Cost: 0.013036292997953551\n",
      "Iteration 203, Cost: 0.012944075204273618\n",
      "Iteration 204, Cost: 0.01285304259473647\n",
      "Iteration 205, Cost: 0.012763175365787912\n",
      "Iteration 206, Cost: 0.012674454112089254\n",
      "Iteration 207, Cost: 0.012586859814558793\n",
      "Iteration 208, Cost: 0.012500373828898759\n",
      "Iteration 209, Cost: 0.012414977874597614\n",
      "Iteration 210, Cost: 0.012330654024397081\n",
      "Iteration 211, Cost: 0.012247384694212493\n",
      "Iteration 212, Cost: 0.012165152633494655\n",
      "Iteration 213, Cost: 0.012083940916020526\n",
      "Iteration 214, Cost: 0.012003732931099735\n",
      "Iteration 215, Cost: 0.011924512375183131\n",
      "Iteration 216, Cost: 0.011846263243859205\n",
      "Iteration 217, Cost: 0.011768969824223811\n",
      "Iteration 218, Cost: 0.011692616687608126\n",
      "Iteration 219, Cost: 0.011617188682649542\n",
      "Iteration 220, Cost: 0.01154267092869001\n",
      "Iteration 221, Cost: 0.011469048809486146\n",
      "Iteration 222, Cost: 0.01139630796721535\n",
      "Iteration 223, Cost: 0.011324434296762276\n",
      "Iteration 224, Cost: 0.011253413940270003\n",
      "Iteration 225, Cost: 0.01118323328194053\n",
      "Iteration 226, Cost: 0.011113878943069373\n",
      "Iteration 227, Cost: 0.01104533777729946\n",
      "Iteration 228, Cost: 0.010977596866079816\n",
      "Iteration 229, Cost: 0.010910643514314994\n",
      "Iteration 230, Cost: 0.010844465246191799\n",
      "Iteration 231, Cost: 0.010779049801170148\n",
      "Iteration 232, Cost: 0.01071438513012577\n",
      "Iteration 233, Cost: 0.010650459391632764\n",
      "Iteration 234, Cost: 0.010587260948374925\n",
      "Iteration 235, Cost: 0.010524778363675102\n",
      "Iteration 236, Cost: 0.010463000398132728\n",
      "Iteration 237, Cost: 0.010401916006360205\n",
      "Iteration 238, Cost: 0.01034151433380942\n",
      "Iteration 239, Cost: 0.010281784713680475\n",
      "Iteration 240, Cost: 0.010222716663905103\n",
      "Iteration 241, Cost: 0.010164299884198074\n",
      "Iteration 242, Cost: 0.01010652425317038\n",
      "Iteration 243, Cost: 0.010049379825498488\n",
      "Iteration 244, Cost: 0.009992856829144681\n",
      "Iteration 245, Cost: 0.009936945662623837\n",
      "Iteration 246, Cost: 0.00988163689231259\n",
      "Iteration 247, Cost: 0.009826921249797253\n",
      "Iteration 248, Cost: 0.009772789629257332\n",
      "Iteration 249, Cost: 0.009719233084881764\n",
      "Iteration 250, Cost: 0.009666242828315595\n",
      "Iteration 251, Cost: 0.009613810226134896\n",
      "Iteration 252, Cost: 0.00956192679734822\n",
      "Iteration 253, Cost: 0.009510584210923146\n",
      "Iteration 254, Cost: 0.009459774283336615\n",
      "Iteration 255, Cost: 0.009409488976148192\n",
      "Iteration 256, Cost: 0.009359720393595375\n",
      "Iteration 257, Cost: 0.009310460780210432\n",
      "Iteration 258, Cost: 0.009261702518458308\n",
      "Iteration 259, Cost: 0.009213438126395355\n",
      "Iteration 260, Cost: 0.009165660255348707\n",
      "Iteration 261, Cost: 0.009118361687616238\n",
      "Iteration 262, Cost: 0.009071535334187222\n",
      "Iteration 263, Cost: 0.009025174232483679\n",
      "Iteration 264, Cost: 0.008979271544122735\n",
      "Iteration 265, Cost: 0.008933820552700134\n",
      "Iteration 266, Cost: 0.00888881466159516\n",
      "Iteration 267, Cost: 0.008844247391797374\n",
      "Iteration 268, Cost: 0.008800112379755342\n",
      "Iteration 269, Cost: 0.008756403375247796\n",
      "Iteration 270, Cost: 0.008713114239277515\n",
      "Iteration 271, Cost: 0.008670238941988264\n",
      "Iteration 272, Cost: 0.00862777156060514\n",
      "Iteration 273, Cost: 0.008585706277398622\n",
      "Iteration 274, Cost: 0.008544037377672675\n",
      "Iteration 275, Cost: 0.008502759247777143\n",
      "Iteration 276, Cost: 0.008461866373144762\n",
      "Iteration 277, Cost: 0.008421353336352992\n",
      "Iteration 278, Cost: 0.008381214815210957\n",
      "Iteration 279, Cost: 0.008341445580871663\n",
      "Iteration 280, Cost: 0.008302040495969661\n",
      "Iteration 281, Cost: 0.008262994512784384\n",
      "Iteration 282, Cost: 0.008224302671429198\n",
      "Iteration 283, Cost: 0.008185960098066393\n",
      "Iteration 284, Cost: 0.008147962003148065\n",
      "Iteration 285, Cost: 0.008110303679683066\n",
      "Iteration 286, Cost: 0.008072980501530031\n",
      "Iteration 287, Cost: 0.008035987921716425\n",
      "Iteration 288, Cost: 0.007999321470783718\n",
      "Iteration 289, Cost: 0.007962976755158595\n",
      "Iteration 290, Cost: 0.00792694945555015\n",
      "Iteration 291, Cost: 0.007891235325373027\n",
      "Iteration 292, Cost: 0.007855830189196396\n",
      "Iteration 293, Cost: 0.007820729941218702\n",
      "Iteration 294, Cost: 0.0077859305437680045\n",
      "Iteration 295, Cost: 0.007751428025827836\n",
      "Iteration 296, Cost: 0.0077172184815883895\n",
      "Iteration 297, Cost: 0.007683298069022881\n",
      "Iteration 298, Cost: 0.0076496630084889276\n",
      "Iteration 299, Cost: 0.007616309581354713\n",
      "Iteration 300, Cost: 0.007583234128649805\n",
      "Iteration 301, Cost: 0.0075504330497403105\n",
      "Iteration 302, Cost: 0.007517902801028291\n",
      "Iteration 303, Cost: 0.0074856398946750525\n",
      "Iteration 304, Cost: 0.007453640897348189\n",
      "Iteration 305, Cost: 0.007421902428992083\n",
      "Iteration 306, Cost: 0.0073904211616216105\n",
      "Iteration 307, Cost: 0.007359193818138796\n",
      "Iteration 308, Cost: 0.007328217171172162\n",
      "Iteration 309, Cost: 0.007297488041938476\n",
      "Iteration 310, Cost: 0.0072670032991266325\n",
      "Iteration 311, Cost: 0.007236759857803404\n",
      "Iteration 312, Cost: 0.007206754678340739\n",
      "Iteration 313, Cost: 0.007176984765364339\n",
      "Iteration 314, Cost: 0.00714744716672322\n",
      "Iteration 315, Cost: 0.007118138972479965\n",
      "Iteration 316, Cost: 0.007089057313921351\n",
      "Iteration 317, Cost: 0.00706019936258907\n",
      "Iteration 318, Cost: 0.007031562329330221\n",
      "Iteration 319, Cost: 0.007003143463367296\n",
      "Iteration 320, Cost: 0.006974940051387301\n",
      "Iteration 321, Cost: 0.006946949416649792\n",
      "Iteration 322, Cost: 0.006919168918113394\n",
      "Iteration 323, Cost: 0.006891595949580641\n",
      "Iteration 324, Cost: 0.006864227938860686\n",
      "Iteration 325, Cost: 0.006837062346949683\n",
      "Iteration 326, Cost: 0.006810096667228468\n",
      "Iteration 327, Cost: 0.006783328424677248\n",
      "Iteration 328, Cost: 0.006756755175107016\n",
      "Iteration 329, Cost: 0.006730374504407346\n",
      "Iteration 330, Cost: 0.006704184027810297\n",
      "Iteration 331, Cost: 0.006678181389170114\n",
      "Iteration 332, Cost: 0.006652364260258423\n",
      "Iteration 333, Cost: 0.006626730340074631\n",
      "Iteration 334, Cost: 0.0066012773541712375\n",
      "Iteration 335, Cost: 0.006576003053993759\n",
      "Iteration 336, Cost: 0.006550905216234993\n",
      "Iteration 337, Cost: 0.006525981642203331\n",
      "Iteration 338, Cost: 0.006501230157204848\n",
      "Iteration 339, Cost: 0.006476648609938897\n",
      "Iteration 340, Cost: 0.006452234871906925\n",
      "Iteration 341, Cost: 0.006427986836834268\n",
      "Iteration 342, Cost: 0.006403902420104665\n",
      "Iteration 343, Cost: 0.006379979558207233\n",
      "Iteration 344, Cost: 0.0063562162081956634\n",
      "Iteration 345, Cost: 0.0063326103471594035\n",
      "Iteration 346, Cost: 0.0063091599717065945\n",
      "Iteration 347, Cost: 0.0062858630974585656\n",
      "Iteration 348, Cost: 0.006262717758555612\n",
      "Iteration 349, Cost: 0.006239722007173931\n",
      "Iteration 350, Cost: 0.006216873913053482\n",
      "Iteration 351, Cost: 0.00619417156303657\n",
      "Iteration 352, Cost: 0.006171613060617013\n",
      "Iteration 353, Cost: 0.006149196525499728\n",
      "Iteration 354, Cost: 0.006126920093170552\n",
      "Iteration 355, Cost: 0.006104781914476207\n",
      "Iteration 356, Cost: 0.0060827801552142255\n",
      "Iteration 357, Cost: 0.006060912995732782\n",
      "Iteration 358, Cost: 0.006039178630540292\n",
      "Iteration 359, Cost: 0.006017575267924704\n",
      "Iteration 360, Cost: 0.0059961011295824075\n",
      "Iteration 361, Cost: 0.005974754450256721\n",
      "Iteration 362, Cost: 0.005953533477385901\n",
      "Iteration 363, Cost: 0.005932436470760647\n",
      "Iteration 364, Cost: 0.005911461702191125\n",
      "Iteration 365, Cost: 0.005890607455183485\n",
      "Iteration 366, Cost: 0.005869872024625948\n",
      "Iteration 367, Cost: 0.005849253716484445\n",
      "Iteration 368, Cost: 0.005828750847507984\n",
      "Iteration 369, Cost: 0.005808361744943712\n",
      "Iteration 370, Cost: 0.005788084746261902\n",
      "Iteration 371, Cost: 0.005767918198890911\n",
      "Iteration 372, Cost: 0.005747860459962318\n",
      "Iteration 373, Cost: 0.005727909896066422\n",
      "Iteration 374, Cost: 0.005708064883018267\n",
      "Iteration 375, Cost: 0.005688323805634475\n",
      "Iteration 376, Cost: 0.005668685057521119\n",
      "Iteration 377, Cost: 0.005649147040872906\n",
      "Iteration 378, Cost: 0.0056297081662840245\n",
      "Iteration 379, Cost: 0.005610366852570946\n",
      "Iteration 380, Cost: 0.0055911215266075676\n",
      "Iteration 381, Cost: 0.005571970623173109\n",
      "Iteration 382, Cost: 0.005552912584813158\n",
      "Iteration 383, Cost: 0.005533945861714322\n",
      "Iteration 384, Cost: 0.005515068911593006\n",
      "Iteration 385, Cost: 0.0054962801995987794\n",
      "Iteration 386, Cost: 0.005477578198232917\n",
      "Iteration 387, Cost: 0.005458961387282671\n",
      "Iteration 388, Cost: 0.005440428253771884\n",
      "Iteration 389, Cost: 0.0054219772919285895\n",
      "Iteration 390, Cost: 0.005403607003170247\n",
      "Iteration 391, Cost: 0.005385315896107345\n",
      "Iteration 392, Cost: 0.005367102486566047\n",
      "Iteration 393, Cost: 0.005348965297630693\n",
      "Iteration 394, Cost: 0.005330902859706889\n",
      "Iteration 395, Cost: 0.005312913710606034\n",
      "Iteration 396, Cost: 0.005294996395652091\n",
      "Iteration 397, Cost: 0.005277149467811462\n",
      "Iteration 398, Cost: 0.005259371487846863\n",
      "Iteration 399, Cost: 0.0052416610244960605\n",
      "Iteration 400, Cost: 0.005224016654676413\n",
      "Iteration 401, Cost: 0.00520643696371612\n",
      "Iteration 402, Cost: 0.005188920545613105\n",
      "Iteration 403, Cost: 0.005171466003322482\n",
      "Iteration 404, Cost: 0.005154071949073522\n",
      "Iteration 405, Cost: 0.005136737004717064\n",
      "Iteration 406, Cost: 0.00511945980210427\n",
      "Iteration 407, Cost: 0.005102238983497625\n",
      "Iteration 408, Cost: 0.005085073202015054\n",
      "Iteration 409, Cost: 0.005067961122108017\n",
      "Iteration 410, Cost: 0.005050901420074356\n",
      "Iteration 411, Cost: 0.00503389278460663\n",
      "Iteration 412, Cost: 0.005016933917376694\n",
      "Iteration 413, Cost: 0.00500002353365707\n",
      "Iteration 414, Cost: 0.0049831603629797\n",
      "Converged!\n",
      "Iteration 1, Cost: 0.2662070948858568\n",
      "Iteration 2, Cost: 0.24575982877183403\n",
      "Iteration 3, Cost: 0.237637771809937\n",
      "Iteration 4, Cost: 0.23365895577672272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5, Cost: 0.23094633010933596\n",
      "Iteration 6, Cost: 0.2286116911758541\n",
      "Iteration 7, Cost: 0.22636806533036144\n",
      "Iteration 8, Cost: 0.22411291443438644\n",
      "Iteration 9, Cost: 0.22180334910180052\n",
      "Iteration 10, Cost: 0.2194158528881398\n",
      "Iteration 11, Cost: 0.21693204887411646\n",
      "Iteration 12, Cost: 0.21433299390276425\n",
      "Iteration 13, Cost: 0.21159632831425412\n",
      "Iteration 14, Cost: 0.20869432797457613\n",
      "Iteration 15, Cost: 0.20559220257518554\n",
      "Iteration 16, Cost: 0.20224656612632091\n",
      "Iteration 17, Cost: 0.19860448365571895\n",
      "Iteration 18, Cost: 0.19460419386012198\n",
      "Iteration 19, Cost: 0.19017975899545606\n",
      "Iteration 20, Cost: 0.18527351301785616\n",
      "Iteration 21, Cost: 0.1798613363154847\n",
      "Iteration 22, Cost: 0.17399306635284248\n",
      "Iteration 23, Cost: 0.16783714254895057\n",
      "Iteration 24, Cost: 0.16169275608333683\n",
      "Iteration 25, Cost: 0.15591987277894467\n",
      "Iteration 26, Cost: 0.1507915390736155\n",
      "Iteration 27, Cost: 0.14637621425683153\n",
      "Iteration 28, Cost: 0.14255653857685513\n",
      "Iteration 29, Cost: 0.13914576338266008\n",
      "Iteration 30, Cost: 0.13598480226644535\n",
      "Iteration 31, Cost: 0.1329726546021151\n",
      "Iteration 32, Cost: 0.1300560265147181\n",
      "Iteration 33, Cost: 0.1272104446846064\n",
      "Iteration 34, Cost: 0.12442620761747127\n",
      "Iteration 35, Cost: 0.12170041738101567\n",
      "Iteration 36, Cost: 0.11903302182189623\n",
      "Iteration 37, Cost: 0.11642498699025904\n",
      "Iteration 38, Cost: 0.11387749057503353\n",
      "Iteration 39, Cost: 0.11139158152881617\n",
      "Iteration 40, Cost: 0.10896804883510793\n",
      "Iteration 41, Cost: 0.10660738337075838\n",
      "Iteration 42, Cost: 0.10430977983885072\n",
      "Iteration 43, Cost: 0.10207515383024081\n",
      "Iteration 44, Cost: 0.09990316236384321\n",
      "Iteration 45, Cost: 0.09779322326775029\n",
      "Iteration 46, Cost: 0.09574453274622508\n",
      "Iteration 47, Cost: 0.0937560825577579\n",
      "Iteration 48, Cost: 0.09182667885224108\n",
      "Iteration 49, Cost: 0.08995496425252031\n",
      "Iteration 50, Cost: 0.08813944368582576\n",
      "Iteration 51, Cost: 0.08637851328220278\n",
      "Iteration 52, Cost: 0.0846704907703403\n",
      "Iteration 53, Cost: 0.08301364542433766\n",
      "Iteration 54, Cost: 0.08140622574048996\n",
      "Iteration 55, Cost: 0.07984648349547768\n",
      "Iteration 56, Cost: 0.07833269345141174\n",
      "Iteration 57, Cost: 0.07686316855504316\n",
      "Iteration 58, Cost: 0.07543627092373904\n",
      "Iteration 59, Cost: 0.07405041918514209\n",
      "Iteration 60, Cost: 0.07270409285564448\n",
      "Iteration 61, Cost: 0.0713958344440885\n",
      "Iteration 62, Cost: 0.07012424989548026\n",
      "Iteration 63, Cost: 0.0688880078826467\n",
      "Iteration 64, Cost: 0.06768583833881209\n",
      "Iteration 65, Cost: 0.06651653051771937\n",
      "Iteration 66, Cost: 0.06537893077856063\n",
      "Iteration 67, Cost: 0.0642719402232483\n",
      "Iteration 68, Cost: 0.06319451226263169\n",
      "Iteration 69, Cost: 0.06214565015353986\n",
      "Iteration 70, Cost: 0.061124404526779516\n",
      "Iteration 71, Cost: 0.06012987091423493\n",
      "Iteration 72, Cost: 0.05916118727817868\n",
      "Iteration 73, Cost: 0.05821753154548255\n",
      "Iteration 74, Cost: 0.05729811915179002\n",
      "Iteration 75, Cost: 0.05640220060448973\n",
      "Iteration 76, Cost: 0.05552905907748373\n",
      "Iteration 77, Cost: 0.054678008054528014\n",
      "Iteration 78, Cost: 0.05384838904081305\n",
      "Iteration 79, Cost: 0.05303956936410673\n",
      "Iteration 80, Cost: 0.05225094008702069\n",
      "Iteration 81, Cost: 0.05148191405074541\n",
      "Iteration 82, Cost: 0.050731924068026124\n",
      "Iteration 83, Cost: 0.05000042127942972\n",
      "Iteration 84, Cost: 0.04928687368238505\n",
      "Iteration 85, Cost: 0.048590764837425464\n",
      "Iteration 86, Cost: 0.04791159275090342\n",
      "Iteration 87, Cost: 0.04724886892854677\n",
      "Iteration 88, Cost: 0.04660211758989343\n",
      "Iteration 89, Cost: 0.045970875030109035\n",
      "Iteration 90, Cost: 0.045354689113099464\n",
      "Iteration 91, Cost: 0.044753118878225126\n",
      "Iteration 92, Cost: 0.04416573424226859\n",
      "Iteration 93, Cost: 0.043592115778499294\n",
      "Iteration 94, Cost: 0.043031854555572936\n",
      "Iteration 95, Cost: 0.04248455202042684\n",
      "Iteration 96, Cost: 0.041949819911117445\n",
      "Iteration 97, Cost: 0.04142728018752962\n",
      "Iteration 98, Cost: 0.0409165649699345\n",
      "Iteration 99, Cost: 0.040417316477373\n",
      "Iteration 100, Cost: 0.039929186959714216\n",
      "Iteration 101, Cost: 0.03945183861893095\n",
      "Iteration 102, Cost: 0.038984943516612934\n",
      "Iteration 103, Cost: 0.038528183465995784\n",
      "Iteration 104, Cost: 0.03808124990781795\n",
      "Iteration 105, Cost: 0.037643843770143376\n",
      "Iteration 106, Cost: 0.03721567531292201\n",
      "Iteration 107, Cost: 0.03679646395852568\n",
      "Iteration 108, Cost: 0.03638593810981637\n",
      "Iteration 109, Cost: 0.03598383495750056\n",
      "Iteration 110, Cost: 0.035589900278619274\n",
      "Iteration 111, Cost: 0.03520388822803892\n",
      "Iteration 112, Cost: 0.034825561124760256\n",
      "Iteration 113, Cost: 0.034454689234769104\n",
      "Iteration 114, Cost: 0.03409105055202442\n",
      "Iteration 115, Cost: 0.03373443057903013\n",
      "Iteration 116, Cost: 0.033384622108275086\n",
      "Iteration 117, Cost: 0.03304142500565797\n",
      "Iteration 118, Cost: 0.03270464599684827\n",
      "Iteration 119, Cost: 0.03237409845737292\n",
      "Iteration 120, Cost: 0.03204960220706644\n",
      "Iteration 121, Cost: 0.031730983309380414\n",
      "Iteration 122, Cost: 0.031418073875919696\n",
      "Iteration 123, Cost: 0.031110711876455855\n",
      "Iteration 124, Cost: 0.03080874095456623\n",
      "Iteration 125, Cost: 0.03051201024895638\n",
      "Iteration 126, Cost: 0.030220374220446686\n",
      "Iteration 127, Cost: 0.0299336924845381\n",
      "Iteration 128, Cost: 0.029651829649417056\n",
      "Iteration 129, Cost: 0.02937465515921509\n",
      "Iteration 130, Cost: 0.029102043142302754\n",
      "Iteration 131, Cost: 0.028833872264370213\n",
      "Iteration 132, Cost: 0.028570025586026282\n",
      "Iteration 133, Cost: 0.028310390424634548\n",
      "Iteration 134, Cost: 0.028054858220096797\n",
      "Iteration 135, Cost: 0.027803324404291688\n",
      "Iteration 136, Cost: 0.027555688273878123\n",
      "Iteration 137, Cost: 0.02731185286617885\n",
      "Iteration 138, Cost: 0.02707172483786917\n",
      "Iteration 139, Cost: 0.02683521434620844\n",
      "Iteration 140, Cost: 0.02660223493256755\n",
      "Iteration 141, Cost: 0.0263727034080235\n",
      "Iteration 142, Cost: 0.02614653974081272\n",
      "Iteration 143, Cost: 0.025923666945456805\n",
      "Iteration 144, Cost: 0.025704010973398236\n",
      "Iteration 145, Cost: 0.02548750060500889\n",
      "Iteration 146, Cost: 0.025274067342860397\n",
      "Iteration 147, Cost: 0.025063645306172427\n",
      "Iteration 148, Cost: 0.024856171126382227\n",
      "Iteration 149, Cost: 0.024651583843806474\n",
      "Iteration 150, Cost: 0.024449824805393462\n",
      "Iteration 151, Cost: 0.02425083756359017\n",
      "Iteration 152, Cost: 0.024054567776374387\n",
      "Iteration 153, Cost: 0.023860963108525958\n",
      "Iteration 154, Cost: 0.023669973134233593\n",
      "Iteration 155, Cost: 0.023481549241153817\n",
      "Iteration 156, Cost: 0.023295644536056506\n",
      "Iteration 157, Cost: 0.023112213752206123\n",
      "Iteration 158, Cost: 0.02293121315864021\n",
      "Iteration 159, Cost: 0.022752600471515123\n",
      "Iteration 160, Cost: 0.02257633476769488\n",
      "Iteration 161, Cost: 0.02240237640076077\n",
      "Iteration 162, Cost: 0.022230686919618433\n",
      "Iteration 163, Cost: 0.022061228989874067\n",
      "Iteration 164, Cost: 0.021893966318144015\n",
      "Iteration 165, Cost: 0.02172886357945054\n",
      "Iteration 166, Cost: 0.02156588634784316\n",
      "Iteration 167, Cost: 0.0214050010303685\n",
      "Iteration 168, Cost: 0.021246174804493187\n",
      "Iteration 169, Cost: 0.021089375559063813\n",
      "Iteration 170, Cost: 0.020934571838866742\n",
      "Iteration 171, Cost: 0.02078173279282735\n",
      "Iteration 172, Cost: 0.02063082812586547\n",
      "Iteration 173, Cost: 0.020481828054400266\n",
      "Iteration 174, Cost: 0.020334703265474686\n",
      "Iteration 175, Cost: 0.02018942487944731\n",
      "Iteration 176, Cost: 0.020045964416177998\n",
      "Iteration 177, Cost: 0.019904293764613605\n",
      "Iteration 178, Cost: 0.01976438515566168\n",
      "Iteration 179, Cost: 0.019626211138223378\n",
      "Iteration 180, Cost: 0.019489744558242112\n",
      "Iteration 181, Cost: 0.019354958540612196\n",
      "Iteration 182, Cost: 0.019221826473781123\n",
      "Iteration 183, Cost: 0.019090321996871332\n",
      "Iteration 184, Cost: 0.01896041898914124\n",
      "Iteration 185, Cost: 0.0188320915616018\n",
      "Iteration 186, Cost: 0.01870531405060307\n",
      "Iteration 187, Cost: 0.018580061013205758\n",
      "Iteration 188, Cost: 0.0184563072241545\n",
      "Iteration 189, Cost: 0.01833402767427389\n",
      "Iteration 190, Cost: 0.01821319757011298\n",
      "Iteration 191, Cost: 0.018093792334670726\n",
      "Iteration 192, Cost: 0.01797578760904233\n",
      "Iteration 193, Cost: 0.017859159254834896\n",
      "Iteration 194, Cost: 0.01774388335720986\n",
      "Iteration 195, Cost: 0.01762993622841944\n",
      "Iteration 196, Cost: 0.017517294411714228\n",
      "Iteration 197, Cost: 0.01740593468550938\n",
      "Iteration 198, Cost: 0.01729583406770701\n",
      "Iteration 199, Cost: 0.017186969820082644\n",
      "Iteration 200, Cost: 0.017079319452653566\n",
      "Iteration 201, Cost: 0.016972860727956642\n",
      "Iteration 202, Cost: 0.016867571665172474\n",
      "Iteration 203, Cost: 0.016763430544041894\n",
      "Iteration 204, Cost: 0.016660415908528924\n",
      "Iteration 205, Cost: 0.01655850657019264\n",
      "Iteration 206, Cost: 0.01645768161123752\n",
      "Iteration 207, Cost: 0.016357920387218783\n",
      "Iteration 208, Cost: 0.01625920252938555\n",
      "Iteration 209, Cost: 0.016161507946650343\n",
      "Iteration 210, Cost: 0.01606481682717859\n",
      "Iteration 211, Cost: 0.01596910963959643\n",
      "Iteration 212, Cost: 0.015874367133819414\n",
      "Iteration 213, Cost: 0.015780570341508014\n",
      "Iteration 214, Cost: 0.015687700576159317\n",
      "Iteration 215, Cost: 0.015595739432847052\n",
      "Iteration 216, Cost: 0.01550466878762404\n",
      "Iteration 217, Cost: 0.015414470796603774\n",
      "Iteration 218, Cost: 0.015325127894738982\n",
      "Iteration 219, Cost: 0.015236622794316632\n",
      "Iteration 220, Cost: 0.0151489384831899\n",
      "Iteration 221, Cost: 0.015062058222768501\n",
      "Iteration 222, Cost: 0.014975965545789262\n",
      "Iteration 223, Cost: 0.014890644253889638\n",
      "Iteration 224, Cost: 0.01480607841500681\n",
      "Iteration 225, Cost: 0.014722252360625505\n",
      "Iteration 226, Cost: 0.01463915068289748\n",
      "Iteration 227, Cost: 0.014556758231655968\n",
      "Iteration 228, Cost: 0.014475060111347908\n",
      "Iteration 229, Cost: 0.014394041677907032\n",
      "Iteration 230, Cost: 0.014313688535590476\n",
      "Iteration 231, Cost: 0.014233986533801526\n",
      "Iteration 232, Cost: 0.014154921763920908\n",
      "Iteration 233, Cost: 0.014076480556168825\n",
      "Iteration 234, Cost: 0.013998649476519751\n",
      "Iteration 235, Cost: 0.013921415323691792\n",
      "Iteration 236, Cost: 0.013844765126232377\n",
      "Iteration 237, Cost: 0.01376868613972173\n",
      "Iteration 238, Cost: 0.013693165844115465\n",
      "Iteration 239, Cost: 0.01361819194124771\n",
      "Iteration 240, Cost: 0.013543752352515714\n",
      "Iteration 241, Cost: 0.013469835216767048\n",
      "Iteration 242, Cost: 0.013396428888410245\n",
      "Iteration 243, Cost: 0.013323521935769612\n",
      "Iteration 244, Cost: 0.013251103139704713\n",
      "Iteration 245, Cost: 0.013179161492514885\n",
      "Iteration 246, Cost: 0.013107686197148763\n",
      "Iteration 247, Cost: 0.013036666666738462\n",
      "Iteration 248, Cost: 0.01296609252447754\n",
      "Iteration 249, Cost: 0.012895953603861358\n",
      "Iteration 250, Cost: 0.012826239949307471\n",
      "Iteration 251, Cost: 0.012756941817173047\n",
      "Iteration 252, Cost: 0.012688049677184782\n",
      "Iteration 253, Cost: 0.012619554214295715\n",
      "Iteration 254, Cost: 0.012551446330981365\n",
      "Iteration 255, Cost: 0.012483717149985847\n",
      "Iteration 256, Cost: 0.012416358017526096\n",
      "Iteration 257, Cost: 0.012349360506959657\n",
      "Iteration 258, Cost: 0.01228271642291843\n",
      "Iteration 259, Cost: 0.012216417805907027\n",
      "Iteration 260, Cost: 0.012150456937360377\n",
      "Iteration 261, Cost: 0.012084826345150736\n",
      "Iteration 262, Cost: 0.012019518809529025\n",
      "Iteration 263, Cost: 0.011954527369479896\n",
      "Iteration 264, Cost: 0.011889845329463807\n",
      "Iteration 265, Cost: 0.011825466266512605\n",
      "Iteration 266, Cost: 0.011761384037638184\n",
      "Iteration 267, Cost: 0.011697592787505878\n",
      "Iteration 268, Cost: 0.01163408695631652\n",
      "Iteration 269, Cost: 0.011570861287832607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 270, Cost: 0.011507910837475384\n",
      "Iteration 271, Cost: 0.011445230980411178\n",
      "Iteration 272, Cost: 0.011382817419536359\n",
      "Iteration 273, Cost: 0.011320666193262045\n",
      "Iteration 274, Cost: 0.01125877368299137\n",
      "Iteration 275, Cost: 0.011197136620174613\n",
      "Iteration 276, Cost: 0.011135752092820741\n",
      "Iteration 277, Cost: 0.011074617551338081\n",
      "Iteration 278, Cost: 0.011013730813572384\n",
      "Iteration 279, Cost: 0.010953090068907604\n",
      "Iteration 280, Cost: 0.010892693881293563\n",
      "Iteration 281, Cost: 0.010832541191065371\n",
      "Iteration 282, Cost: 0.010772631315422906\n",
      "Iteration 283, Cost: 0.010712963947443817\n",
      "Iteration 284, Cost: 0.010653539153512059\n",
      "Iteration 285, Cost: 0.010594357369054617\n",
      "Iteration 286, Cost: 0.010535419392492945\n",
      "Iteration 287, Cost: 0.010476726377331875\n",
      "Iteration 288, Cost: 0.010418279822328146\n",
      "Iteration 289, Cost: 0.010360081559702016\n",
      "Iteration 290, Cost: 0.010302133741379525\n",
      "Iteration 291, Cost: 0.010244438823278489\n",
      "Iteration 292, Cost: 0.010186999547678714\n",
      "Iteration 293, Cost: 0.010129818923744907\n",
      "Iteration 294, Cost: 0.010072900206299374\n",
      "Iteration 295, Cost: 0.010016246872969896\n",
      "Iteration 296, Cost: 0.009959862599865492\n",
      "Iteration 297, Cost: 0.009903751235958665\n",
      "Iteration 298, Cost: 0.00984791677637623\n",
      "Iteration 299, Cost: 0.009792363334821533\n",
      "Iteration 300, Cost: 0.009737095115367966\n",
      "Iteration 301, Cost: 0.009682116383877157\n",
      "Iteration 302, Cost: 0.009627431439303787\n",
      "Iteration 303, Cost: 0.009573044585153356\n",
      "Iteration 304, Cost: 0.009518960101358626\n",
      "Iteration 305, Cost: 0.009465182216834829\n",
      "Iteration 306, Cost: 0.00941171508296376\n",
      "Iteration 307, Cost: 0.009358562748241999\n",
      "Iteration 308, Cost: 0.00930572913430993\n",
      "Iteration 309, Cost: 0.009253218013555753\n",
      "Iteration 310, Cost: 0.009201032988463417\n",
      "Iteration 311, Cost: 0.009149177472845577\n",
      "Iteration 312, Cost: 0.009097654675073482\n",
      "Iteration 313, Cost: 0.009046467583385289\n",
      "Iteration 314, Cost: 0.008995618953323854\n",
      "Iteration 315, Cost: 0.008945111297325144\n",
      "Iteration 316, Cost: 0.008894946876449497\n",
      "Iteration 317, Cost: 0.008845127694220791\n",
      "Iteration 318, Cost: 0.008795655492513774\n",
      "Iteration 319, Cost: 0.008746531749407283\n",
      "Iteration 320, Cost: 0.008697757678901756\n",
      "Iteration 321, Cost: 0.008649334232382958\n",
      "Iteration 322, Cost: 0.008601262101700707\n",
      "Iteration 323, Cost: 0.00855354172372133\n",
      "Iteration 324, Cost: 0.008506173286205657\n",
      "Iteration 325, Cost: 0.008459156734860467\n",
      "Iteration 326, Cost: 0.008412491781410066\n",
      "Iteration 327, Cost: 0.008366177912535988\n",
      "Iteration 328, Cost: 0.008320214399536344\n",
      "Iteration 329, Cost: 0.008274600308561817\n",
      "Iteration 330, Cost: 0.0082293345112922\n",
      "Iteration 331, Cost: 0.008184415695925876\n",
      "Iteration 332, Cost: 0.008139842378363727\n",
      "Iteration 333, Cost: 0.008095612913478977\n",
      "Iteration 334, Cost: 0.008051725506374974\n",
      "Iteration 335, Cost: 0.008008178223543237\n",
      "Iteration 336, Cost: 0.007964969003844899\n",
      "Iteration 337, Cost: 0.007922095669248732\n",
      "Iteration 338, Cost: 0.007879555935269166\n",
      "Iteration 339, Cost: 0.007837347421056977\n",
      "Iteration 340, Cost: 0.007795467659104423\n",
      "Iteration 341, Cost: 0.0077539141045347105\n",
      "Iteration 342, Cost: 0.00771268414395347\n",
      "Iteration 343, Cost: 0.007671775103846642\n",
      "Iteration 344, Cost: 0.007631184258515449\n",
      "Iteration 345, Cost: 0.0075909088375446165\n",
      "Iteration 346, Cost: 0.0075509460328047045\n",
      "Iteration 347, Cost: 0.007511293004993699\n",
      "Iteration 348, Cost: 0.0074719468897263795\n",
      "Iteration 349, Cost: 0.007432904803183016\n",
      "Iteration 350, Cost: 0.0073941638473313575\n",
      "Iteration 351, Cost: 0.007355721114737704\n",
      "Iteration 352, Cost: 0.0073175736929844675\n",
      "Iteration 353, Cost: 0.007279718668712635\n",
      "Iteration 354, Cost: 0.0072421531313082675\n",
      "Iteration 355, Cost: 0.007204874176252725\n",
      "Iteration 356, Cost: 0.00716787890815632\n",
      "Iteration 357, Cost: 0.007131164443495224\n",
      "Iteration 358, Cost: 0.007094727913071059\n",
      "Iteration 359, Cost: 0.0070585664642123435\n",
      "Iteration 360, Cost: 0.007022677262736308\n",
      "Iteration 361, Cost: 0.006987057494689103\n",
      "Iteration 362, Cost: 0.006951704367881592\n",
      "Iteration 363, Cost: 0.006916615113237251\n",
      "Iteration 364, Cost: 0.006881786985967815\n",
      "Iteration 365, Cost: 0.006847217266591543\n",
      "Iteration 366, Cost: 0.006812903261808089\n",
      "Iteration 367, Cost: 0.006778842305243103\n",
      "Iteration 368, Cost: 0.00674503175807492\n",
      "Iteration 369, Cost: 0.006711469009554755\n",
      "Iteration 370, Cost: 0.006678151477431156\n",
      "Iteration 371, Cost: 0.0066450766082885296\n",
      "Iteration 372, Cost: 0.0066122418778089736\n",
      "Iteration 373, Cost: 0.0065796447909657885\n",
      "Iteration 374, Cost: 0.00654728288215647\n",
      "Iteration 375, Cost: 0.006515153715282249\n",
      "Iteration 376, Cost: 0.006483254883780706\n",
      "Iteration 377, Cost: 0.006451584010617371\n",
      "Iteration 378, Cost: 0.00642013874824172\n",
      "Iteration 379, Cost: 0.006388916778512403\n",
      "Iteration 380, Cost: 0.006357915812596196\n",
      "Iteration 381, Cost: 0.0063271335908445855\n",
      "Iteration 382, Cost: 0.006296567882651609\n",
      "Iteration 383, Cost: 0.006266216486296184\n",
      "Iteration 384, Cost: 0.006236077228771703\n",
      "Iteration 385, Cost: 0.006206147965605547\n",
      "Iteration 386, Cost: 0.006176426580670699\n",
      "Iteration 387, Cost: 0.0061469109859914875\n",
      "Iteration 388, Cost: 0.0061175991215452\n",
      "Iteration 389, Cost: 0.006088488955061075\n",
      "Iteration 390, Cost: 0.006059578481818024\n",
      "Iteration 391, Cost: 0.006030865724442166\n",
      "Iteration 392, Cost: 0.006002348732705206\n",
      "Iteration 393, Cost: 0.005974025583324411\n",
      "Iteration 394, Cost: 0.005945894379764886\n",
      "Iteration 395, Cost: 0.005917953252044674\n",
      "Iteration 396, Cost: 0.005890200356543135\n",
      "Iteration 397, Cost: 0.005862633875812894\n",
      "Iteration 398, Cost: 0.005835252018395621\n",
      "Iteration 399, Cost: 0.005808053018641755\n",
      "Iteration 400, Cost: 0.005781035136534259\n",
      "Iteration 401, Cost: 0.005754196657516401\n",
      "Iteration 402, Cost: 0.00572753589232348\n",
      "Iteration 403, Cost: 0.00570105117681838\n",
      "Iteration 404, Cost: 0.005674740871830777\n",
      "Iteration 405, Cost: 0.005648603362999779\n",
      "Iteration 406, Cost: 0.005622637060619708\n",
      "Iteration 407, Cost: 0.005596840399488761\n",
      "Iteration 408, Cost: 0.0055712118387601945\n",
      "Iteration 409, Cost: 0.0055457498617956305\n",
      "Iteration 410, Cost: 0.005520452976020165\n",
      "Iteration 411, Cost: 0.005495319712778776\n",
      "Iteration 412, Cost: 0.005470348627193683\n",
      "Iteration 413, Cost: 0.005445538298022076\n",
      "Iteration 414, Cost: 0.005420887327513912\n",
      "Iteration 415, Cost: 0.0053963943412690995\n",
      "Iteration 416, Cost: 0.0053720579880937374\n",
      "Iteration 417, Cost: 0.005347876939854803\n",
      "Iteration 418, Cost: 0.0053238498913328\n",
      "Iteration 419, Cost: 0.005299975560071893\n",
      "Iteration 420, Cost: 0.005276252686226926\n",
      "Iteration 421, Cost: 0.005252680032406884\n",
      "Iteration 422, Cost: 0.005229256383514231\n",
      "Iteration 423, Cost: 0.00520598054657964\n",
      "Iteration 424, Cost: 0.005182851350591591\n",
      "Iteration 425, Cost: 0.005159867646320339\n",
      "Iteration 426, Cost: 0.0051370283061357705\n",
      "Iteration 427, Cost: 0.0051143322238186755\n",
      "Iteration 428, Cost: 0.005091778314364962\n",
      "Iteration 429, Cost: 0.005069365513782367\n",
      "Iteration 430, Cost: 0.005047092778879266\n",
      "Iteration 431, Cost: 0.005024959087045142\n",
      "Iteration 432, Cost: 0.005002963436022338\n",
      "Iteration 433, Cost: 0.004981104843668754\n",
      "Converged!\n",
      "Iteration 1, Cost: 0.4205418899646758\n",
      "Iteration 2, Cost: 0.37318806943337823\n",
      "Iteration 3, Cost: 0.30975054181752765\n",
      "Iteration 4, Cost: 0.2432861274714037\n",
      "Iteration 5, Cost: 0.20485511775606613\n",
      "Iteration 6, Cost: 0.1906314305187264\n",
      "Iteration 7, Cost: 0.18374156856078838\n",
      "Iteration 8, Cost: 0.17885207461160416\n",
      "Iteration 9, Cost: 0.17460339734529454\n",
      "Iteration 10, Cost: 0.1706046682916333\n",
      "Iteration 11, Cost: 0.16673603091226405\n",
      "Iteration 12, Cost: 0.16296122176259487\n",
      "Iteration 13, Cost: 0.1592707327255836\n",
      "Iteration 14, Cost: 0.15566335482506452\n",
      "Iteration 15, Cost: 0.15214004733601272\n",
      "Iteration 16, Cost: 0.14870194982771065\n",
      "Iteration 17, Cost: 0.14534980079016566\n",
      "Iteration 18, Cost: 0.142083823307308\n",
      "Iteration 19, Cost: 0.13890375610697028\n",
      "Iteration 20, Cost: 0.1358089227491496\n",
      "Iteration 21, Cost: 0.13279830499644435\n",
      "Iteration 22, Cost: 0.1298706105764549\n",
      "Iteration 23, Cost: 0.12702433306214667\n",
      "Iteration 24, Cost: 0.12425780372885538\n",
      "Iteration 25, Cost: 0.12156923577651026\n",
      "Iteration 26, Cost: 0.11895676143044882\n",
      "Iteration 27, Cost: 0.11641846249123514\n",
      "Iteration 28, Cost: 0.11395239496107926\n",
      "Iteration 29, Cost: 0.11155660842690238\n",
      "Iteration 30, Cost: 0.10922916091218746\n",
      "Iteration 31, Cost: 0.10696812991206661\n",
      "Iteration 32, Cost: 0.10477162029755446\n",
      "Iteration 33, Cost: 0.10263776972051916\n",
      "Iteration 34, Cost: 0.10056475207912187\n",
      "Iteration 35, Cost: 0.0985507795228818\n",
      "Iteration 36, Cost: 0.09659410339496567\n",
      "Iteration 37, Cost: 0.09469301443248784\n",
      "Iteration 38, Cost: 0.0928458424770812\n",
      "Iteration 39, Cost: 0.09105095588934826\n",
      "Iteration 40, Cost: 0.08930676081215289\n",
      "Iteration 41, Cost: 0.0876117003882511\n",
      "Iteration 42, Cost: 0.08596425400621562\n",
      "Iteration 43, Cost: 0.0843629366235953\n",
      "Iteration 44, Cost: 0.08280629819650184\n",
      "Iteration 45, Cost: 0.0812929232292778\n",
      "Iteration 46, Cost: 0.07982143044577633\n",
      "Iteration 47, Cost: 0.07839047257448728\n",
      "Iteration 48, Cost: 0.07699873623286836\n",
      "Iteration 49, Cost: 0.07564494189147822\n",
      "Iteration 50, Cost: 0.074327843895615\n",
      "Iteration 51, Cost: 0.07304623052091312\n",
      "Iteration 52, Cost: 0.07179892403950933\n",
      "Iteration 53, Cost: 0.07058478077471168\n",
      "Iteration 54, Cost: 0.06940269112433167\n",
      "Iteration 55, Cost: 0.06825157953570757\n",
      "Iteration 56, Cost: 0.0671304044187028\n",
      "Iteration 57, Cost: 0.06603815798637122\n",
      "Iteration 58, Cost: 0.06497386601634131\n",
      "Iteration 59, Cost: 0.06393658752911684\n",
      "Iteration 60, Cost: 0.0629254143823035\n",
      "Iteration 61, Cost: 0.06193947078216587\n",
      "Iteration 62, Cost: 0.060977912715860096\n",
      "Iteration 63, Cost: 0.06003992730916623\n",
      "Iteration 64, Cost: 0.05912473211558959\n",
      "Iteration 65, Cost: 0.0582315743433573\n",
      "Iteration 66, Cost: 0.05735973002717535\n",
      "Iteration 67, Cost: 0.056508503151709744\n",
      "Iteration 68, Cost: 0.05567722473369595\n",
      "Iteration 69, Cost: 0.05486525186945087\n",
      "Iteration 70, Cost: 0.05407196675443498\n",
      "Iteration 71, Cost: 0.05329677568145664\n",
      "Iteration 72, Cost: 0.05253910802417564\n",
      "Iteration 73, Cost: 0.05179841521277275\n",
      "Iteration 74, Cost: 0.05107416970901457\n",
      "Iteration 75, Cost: 0.05036586398843289\n",
      "Iteration 76, Cost: 0.0496730095379105\n",
      "Iteration 77, Cost: 0.04899513587755504\n",
      "Iteration 78, Cost: 0.04833178961626478\n",
      "Iteration 79, Cost: 0.04768253355075738\n",
      "Iteration 80, Cost: 0.04704694581795538\n",
      "Iteration 81, Cost: 0.046424619110421984\n",
      "Iteration 82, Cost: 0.04581515996395816\n",
      "Iteration 83, Cost: 0.045218188125472514\n",
      "Iteration 84, Cost: 0.04463333600781217\n",
      "Iteration 85, Cost: 0.044060248236418714\n",
      "Iteration 86, Cost: 0.043498581290496656\n",
      "Iteration 87, Cost: 0.04294800323892328\n",
      "Iteration 88, Cost: 0.04240819356847031\n",
      "Iteration 89, Cost: 0.041878843099140695\n",
      "Iteration 90, Cost: 0.04135965397863892\n",
      "Iteration 91, Cost: 0.04085033974527554\n",
      "Iteration 92, Cost: 0.04035062544603853\n",
      "Iteration 93, Cost: 0.03986024779421979\n",
      "Iteration 94, Cost: 0.03937895534893946\n",
      "Iteration 95, Cost: 0.03890650869723872\n",
      "Iteration 96, Cost: 0.03844268061819776\n",
      "Iteration 97, Cost: 0.03798725620787046\n",
      "Iteration 98, Cost: 0.037540032943812716\n",
      "Iteration 99, Cost: 0.03710082066872262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100, Cost: 0.036669441474305696\n",
      "Iteration 101, Cost: 0.03624572946900756\n",
      "Iteration 102, Cost: 0.03582953041675674\n",
      "Iteration 103, Cost: 0.03542070123831709\n",
      "Iteration 104, Cost: 0.03501910937216316\n",
      "Iteration 105, Cost: 0.03462463199778382\n",
      "Iteration 106, Cost: 0.034237155130710045\n",
      "Iteration 107, Cost: 0.03385657260499785\n",
      "Iteration 108, Cost: 0.033482784964955874\n",
      "Iteration 109, Cost: 0.03311569829315065\n",
      "Iteration 110, Cost: 0.032755223005727335\n",
      "Iteration 111, Cost: 0.032401272648502025\n",
      "Iteration 112, Cost: 0.032053762727876016\n",
      "Iteration 113, Cost: 0.03171260960930735\n",
      "Iteration 114, Cost: 0.03137772951292893\n",
      "Iteration 115, Cost: 0.031049037631163956\n",
      "Iteration 116, Cost: 0.030726447387234134\n",
      "Iteration 117, Cost: 0.030409869846748604\n",
      "Iteration 118, Cost: 0.030099213287609474\n",
      "Iteration 119, Cost: 0.029794382926763167\n",
      "Iteration 120, Cost: 0.029495280796293395\n",
      "Iteration 121, Cost: 0.02920180575631792\n",
      "Iteration 122, Cost: 0.028913853628325423\n",
      "Iteration 123, Cost: 0.02863131743005426\n",
      "Iteration 124, Cost: 0.028354087691743\n",
      "Iteration 125, Cost: 0.028082052833454198\n",
      "Iteration 126, Cost: 0.02781509958400511\n",
      "Iteration 127, Cost: 0.027553113423613693\n",
      "Iteration 128, Cost: 0.027295979034458136\n",
      "Iteration 129, Cost: 0.027043580745739402\n",
      "Iteration 130, Cost: 0.026795802962337783\n",
      "Iteration 131, Cost: 0.02655253056861586\n",
      "Iteration 132, Cost: 0.026313649301223596\n",
      "Iteration 133, Cost: 0.026079046086830676\n",
      "Iteration 134, Cost: 0.025848609342499943\n",
      "Iteration 135, Cost: 0.025622229237907707\n",
      "Iteration 136, Cost: 0.025399797919814095\n",
      "Iteration 137, Cost: 0.02518120970010844\n",
      "Iteration 138, Cost: 0.02496636120942952\n",
      "Iteration 139, Cost: 0.024755151518820424\n",
      "Iteration 140, Cost: 0.02454748223215839\n",
      "Iteration 141, Cost: 0.024343257552235167\n",
      "Iteration 142, Cost: 0.024142384323384402\n",
      "Iteration 143, Cost: 0.023944772053487197\n",
      "Iteration 144, Cost: 0.023750332918059423\n",
      "Iteration 145, Cost: 0.023558981748954756\n",
      "Iteration 146, Cost: 0.0233706360100214\n",
      "Iteration 147, Cost: 0.023185215761841283\n",
      "Iteration 148, Cost: 0.023002643617468045\n",
      "Iteration 149, Cost: 0.022822844690870488\n",
      "Iteration 150, Cost: 0.022645746539588178\n",
      "Iteration 151, Cost: 0.022471279102917213\n",
      "Iteration 152, Cost: 0.022299374636769936\n",
      "Iteration 153, Cost: 0.022129967646193273\n",
      "Iteration 154, Cost: 0.021962994816386736\n",
      "Iteration 155, Cost: 0.02179839494293243\n",
      "Iteration 156, Cost: 0.021636108861835594\n",
      "Iteration 157, Cost: 0.021476079379873694\n",
      "Iteration 158, Cost: 0.021318251205664284\n",
      "Iteration 159, Cost: 0.02116257088178548\n",
      "Iteration 160, Cost: 0.021008986718217066\n",
      "Iteration 161, Cost: 0.020857448727313318\n",
      "Iteration 162, Cost: 0.020707908560470365\n",
      "Iteration 163, Cost: 0.02056031944660949\n",
      "Iteration 164, Cost: 0.020414636132563134\n",
      "Iteration 165, Cost: 0.020270814825420782\n",
      "Iteration 166, Cost: 0.02012881313686766\n",
      "Iteration 167, Cost: 0.01998859002952867\n",
      "Iteration 168, Cost: 0.019850105765313344\n",
      "Iteration 169, Cost: 0.0197133218557437\n",
      "Iteration 170, Cost: 0.019578201014236307\n",
      "Iteration 171, Cost: 0.019444707110300278\n",
      "Iteration 172, Cost: 0.019312805125606656\n",
      "Iteration 173, Cost: 0.019182461111879073\n",
      "Iteration 174, Cost: 0.019053642150551307\n",
      "Iteration 175, Cost: 0.018926316314134915\n",
      "Iteration 176, Cost: 0.01880045262923788\n",
      "Iteration 177, Cost: 0.01867602104117418\n",
      "Iteration 178, Cost: 0.018552992380103592\n",
      "Iteration 179, Cost: 0.018431338328641172\n",
      "Iteration 180, Cost: 0.01831103139087611\n",
      "Iteration 181, Cost: 0.018192044862740734\n",
      "Iteration 182, Cost: 0.01807435280367107\n",
      "Iteration 183, Cost: 0.017957930009502087\n",
      "Iteration 184, Cost: 0.017842751986541977\n",
      "Iteration 185, Cost: 0.01772879492677135\n",
      "Iteration 186, Cost: 0.017616035684114975\n",
      "Iteration 187, Cost: 0.01750445175173556\n",
      "Iteration 188, Cost: 0.01739402124030046\n",
      "Iteration 189, Cost: 0.01728472285717447\n",
      "Iteration 190, Cost: 0.017176535886493288\n",
      "Iteration 191, Cost: 0.01706944017007414\n",
      "Iteration 192, Cost: 0.016963416089122045\n",
      "Iteration 193, Cost: 0.01685844454669153\n",
      "Iteration 194, Cost: 0.016754506950865746\n",
      "Iteration 195, Cost: 0.016651585198616425\n",
      "Iteration 196, Cost: 0.016549661660309703\n",
      "Iteration 197, Cost: 0.01644871916482463\n",
      "Iteration 198, Cost: 0.01634874098525261\n",
      "Iteration 199, Cost: 0.016249710825147545\n",
      "Iteration 200, Cost: 0.01615161280529806\n",
      "Iteration 201, Cost: 0.016054431450994337\n",
      "Iteration 202, Cost: 0.015958151679763846\n",
      "Iteration 203, Cost: 0.015862758789551203\n",
      "Iteration 204, Cost: 0.01576823844731895\n",
      "Iteration 205, Cost: 0.01567457667804718\n",
      "Iteration 206, Cost: 0.015581759854111133\n",
      "Iteration 207, Cost: 0.015489774685017051\n",
      "Iteration 208, Cost: 0.015398608207477682\n",
      "Iteration 209, Cost: 0.015308247775809924\n",
      "Iteration 210, Cost: 0.015218681052638053\n",
      "Iteration 211, Cost: 0.01512989599988705\n",
      "Iteration 212, Cost: 0.015041880870051405\n",
      "Iteration 213, Cost: 0.014954624197725738\n",
      "Iteration 214, Cost: 0.014868114791384399\n",
      "Iteration 215, Cost: 0.014782341725398098\n",
      "Iteration 216, Cost: 0.014697294332276315\n",
      "Iteration 217, Cost: 0.01461296219512509\n",
      "Iteration 218, Cost: 0.014529335140310413\n",
      "Iteration 219, Cost: 0.014446403230318284\n",
      "Iteration 220, Cost: 0.014364156756802832\n",
      "Iteration 221, Cost: 0.014282586233815009\n",
      "Iteration 222, Cost: 0.014201682391204395\n",
      "Iteration 223, Cost: 0.01412143616818763\n",
      "Iteration 224, Cost: 0.01404183870707727\n",
      "Iteration 225, Cost: 0.013962881347165473\n",
      "Iteration 226, Cost: 0.013884555618757314\n",
      "Iteration 227, Cost: 0.013806853237348975\n",
      "Iteration 228, Cost: 0.013729766097946437\n",
      "Iteration 229, Cost: 0.01365328626952078\n",
      "Iteration 230, Cost: 0.013577405989596312\n",
      "Iteration 231, Cost: 0.013502117658968395\n",
      "Iteration 232, Cost: 0.01342741383654775\n",
      "Iteration 233, Cost: 0.013353287234328656\n",
      "Iteration 234, Cost: 0.013279730712478446\n",
      "Iteration 235, Cost: 0.013206737274546038\n",
      "Iteration 236, Cost: 0.013134300062787443\n",
      "Iteration 237, Cost: 0.013062412353606287\n",
      "Iteration 238, Cost: 0.012991067553107714\n",
      "Iteration 239, Cost: 0.012920259192763971\n",
      "Iteration 240, Cost: 0.012849980925190239\n",
      "Iteration 241, Cost: 0.012780226520029398\n",
      "Iteration 242, Cost: 0.012710989859944342\n",
      "Iteration 243, Cost: 0.01264226493671679\n",
      "Iteration 244, Cost: 0.012574045847451315\n",
      "Iteration 245, Cost: 0.01250632679088364\n",
      "Iteration 246, Cost: 0.012439102063792073\n",
      "Iteration 247, Cost: 0.012372366057511108\n",
      "Iteration 248, Cost: 0.012306113254546214\n",
      "Iteration 249, Cost: 0.012240338225288767\n",
      "Iteration 250, Cost: 0.012175035624830239\n",
      "Iteration 251, Cost: 0.01211020018987458\n",
      "Iteration 252, Cost: 0.012045826735747847\n",
      "Iteration 253, Cost: 0.011981910153504067\n",
      "Iteration 254, Cost: 0.011918445407126308\n",
      "Iteration 255, Cost: 0.011855427530821914\n",
      "Iteration 256, Cost: 0.011792851626410852\n",
      "Iteration 257, Cost: 0.011730712860806056\n",
      "Iteration 258, Cost: 0.011669006463584665\n",
      "Iteration 259, Cost: 0.011607727724649053\n",
      "Iteration 260, Cost: 0.011546871991976404\n",
      "Iteration 261, Cost: 0.01148643466945572\n",
      "Iteration 262, Cost: 0.011426411214811031\n",
      "Iteration 263, Cost: 0.01136679713760956\n",
      "Iteration 264, Cost: 0.011307587997353636\n",
      "Iteration 265, Cost: 0.011248779401655085\n",
      "Iteration 266, Cost: 0.011190367004490824\n",
      "Iteration 267, Cost: 0.011132346504538423\n",
      "Iteration 268, Cost: 0.011074713643590368\n",
      "Iteration 269, Cost: 0.011017464205045733\n",
      "Iteration 270, Cost: 0.010960594012478052\n",
      "Iteration 271, Cost: 0.010904098928278153\n",
      "Iteration 272, Cost: 0.01084797485237073\n",
      "Iteration 273, Cost: 0.010792217721003493\n",
      "Iteration 274, Cost: 0.01073682350560774\n",
      "Iteration 275, Cost: 0.010681788211729234\n",
      "Iteration 276, Cost: 0.010627107878028333\n",
      "Iteration 277, Cost: 0.01057277857534837\n",
      "Iteration 278, Cost: 0.010518796405851313\n",
      "Iteration 279, Cost: 0.010465157502219786\n",
      "Iteration 280, Cost: 0.0104118580269247\n",
      "Iteration 281, Cost: 0.010358894171557661\n",
      "Iteration 282, Cost: 0.01030626215622752\n",
      "Iteration 283, Cost: 0.010253958229020466\n",
      "Iteration 284, Cost: 0.010201978665523156\n",
      "Iteration 285, Cost: 0.010150319768408472\n",
      "Iteration 286, Cost: 0.010098977867083572\n",
      "Iteration 287, Cost: 0.010047949317400034\n",
      "Iteration 288, Cost: 0.009997230501425947\n",
      "Iteration 289, Cost: 0.00994681782727999\n",
      "Iteration 290, Cost: 0.009896707729027506\n",
      "Iteration 291, Cost: 0.009846896666638839\n",
      "Iteration 292, Cost: 0.009797381126010205\n",
      "Iteration 293, Cost: 0.009748157619047503\n",
      "Iteration 294, Cost: 0.009699222683813593\n",
      "Iteration 295, Cost: 0.009650572884739651\n",
      "Iteration 296, Cost: 0.00960220481290129\n",
      "Iteration 297, Cost: 0.0095541150863603\n",
      "Iteration 298, Cost: 0.009506300350572858\n",
      "Iteration 299, Cost: 0.009458757278865186\n",
      "Iteration 300, Cost: 0.009411482572977714\n",
      "Iteration 301, Cost: 0.00936447296367888\n",
      "Iteration 302, Cost: 0.00931772521144958\n",
      "Iteration 303, Cost: 0.009271236107239657\n",
      "Iteration 304, Cost: 0.009225002473297439\n",
      "Iteration 305, Cost: 0.00917902116407363\n",
      "Iteration 306, Cost: 0.009133289067200664\n",
      "Iteration 307, Cost: 0.009087803104548682\n",
      "Iteration 308, Cost: 0.009042560233359151\n",
      "Iteration 309, Cost: 0.00899755744745708\n",
      "Iteration 310, Cost: 0.008952791778542649\n",
      "Iteration 311, Cost: 0.008908260297562847\n",
      "Iteration 312, Cost: 0.00886396011616363\n",
      "Iteration 313, Cost: 0.008819888388222685\n",
      "Iteration 314, Cost: 0.008776042311462779\n",
      "Iteration 315, Cost: 0.00873241912914519\n",
      "Iteration 316, Cost: 0.008689016131842397\n",
      "Iteration 317, Cost: 0.008645830659288768\n",
      "Iteration 318, Cost: 0.008602860102307496\n",
      "Iteration 319, Cost: 0.008560101904811393\n",
      "Iteration 320, Cost: 0.008517553565874705\n",
      "Iteration 321, Cost: 0.0084752126418723\n",
      "Iteration 322, Cost: 0.008433076748681918\n",
      "Iteration 323, Cost: 0.008391143563944364\n",
      "Iteration 324, Cost: 0.008349410829375712\n",
      "Iteration 325, Cost: 0.008307876353124627\n",
      "Iteration 326, Cost: 0.00826653801216698\n",
      "Iteration 327, Cost: 0.008225393754728957\n",
      "Iteration 328, Cost: 0.008184441602728753\n",
      "Iteration 329, Cost: 0.008143679654225836\n",
      "Iteration 330, Cost: 0.00810310608586575\n",
      "Iteration 331, Cost: 0.008062719155307171\n",
      "Iteration 332, Cost: 0.008022517203616776\n",
      "Iteration 333, Cost: 0.007982498657616532\n",
      "Iteration 334, Cost: 0.00794266203216656\n",
      "Iteration 335, Cost: 0.00790300593236601\n",
      "Iteration 336, Cost: 0.007863529055653053\n",
      "Iteration 337, Cost: 0.007824230193784365\n",
      "Iteration 338, Cost: 0.007785108234673517\n",
      "Iteration 339, Cost: 0.007746162164067033\n",
      "Iteration 340, Cost: 0.007707391067036215\n",
      "Iteration 341, Cost: 0.007668794129262576\n",
      "Iteration 342, Cost: 0.007630370638094416\n",
      "Iteration 343, Cost: 0.007592119983352112\n",
      "Iteration 344, Cost: 0.007554041657860036\n",
      "Iteration 345, Cost: 0.007516135257683468\n",
      "Iteration 346, Cost: 0.007478400482049797\n",
      "Iteration 347, Cost: 0.007440837132934313\n",
      "Iteration 348, Cost: 0.007403445114292498\n",
      "Iteration 349, Cost: 0.007366224430922365\n",
      "Iteration 350, Cost: 0.007329175186942612\n",
      "Iteration 351, Cost: 0.007292297583874717\n",
      "Iteration 352, Cost: 0.007255591918319898\n",
      "Iteration 353, Cost: 0.0072190585792248785\n",
      "Iteration 354, Cost: 0.007182698044733788\n",
      "Iteration 355, Cost: 0.007146510878627065\n",
      "Iteration 356, Cost: 0.007110497726351961\n",
      "Iteration 357, Cost: 0.007074659310653338\n",
      "Iteration 358, Cost: 0.0070389964268173935\n",
      "Iteration 359, Cost: 0.007003509937545061\n",
      "Iteration 360, Cost: 0.0069682007674760444\n",
      "Iteration 361, Cost: 0.006933069897388354\n",
      "Iteration 362, Cost: 0.006898118358102169\n",
      "Iteration 363, Cost: 0.006863347224120567\n",
      "Iteration 364, Cost: 0.006828757607042892\n",
      "Iteration 365, Cost: 0.00679435064878981\n",
      "Iteration 366, Cost: 0.006760127514681573\n",
      "Iteration 367, Cost: 0.006726089386413228\n",
      "Iteration 368, Cost: 0.006692237454972252\n",
      "Iteration 369, Cost: 0.006658572913545103\n",
      "Iteration 370, Cost: 0.006625096950459791\n",
      "Iteration 371, Cost: 0.006591810742211508\n",
      "Iteration 372, Cost: 0.006558715446617692\n",
      "Iteration 373, Cost: 0.006525812196147669\n",
      "Iteration 374, Cost: 0.006493102091470199\n",
      "Iteration 375, Cost: 0.006460586195259878\n",
      "Iteration 376, Cost: 0.006428265526300556\n",
      "Iteration 377, Cost: 0.00639614105392051\n",
      "Iteration 378, Cost: 0.006364213692790587\n",
      "Iteration 379, Cost: 0.006332484298112327\n",
      "Iteration 380, Cost: 0.00630095366121899\n",
      "Iteration 381, Cost: 0.006269622505607847\n",
      "Iteration 382, Cost: 0.006238491483417671\n",
      "Iteration 383, Cost: 0.006207561172360677\n",
      "Iteration 384, Cost: 0.006176832073113819\n",
      "Iteration 385, Cost: 0.006146304607169796\n",
      "Iteration 386, Cost: 0.006115979115144057\n",
      "Iteration 387, Cost: 0.006085855855530031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 388, Cost: 0.006055935003891196\n",
      "Iteration 389, Cost: 0.006026216652475231\n",
      "Iteration 390, Cost: 0.005996700810232482\n",
      "Iteration 391, Cost: 0.005967387403218387\n",
      "Iteration 392, Cost: 0.00593827627535737\n",
      "Iteration 393, Cost: 0.005909367189543724\n",
      "Iteration 394, Cost: 0.005880659829053783\n",
      "Iteration 395, Cost: 0.005852153799242625\n",
      "Iteration 396, Cost: 0.005823848629497759\n",
      "Iteration 397, Cost: 0.005795743775422228\n",
      "Iteration 398, Cost: 0.0057678386212193355\n",
      "Iteration 399, Cost: 0.005740132482251735\n",
      "Iteration 400, Cost: 0.005712624607748157\n",
      "Iteration 401, Cost: 0.005685314183631866\n",
      "Iteration 402, Cost: 0.005658200335446071\n",
      "Iteration 403, Cost: 0.005631282131352669\n",
      "Iteration 404, Cost: 0.005604558585182057\n",
      "Iteration 405, Cost: 0.005578028659513232\n",
      "Iteration 406, Cost: 0.005551691268764928\n",
      "Iteration 407, Cost: 0.005525545282280093\n",
      "Iteration 408, Cost: 0.005499589527387633\n",
      "Iteration 409, Cost: 0.00547382279242694\n",
      "Iteration 410, Cost: 0.005448243829722327\n",
      "Iteration 411, Cost: 0.005422851358495965\n",
      "Iteration 412, Cost: 0.005397644067709537\n",
      "Iteration 413, Cost: 0.005372620618826072\n",
      "Iteration 414, Cost: 0.0053477796484849745\n",
      "Iteration 415, Cost: 0.005323119771084367\n",
      "Iteration 416, Cost: 0.005298639581266147\n",
      "Iteration 417, Cost: 0.005274337656300253\n",
      "Iteration 418, Cost: 0.0052502125583655645\n",
      "Iteration 419, Cost: 0.005226262836725921\n",
      "Iteration 420, Cost: 0.005202487029800408\n",
      "Iteration 421, Cost: 0.005178883667127927\n",
      "Iteration 422, Cost: 0.005155451271226655\n",
      "Iteration 423, Cost: 0.0051321883593496015\n",
      "Iteration 424, Cost: 0.0051090934451379875\n",
      "Iteration 425, Cost: 0.005086165040174521\n",
      "Iteration 426, Cost: 0.005063401655439133\n",
      "Iteration 427, Cost: 0.005040801802669912\n",
      "Iteration 428, Cost: 0.005018363995632312\n",
      "Iteration 429, Cost: 0.004996086751299852\n",
      "Converged!\n",
      "Iteration 1, Cost: 0.2709129551938508\n",
      "Iteration 2, Cost: 0.24559681735266659\n",
      "Iteration 3, Cost: 0.22803473957459056\n",
      "Iteration 4, Cost: 0.21440479044935404\n",
      "Iteration 5, Cost: 0.20218423834287222\n",
      "Iteration 6, Cost: 0.19093882824144318\n",
      "Iteration 7, Cost: 0.18123792907153147\n",
      "Iteration 8, Cost: 0.17350478875216194\n",
      "Iteration 9, Cost: 0.1675073511527103\n",
      "Iteration 10, Cost: 0.16267262636527813\n",
      "Iteration 11, Cost: 0.15851451185223872\n",
      "Iteration 12, Cost: 0.15474908281145738\n",
      "Iteration 13, Cost: 0.15123597421504245\n",
      "Iteration 14, Cost: 0.1479086261579098\n",
      "Iteration 15, Cost: 0.14473355830340642\n",
      "Iteration 16, Cost: 0.1416913500747105\n",
      "Iteration 17, Cost: 0.13876855103286037\n",
      "Iteration 18, Cost: 0.13595436910145056\n",
      "Iteration 19, Cost: 0.13323933308881558\n",
      "Iteration 20, Cost: 0.13061475842296372\n",
      "Iteration 21, Cost: 0.12807254070748006\n",
      "Iteration 22, Cost: 0.12560508299505163\n",
      "Iteration 23, Cost: 0.12320527553792382\n",
      "Iteration 24, Cost: 0.12086649329038578\n",
      "Iteration 25, Cost: 0.11858259658322344\n",
      "Iteration 26, Cost: 0.11634792953299043\n",
      "Iteration 27, Cost: 0.1141573149367839\n",
      "Iteration 28, Cost: 0.11200604619077559\n",
      "Iteration 29, Cost: 0.109889877356003\n",
      "Iteration 30, Cost: 0.10780501249485724\n",
      "Iteration 31, Cost: 0.10574809514893453\n",
      "Iteration 32, Cost: 0.10371619850047147\n",
      "Iteration 33, Cost: 0.10170681644266256\n",
      "Iteration 34, Cost: 0.09971785551410545\n",
      "Iteration 35, Cost: 0.0977476274360303\n",
      "Iteration 36, Cost: 0.09579484181996248\n",
      "Iteration 37, Cost: 0.09385859847468214\n",
      "Iteration 38, Cost: 0.09193837862189276\n",
      "Iteration 39, Cost: 0.0900340342219066\n",
      "Iteration 40, Cost: 0.08814577451441061\n",
      "Iteration 41, Cost: 0.08627414880683286\n",
      "Iteration 42, Cost: 0.08442002451855153\n",
      "Iteration 43, Cost: 0.08258455954909745\n",
      "Iteration 44, Cost: 0.08076916822419149\n",
      "Iteration 45, Cost: 0.0789754804211719\n",
      "Iteration 46, Cost: 0.07720529400034919\n",
      "Iteration 47, Cost: 0.07546052134868625\n",
      "Iteration 48, Cost: 0.07374313160581304\n",
      "Iteration 49, Cost: 0.0720550908720586\n",
      "Iteration 50, Cost: 0.07039830325005879\n",
      "Iteration 51, Cost: 0.06877455581204091\n",
      "Iteration 52, Cost: 0.0671854704325728\n",
      "Iteration 53, Cost: 0.06563246488241925\n",
      "Iteration 54, Cost: 0.0641167247319919\n",
      "Iteration 55, Cost: 0.06263918661562985\n",
      "Iteration 56, Cost: 0.06120053243410931\n",
      "Iteration 57, Cost: 0.059801193270570784\n",
      "Iteration 58, Cost: 0.05844136125568515\n",
      "Iteration 59, Cost: 0.05712100736606462\n",
      "Iteration 60, Cost: 0.05583990314441137\n",
      "Iteration 61, Cost: 0.05459764452526701\n",
      "Iteration 62, Cost: 0.05339367626005254\n",
      "Iteration 63, Cost: 0.052227315789693675\n",
      "Iteration 64, Cost: 0.051097775759425396\n",
      "Iteration 65, Cost: 0.05000418467460945\n",
      "Iteration 66, Cost: 0.04894560544216152\n",
      "Iteration 67, Cost: 0.047921051726123376\n",
      "Iteration 68, Cost: 0.046929502173081145\n",
      "Iteration 69, Cost: 0.0459699126429395\n",
      "Iteration 70, Cost: 0.045041226623823004\n",
      "Iteration 71, Cost: 0.04414238402689523\n",
      "Iteration 72, Cost: 0.04327232855645088\n",
      "Iteration 73, Cost: 0.04243001383964408\n",
      "Iteration 74, Cost: 0.0416144084837044\n",
      "Iteration 75, Cost: 0.04082450020988626\n",
      "Iteration 76, Cost: 0.040059299194843065\n",
      "Iteration 77, Cost: 0.039317840732818514\n",
      "Iteration 78, Cost: 0.038599187316560056\n",
      "Iteration 79, Cost: 0.03790243022132662\n",
      "Iteration 80, Cost: 0.037226690664691184\n",
      "Iteration 81, Cost: 0.03657112060481797\n",
      "Iteration 82, Cost: 0.03593490323127517\n",
      "Iteration 83, Cost: 0.03531725319498565\n",
      "Iteration 84, Cost: 0.03471741661741023\n",
      "Iteration 85, Cost: 0.03413467091333152\n",
      "Iteration 86, Cost: 0.03356832445653516\n",
      "Iteration 87, Cost: 0.03301771611317932\n",
      "Iteration 88, Cost: 0.03248221466364799\n",
      "Iteration 89, Cost: 0.031961218130166155\n",
      "Iteration 90, Cost: 0.03145415302440412\n",
      "Iteration 91, Cost: 0.030960473526712714\n",
      "Iteration 92, Cost: 0.030479660606514643\n",
      "Iteration 93, Cost: 0.030011221091735173\n",
      "Iteration 94, Cost: 0.029554686693982084\n",
      "Iteration 95, Cost: 0.02910961299546754\n",
      "Iteration 96, Cost: 0.028675578403371316\n",
      "Iteration 97, Cost: 0.02825218307742561\n",
      "Iteration 98, Cost: 0.027839047836888186\n",
      "Iteration 99, Cost: 0.027435813053673028\n",
      "Iteration 100, Cost: 0.027042137539125558\n",
      "Iteration 101, Cost: 0.02665769743264988\n",
      "Iteration 102, Cost: 0.02628218510100894\n",
      "Iteration 103, Cost: 0.02591530805752141\n",
      "Iteration 104, Cost: 0.02555678791048933\n",
      "Iteration 105, Cost: 0.025206359349949928\n",
      "Iteration 106, Cost: 0.024863769181229765\n",
      "Iteration 107, Cost: 0.02452877541279862\n",
      "Iteration 108, Cost: 0.02420114640461842\n",
      "Iteration 109, Cost: 0.02388066008162983\n",
      "Iteration 110, Cost: 0.0235671032153081\n",
      "Iteration 111, Cost: 0.023260270774452117\n",
      "Iteration 112, Cost: 0.022959965344648607\n",
      "Iteration 113, Cost: 0.022665996614269872\n",
      "Iteration 114, Cost: 0.022378180923492332\n",
      "Iteration 115, Cost: 0.022096340871719796\n",
      "Iteration 116, Cost: 0.021820304977986312\n",
      "Iteration 117, Cost: 0.02154990738840611\n",
      "Iteration 118, Cost: 0.021284987624516447\n",
      "Iteration 119, Cost: 0.021025390366392668\n",
      "Iteration 120, Cost: 0.020770965264661108\n",
      "Iteration 121, Cost: 0.020521566775947502\n",
      "Iteration 122, Cost: 0.02027705401682724\n",
      "Iteration 123, Cost: 0.0200372906319433\n",
      "Iteration 124, Cost: 0.019802144672587552\n",
      "Iteration 125, Cost: 0.019571488482667303\n",
      "Iteration 126, Cost: 0.019345198589575556\n",
      "Iteration 127, Cost: 0.01912315559803194\n",
      "Iteration 128, Cost: 0.018905244085449677\n",
      "Iteration 129, Cost: 0.018691352497806245\n",
      "Iteration 130, Cost: 0.01848137304535078\n",
      "Iteration 131, Cost: 0.01827520159777115\n",
      "Iteration 132, Cost: 0.018072737578673777\n",
      "Iteration 133, Cost: 0.01787388385940501\n",
      "Iteration 134, Cost: 0.017678546652371875\n",
      "Iteration 135, Cost: 0.017486635404109156\n",
      "Iteration 136, Cost: 0.01729806268839617\n",
      "Iteration 137, Cost: 0.017112744099756813\n",
      "Iteration 138, Cost: 0.01693059814768604\n",
      "Iteration 139, Cost: 0.016751546151940078\n",
      "Iteration 140, Cost: 0.016575512139210862\n",
      "Iteration 141, Cost: 0.01640242274148014\n",
      "Iteration 142, Cost: 0.016232207096319312\n",
      "Iteration 143, Cost: 0.016064796749368768\n",
      "Iteration 144, Cost: 0.015900125559197103\n",
      "Iteration 145, Cost: 0.015738129604707975\n",
      "Iteration 146, Cost: 0.015578747095230319\n",
      "Iteration 147, Cost: 0.015421918283398048\n",
      "Iteration 148, Cost: 0.015267585380897635\n",
      "Iteration 149, Cost: 0.015115692477136589\n",
      "Iteration 150, Cost: 0.01496618546086339\n",
      "Iteration 151, Cost: 0.014819011944748987\n",
      "Iteration 152, Cost: 0.014674121192922208\n",
      "Iteration 153, Cost: 0.01453146405143598\n",
      "Iteration 154, Cost: 0.014390992881627701\n",
      "Iteration 155, Cost: 0.014252661496325751\n",
      "Iteration 156, Cost: 0.014116425098844396\n",
      "Iteration 157, Cost: 0.013982240224701382\n",
      "Iteration 158, Cost: 0.01385006468598583\n",
      "Iteration 159, Cost: 0.01371985751829882\n",
      "Iteration 160, Cost: 0.013591578930184882\n",
      "Iteration 161, Cost: 0.01346519025496954\n",
      "Iteration 162, Cost: 0.013340653904915812\n",
      "Iteration 163, Cost: 0.013217933327611191\n",
      "Iteration 164, Cost: 0.013096992964495967\n",
      "Iteration 165, Cost: 0.012977798211443525\n",
      "Iteration 166, Cost: 0.012860315381303876\n",
      "Iteration 167, Cost: 0.012744511668322417\n",
      "Iteration 168, Cost: 0.01263035511434723\n",
      "Iteration 169, Cost: 0.012517814576739979\n",
      "Iteration 170, Cost: 0.012406859697907174\n",
      "Iteration 171, Cost: 0.012297460876370922\n",
      "Iteration 172, Cost: 0.01218958923930042\n",
      "Iteration 173, Cost: 0.012083216616428177\n",
      "Iteration 174, Cost: 0.011978315515277347\n",
      "Iteration 175, Cost: 0.01187485909762945\n",
      "Iteration 176, Cost: 0.011772821157164442\n",
      "Iteration 177, Cost: 0.011672176098207952\n",
      "Iteration 178, Cost: 0.011572898915523317\n",
      "Iteration 179, Cost: 0.011474965175088959\n",
      "Iteration 180, Cost: 0.011378350995804365\n",
      "Iteration 181, Cost: 0.011283033032070797\n",
      "Iteration 182, Cost: 0.011188988457195594\n",
      "Iteration 183, Cost: 0.011096194947571551\n",
      "Iteration 184, Cost: 0.011004630667585582\n",
      "Iteration 185, Cost: 0.01091427425521329\n",
      "Iteration 186, Cost: 0.01082510480825867\n",
      "Iteration 187, Cost: 0.010737101871200366\n",
      "Iteration 188, Cost: 0.010650245422608395\n",
      "Iteration 189, Cost: 0.01056451586309726\n",
      "Iteration 190, Cost: 0.010479894003783541\n",
      "Iteration 191, Cost: 0.010396361055218094\n",
      "Iteration 192, Cost: 0.010313898616764744\n",
      "Iteration 193, Cost: 0.010232488666399406\n",
      "Iteration 194, Cost: 0.010152113550904921\n",
      "Iteration 195, Cost: 0.010072755976438927\n",
      "Iteration 196, Cost: 0.009994398999453194\n",
      "Iteration 197, Cost: 0.009917026017944648\n",
      "Iteration 198, Cost: 0.0098406207630194\n",
      "Iteration 199, Cost: 0.009765167290752467\n",
      "Iteration 200, Cost: 0.009690649974327063\n",
      "Iteration 201, Cost: 0.009617053496438419\n",
      "Iteration 202, Cost: 0.009544362841948146\n",
      "Iteration 203, Cost: 0.009472563290776073\n",
      "Iteration 204, Cost: 0.009401640411017492\n",
      "Iteration 205, Cost: 0.009331580052274456\n",
      "Iteration 206, Cost: 0.009262368339190637\n",
      "Iteration 207, Cost: 0.009193991665179986\n",
      "Iteration 208, Cost: 0.00912643668633999\n",
      "Iteration 209, Cost: 0.00905969031554114\n",
      "Iteration 210, Cost: 0.008993739716684572\n",
      "Iteration 211, Cost: 0.008928572299120545\n",
      "Iteration 212, Cost: 0.008864175712220888\n",
      "Iteration 213, Cost: 0.008800537840098871\n",
      "Iteration 214, Cost: 0.008737646796470582\n",
      "Iteration 215, Cost: 0.008675490919652086\n",
      "Iteration 216, Cost: 0.008614058767687142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 217, Cost: 0.0085533391136005\n",
      "Iteration 218, Cost: 0.008493320940772122\n",
      "Iteration 219, Cost: 0.008433993438428003\n",
      "Iteration 220, Cost: 0.008375345997243426\n",
      "Iteration 221, Cost: 0.00831736820505482\n",
      "Iteration 222, Cost: 0.008260049842676587\n",
      "Iteration 223, Cost: 0.008203380879819366\n",
      "Iteration 224, Cost: 0.008147351471106606\n",
      "Iteration 225, Cost: 0.008091951952186238\n",
      "Iteration 226, Cost: 0.008037172835934574\n",
      "Iteration 227, Cost: 0.007983004808749682\n",
      "Iteration 228, Cost: 0.007929438726931488\n",
      "Iteration 229, Cost: 0.007876465613146238\n",
      "Iteration 230, Cost: 0.007824076652972745\n",
      "Iteration 231, Cost: 0.007772263191528295\n",
      "Iteration 232, Cost: 0.007721016730171867\n",
      "Iteration 233, Cost: 0.007670328923282696\n",
      "Iteration 234, Cost: 0.00762019157511203\n",
      "Iteration 235, Cost: 0.0075705966367063055\n",
      "Iteration 236, Cost: 0.007521536202899719\n",
      "Iteration 237, Cost: 0.007473002509374522\n",
      "Iteration 238, Cost: 0.0074249879297872586\n",
      "Iteration 239, Cost: 0.007377484972959308\n",
      "Iteration 240, Cost: 0.0073304862801301055\n",
      "Iteration 241, Cost: 0.007283984622271517\n",
      "Iteration 242, Cost: 0.007237972897461838\n",
      "Iteration 243, Cost: 0.007192444128317984\n",
      "Iteration 244, Cost: 0.0071473914594844794\n",
      "Iteration 245, Cost: 0.007102808155177821\n",
      "Iteration 246, Cost: 0.007058687596784982\n",
      "Iteration 247, Cost: 0.007015023280514691\n",
      "Iteration 248, Cost: 0.006971808815100285\n",
      "Iteration 249, Cost: 0.006929037919552906\n",
      "Iteration 250, Cost: 0.0068867044209638805\n",
      "Iteration 251, Cost: 0.006844802252355091\n",
      "Iteration 252, Cost: 0.006803325450576285\n",
      "Iteration 253, Cost: 0.006762268154248171\n",
      "Iteration 254, Cost: 0.006721624601750297\n",
      "Iteration 255, Cost: 0.006681389129252646\n",
      "Iteration 256, Cost: 0.006641556168789936\n",
      "Iteration 257, Cost: 0.006602120246377684\n",
      "Iteration 258, Cost: 0.006563075980169041\n",
      "Iteration 259, Cost: 0.006524418078651465\n",
      "Iteration 260, Cost: 0.0064861413388823536\n",
      "Iteration 261, Cost: 0.006448240644762711\n",
      "Iteration 262, Cost: 0.006410710965348005\n",
      "Iteration 263, Cost: 0.0063735473531953595\n",
      "Iteration 264, Cost: 0.006336744942746264\n",
      "Iteration 265, Cost: 0.006300298948743979\n",
      "Iteration 266, Cost: 0.006264204664684855\n",
      "Iteration 267, Cost: 0.006228457461302814\n",
      "Iteration 268, Cost: 0.006193052785086215\n",
      "Iteration 269, Cost: 0.006157986156826378\n",
      "Iteration 270, Cost: 0.006123253170197079\n",
      "Iteration 271, Cost: 0.0060888494903642496\n",
      "Iteration 272, Cost: 0.006054770852625274\n",
      "Iteration 273, Cost: 0.006021013061077166\n",
      "Iteration 274, Cost: 0.005987571987312981\n",
      "Iteration 275, Cost: 0.005954443569145845\n",
      "Iteration 276, Cost: 0.005921623809359952\n",
      "Iteration 277, Cost: 0.005889108774487947\n",
      "Iteration 278, Cost: 0.0058568945936140655\n",
      "Iteration 279, Cost: 0.005824977457202496\n",
      "Iteration 280, Cost: 0.005793353615950345\n",
      "Iteration 281, Cost: 0.005762019379664706\n",
      "Iteration 282, Cost: 0.0057309711161632375\n",
      "Iteration 283, Cost: 0.005700205250197753\n",
      "Iteration 284, Cost: 0.0056697182624002975\n",
      "Iteration 285, Cost: 0.00563950668825122\n",
      "Iteration 286, Cost: 0.005609567117068716\n",
      "Iteration 287, Cost: 0.005579896191019377\n",
      "Iteration 288, Cost: 0.005550490604149294\n",
      "Iteration 289, Cost: 0.005521347101435215\n",
      "Iteration 290, Cost: 0.005492462477855347\n",
      "Iteration 291, Cost: 0.005463833577479336\n",
      "Iteration 292, Cost: 0.005435457292576999\n",
      "Iteration 293, Cost: 0.00540733056274542\n",
      "Iteration 294, Cost: 0.005379450374053932\n",
      "Iteration 295, Cost: 0.005351813758206684\n",
      "Iteration 296, Cost: 0.005324417791722295\n",
      "Iteration 297, Cost: 0.00529725959513031\n",
      "Iteration 298, Cost: 0.005270336332184004\n",
      "Iteration 299, Cost: 0.005243645209089238\n",
      "Iteration 300, Cost: 0.005217183473748951\n",
      "Iteration 301, Cost: 0.005190948415023001\n",
      "Iteration 302, Cost: 0.00516493736200295\n",
      "Iteration 303, Cost: 0.005139147683301533\n",
      "Iteration 304, Cost: 0.00511357678635641\n",
      "Iteration 305, Cost: 0.005088222116747968\n",
      "Iteration 306, Cost: 0.005063081157530771\n",
      "Iteration 307, Cost: 0.00503815142857846\n",
      "Iteration 308, Cost: 0.005013430485941702\n",
      "Iteration 309, Cost: 0.004988915921218993\n",
      "Converged!\n",
      "Iteration 1, Cost: 0.2341835632383078\n",
      "Iteration 2, Cost: 0.2069120248341481\n",
      "Iteration 3, Cost: 0.18935830499248582\n",
      "Iteration 4, Cost: 0.17810796660881034\n",
      "Iteration 5, Cost: 0.17033500425985723\n",
      "Iteration 6, Cost: 0.16437002608527918\n",
      "Iteration 7, Cost: 0.15934991327747322\n",
      "Iteration 8, Cost: 0.1548466782566278\n",
      "Iteration 9, Cost: 0.15064946466700874\n",
      "Iteration 10, Cost: 0.14665405138697832\n",
      "Iteration 11, Cost: 0.14280863445409173\n",
      "Iteration 12, Cost: 0.13908718971115766\n",
      "Iteration 13, Cost: 0.13547631581130093\n",
      "Iteration 14, Cost: 0.13196873763813335\n",
      "Iteration 15, Cost: 0.12856012118070176\n",
      "Iteration 16, Cost: 0.12524752969588335\n",
      "Iteration 17, Cost: 0.12202868424669266\n",
      "Iteration 18, Cost: 0.11890161233849271\n",
      "Iteration 19, Cost: 0.11586448105614673\n",
      "Iteration 20, Cost: 0.1129155173095522\n",
      "Iteration 21, Cost: 0.1100529696255551\n",
      "Iteration 22, Cost: 0.10727509046377295\n",
      "Iteration 23, Cost: 0.10458012925297624\n",
      "Iteration 24, Cost: 0.10196633130068898\n",
      "Iteration 25, Cost: 0.0994319398808279\n",
      "Iteration 26, Cost: 0.09697519978175352\n",
      "Iteration 27, Cost: 0.09459436112500197\n",
      "Iteration 28, Cost: 0.09228768263724924\n",
      "Iteration 29, Cost: 0.09005343387409821\n",
      "Iteration 30, Cost: 0.08788989617335027\n",
      "Iteration 31, Cost: 0.08579536235015715\n",
      "Iteration 32, Cost: 0.08376813532598508\n",
      "Iteration 33, Cost: 0.0818065260018915\n",
      "Iteration 34, Cost: 0.07990885074533001\n",
      "Iteration 35, Cost: 0.07807342886553696\n",
      "Iteration 36, Cost: 0.07629858041637164\n",
      "Iteration 37, Cost: 0.0745826245999552\n",
      "Iteration 38, Cost: 0.07292387896236076\n",
      "Iteration 39, Cost: 0.07132065948546962\n",
      "Iteration 40, Cost: 0.06977128159636527\n",
      "Iteration 41, Cost: 0.06827406204415584\n",
      "Iteration 42, Cost: 0.06682732153813287\n",
      "Iteration 43, Cost: 0.06542938800248076\n",
      "Iteration 44, Cost: 0.06407860028110617\n",
      "Iteration 45, Cost: 0.0627733121198086\n",
      "Iteration 46, Cost: 0.06151189625924649\n",
      "Iteration 47, Cost: 0.06029274848780346\n",
      "Iteration 48, Cost: 0.059114291525331826\n",
      "Iteration 49, Cost: 0.05797497863393055\n",
      "Iteration 50, Cost: 0.05687329687796133\n",
      "Iteration 51, Cost: 0.05580776998053896\n",
      "Iteration 52, Cost: 0.054776960746435344\n",
      "Iteration 53, Cost: 0.05377947304091506\n",
      "Iteration 54, Cost: 0.05281395333010396\n",
      "Iteration 55, Cost: 0.05187909180104943\n",
      "Iteration 56, Cost: 0.05097362308886194\n",
      "Iteration 57, Cost: 0.050096326644585264\n",
      "Iteration 58, Cost: 0.04924602678115896\n",
      "Iteration 59, Cost: 0.0484215924364682\n",
      "Iteration 60, Cost: 0.047621936692466275\n",
      "Iteration 61, Cost: 0.046846016088113236\n",
      "Iteration 62, Cost: 0.0460928297617528\n",
      "Iteration 63, Cost: 0.04536141845585115\n",
      "Iteration 64, Cost: 0.04465086341398671\n",
      "Iteration 65, Cost: 0.04396028519680158\n",
      "Iteration 66, Cost: 0.043288842440444425\n",
      "Iteration 67, Cost: 0.04263573057795912\n",
      "Iteration 68, Cost: 0.04200018054116947\n",
      "Iteration 69, Cost: 0.04138145745792727\n",
      "Iteration 70, Cost: 0.040778859357148194\n",
      "Iteration 71, Cost: 0.04019171589186815\n",
      "Iteration 72, Cost: 0.03961938708860808\n",
      "Iteration 73, Cost: 0.039061262129624214\n",
      "Iteration 74, Cost: 0.038516758173129335\n",
      "Iteration 75, Cost: 0.03798531921527696\n",
      "Iteration 76, Cost: 0.037466414996583236\n",
      "Iteration 77, Cost: 0.03695953995449974\n",
      "Iteration 78, Cost: 0.03646421222302254\n",
      "Iteration 79, Cost: 0.03597997267950946\n",
      "Iteration 80, Cost: 0.0355063840382602\n",
      "Iteration 81, Cost: 0.035043029989877565\n",
      "Iteration 82, Cost: 0.03458951438495846\n",
      "Iteration 83, Cost: 0.03414546046025018\n",
      "Iteration 84, Cost: 0.03371051010504184\n",
      "Iteration 85, Cost: 0.033284323165238064\n",
      "Iteration 86, Cost: 0.032866576782277605\n",
      "Iteration 87, Cost: 0.03245696476381454\n",
      "Iteration 88, Cost: 0.03205519698287515\n",
      "Iteration 89, Cost: 0.03166099880204374\n",
      "Iteration 90, Cost: 0.03127411051912175\n",
      "Iteration 91, Cost: 0.030894286830653563\n",
      "Iteration 92, Cost: 0.03052129630972678\n",
      "Iteration 93, Cost: 0.030154920894543285\n",
      "Iteration 94, Cost: 0.029794955384424972\n",
      "Iteration 95, Cost: 0.029441206940170116\n",
      "Iteration 96, Cost: 0.02909349458601362\n",
      "Iteration 97, Cost: 0.02875164871086357\n",
      "Iteration 98, Cost: 0.028415510566979368\n",
      "Iteration 99, Cost: 0.028084931764809983\n",
      "Iteration 100, Cost: 0.02775977376330438\n",
      "Iteration 101, Cost: 0.027439907355616425\n",
      "Iteration 102, Cost: 0.027125212150724384\n",
      "Iteration 103, Cost: 0.02681557605204013\n",
      "Iteration 104, Cost: 0.026510894734564628\n",
      "Iteration 105, Cost: 0.026211071122526228\n",
      "Iteration 106, Cost: 0.025916014869693756\n",
      "Iteration 107, Cost: 0.025625641844674484\n",
      "Iteration 108, Cost: 0.02533987362348259\n",
      "Iteration 109, Cost: 0.02505863699150633\n",
      "Iteration 110, Cost: 0.024781863456732084\n",
      "Iteration 111, Cost: 0.024509488775732273\n",
      "Iteration 112, Cost: 0.02424145249353303\n",
      "Iteration 113, Cost: 0.02397769749809212\n",
      "Iteration 114, Cost: 0.02371816958978408\n",
      "Iteration 115, Cost: 0.023462817066050405\n",
      "Iteration 116, Cost: 0.023211590321258954\n",
      "Iteration 117, Cost: 0.0229644414618485\n",
      "Iteration 118, Cost: 0.02272132393701177\n",
      "Iteration 119, Cost: 0.022482192185478325\n",
      "Iteration 120, Cost: 0.02224700129936345\n",
      "Iteration 121, Cost: 0.022015706706504504\n",
      "Iteration 122, Cost: 0.021788263873155956\n",
      "Iteration 123, Cost: 0.02156462802929974\n",
      "Iteration 124, Cost: 0.021344753919094978\n",
      "Iteration 125, Cost: 0.02112859557909488\n",
      "Iteration 126, Cost: 0.02091610614677082\n",
      "Iteration 127, Cost: 0.02070723770159319\n",
      "Iteration 128, Cost: 0.020501941140435326\n",
      "Iteration 129, Cost: 0.020300166088418045\n",
      "Iteration 130, Cost: 0.02010186084554114\n",
      "Iteration 131, Cost: 0.019906972368607043\n",
      "Iteration 132, Cost: 0.019715446287088326\n",
      "Iteration 133, Cost: 0.019527226950780175\n",
      "Iteration 134, Cost: 0.019342257506360643\n",
      "Iteration 135, Cost: 0.01916047999939476\n",
      "Iteration 136, Cost: 0.018981835497888205\n",
      "Iteration 137, Cost: 0.018806264233235847\n",
      "Iteration 138, Cost: 0.018633705754318747\n",
      "Iteration 139, Cost: 0.018464099090568738\n",
      "Iteration 140, Cost: 0.018297382920022077\n",
      "Iteration 141, Cost: 0.018133495738697446\n",
      "Iteration 142, Cost: 0.017972376028029497\n",
      "Iteration 143, Cost: 0.017813962417538444\n",
      "Iteration 144, Cost: 0.017658193840392324\n",
      "Iteration 145, Cost: 0.01750500967999611\n",
      "Iteration 146, Cost: 0.017354349906203416\n",
      "Iteration 147, Cost: 0.0172061552001745\n",
      "Iteration 148, Cost: 0.017060367067290385\n",
      "Iteration 149, Cost: 0.016916927937868812\n",
      "Iteration 150, Cost: 0.016775781255712018\n",
      "Iteration 151, Cost: 0.016636871554748113\n",
      "Iteration 152, Cost: 0.016500144524210225\n",
      "Iteration 153, Cost: 0.016365547062933583\n",
      "Iteration 154, Cost: 0.016233027323446437\n",
      "Iteration 155, Cost: 0.016102534746590378\n",
      "Iteration 156, Cost: 0.015974020087435755\n",
      "Iteration 157, Cost: 0.01584743543326357\n",
      "Iteration 158, Cost: 0.0157227342143712\n",
      "Iteration 159, Cost: 0.0155998712084311\n",
      "Iteration 160, Cost: 0.01547880253909213\n",
      "Iteration 161, Cost: 0.015359485669466953\n",
      "Iteration 162, Cost: 0.015241879391098057\n",
      "Iteration 163, Cost: 0.015125943808942499\n",
      "Iteration 164, Cost: 0.015011640322862951\n",
      "Iteration 165, Cost: 0.014898931606061665\n",
      "Iteration 166, Cost: 0.014787781580845604\n",
      "Iteration 167, Cost: 0.014678155392066537\n",
      "Iteration 168, Cost: 0.01457001937853887\n",
      "Iteration 169, Cost: 0.014463341042702105\n",
      "Iteration 170, Cost: 0.01435808901876276\n",
      "Iteration 171, Cost: 0.014254233039523789\n",
      "Iteration 172, Cost: 0.014151743902086648\n",
      "Iteration 173, Cost: 0.014050593432592774\n",
      "Iteration 174, Cost: 0.013950754450156826\n",
      "Iteration 175, Cost: 0.01385220073013297\n",
      "Iteration 176, Cost: 0.013754906966848076\n",
      "Iteration 177, Cost: 0.01365884873593039\n",
      "Iteration 178, Cost: 0.013564002456359928\n",
      "Iteration 179, Cost: 0.013470345352365822\n",
      "Iteration 180, Cost: 0.013377855415296458\n",
      "Iteration 181, Cost: 0.013286511365589533\n",
      "Iteration 182, Cost: 0.013196292614971036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 183, Cost: 0.013107179229013808\n",
      "Iteration 184, Cost: 0.013019151890187595\n",
      "Iteration 185, Cost: 0.012932191861533052\n",
      "Iteration 186, Cost: 0.012846280951091357\n",
      "Iteration 187, Cost: 0.012761401477219133\n",
      "Iteration 188, Cost: 0.012677536234914667\n",
      "Iteration 189, Cost: 0.012594668463276032\n",
      "Iteration 190, Cost: 0.012512781814204704\n",
      "Iteration 191, Cost: 0.012431860322459253\n",
      "Iteration 192, Cost: 0.012351888377153295\n",
      "Iteration 193, Cost: 0.012272850694779725\n",
      "Iteration 194, Cost: 0.012194732293829896\n",
      "Iteration 195, Cost: 0.012117518471062053\n",
      "Iteration 196, Cost: 0.012041194779457875\n",
      "Iteration 197, Cost: 0.011965747007890448\n",
      "Iteration 198, Cost: 0.011891161162510941\n",
      "Iteration 199, Cost: 0.0118174234498456\n",
      "Iteration 200, Cost: 0.011744520261579306\n",
      "Iteration 201, Cost: 0.011672438160987522\n",
      "Iteration 202, Cost: 0.011601163870964857\n",
      "Iteration 203, Cost: 0.011530684263586327\n",
      "Iteration 204, Cost: 0.011460986351126464\n",
      "Iteration 205, Cost: 0.011392057278452317\n",
      "Iteration 206, Cost: 0.011323884316698675\n",
      "Iteration 207, Cost: 0.011256454858128042\n",
      "Iteration 208, Cost: 0.011189756412073721\n",
      "Iteration 209, Cost: 0.011123776601861845\n",
      "Iteration 210, Cost: 0.011058503162607237\n",
      "Iteration 211, Cost: 0.010993923939778648\n",
      "Iteration 212, Cost: 0.010930026888430654\n",
      "Iteration 213, Cost: 0.010866800073002836\n",
      "Iteration 214, Cost: 0.01080423166759067\n",
      "Iteration 215, Cost: 0.010742309956597895\n",
      "Iteration 216, Cost: 0.010681023335685512\n",
      "Iteration 217, Cost: 0.010620360312938924\n",
      "Iteration 218, Cost: 0.010560309510181231\n",
      "Iteration 219, Cost: 0.01050085966436747\n",
      "Iteration 220, Cost: 0.010441999629001475\n",
      "Iteration 221, Cost: 0.01038371837552373\n",
      "Iteration 222, Cost: 0.010326004994625455\n",
      "Iteration 223, Cost: 0.010268848697450334\n",
      "Iteration 224, Cost: 0.010212238816651696\n",
      "Iteration 225, Cost: 0.01015616480727849\n",
      "Iteration 226, Cost: 0.010100616247468934\n",
      "Iteration 227, Cost: 0.010045582838935587\n",
      "Iteration 228, Cost: 0.009991054407230098\n",
      "Iteration 229, Cost: 0.00993702090178001\n",
      "Iteration 230, Cost: 0.009883472395693456\n",
      "Iteration 231, Cost: 0.009830399085330998\n",
      "Iteration 232, Cost: 0.009777791289646376\n",
      "Iteration 233, Cost: 0.009725639449300575\n",
      "Iteration 234, Cost: 0.009673934125555388\n",
      "Iteration 235, Cost: 0.009622665998954531\n",
      "Iteration 236, Cost: 0.009571825867801633\n",
      "Iteration 237, Cost: 0.009521404646445542\n",
      "Iteration 238, Cost: 0.009471393363384336\n",
      "Iteration 239, Cost: 0.00942178315919991\n",
      "Iteration 240, Cost: 0.009372565284335596\n",
      "Iteration 241, Cost: 0.009323731096729595\n",
      "Iteration 242, Cost: 0.009275272059317056\n",
      "Iteration 243, Cost: 0.009227179737413792\n",
      "Iteration 244, Cost: 0.009179445795994633\n",
      "Iteration 245, Cost: 0.009132061996879296\n",
      "Iteration 246, Cost: 0.009085020195838487\n",
      "Iteration 247, Cost: 0.00903831233963304\n",
      "Iteration 248, Cost: 0.008991930462998432\n",
      "Iteration 249, Cost: 0.008945866685587181\n",
      "Iteration 250, Cost: 0.008900113208881375\n",
      "Iteration 251, Cost: 0.008854662313087524\n",
      "Iteration 252, Cost: 0.008809506354025947\n",
      "Iteration 253, Cost: 0.008764637760026934\n",
      "Iteration 254, Cost: 0.008720049028846011\n",
      "Iteration 255, Cost: 0.008675732724610793\n",
      "Iteration 256, Cost: 0.00863168147481231\n",
      "Iteration 257, Cost: 0.008587887967353873\n",
      "Iteration 258, Cost: 0.00854434494767115\n",
      "Iteration 259, Cost: 0.00850104521593765\n",
      "Iteration 260, Cost: 0.008457981624370526\n",
      "Iteration 261, Cost: 0.00841514707465239\n",
      "Iteration 262, Cost: 0.008372534515485874\n",
      "Iteration 263, Cost: 0.008330136940298646\n",
      "Iteration 264, Cost: 0.00828794738511794\n",
      "Iteration 265, Cost: 0.008245958926634947\n",
      "Iteration 266, Cost: 0.008204164680481116\n",
      "Iteration 267, Cost: 0.008162557799739898\n",
      "Iteration 268, Cost: 0.008121131473719581\n",
      "Iteration 269, Cost: 0.008079878927014774\n",
      "Iteration 270, Cost: 0.008038793418886341\n",
      "Iteration 271, Cost: 0.007997868242992041\n",
      "Iteration 272, Cost: 0.00795709672750263\n",
      "Iteration 273, Cost: 0.007916472235640931\n",
      "Iteration 274, Cost: 0.007875988166684339\n",
      "Iteration 275, Cost: 0.007835637957474067\n",
      "Iteration 276, Cost: 0.007795415084477883\n",
      "Iteration 277, Cost: 0.007755313066456027\n",
      "Iteration 278, Cost: 0.007715325467783506\n",
      "Iteration 279, Cost: 0.00767544590248526\n",
      "Iteration 280, Cost: 0.007635668039043929\n",
      "Iteration 281, Cost: 0.007595985606043198\n",
      "Iteration 282, Cost: 0.00755639239871269\n",
      "Iteration 283, Cost: 0.007516882286443108\n",
      "Iteration 284, Cost: 0.007477449221342641\n",
      "Iteration 285, Cost: 0.007438087247907504\n",
      "Iteration 286, Cost: 0.007398790513880516\n",
      "Iteration 287, Cost: 0.007359553282371852\n",
      "Iteration 288, Cost: 0.007320369945315287\n",
      "Iteration 289, Cost: 0.0072812350383309795\n",
      "Iteration 290, Cost: 0.007242143257062047\n",
      "Iteration 291, Cost: 0.007203089475046478\n",
      "Iteration 292, Cost: 0.007164068763178009\n",
      "Iteration 293, Cost: 0.0071250764107989705\n",
      "Iteration 294, Cost: 0.0070861079484547925\n",
      "Iteration 295, Cost: 0.007047159172322914\n",
      "Iteration 296, Cost: 0.007008226170308409\n",
      "Iteration 297, Cost: 0.006969305349774035\n",
      "Iteration 298, Cost: 0.006930393466843326\n",
      "Iteration 299, Cost: 0.006891487657181729\n",
      "Iteration 300, Cost: 0.0068525854681219705\n",
      "Iteration 301, Cost: 0.006813684891956255\n",
      "Iteration 302, Cost: 0.006774784400169143\n",
      "Iteration 303, Cost: 0.006735882978331582\n",
      "Iteration 304, Cost: 0.0066969801613187795\n",
      "Iteration 305, Cost: 0.006658076068453385\n",
      "Iteration 306, Cost: 0.006619171438111665\n",
      "Iteration 307, Cost: 0.0065802676612654685\n",
      "Iteration 308, Cost: 0.0065413668133687255\n",
      "Iteration 309, Cost: 0.006502471683935921\n",
      "Iteration 310, Cost: 0.006463585803104416\n",
      "Iteration 311, Cost: 0.006424713464425228\n",
      "Iteration 312, Cost: 0.006385859743091631\n",
      "Iteration 313, Cost: 0.006347030508794977\n",
      "Iteration 314, Cost: 0.006308232432396038\n",
      "Iteration 315, Cost: 0.006269472985621711\n",
      "Iteration 316, Cost: 0.006230760433044214\n",
      "Iteration 317, Cost: 0.006192103815675388\n",
      "Iteration 318, Cost: 0.006153512925614673\n",
      "Iteration 319, Cost: 0.006114998271326178\n",
      "Iteration 320, Cost: 0.0060765710332875405\n",
      "Iteration 321, Cost: 0.006038243009948643\n",
      "Iteration 322, Cost: 0.006000026554157958\n",
      "Iteration 323, Cost: 0.005961934500452441\n",
      "Iteration 324, Cost: 0.005923980083855932\n",
      "Iteration 325, Cost: 0.005886176851082358\n",
      "Iteration 326, Cost: 0.005848538565282791\n",
      "Iteration 327, Cost: 0.00581107910569965\n",
      "Iteration 328, Cost: 0.005773812363785238\n",
      "Iteration 329, Cost: 0.005736752137495839\n",
      "Iteration 330, Cost: 0.005699912025577386\n",
      "Iteration 331, Cost: 0.005663305323707491\n",
      "Iteration 332, Cost: 0.005626944924347112\n",
      "Iteration 333, Cost: 0.00559084322208196\n",
      "Iteration 334, Cost: 0.0055550120261007715\n",
      "Iteration 335, Cost: 0.005519462481270242\n",
      "Iteration 336, Cost: 0.005484204999032141\n",
      "Iteration 337, Cost: 0.005449249199077964\n",
      "Iteration 338, Cost: 0.005414603862462137\n",
      "Iteration 339, Cost: 0.005380276896509449\n",
      "Iteration 340, Cost: 0.005346275311569196\n",
      "Iteration 341, Cost: 0.005312605209379807\n",
      "Iteration 342, Cost: 0.0052792717825442545\n",
      "Iteration 343, Cost: 0.0052462793243874236\n",
      "Iteration 344, Cost: 0.005213631248277932\n",
      "Iteration 345, Cost: 0.005181330115352835\n",
      "Iteration 346, Cost: 0.005149377669485234\n",
      "Iteration 347, Cost: 0.0051177748782812935\n",
      "Iteration 348, Cost: 0.005086521978881367\n",
      "Iteration 349, Cost: 0.00505561852736523\n",
      "Iteration 350, Cost: 0.005025063450618382\n",
      "Iteration 351, Cost: 0.0049948550995984695\n",
      "Converged!\n",
      "Iteration 1, Cost: 0.36697014014755064\n",
      "Iteration 2, Cost: 0.3574216032923259\n",
      "Iteration 3, Cost: 0.34764670327718783\n",
      "Iteration 4, Cost: 0.33609791637554703\n",
      "Iteration 5, Cost: 0.3214685643653085\n",
      "Iteration 6, Cost: 0.30267910536386977\n",
      "Iteration 7, Cost: 0.27998888802262917\n",
      "Iteration 8, Cost: 0.25698420138839356\n",
      "Iteration 9, Cost: 0.23925801163466412\n",
      "Iteration 10, Cost: 0.22811634191321958\n",
      "Iteration 11, Cost: 0.22074905918849846\n",
      "Iteration 12, Cost: 0.2148972524223481\n",
      "Iteration 13, Cost: 0.20957490741732668\n",
      "Iteration 14, Cost: 0.2044099810303454\n",
      "Iteration 15, Cost: 0.19928186247805918\n",
      "Iteration 16, Cost: 0.194178288720622\n",
      "Iteration 17, Cost: 0.1891336507385115\n",
      "Iteration 18, Cost: 0.18419767395293463\n",
      "Iteration 19, Cost: 0.1794185205498693\n",
      "Iteration 20, Cost: 0.1748344500682906\n",
      "Iteration 21, Cost: 0.17047070965098926\n",
      "Iteration 22, Cost: 0.16633951768112587\n",
      "Iteration 23, Cost: 0.16244185122799268\n",
      "Iteration 24, Cost: 0.15877013437729262\n",
      "Iteration 25, Cost: 0.1553110998712481\n",
      "Iteration 26, Cost: 0.15204831552058085\n",
      "Iteration 27, Cost: 0.14896413039755868\n",
      "Iteration 28, Cost: 0.1460410102889142\n",
      "Iteration 29, Cost: 0.14326235302719817\n",
      "Iteration 30, Cost: 0.14061291538623033\n",
      "Iteration 31, Cost: 0.13807897716886927\n",
      "Iteration 32, Cost: 0.13564834272759851\n",
      "Iteration 33, Cost: 0.1333102517571963\n",
      "Iteration 34, Cost: 0.1310552469538\n",
      "Iteration 35, Cost: 0.12887502795874597\n",
      "Iteration 36, Cost: 0.12676230843746952\n",
      "Iteration 37, Cost: 0.12471068495883635\n",
      "Iteration 38, Cost: 0.12271452128479099\n",
      "Iteration 39, Cost: 0.1207688487305402\n",
      "Iteration 40, Cost: 0.11886928166045246\n",
      "Iteration 41, Cost: 0.11701194642703441\n",
      "Iteration 42, Cost: 0.11519342179855366\n",
      "Iteration 43, Cost: 0.1134106889376868\n",
      "Iteration 44, Cost: 0.11166108915459706\n",
      "Iteration 45, Cost: 0.10994228788262492\n",
      "Iteration 46, Cost: 0.1082522435669198\n",
      "Iteration 47, Cost: 0.10658918039046414\n",
      "Iteration 48, Cost: 0.10495156397527958\n",
      "Iteration 49, Cost: 0.10333807938375314\n",
      "Iteration 50, Cost: 0.10174761090471679\n",
      "Iteration 51, Cost: 0.10017922324211495\n",
      "Iteration 52, Cost: 0.09863214383286827\n",
      "Iteration 53, Cost: 0.09710574610743725\n",
      "Iteration 54, Cost: 0.09559953357430925\n",
      "Iteration 55, Cost: 0.09411312466084927\n",
      "Iteration 56, Cost: 0.09264623828022722\n",
      "Iteration 57, Cost: 0.09119868011985176\n",
      "Iteration 58, Cost: 0.0897703296630906\n",
      "Iteration 59, Cost: 0.08836112796501056\n",
      "Iteration 60, Cost: 0.08697106620615258\n",
      "Iteration 61, Cost: 0.08560017504746169\n",
      "Iteration 62, Cost: 0.08424851480565561\n",
      "Iteration 63, Cost: 0.0829161664625588\n",
      "Iteration 64, Cost: 0.08160322351504944\n",
      "Iteration 65, Cost: 0.08030978466488402\n",
      "Iteration 66, Cost: 0.07903594734023511\n",
      "Iteration 67, Cost: 0.0777818020336378\n",
      "Iteration 68, Cost: 0.07654742743442106\n",
      "Iteration 69, Cost: 0.07533288632776054\n",
      "Iteration 70, Cost: 0.07413822222733335\n",
      "Iteration 71, Cost: 0.07296345670424381\n",
      "Iteration 72, Cost: 0.07180858737145242\n",
      "Iteration 73, Cost: 0.07067358648038231\n",
      "Iteration 74, Cost: 0.06955840008467498\n",
      "Iteration 75, Cost: 0.06846294772517447\n",
      "Iteration 76, Cost: 0.06738712259006148\n",
      "Iteration 77, Cost: 0.06633079210453233\n",
      "Iteration 78, Cost: 0.06529379890538939\n",
      "Iteration 79, Cost: 0.06427596215721763\n",
      "Iteration 80, Cost: 0.06327707916827335\n",
      "Iteration 81, Cost: 0.06229692726560126\n",
      "Iteration 82, Cost: 0.06133526589000081\n",
      "Iteration 83, Cost: 0.06039183887207255\n",
      "Iteration 84, Cost: 0.059466376850497334\n",
      "Iteration 85, Cost: 0.05855859979280647\n",
      "Iteration 86, Cost: 0.05766821957712796\n",
      "Iteration 87, Cost: 0.05679494259080897\n",
      "Iteration 88, Cost: 0.05593847229861776\n",
      "Iteration 89, Cost: 0.055098511729793194\n",
      "Iteration 90, Cost: 0.05427476583007792\n",
      "Iteration 91, Cost: 0.05346694362274465\n",
      "Iteration 92, Cost: 0.05267476012230704\n",
      "Iteration 93, Cost: 0.05189793794692385\n",
      "Iteration 94, Cost: 0.05113620858118249\n",
      "Iteration 95, Cost: 0.05038931325046909\n",
      "Iteration 96, Cost: 0.049657003381594005\n",
      "Iteration 97, Cost: 0.04893904064134252\n",
      "Iteration 98, Cost: 0.048235196564207834\n",
      "Iteration 99, Cost: 0.047545251801271796\n",
      "Iteration 100, Cost: 0.04686899504219233\n",
      "Iteration 101, Cost: 0.046206221679559574\n",
      "Iteration 102, Cost: 0.04555673229767309\n",
      "Iteration 103, Cost: 0.04492033107468618\n",
      "Iteration 104, Cost: 0.04429682418735203\n",
      "Iteration 105, Cost: 0.04368601830136947\n",
      "Iteration 106, Cost: 0.043087719218415726\n",
      "Iteration 107, Cost: 0.04250173073483609\n",
      "Iteration 108, Cost: 0.04192785374847687\n",
      "Iteration 109, Cost: 0.0413658856312132\n",
      "Iteration 110, Cost: 0.04081561986707223\n",
      "Iteration 111, Cost: 0.04027684594082497\n",
      "Iteration 112, Cost: 0.03974934945036395\n",
      "Iteration 113, Cost: 0.039232912408433025\n",
      "Iteration 114, Cost: 0.03872731369521959\n",
      "Iteration 115, Cost: 0.03823232962250904\n",
      "Iteration 116, Cost: 0.037747734571889076\n",
      "Iteration 117, Cost: 0.037273301673142994\n",
      "Iteration 118, Cost: 0.03680880349377648\n",
      "Iteration 119, Cost: 0.03635401271595877\n",
      "Iteration 120, Cost: 0.035908702782526586\n",
      "Iteration 121, Cost: 0.035472648498739566\n",
      "Iteration 122, Cost: 0.03504562658095606\n",
      "Iteration 123, Cost: 0.03462741614719672\n",
      "Iteration 124, Cost: 0.03421779914764539\n",
      "Iteration 125, Cost: 0.03381656073552646\n",
      "Iteration 126, Cost: 0.033423489580560906\n",
      "Iteration 127, Cost: 0.03303837812842382\n",
      "Iteration 128, Cost: 0.032661022810398864\n",
      "Iteration 129, Cost: 0.03229122420783959\n",
      "Iteration 130, Cost: 0.031928787176188814\n",
      "Iteration 131, Cost: 0.03157352093324662\n",
      "Iteration 132, Cost: 0.03122523911617769\n",
      "Iteration 133, Cost: 0.03088375981145629\n",
      "Iteration 134, Cost: 0.03054890556160291\n",
      "Iteration 135, Cost: 0.030220503352197482\n",
      "Iteration 136, Cost: 0.02989838458228184\n",
      "Iteration 137, Cost: 0.029582385020903133\n",
      "Iteration 138, Cost: 0.029272344752209835\n",
      "Iteration 139, Cost: 0.028968108111197984\n",
      "Iteration 140, Cost: 0.028669523611920888\n",
      "Iteration 141, Cost: 0.02837644386972095\n",
      "Iteration 142, Cost: 0.028088725518816877\n",
      "Iteration 143, Cost: 0.027806229126382354\n",
      "Iteration 144, Cost: 0.02752881910408025\n",
      "Iteration 145, Cost: 0.027256363617868453\n",
      "Iteration 146, Cost: 0.026988734496765362\n",
      "Iteration 147, Cost: 0.026725807141154495\n",
      "Iteration 148, Cost: 0.02646746043111446\n",
      "Iteration 149, Cost: 0.026213576635181594\n",
      "Iteration 150, Cost: 0.02596404131988598\n",
      "Iteration 151, Cost: 0.02571874326034479\n",
      "Iteration 152, Cost: 0.025477574352149437\n",
      "Iteration 153, Cost: 0.02524042952474294\n",
      "Iteration 154, Cost: 0.025007206656450027\n",
      "Iteration 155, Cost: 0.024777806491294147\n",
      "Iteration 156, Cost: 0.024552132557711857\n",
      "Iteration 157, Cost: 0.02433009108925458\n",
      "Iteration 158, Cost: 0.024111590947351314\n",
      "Iteration 159, Cost: 0.023896543546191012\n",
      "Iteration 160, Cost: 0.02368486277977174\n",
      "Iteration 161, Cost: 0.02347646495115298\n",
      "Iteration 162, Cost: 0.02327126870393874\n",
      "Iteration 163, Cost: 0.023069194956011358\n",
      "Iteration 164, Cost: 0.02287016683552898\n",
      "Iteration 165, Cost: 0.022674109619193597\n",
      "Iteration 166, Cost: 0.02248095067279073\n",
      "Iteration 167, Cost: 0.022290619393996534\n",
      "Iteration 168, Cost: 0.022103047157442712\n",
      "Iteration 169, Cost: 0.02191816726202449\n",
      "Iteration 170, Cost: 0.021735914880431303\n",
      "Iteration 171, Cost: 0.02155622701087414\n",
      "Iteration 172, Cost: 0.021379042430977612\n",
      "Iteration 173, Cost: 0.021204301653797948\n",
      "Iteration 174, Cost: 0.021031946885921124\n",
      "Iteration 175, Cost: 0.020861921987587767\n",
      "Iteration 176, Cost: 0.020694172434782693\n",
      "Iteration 177, Cost: 0.020528645283218242\n",
      "Iteration 178, Cost: 0.0203652891341306\n",
      "Iteration 179, Cost: 0.020204054101798017\n",
      "Iteration 180, Cost: 0.020044891782678837\n",
      "Iteration 181, Cost: 0.019887755226055993\n",
      "Iteration 182, Cost: 0.019732598906062715\n",
      "Iteration 183, Cost: 0.01957937869495279\n",
      "Iteration 184, Cost: 0.01942805183746676\n",
      "Iteration 185, Cost: 0.01927857692613431\n",
      "Iteration 186, Cost: 0.01913091387734294\n",
      "Iteration 187, Cost: 0.018985023907993347\n",
      "Iteration 188, Cost: 0.018840869512554353\n",
      "Iteration 189, Cost: 0.018698414440324412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 190, Cost: 0.018557623672703166\n",
      "Iteration 191, Cost: 0.018418463400275926\n",
      "Iteration 192, Cost: 0.018280900999516486\n",
      "Iteration 193, Cost: 0.0181449050089198\n",
      "Iteration 194, Cost: 0.018010445104385837\n",
      "Iteration 195, Cost: 0.017877492073690174\n",
      "Iteration 196, Cost: 0.017746017789894684\n",
      "Iteration 197, Cost: 0.017615995183574044\n",
      "Iteration 198, Cost: 0.01748739821375986\n",
      "Iteration 199, Cost: 0.017360201837533822\n",
      "Iteration 200, Cost: 0.017234381978233966\n",
      "Iteration 201, Cost: 0.017109915492273458\n",
      "Iteration 202, Cost: 0.01698678013460776\n",
      "Iteration 203, Cost: 0.016864954522923664\n",
      "Iteration 204, Cost: 0.016744418100660737\n",
      "Iteration 205, Cost: 0.016625151099011203\n",
      "Iteration 206, Cost: 0.01650713449807777\n",
      "Iteration 207, Cost: 0.016390349987398176\n",
      "Iteration 208, Cost: 0.016274779926070633\n",
      "Iteration 209, Cost: 0.01616040730273379\n",
      "Iteration 210, Cost: 0.01604721569566872\n",
      "Iteration 211, Cost: 0.01593518923329742\n",
      "Iteration 212, Cost: 0.01582431255535298\n",
      "Iteration 213, Cost: 0.015714570774989983\n",
      "Iteration 214, Cost: 0.015605949442091213\n",
      "Iteration 215, Cost: 0.015498434508007813\n",
      "Iteration 216, Cost: 0.01539201229194612\n",
      "Iteration 217, Cost: 0.01528666944918592\n",
      "Iteration 218, Cost: 0.015182392941282978\n",
      "Iteration 219, Cost: 0.015079170008374589\n",
      "Iteration 220, Cost: 0.014976988143671405\n",
      "Iteration 221, Cost: 0.01487583507018328\n",
      "Iteration 222, Cost: 0.014775698719692183\n",
      "Iteration 223, Cost: 0.014676567213952371\n",
      "Iteration 224, Cost: 0.014578428848067837\n",
      "Iteration 225, Cost: 0.014481272075969954\n",
      "Iteration 226, Cost: 0.014385085497895026\n",
      "Iteration 227, Cost: 0.01428985784974217\n",
      "Iteration 228, Cost: 0.01419557799417679\n",
      "Iteration 229, Cost: 0.014102234913333974\n",
      "Iteration 230, Cost: 0.01400981770296918\n",
      "Iteration 231, Cost: 0.013918315567900214\n",
      "Iteration 232, Cost: 0.01382771781858478\n",
      "Iteration 233, Cost: 0.013738013868680907\n",
      "Iteration 234, Cost: 0.013649193233443295\n",
      "Iteration 235, Cost: 0.013561245528816289\n",
      "Iteration 236, Cost: 0.013474160471093627\n",
      "Iteration 237, Cost: 0.013387927877025639\n",
      "Iteration 238, Cost: 0.013302537664265838\n",
      "Iteration 239, Cost: 0.013217979852060616\n",
      "Iteration 240, Cost: 0.013134244562097488\n",
      "Iteration 241, Cost: 0.013051322019438953\n",
      "Iteration 242, Cost: 0.01296920255348013\n",
      "Iteration 243, Cost: 0.012887876598878953\n",
      "Iteration 244, Cost: 0.012807334696417372\n",
      "Iteration 245, Cost: 0.012727567493761105\n",
      "Iteration 246, Cost: 0.012648565746093512\n",
      "Iteration 247, Cost: 0.01257032031660624\n",
      "Iteration 248, Cost: 0.012492822176835741\n",
      "Iteration 249, Cost: 0.01241606240683989\n",
      "Iteration 250, Cost: 0.012340032195213724\n",
      "Iteration 251, Cost: 0.012264722838946794\n",
      "Iteration 252, Cost: 0.012190125743127913\n",
      "Iteration 253, Cost: 0.012116232420505196\n",
      "Iteration 254, Cost: 0.012043034490911252\n",
      "Iteration 255, Cost: 0.01197052368056456\n",
      "Iteration 256, Cost: 0.011898691821258798\n",
      "Iteration 257, Cost: 0.01182753084945252\n",
      "Iteration 258, Cost: 0.011757032805271413\n",
      "Iteration 259, Cost: 0.011687189831435485\n",
      "Iteration 260, Cost: 0.011617994172122911\n",
      "Iteration 261, Cost: 0.011549438171781834\n",
      "Iteration 262, Cost: 0.011481514273900865\n",
      "Iteration 263, Cost: 0.011414215019748002\n",
      "Iteration 264, Cost: 0.011347533047087112\n",
      "Iteration 265, Cost: 0.011281461088880198\n",
      "Iteration 266, Cost: 0.011215991971982706\n",
      "Iteration 267, Cost: 0.011151118615838554\n",
      "Iteration 268, Cost: 0.011086834031180349\n",
      "Iteration 269, Cost: 0.01102313131873987\n",
      "Iteration 270, Cost: 0.010960003667972843\n",
      "Iteration 271, Cost: 0.010897444355801462\n",
      "Iteration 272, Cost: 0.010835446745377508\n",
      "Iteration 273, Cost: 0.010774004284868138\n",
      "Iteration 274, Cost: 0.010713110506266142\n",
      "Iteration 275, Cost: 0.010652759024225694\n",
      "Iteration 276, Cost: 0.010592943534924513\n",
      "Iteration 277, Cost: 0.010533657814952599\n",
      "Iteration 278, Cost: 0.010474895720227783\n",
      "Iteration 279, Cost: 0.010416651184937741\n",
      "Iteration 280, Cost: 0.010358918220508037\n",
      "Iteration 281, Cost: 0.010301690914595556\n",
      "Iteration 282, Cost: 0.010244963430106462\n",
      "Iteration 283, Cost: 0.01018873000423783\n",
      "Iteration 284, Cost: 0.01013298494754181\n",
      "Iteration 285, Cost: 0.010077722643011327\n",
      "Iteration 286, Cost: 0.010022937545186114\n",
      "Iteration 287, Cost: 0.009968624179277978\n",
      "Iteration 288, Cost: 0.00991477714031407\n",
      "Iteration 289, Cost: 0.009861391092297068\n",
      "Iteration 290, Cost: 0.009808460767381134\n",
      "Iteration 291, Cost: 0.009755980965062519\n",
      "Iteration 292, Cost: 0.00970394655138384\n",
      "Iteration 293, Cost: 0.009652352458150961\n",
      "Iteration 294, Cost: 0.009601193682161582\n",
      "Iteration 295, Cost: 0.00955046528444465\n",
      "Iteration 296, Cost: 0.009500162389509749\n",
      "Iteration 297, Cost: 0.009450280184605737\n",
      "Iteration 298, Cost: 0.009400813918987913\n",
      "Iteration 299, Cost: 0.009351758903193105\n",
      "Iteration 300, Cost: 0.009303110508322086\n",
      "Iteration 301, Cost: 0.009254864165328785\n",
      "Iteration 302, Cost: 0.009207015364315917\n",
      "Iteration 303, Cost: 0.009159559653836516\n",
      "Iteration 304, Cost: 0.009112492640201128\n",
      "Iteration 305, Cost: 0.009065809986790267\n",
      "Iteration 306, Cost: 0.009019507413371987\n",
      "Iteration 307, Cost: 0.008973580695424239\n",
      "Iteration 308, Cost: 0.008928025663461911\n",
      "Iteration 309, Cost: 0.00888283820236841\n",
      "Iteration 310, Cost: 0.008838014250731663\n",
      "Iteration 311, Cost: 0.008793549800184437\n",
      "Iteration 312, Cost: 0.008749440894748989\n",
      "Iteration 313, Cost: 0.008705683630185968\n",
      "Iteration 314, Cost: 0.00866227415334758\n",
      "Iteration 315, Cost: 0.008619208661535045\n",
      "Iteration 316, Cost: 0.008576483401860343\n",
      "Iteration 317, Cost: 0.008534094670612337\n",
      "Iteration 318, Cost: 0.008492038812627268\n",
      "Iteration 319, Cost: 0.008450312220663746\n",
      "Iteration 320, Cost: 0.00840891133478228\n",
      "Iteration 321, Cost: 0.008367832641729415\n",
      "Iteration 322, Cost: 0.00832707267432657\n",
      "Iteration 323, Cost: 0.008286628010863698\n",
      "Iteration 324, Cost: 0.00824649527449776\n",
      "Iteration 325, Cost: 0.008206671132656232\n",
      "Iteration 326, Cost: 0.008167152296445589\n",
      "Iteration 327, Cost: 0.008127935520065\n",
      "Iteration 328, Cost: 0.008089017600225188\n",
      "Iteration 329, Cost: 0.008050395375572634\n",
      "Iteration 330, Cost: 0.00801206572611912\n",
      "Iteration 331, Cost: 0.007974025572676798\n",
      "Iteration 332, Cost: 0.007936271876298709\n",
      "Iteration 333, Cost: 0.007898801637724972\n",
      "Iteration 334, Cost: 0.007861611896834585\n",
      "Iteration 335, Cost: 0.007824699732102964\n",
      "Iteration 336, Cost: 0.007788062260065242\n",
      "Iteration 337, Cost: 0.007751696634785389\n",
      "Iteration 338, Cost: 0.0077156000473311845\n",
      "Iteration 339, Cost: 0.007679769725255105\n",
      "Iteration 340, Cost: 0.00764420293208109\n",
      "Iteration 341, Cost: 0.0076088969667973344\n",
      "Iteration 342, Cost: 0.0075738491633549595\n",
      "Iteration 343, Cost: 0.00753905689017277\n",
      "Iteration 344, Cost: 0.007504517549647945\n",
      "Iteration 345, Cost: 0.007470228577672753\n",
      "Iteration 346, Cost: 0.0074361874431572985\n",
      "Iteration 347, Cost: 0.007402391647558237\n",
      "Iteration 348, Cost: 0.007368838724413522\n",
      "Iteration 349, Cost: 0.0073355262388831105\n",
      "Iteration 350, Cost: 0.0073024517872956754\n",
      "Iteration 351, Cost: 0.007269612996701241\n",
      "Iteration 352, Cost: 0.007237007524429766\n",
      "Iteration 353, Cost: 0.007204633057655625\n",
      "Iteration 354, Cost: 0.007172487312967972\n",
      "Iteration 355, Cost: 0.007140568035946951\n",
      "Iteration 356, Cost: 0.007108873000745704\n",
      "Iteration 357, Cost: 0.007077400009678157\n",
      "Iteration 358, Cost: 0.007046146892812551\n",
      "Iteration 359, Cost: 0.007015111507570643\n",
      "Iteration 360, Cost: 0.006984291738332554\n",
      "Iteration 361, Cost: 0.0069536854960472365\n",
      "Iteration 362, Cost: 0.006923290717848464\n",
      "Iteration 363, Cost: 0.006893105366676336\n",
      "Iteration 364, Cost: 0.006863127430904233\n",
      "Iteration 365, Cost: 0.006833354923971165\n",
      "Iteration 366, Cost: 0.00680378588401944\n",
      "Iteration 367, Cost: 0.006774418373537667\n",
      "Iteration 368, Cost: 0.006745250479008935\n",
      "Iteration 369, Cost: 0.0067162803105641974\n",
      "Iteration 370, Cost: 0.006687506001640763\n",
      "Iteration 371, Cost: 0.006658925708645833\n",
      "Iteration 372, Cost: 0.0066305376106250554\n",
      "Iteration 373, Cost: 0.006602339908935986\n",
      "Iteration 374, Cost: 0.006574330826926457\n",
      "Iteration 375, Cost: 0.006546508609617735\n",
      "Iteration 376, Cost: 0.006518871523392459\n",
      "Iteration 377, Cost: 0.006491417855687245\n",
      "Iteration 378, Cost: 0.006464145914689961\n",
      "Iteration 379, Cost: 0.0064370540290415255\n",
      "Iteration 380, Cost: 0.006410140547542268\n",
      "Iteration 381, Cost: 0.006383403838862714\n",
      "Iteration 382, Cost: 0.006356842291258758\n",
      "Iteration 383, Cost: 0.006330454312291179\n",
      "Iteration 384, Cost: 0.006304238328549426\n",
      "Iteration 385, Cost: 0.006278192785379595\n",
      "Iteration 386, Cost: 0.006252316146616592\n",
      "Iteration 387, Cost: 0.006226606894320347\n",
      "Iteration 388, Cost: 0.006201063528516101\n",
      "Iteration 389, Cost: 0.006175684566938636\n",
      "Iteration 390, Cost: 0.0061504685447804335\n",
      "Iteration 391, Cost: 0.006125414014443706\n",
      "Iteration 392, Cost: 0.006100519545296216\n",
      "Iteration 393, Cost: 0.006075783723430839\n",
      "Iteration 394, Cost: 0.006051205151428842\n",
      "Iteration 395, Cost: 0.006026782448126766\n",
      "Iteration 396, Cost: 0.006002514248386914\n",
      "Iteration 397, Cost: 0.005978399202871361\n",
      "Iteration 398, Cost: 0.005954435977819426\n",
      "Iteration 399, Cost: 0.0059306232548285995\n",
      "Iteration 400, Cost: 0.005906959730638787\n",
      "Iteration 401, Cost: 0.005883444116919928\n",
      "Iteration 402, Cost: 0.005860075140062849\n",
      "Iteration 403, Cost: 0.005836851540973331\n",
      "Iteration 404, Cost: 0.005813772074869365\n",
      "Iteration 405, Cost: 0.0057908355110815185\n",
      "Iteration 406, Cost: 0.005768040632856367\n",
      "Iteration 407, Cost: 0.005745386237162948\n",
      "Iteration 408, Cost: 0.005722871134502204\n",
      "Iteration 409, Cost: 0.005700494148719341\n",
      "Iteration 410, Cost: 0.005678254116819082\n",
      "Iteration 411, Cost: 0.005656149888783754\n",
      "Iteration 412, Cost: 0.005634180327394172\n",
      "Iteration 413, Cost: 0.005612344308053269\n",
      "Iteration 414, Cost: 0.005590640718612438\n",
      "Iteration 415, Cost: 0.005569068459200559\n",
      "Iteration 416, Cost: 0.005547626442055601\n",
      "Iteration 417, Cost: 0.005526313591358866\n",
      "Iteration 418, Cost: 0.005505128843071721\n",
      "Iteration 419, Cost: 0.005484071144774874\n",
      "Iteration 420, Cost: 0.005463139455510092\n",
      "Iteration 421, Cost: 0.005442332745624347\n",
      "Iteration 422, Cost: 0.005421649996616331\n",
      "Iteration 423, Cost: 0.005401090200985367\n",
      "Iteration 424, Cost: 0.005380652362082572\n",
      "Iteration 425, Cost: 0.005360335493964334\n",
      "Iteration 426, Cost: 0.005340138621248007\n",
      "Iteration 427, Cost: 0.005320060778969832\n",
      "Iteration 428, Cost: 0.005300101012444996\n",
      "Iteration 429, Cost: 0.005280258377129852\n",
      "Iteration 430, Cost: 0.005260531938486219\n",
      "Iteration 431, Cost: 0.005240920771847767\n",
      "Iteration 432, Cost: 0.005221423962288427\n",
      "Iteration 433, Cost: 0.005202040604492797\n",
      "Iteration 434, Cost: 0.005182769802628534\n",
      "Iteration 435, Cost: 0.005163610670220672\n",
      "Iteration 436, Cost: 0.005144562330027861\n",
      "Iteration 437, Cost: 0.005125623913920468\n",
      "Iteration 438, Cost: 0.005106794562760544\n",
      "Iteration 439, Cost: 0.0050880734262836\n",
      "Iteration 440, Cost: 0.005069459662982178\n",
      "Iteration 441, Cost: 0.005050952439991178\n",
      "Iteration 442, Cost: 0.005032550932974934\n",
      "Iteration 443, Cost: 0.005014254326015979\n",
      "Iteration 444, Cost: 0.004996061811505501\n",
      "Converged!\n",
      "Iteration 1, Cost: 0.2716397104846159\n",
      "Iteration 2, Cost: 0.24939519908704505\n",
      "Iteration 3, Cost: 0.23396470004883532\n",
      "Iteration 4, Cost: 0.2232787087332875\n",
      "Iteration 5, Cost: 0.21519260408046795\n",
      "Iteration 6, Cost: 0.20876184639633175\n",
      "Iteration 7, Cost: 0.20341824927831847\n",
      "Iteration 8, Cost: 0.19871933001401948\n",
      "Iteration 9, Cost: 0.19436813778637857\n",
      "Iteration 10, Cost: 0.19019462630334721\n",
      "Iteration 11, Cost: 0.1861069519698949\n",
      "Iteration 12, Cost: 0.18205430190317576\n",
      "Iteration 13, Cost: 0.17800802898775125\n",
      "Iteration 14, Cost: 0.17395476571772106\n",
      "Iteration 15, Cost: 0.16989540855256224\n",
      "Iteration 16, Cost: 0.16584596930722806\n",
      "Iteration 17, Cost: 0.16183746267860807\n",
      "Iteration 18, Cost: 0.15791268063137848\n",
      "Iteration 19, Cost: 0.1541188992359699\n",
      "Iteration 20, Cost: 0.15049773739949493\n",
      "Iteration 21, Cost: 0.14707569347851077\n",
      "Iteration 22, Cost: 0.14385941982369269\n",
      "Iteration 23, Cost: 0.1408375200499686\n",
      "Iteration 24, Cost: 0.13798711108431067\n",
      "Iteration 25, Cost: 0.13528142729238885\n",
      "Iteration 26, Cost: 0.1326954692112407\n",
      "Iteration 27, Cost: 0.1302087519271108\n",
      "Iteration 28, Cost: 0.1278057753695885\n",
      "Iteration 29, Cost: 0.12547530506483007\n",
      "Iteration 30, Cost: 0.12320930922207783\n",
      "Iteration 31, Cost: 0.1210019902219733\n",
      "Iteration 32, Cost: 0.11884905086420856\n",
      "Iteration 33, Cost: 0.11674718808214089\n",
      "Iteration 34, Cost: 0.1146937591309104\n",
      "Iteration 35, Cost: 0.11268656436397025\n",
      "Iteration 36, Cost: 0.11072370460036589\n",
      "Iteration 37, Cost: 0.10880348561662578\n",
      "Iteration 38, Cost: 0.10692435314773487\n",
      "Iteration 39, Cost: 0.10508484872141088\n",
      "Iteration 40, Cost: 0.10328358066494878\n",
      "Iteration 41, Cost: 0.10151920677210545\n",
      "Iteration 42, Cost: 0.09979042619571302\n",
      "Iteration 43, Cost: 0.09809597865206304\n",
      "Iteration 44, Cost: 0.09643464927821167\n",
      "Iteration 45, Cost: 0.09480527762030778\n",
      "Iteration 46, Cost: 0.09320676931157779\n",
      "Iteration 47, Cost: 0.09163810904295822\n",
      "Iteration 48, Cost: 0.09009837344648101\n",
      "Iteration 49, Cost: 0.08858674251825004\n",
      "Iteration 50, Cost: 0.08710250823835831\n",
      "Iteration 51, Cost: 0.08564507915001618\n",
      "Iteration 52, Cost: 0.08421397989485117\n",
      "Iteration 53, Cost: 0.08280884510471655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 54, Cost: 0.08142940762012396\n",
      "Iteration 55, Cost: 0.0800754816806813\n",
      "Iteration 56, Cost: 0.0787469423962932\n",
      "Iteration 57, Cost: 0.07744370331374408\n",
      "Iteration 58, Cost: 0.07616569411566854\n",
      "Iteration 59, Cost: 0.07491284036950278\n",
      "Iteration 60, Cost: 0.07368504681675005\n",
      "Iteration 61, Cost: 0.07248218507203129\n",
      "Iteration 62, Cost: 0.07130408593895492\n",
      "Iteration 63, Cost: 0.07015053598383951\n",
      "Iteration 64, Cost: 0.06902127762494083\n",
      "Iteration 65, Cost: 0.06791601181678333\n",
      "Iteration 66, Cost: 0.06683440240777362\n",
      "Iteration 67, Cost: 0.06577608136828095\n",
      "Iteration 68, Cost: 0.06474065426549003\n",
      "Iteration 69, Cost: 0.0637277055515429\n",
      "Iteration 70, Cost: 0.06273680340113107\n",
      "Iteration 71, Cost: 0.06176750396861887\n",
      "Iteration 72, Cost: 0.060819355029988614\n",
      "Iteration 73, Cost: 0.05989189903562244\n",
      "Iteration 74, Cost: 0.05898467563352903\n",
      "Iteration 75, Cost: 0.05809722373687927\n",
      "Iteration 76, Cost: 0.05722908321143624\n",
      "Iteration 77, Cost: 0.056379796252919966\n",
      "Iteration 78, Cost: 0.05554890851526554\n",
      "Iteration 79, Cost: 0.05473597004054016\n",
      "Iteration 80, Cost: 0.05394053603142001\n",
      "Iteration 81, Cost: 0.05316216749834052\n",
      "Iteration 82, Cost: 0.05240043180602331\n",
      "Iteration 83, Cost: 0.05165490313808302\n",
      "Iteration 84, Cost: 0.05092516289371037\n",
      "Iteration 85, Cost: 0.05021080002683634\n",
      "Iteration 86, Cost: 0.049511411335506315\n",
      "Iteration 87, Cost: 0.0488266017072452\n",
      "Iteration 88, Cost: 0.048155984324807735\n",
      "Iteration 89, Cost: 0.047499180835743705\n",
      "Iteration 90, Cost: 0.04685582148855244\n",
      "Iteration 91, Cost: 0.046225545237766986\n",
      "Iteration 92, Cost: 0.04560799982002794\n",
      "Iteration 93, Cost: 0.04500284180302912\n",
      "Iteration 94, Cost: 0.04440973660910627\n",
      "Iteration 95, Cost: 0.04382835851516763\n",
      "Iteration 96, Cost: 0.04325839063061466\n",
      "Iteration 97, Cost: 0.04269952485485974\n",
      "Iteration 98, Cost: 0.042151461816006285\n",
      "Iteration 99, Cost: 0.041613910792213\n",
      "Iteration 100, Cost: 0.041086589617213135\n",
      "Iteration 101, Cost: 0.0405692245714031\n",
      "Iteration 102, Cost: 0.04006155025985094\n",
      "Iteration 103, Cost: 0.039563309478506684\n",
      "Iteration 104, Cost: 0.03907425306982396\n",
      "Iteration 105, Cost: 0.03859413976892709\n",
      "Iteration 106, Cost: 0.038122736041382645\n",
      "Iteration 107, Cost: 0.03765981591356031\n",
      "Iteration 108, Cost: 0.0372051607964961\n",
      "Iteration 109, Cost: 0.03675855930410409\n",
      "Iteration 110, Cost: 0.036319807066521145\n",
      "Iteration 111, Cost: 0.03588870653931363\n",
      "Iteration 112, Cost: 0.03546506680922736\n",
      "Iteration 113, Cost: 0.03504870339712177\n",
      "Iteration 114, Cost: 0.03463943805869652\n",
      "Iteration 115, Cost: 0.034237098583594215\n",
      "Iteration 116, Cost: 0.03384151859344531\n",
      "Iteration 117, Cost: 0.03345253733941014\n",
      "Iteration 118, Cost: 0.033069999499767695\n",
      "Iteration 119, Cost: 0.03269375497809932\n",
      "Iteration 120, Cost: 0.032323658702617636\n",
      "Iteration 121, Cost: 0.0319595704271939\n",
      "Iteration 122, Cost: 0.03160135453464001\n",
      "Iteration 123, Cost: 0.031248879842803175\n",
      "Iteration 124, Cost: 0.030902019414028523\n",
      "Iteration 125, Cost: 0.030560650368539114\n",
      "Iteration 126, Cost: 0.030224653702270225\n",
      "Iteration 127, Cost: 0.029893914109675714\n",
      "Iteration 128, Cost: 0.029568319811998225\n",
      "Iteration 129, Cost: 0.029247762391461075\n",
      "Iteration 130, Cost: 0.02893213663179842\n",
      "Iteration 131, Cost: 0.02862134036549192\n",
      "Iteration 132, Cost: 0.028315274328027377\n",
      "Iteration 133, Cost: 0.02801384201942467\n",
      "Iteration 134, Cost: 0.027716949573230128\n",
      "Iteration 135, Cost: 0.02742450563309335\n",
      "Iteration 136, Cost: 0.027136421236982327\n",
      "Iteration 137, Cost: 0.02685260970902245\n",
      "Iteration 138, Cost: 0.026572986558879037\n",
      "Iteration 139, Cost: 0.02629746938853957\n",
      "Iteration 140, Cost: 0.0260259778062934\n",
      "Iteration 141, Cost: 0.025758433347653306\n",
      "Iteration 142, Cost: 0.02549475940291647\n",
      "Iteration 143, Cost: 0.025234881151022577\n",
      "Iteration 144, Cost: 0.024978725499334123\n",
      "Iteration 145, Cost: 0.024726221028938983\n",
      "Iteration 146, Cost: 0.0244772979450576\n",
      "Iteration 147, Cost: 0.02423188803212674\n",
      "Iteration 148, Cost: 0.023989924613127742\n",
      "Iteration 149, Cost: 0.023751342512730072\n",
      "Iteration 150, Cost: 0.023516078023828168\n",
      "Iteration 151, Cost: 0.023284068877062993\n",
      "Iteration 152, Cost: 0.023055254212935743\n",
      "Iteration 153, Cost: 0.022829574556141704\n",
      "Iteration 154, Cost: 0.022606971791774084\n",
      "Iteration 155, Cost: 0.022387389143072572\n",
      "Iteration 156, Cost: 0.022170771150416393\n",
      "Iteration 157, Cost: 0.021957063651287696\n",
      "Iteration 158, Cost: 0.021746213760957495\n",
      "Iteration 159, Cost: 0.02153816985367178\n",
      "Iteration 160, Cost: 0.021332881544140633\n",
      "Iteration 161, Cost: 0.021130299669157207\n",
      "Iteration 162, Cost: 0.020930376269196197\n",
      "Iteration 163, Cost: 0.02073306456986297\n",
      "Iteration 164, Cost: 0.02053831896308433\n",
      "Iteration 165, Cost: 0.020346094987950977\n",
      "Iteration 166, Cost: 0.02015634931113804\n",
      "Iteration 167, Cost: 0.01996903970684629\n",
      "Iteration 168, Cost: 0.019784125036220724\n",
      "Iteration 169, Cost: 0.01960156522621593\n",
      "Iteration 170, Cost: 0.019421321247889776\n",
      "Iteration 171, Cost: 0.019243355094117398\n",
      "Iteration 172, Cost: 0.01906762975672719\n",
      "Iteration 173, Cost: 0.018894109203069415\n",
      "Iteration 174, Cost: 0.01872275835203599\n",
      "Iteration 175, Cost: 0.018553543049557406\n",
      "Iteration 176, Cost: 0.01838643004360957\n",
      "Iteration 177, Cost: 0.018221386958769403\n",
      "Iteration 178, Cost: 0.01805838227036394\n",
      "Iteration 179, Cost: 0.017897385278262845\n",
      "Iteration 180, Cost: 0.017738366080369182\n",
      "Iteration 181, Cost: 0.017581295545867577\n",
      "Iteration 182, Cost: 0.01742614528829303\n",
      "Iteration 183, Cost: 0.017272887638487104\n",
      "Iteration 184, Cost: 0.01712149561751129\n",
      "Iteration 185, Cost: 0.016971942909589775\n",
      "Iteration 186, Cost: 0.01682420383515605\n",
      "Iteration 187, Cost: 0.0166782533240789\n",
      "Iteration 188, Cost: 0.016534066889144244\n",
      "Iteration 189, Cost: 0.016391620599869254\n",
      "Iteration 190, Cost: 0.016250891056724576\n",
      "Iteration 191, Cost: 0.016111855365838944\n",
      "Iteration 192, Cost: 0.015974491114258704\n",
      "Iteration 193, Cost: 0.015838776345831414\n",
      "Iteration 194, Cost: 0.015704689537779523\n",
      "Iteration 195, Cost: 0.015572209578025633\n",
      "Iteration 196, Cost: 0.015441315743325976\n",
      "Iteration 197, Cost: 0.015311987678263273\n",
      "Iteration 198, Cost: 0.015184205375144064\n",
      "Iteration 199, Cost: 0.015057949154839247\n",
      "Iteration 200, Cost: 0.014933199648599575\n",
      "Iteration 201, Cost: 0.014809937780871153\n",
      "Iteration 202, Cost: 0.01468814475312849\n",
      "Iteration 203, Cost: 0.014567802028735637\n",
      "Iteration 204, Cost: 0.014448891318838857\n",
      "Iteration 205, Cost: 0.014331394569287143\n",
      "Iteration 206, Cost: 0.014215293948570422\n",
      "Iteration 207, Cost: 0.014100571836758778\n",
      "Iteration 208, Cost: 0.013987210815420209\n",
      "Iteration 209, Cost: 0.013875193658489004\n",
      "Iteration 210, Cost: 0.013764503324051948\n",
      "Iteration 211, Cost: 0.013655122947015243\n",
      "Iteration 212, Cost: 0.013547035832611404\n",
      "Iteration 213, Cost: 0.013440225450702363\n",
      "Iteration 214, Cost: 0.01333467543083244\n",
      "Iteration 215, Cost: 0.01323036955798327\n",
      "Iteration 216, Cost: 0.013127291768981312\n",
      "Iteration 217, Cost: 0.013025426149508168\n",
      "Iteration 218, Cost: 0.012924756931663579\n",
      "Iteration 219, Cost: 0.012825268492031635\n",
      "Iteration 220, Cost: 0.012726945350201274\n",
      "Iteration 221, Cost: 0.012629772167693544\n",
      "Iteration 222, Cost: 0.012533733747249693\n",
      "Iteration 223, Cost: 0.01243881503243579\n",
      "Iteration 224, Cost: 0.012345001107521967\n",
      "Iteration 225, Cost: 0.012252277197596322\n",
      "Iteration 226, Cost: 0.012160628668876308\n",
      "Iteration 227, Cost: 0.01207004102918254\n",
      "Iteration 228, Cost: 0.011980499928542906\n",
      "Iteration 229, Cost: 0.011891991159897206\n",
      "Iteration 230, Cost: 0.011804500659875162\n",
      "Iteration 231, Cost: 0.01171801450962328\n",
      "Iteration 232, Cost: 0.011632518935658344\n",
      "Iteration 233, Cost: 0.011548000310727873\n",
      "Iteration 234, Cost: 0.011464445154659938\n",
      "Iteration 235, Cost: 0.011381840135187093\n",
      "Iteration 236, Cost: 0.011300172068731044\n",
      "Iteration 237, Cost: 0.011219427921136702\n",
      "Iteration 238, Cost: 0.011139594808345853\n",
      "Iteration 239, Cost: 0.0110606599970025\n",
      "Iteration 240, Cost: 0.010982610904983285\n",
      "Iteration 241, Cost: 0.010905435101847705\n",
      "Iteration 242, Cost: 0.010829120309204283\n",
      "Iteration 243, Cost: 0.010753654400989628\n",
      "Iteration 244, Cost: 0.010679025403658539\n",
      "Iteration 245, Cost: 0.01060522149628411\n",
      "Iteration 246, Cost: 0.0105322310105675\n",
      "Iteration 247, Cost: 0.010460042430757687\n",
      "Iteration 248, Cost: 0.010388644393482204\n",
      "Iteration 249, Cost: 0.010318025687490228\n",
      "Iteration 250, Cost: 0.010248175253309863\n",
      "Iteration 251, Cost: 0.01017908218282175\n",
      "Iteration 252, Cost: 0.010110735718751542\n",
      "Iteration 253, Cost: 0.010043125254083811\n",
      "Iteration 254, Cost: 0.009976240331400358\n",
      "Iteration 255, Cost: 0.009910070642145875\n",
      "Iteration 256, Cost: 0.00984460602582402\n",
      "Iteration 257, Cost: 0.009779836469127197\n",
      "Iteration 258, Cost: 0.009715752105003063\n",
      "Iteration 259, Cost: 0.009652343211661172\n",
      "Iteration 260, Cost: 0.009589600211522794\n",
      "Iteration 261, Cost: 0.009527513670117263\n",
      "Iteration 262, Cost: 0.009466074294927844\n",
      "Iteration 263, Cost: 0.009405272934190348\n",
      "Iteration 264, Cost: 0.009345100575647438\n",
      "Iteration 265, Cost: 0.009285548345261671\n",
      "Iteration 266, Cost: 0.009226607505890128\n",
      "Iteration 267, Cost: 0.009168269455923419\n",
      "Iteration 268, Cost: 0.009110525727891918\n",
      "Iteration 269, Cost: 0.009053367987041703\n",
      "Iteration 270, Cost: 0.008996788029882935\n",
      "Iteration 271, Cost: 0.008940777782713032\n",
      "Iteration 272, Cost: 0.008885329300117081\n",
      "Iteration 273, Cost: 0.008830434763447793\n",
      "Iteration 274, Cost: 0.008776086479287187\n",
      "Iteration 275, Cost: 0.008722276877892185\n",
      "Iteration 276, Cost: 0.00866899851162611\n",
      "Iteration 277, Cost: 0.008616244053378096\n",
      "Iteration 278, Cost: 0.008564006294972296\n",
      "Iteration 279, Cost: 0.008512278145568668\n",
      "Iteration 280, Cost: 0.008461052630057125\n",
      "Iteration 281, Cost: 0.008410322887446624\n",
      "Iteration 282, Cost: 0.008360082169250896\n",
      "Iteration 283, Cost: 0.008310323837872207\n",
      "Iteration 284, Cost: 0.008261041364984711\n",
      "Iteration 285, Cost: 0.008212228329918678\n",
      "Iteration 286, Cost: 0.008163878418046989\n",
      "Iteration 287, Cost: 0.008115985419175104\n",
      "Iteration 288, Cost: 0.008068543225935691\n",
      "Iteration 289, Cost: 0.008021545832189074\n",
      "Iteration 290, Cost: 0.007974987331430552\n",
      "Iteration 291, Cost: 0.007928861915205592\n",
      "Iteration 292, Cost: 0.007883163871533885\n",
      "Iteration 293, Cost: 0.007837887583343127\n",
      "Iteration 294, Cost: 0.007793027526913437\n",
      "Iteration 295, Cost: 0.0077485782703331455\n",
      "Iteration 296, Cost: 0.007704534471966762\n",
      "Iteration 297, Cost: 0.007660890878935794\n",
      "Iteration 298, Cost: 0.007617642325613068\n",
      "Iteration 299, Cost: 0.007574783732131236\n",
      "Iteration 300, Cost: 0.007532310102905924\n",
      "Iteration 301, Cost: 0.007490216525174144\n",
      "Iteration 302, Cost: 0.0074484981675484166\n",
      "Iteration 303, Cost: 0.0074071502785870574\n",
      "Iteration 304, Cost: 0.007366168185381035\n",
      "Iteration 305, Cost: 0.007325547292157759\n",
      "Iteration 306, Cost: 0.007285283078902182\n",
      "Iteration 307, Cost: 0.007245371099995456\n",
      "Iteration 308, Cost: 0.007205806982871447\n",
      "Iteration 309, Cost: 0.007166586426691342\n",
      "Iteration 310, Cost: 0.007127705201036555\n",
      "Iteration 311, Cost: 0.007089159144620097\n",
      "Iteration 312, Cost: 0.007050944164016578\n",
      "Iteration 313, Cost: 0.007013056232410945\n",
      "Iteration 314, Cost: 0.0069754913883660705\n",
      "Iteration 315, Cost: 0.006938245734609254\n",
      "Iteration 316, Cost: 0.006901315436837682\n",
      "Iteration 317, Cost: 0.00686469672254288\n",
      "Iteration 318, Cost: 0.0068283858798541645\n",
      "Iteration 319, Cost: 0.006792379256401071\n",
      "Iteration 320, Cost: 0.006756673258194734\n",
      "Iteration 321, Cost: 0.00672126434852816\n",
      "Iteration 322, Cost: 0.006686149046895331\n",
      "Iteration 323, Cost: 0.006651323927929051\n",
      "Iteration 324, Cost: 0.006616785620357432\n",
      "Iteration 325, Cost: 0.0065825308059789195\n",
      "Iteration 326, Cost: 0.006548556218655714\n",
      "Iteration 327, Cost: 0.006514858643325454\n",
      "Iteration 328, Cost: 0.006481434915031039\n",
      "Iteration 329, Cost: 0.006448281917968356\n",
      "Iteration 330, Cost: 0.006415396584551845\n",
      "Iteration 331, Cost: 0.006382775894497619\n",
      "Iteration 332, Cost: 0.00635041687392403\n",
      "Iteration 333, Cost: 0.006318316594469433\n",
      "Iteration 334, Cost: 0.006286472172426969\n",
      "Iteration 335, Cost: 0.00625488076789615\n",
      "Iteration 336, Cost: 0.006223539583951059\n",
      "Iteration 337, Cost: 0.006192445865824866\n",
      "Iteration 338, Cost: 0.006161596900110548\n",
      "Iteration 339, Cost: 0.006130990013977474\n",
      "Iteration 340, Cost: 0.006100622574403715\n",
      "Iteration 341, Cost: 0.006070491987423775\n",
      "Iteration 342, Cost: 0.0060405956973915475\n",
      "Iteration 343, Cost: 0.006010931186258235\n",
      "Iteration 344, Cost: 0.005981495972865008\n",
      "Iteration 345, Cost: 0.005952287612250148\n",
      "Iteration 346, Cost: 0.005923303694970428\n",
      "Iteration 347, Cost: 0.0058945418464365065\n",
      "Iteration 348, Cost: 0.005865999726262081\n",
      "Iteration 349, Cost: 0.005837675027626532\n",
      "Iteration 350, Cost: 0.005809565476650869\n",
      "Iteration 351, Cost: 0.0057816688317866875\n",
      "Iteration 352, Cost: 0.005753982883217914\n",
      "Iteration 353, Cost: 0.005726505452275099\n",
      "Iteration 354, Cost: 0.005699234390862032\n",
      "Iteration 355, Cost: 0.005672167580894399\n",
      "Iteration 356, Cost: 0.005645302933750302\n",
      "Iteration 357, Cost: 0.005618638389732388\n",
      "Iteration 358, Cost: 0.005592171917541311\n",
      "Iteration 359, Cost: 0.005565901513760388\n",
      "Iteration 360, Cost: 0.005539825202351142\n",
      "Iteration 361, Cost: 0.00551394103415954\n",
      "Iteration 362, Cost: 0.005488247086432733\n",
      "Iteration 363, Cost: 0.0054627414623460244\n",
      "Iteration 364, Cost: 0.00543742229053989\n",
      "Iteration 365, Cost: 0.005412287724666835\n",
      "Iteration 366, Cost: 0.005387335942947867\n",
      "Iteration 367, Cost: 0.005362565147738387\n",
      "Iteration 368, Cost: 0.005337973565103288\n",
      "Iteration 369, Cost: 0.00531355944440108\n",
      "Iteration 370, Cost: 0.00528932105787683\n",
      "Iteration 371, Cost: 0.005265256700263717\n",
      "Iteration 372, Cost: 0.005241364688393042\n",
      "Iteration 373, Cost: 0.00521764336081247\n",
      "Iteration 374, Cost: 0.005194091077412358\n",
      "Iteration 375, Cost: 0.005170706219059961\n",
      "Iteration 376, Cost: 0.005147487187241346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 377, Cost: 0.005124432403710864\n",
      "Iteration 378, Cost: 0.005101540310147972\n",
      "Iteration 379, Cost: 0.005078809367821275\n",
      "Iteration 380, Cost: 0.005056238057259603\n",
      "Iteration 381, Cost: 0.0050338248779299615\n",
      "Iteration 382, Cost: 0.005011568347922233\n",
      "Iteration 383, Cost: 0.004989467003640417\n",
      "Converged!\n",
      "Accuracy Results:\n",
      "      Architecture  Regularization  Accuracy\n",
      "0   [13, 10, 8, 3]            0.01  1.000000\n",
      "1   [13, 10, 8, 3]            0.10  0.944444\n",
      "2   [13, 10, 8, 3]            1.00  1.000000\n",
      "3   [13, 10, 8, 3]            0.01  0.944444\n",
      "4   [13, 10, 8, 3]            0.10  0.944444\n",
      "5   [13, 10, 8, 3]            1.00  0.944444\n",
      "6   [13, 10, 8, 3]            0.01  1.000000\n",
      "7   [13, 10, 8, 3]            0.10  1.000000\n",
      "8   [13, 10, 8, 3]            1.00  1.000000\n",
      "9   [13, 10, 8, 3]            0.01  0.888889\n",
      "10  [13, 10, 8, 3]            0.10  0.888889\n",
      "11  [13, 10, 8, 3]            1.00  0.944444\n",
      "12  [13, 10, 8, 3]            0.01  0.888889\n",
      "13  [13, 10, 8, 3]            0.10  0.888889\n",
      "14  [13, 10, 8, 3]            1.00  0.944444\n",
      "15  [13, 10, 8, 3]            0.01  1.000000\n",
      "16  [13, 10, 8, 3]            0.10  0.888889\n",
      "17  [13, 10, 8, 3]            1.00  1.000000\n",
      "18  [13, 10, 8, 3]            0.01  1.000000\n",
      "19  [13, 10, 8, 3]            0.10  1.000000\n",
      "20  [13, 10, 8, 3]            1.00  1.000000\n",
      "21  [13, 10, 8, 3]            0.01  0.944444\n",
      "22  [13, 10, 8, 3]            0.10  1.000000\n",
      "23  [13, 10, 8, 3]            1.00  0.944444\n",
      "24  [13, 10, 8, 3]            0.01  0.941176\n",
      "25  [13, 10, 8, 3]            0.10  0.941176\n",
      "26  [13, 10, 8, 3]            1.00  1.000000\n",
      "27  [13, 10, 8, 3]            0.01  1.000000\n",
      "28  [13, 10, 8, 3]            0.10  1.000000\n",
      "29  [13, 10, 8, 3]            1.00  1.000000\n",
      "\n",
      "F1 Score Results:\n",
      "      Architecture  Regularization  F1 Score\n",
      "0   [13, 10, 8, 3]            0.01  1.000000\n",
      "1   [13, 10, 8, 3]            0.10  0.944444\n",
      "2   [13, 10, 8, 3]            1.00  1.000000\n",
      "3   [13, 10, 8, 3]            0.01  0.944444\n",
      "4   [13, 10, 8, 3]            0.10  0.944444\n",
      "5   [13, 10, 8, 3]            1.00  0.944444\n",
      "6   [13, 10, 8, 3]            0.01  1.000000\n",
      "7   [13, 10, 8, 3]            0.10  1.000000\n",
      "8   [13, 10, 8, 3]            1.00  1.000000\n",
      "9   [13, 10, 8, 3]            0.01  0.944444\n",
      "10  [13, 10, 8, 3]            0.10  0.941176\n",
      "11  [13, 10, 8, 3]            1.00  0.944444\n",
      "12  [13, 10, 8, 3]            0.01  0.888889\n",
      "13  [13, 10, 8, 3]            0.10  0.918919\n",
      "14  [13, 10, 8, 3]            1.00  0.972973\n",
      "15  [13, 10, 8, 3]            0.01  1.000000\n",
      "16  [13, 10, 8, 3]            0.10  0.941176\n",
      "17  [13, 10, 8, 3]            1.00  1.000000\n",
      "18  [13, 10, 8, 3]            0.01  1.000000\n",
      "19  [13, 10, 8, 3]            0.10  1.000000\n",
      "20  [13, 10, 8, 3]            1.00  1.000000\n",
      "21  [13, 10, 8, 3]            0.01  0.944444\n",
      "22  [13, 10, 8, 3]            0.10  1.000000\n",
      "23  [13, 10, 8, 3]            1.00  0.972973\n",
      "24  [13, 10, 8, 3]            0.01  0.941176\n",
      "25  [13, 10, 8, 3]            0.10  0.941176\n",
      "26  [13, 10, 8, 3]            1.00  1.000000\n",
      "27  [13, 10, 8, 3]            0.01  1.000000\n",
      "28  [13, 10, 8, 3]            0.10  1.000000\n",
      "29  [13, 10, 8, 3]            1.00  1.000000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Load dataset\n",
    "df_wine = pd.read_csv(\"/Users/noshitha/Downloads/hw3/datasets/hw3_wine.csv\", delimiter=\"\\t\")\n",
    "\n",
    "# Preprocess data\n",
    "X_wine = pd.get_dummies(df_wine.iloc[:, 1:])\n",
    "y_wine = df_wine.iloc[:, 0]\n",
    "\n",
    "# Normalize data\n",
    "y_wine_resized = y_wine.values.reshape(-1, 1)\n",
    "\n",
    "# Initialize the OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Fit and transform the target variable\n",
    "y_encoded = encoder.fit_transform(y_wine_resized)\n",
    "\n",
    "# Define model architectures and regularization parameters\n",
    "architectures = [\n",
    "    [X_wine.shape[1], 10, 8, 3]  # Example architecture\n",
    "]\n",
    "regularization_params = [0.01, 0.1, 1.0]  # Example regularization parameters\n",
    "\n",
    "# Initialize lists to store results\n",
    "results_accuracy = []\n",
    "results_f1_score = []\n",
    "\n",
    "# Perform stratified k-fold cross-validation\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "for train_index, test_index in skf.split(X_wine, y_wine):\n",
    "    X_train, X_test = X_wine.iloc[train_index], X_wine.iloc[test_index]\n",
    "    y_train, y_test = y_encoded[train_index], y_encoded[test_index]\n",
    "    \n",
    "    # Normalize training and test data manually\n",
    "    mean = np.mean(X_train, axis=0)\n",
    "    std = np.std(X_train, axis=0)\n",
    "    X_train_normalized = (X_train - mean) / std\n",
    "    X_test_normalized = (X_test - mean) / std\n",
    "    \n",
    "    for arch in architectures:\n",
    "        for lam in regularization_params:\n",
    "            # Instantiate the NeuralNetwork class with the desired architecture\n",
    "            model = NeuralNetwork(arch)\n",
    "\n",
    "            # Train the neural network\n",
    "            model.train(X_train_normalized, y_train, learning_rate=0.01, lam=lam, max_iterations=1000, epsilon=0.005)\n",
    "\n",
    "            # Evaluate the trained model\n",
    "            accuracy, f1_score = model.evaluate(X_test_normalized, y_test)\n",
    "            results_accuracy.append([arch, lam, accuracy])\n",
    "            results_f1_score.append([arch, lam, f1_score])\n",
    "\n",
    "# Create tables summarizing the results\n",
    "results_accuracy_df = pd.DataFrame(results_accuracy, columns=['Architecture', 'Regularization', 'Accuracy'])\n",
    "results_f1_score_df = pd.DataFrame(results_f1_score, columns=['Architecture', 'Regularization', 'F1 Score'])\n",
    "\n",
    "print(\"Accuracy Results:\")\n",
    "print(results_accuracy_df)\n",
    "\n",
    "print(\"\\nF1 Score Results:\")\n",
    "print(results_f1_score_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9606b510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged at cost :0.004995708281810879 while Epsilon:0.005 \n",
      "Converged at cost :0.004999099262411388 while Epsilon:0.005 \n",
      "Converged at cost :0.004996840279093582 while Epsilon:0.005 \n",
      "Converged at cost :0.004999214392216478 while Epsilon:0.005 \n",
      "Converged at cost :0.004995906779296381 while Epsilon:0.005 \n",
      "Converged at cost :0.004994926453779626 while Epsilon:0.005 \n",
      "Converged at cost :0.004990890854417779 while Epsilon:0.005 \n",
      "Converged at cost :0.00499267698706461 while Epsilon:0.005 \n",
      "Converged at cost :0.0049986943666625065 while Epsilon:0.005 \n",
      "Converged at cost :0.004996857724769206 while Epsilon:0.005 \n",
      "Converged at cost :0.004996479495347855 while Epsilon:0.005 \n",
      "Converged at cost :0.004992656352211297 while Epsilon:0.005 \n",
      "Converged at cost :0.004994749935123214 while Epsilon:0.005 \n",
      "Converged at cost :0.004991603167493854 while Epsilon:0.005 \n",
      "Converged at cost :0.004988878559649432 while Epsilon:0.005 \n",
      "Converged at cost :0.004992794932852111 while Epsilon:0.005 \n",
      "Converged at cost :0.004992477671584373 while Epsilon:0.005 \n",
      "Converged at cost :0.004988837557353163 while Epsilon:0.005 \n",
      "Converged at cost :0.004989799598394832 while Epsilon:0.005 \n",
      "Converged at cost :0.004996447529691293 while Epsilon:0.005 \n",
      "Converged at cost :0.0049885798940678405 while Epsilon:0.005 \n",
      "Converged at cost :0.004996598819628364 while Epsilon:0.005 \n",
      "Converged at cost :0.004986409059637224 while Epsilon:0.005 \n",
      "Converged at cost :0.004994859831840159 while Epsilon:0.005 \n",
      "Converged at cost :0.00498926578862623 while Epsilon:0.005 \n",
      "Converged at cost :0.004986896961204478 while Epsilon:0.005 \n",
      "Converged at cost :0.0049911728418423435 while Epsilon:0.005 \n",
      "Converged at cost :0.0049980422287007565 while Epsilon:0.005 \n",
      "Converged at cost :0.004984686223613351 while Epsilon:0.005 \n",
      "Converged at cost :0.004992202765624371 while Epsilon:0.005 \n",
      "Converged at cost :0.004992296628658405 while Epsilon:0.005 \n",
      "Converged at cost :0.0049887253586581234 while Epsilon:0.005 \n",
      "Converged at cost :0.0049843679455663925 while Epsilon:0.005 \n",
      "Converged at cost :0.004988074837610723 while Epsilon:0.005 \n",
      "Converged at cost :0.004999079937051119 while Epsilon:0.005 \n",
      "Converged at cost :0.004992481174335833 while Epsilon:0.005 \n",
      "Converged at cost :0.004984266271689652 while Epsilon:0.005 \n",
      "Converged at cost :0.004985314189284514 while Epsilon:0.005 \n",
      "Converged at cost :0.004991407653375531 while Epsilon:0.005 \n",
      "Converged at cost :0.0049897029954173965 while Epsilon:0.005 \n",
      "Converged at cost :0.004997660883706365 while Epsilon:0.005 \n",
      "Converged at cost :0.004999644122698693 while Epsilon:0.005 \n",
      "Converged at cost :0.004994647619640025 while Epsilon:0.005 \n",
      "Converged at cost :0.00499057851823011 while Epsilon:0.005 \n",
      "Converged at cost :0.004999006421767749 while Epsilon:0.005 \n",
      "Converged at cost :0.004995242612293727 while Epsilon:0.005 \n",
      "Converged at cost :0.004993813143483098 while Epsilon:0.005 \n",
      "Converged at cost :0.004990282184409343 while Epsilon:0.005 \n",
      "Converged at cost :0.004982612138133533 while Epsilon:0.005 \n",
      "Converged at cost :0.0049997749661927935 while Epsilon:0.005 \n",
      "Converged at cost :0.004989673408806045 while Epsilon:0.005 \n",
      "Converged at cost :0.004992183728946687 while Epsilon:0.005 \n",
      "Converged at cost :0.00499115060984251 while Epsilon:0.005 \n",
      "Converged at cost :0.0049775438056634015 while Epsilon:0.005 \n",
      "Converged at cost :0.004996576528923504 while Epsilon:0.005 \n",
      "Converged at cost :0.004982029056844992 while Epsilon:0.005 \n",
      "Converged at cost :0.0049945626953175415 while Epsilon:0.005 \n",
      "Converged at cost :0.004993237946328444 while Epsilon:0.005 \n",
      "Converged at cost :0.004993960739967868 while Epsilon:0.005 \n",
      "Converged at cost :0.004977723480991608 while Epsilon:0.005 \n",
      "Converged at cost :0.004995044791139512 while Epsilon:0.005 \n",
      "Converged at cost :0.0049810080967240674 while Epsilon:0.005 \n",
      "Converged at cost :0.0049802046009147375 while Epsilon:0.005 \n",
      "Converged at cost :0.00498868714524158 while Epsilon:0.005 \n",
      "Converged at cost :0.004988066590374411 while Epsilon:0.005 \n",
      "Converged at cost :0.00498452256966383 while Epsilon:0.005 \n",
      "Converged at cost :0.004984199456298278 while Epsilon:0.005 \n",
      "Converged at cost :0.004990979293897901 while Epsilon:0.005 \n",
      "Converged at cost :0.0049906359192253965 while Epsilon:0.005 \n",
      "Converged at cost :0.00497869230799284 while Epsilon:0.005 \n",
      "Converged at cost :0.0049966818960248205 while Epsilon:0.005 \n",
      "Converged at cost :0.0049919974041318154 while Epsilon:0.005 \n",
      "Converged at cost :0.0049987827808361494 while Epsilon:0.005 \n",
      "Converged at cost :0.004993890927669873 while Epsilon:0.005 \n",
      "Converged at cost :0.004994467500799256 while Epsilon:0.005 \n",
      "Converged at cost :0.004997473706384822 while Epsilon:0.005 \n",
      "Converged at cost :0.004994819377398598 while Epsilon:0.005 \n",
      "Converged at cost :0.004993865654879549 while Epsilon:0.005 \n",
      "Converged at cost :0.004992428308321422 while Epsilon:0.005 \n",
      "Converged at cost :0.004995719149146396 while Epsilon:0.005 \n",
      "Converged at cost :0.004989119419204759 while Epsilon:0.005 \n",
      "Converged at cost :0.004993555030884065 while Epsilon:0.005 \n",
      "Converged at cost :0.004989276880787726 while Epsilon:0.005 \n",
      "Converged at cost :0.004986357363788034 while Epsilon:0.005 \n",
      "Converged at cost :0.0049999602788447966 while Epsilon:0.005 \n",
      "Converged at cost :0.004991536334788517 while Epsilon:0.005 \n",
      "Converged at cost :0.004993965770391157 while Epsilon:0.005 \n",
      "Converged at cost :0.004996711190967221 while Epsilon:0.005 \n",
      "Converged at cost :0.004996640684551154 while Epsilon:0.005 \n",
      "Converged at cost :0.004993373655164497 while Epsilon:0.005 \n",
      "Converged at cost :0.004993388603821971 while Epsilon:0.005 \n",
      "Converged at cost :0.004990346666136007 while Epsilon:0.005 \n",
      "Converged at cost :0.004991935253616768 while Epsilon:0.005 \n",
      "Converged at cost :0.004991516286841937 while Epsilon:0.005 \n",
      "Converged at cost :0.004995792649434575 while Epsilon:0.005 \n",
      "Converged at cost :0.004991070551644771 while Epsilon:0.005 \n",
      "Converged at cost :0.004994951660259956 while Epsilon:0.005 \n",
      "Converged at cost :0.004994462158514418 while Epsilon:0.005 \n",
      "Converged at cost :0.0049977107341206365 while Epsilon:0.005 \n",
      "Converged at cost :0.004985207411632977 while Epsilon:0.005 \n",
      "Converged at cost :0.004991641154152518 while Epsilon:0.005 \n",
      "Converged at cost :0.004986175090055798 while Epsilon:0.005 \n",
      "Converged at cost :0.004998088805810075 while Epsilon:0.005 \n",
      "Converged at cost :0.004995927743529142 while Epsilon:0.005 \n",
      "Converged at cost :0.004997361276105801 while Epsilon:0.005 \n",
      "Converged at cost :0.004998564644757953 while Epsilon:0.005 \n",
      "Converged at cost :0.0049899507077991795 while Epsilon:0.005 \n",
      "Converged at cost :0.004992189454951193 while Epsilon:0.005 \n",
      "Converged at cost :0.004992581270303065 while Epsilon:0.005 \n",
      "Converged at cost :0.00499457203779426 while Epsilon:0.005 \n",
      "Converged at cost :0.0049885613023264875 while Epsilon:0.005 \n",
      "Converged at cost :0.004994147745731712 while Epsilon:0.005 \n",
      "Converged at cost :0.004989749539654468 while Epsilon:0.005 \n",
      "Converged at cost :0.004987970041746867 while Epsilon:0.005 \n",
      "Converged at cost :0.004979973933323134 while Epsilon:0.005 \n",
      "Converged at cost :0.004997572783730625 while Epsilon:0.005 \n",
      "Converged at cost :0.004999601952756673 while Epsilon:0.005 \n",
      "Converged at cost :0.004995218982048285 while Epsilon:0.005 \n",
      "Converged at cost :0.004988541645518815 while Epsilon:0.005 \n",
      "Converged at cost :0.004999982101478286 while Epsilon:0.005 \n",
      "Converged at cost :0.004992910980321729 while Epsilon:0.005 \n",
      "Converged at cost :0.004992626029987733 while Epsilon:0.005 \n",
      "Converged at cost :0.004984973195367744 while Epsilon:0.005 \n",
      "Converged at cost :0.004978551265563541 while Epsilon:0.005 \n",
      "Converged at cost :0.004986872228836 while Epsilon:0.005 \n",
      "Converged at cost :0.004995410280585132 while Epsilon:0.005 \n",
      "Converged at cost :0.004989241973365464 while Epsilon:0.005 \n",
      "Converged at cost :0.0049906269522769905 while Epsilon:0.005 \n",
      "Converged at cost :0.004990790458376632 while Epsilon:0.005 \n",
      "Converged at cost :0.0049871545279175054 while Epsilon:0.005 \n",
      "Converged at cost :0.0049854949230048375 while Epsilon:0.005 \n",
      "Converged at cost :0.0049845874638293946 while Epsilon:0.005 \n",
      "Converged at cost :0.004997505642137656 while Epsilon:0.005 \n",
      "Converged at cost :0.0049795038541628036 while Epsilon:0.005 \n",
      "Converged at cost :0.0049768970968614495 while Epsilon:0.005 \n",
      "Converged at cost :0.004984356887470767 while Epsilon:0.005 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged at cost :0.004981946334887664 while Epsilon:0.005 \n",
      "Converged at cost :0.0049764689331048256 while Epsilon:0.005 \n",
      "Converged at cost :0.004998996734937718 while Epsilon:0.005 \n",
      "Converged at cost :0.004995169430303731 while Epsilon:0.005 \n",
      "Converged at cost :0.004988518699668125 while Epsilon:0.005 \n",
      "Converged at cost :0.0049994873471272535 while Epsilon:0.005 \n",
      "Converged at cost :0.004996980950646415 while Epsilon:0.005 \n",
      "Converged at cost :0.00497815096111877 while Epsilon:0.005 \n",
      "Converged at cost :0.004990716440621963 while Epsilon:0.005 \n",
      "Converged at cost :0.004980103040422597 while Epsilon:0.005 \n",
      "Converged at cost :0.004995356898657715 while Epsilon:0.005 \n",
      "Converged at cost :0.0049992295575231735 while Epsilon:0.005 \n",
      "Converged at cost :0.004992179954380557 while Epsilon:0.005 \n",
      "Converged at cost :0.004997744763586371 while Epsilon:0.005 \n",
      "Converged at cost :0.004983347759113591 while Epsilon:0.005 \n",
      "Converged at cost :0.004994213491854746 while Epsilon:0.005 \n",
      "Converged at cost :0.004990871169585474 while Epsilon:0.005 \n",
      "Converged at cost :0.004978354297843992 while Epsilon:0.005 \n",
      "Converged at cost :0.004968972514953072 while Epsilon:0.005 \n",
      "Converged at cost :0.004982868617167669 while Epsilon:0.005 \n",
      "Converged at cost :0.004989922027918969 while Epsilon:0.005 \n",
      "Converged at cost :0.004983358376314961 while Epsilon:0.005 \n",
      "Converged at cost :0.004986564082262441 while Epsilon:0.005 \n",
      "Converged at cost :0.004983080610488819 while Epsilon:0.005 \n",
      "Converged at cost :0.0049919193757054295 while Epsilon:0.005 \n",
      "Converged at cost :0.004976312543956888 while Epsilon:0.005 \n",
      "Mean Accuracy Results:\n",
      "         Architecture, Lambda  Mean Accuracy\n",
      "0       ([13, 2, 2, 3], 0.01)       0.949673\n",
      "1        ([13, 2, 2, 3], 0.1)       0.938562\n",
      "2        ([13, 2, 2, 3], 1.0)       0.950000\n",
      "3       ([13, 5, 4, 3], 0.01)       0.971895\n",
      "4        ([13, 5, 4, 3], 0.1)       0.972222\n",
      "5        ([13, 5, 4, 3], 1.0)       0.972222\n",
      "6      ([13, 10, 8, 3], 0.01)       0.983333\n",
      "7       ([13, 10, 8, 3], 0.1)       0.972222\n",
      "8       ([13, 10, 8, 3], 1.0)       0.977778\n",
      "9          ([13, 5, 3], 0.01)       0.960784\n",
      "10          ([13, 5, 3], 0.1)       0.977778\n",
      "11          ([13, 5, 3], 1.0)       0.977778\n",
      "12      ([13, 5, 5, 3], 0.01)       0.972222\n",
      "13       ([13, 5, 5, 3], 0.1)       0.961111\n",
      "14       ([13, 5, 5, 3], 1.0)       0.977451\n",
      "15  ([13, 10, 8, 5, 3], 0.01)       0.971895\n",
      "16   ([13, 10, 8, 5, 3], 0.1)       0.977778\n",
      "17   ([13, 10, 8, 5, 3], 1.0)       0.972222\n",
      "\n",
      "Mean F1 Score Results:\n",
      "         Architecture, Lambda  Mean F1 Score\n",
      "0       ([13, 2, 2, 3], 0.01)       0.949673\n",
      "1        ([13, 2, 2, 3], 0.1)       0.938562\n",
      "2        ([13, 2, 2, 3], 1.0)       0.956061\n",
      "3       ([13, 5, 4, 3], 0.01)       0.974747\n",
      "4        ([13, 5, 4, 3], 0.1)       0.972222\n",
      "5        ([13, 5, 4, 3], 1.0)       0.974921\n",
      "6      ([13, 10, 8, 3], 0.01)       0.988885\n",
      "7       ([13, 10, 8, 3], 0.1)       0.977773\n",
      "8       ([13, 10, 8, 3], 1.0)       0.986178\n",
      "9          ([13, 5, 3], 0.01)       0.971887\n",
      "10          ([13, 5, 3], 0.1)       0.983016\n",
      "11          ([13, 5, 3], 1.0)       0.989451\n",
      "12      ([13, 5, 5, 3], 0.01)       0.977924\n",
      "13       ([13, 5, 5, 3], 0.1)       0.969507\n",
      "14       ([13, 5, 5, 3], 1.0)       0.985859\n",
      "15  ([13, 10, 8, 5, 3], 0.01)       0.974747\n",
      "16   ([13, 10, 8, 5, 3], 0.1)       0.983626\n",
      "17   ([13, 10, 8, 5, 3], 1.0)       0.974921\n",
      "\n",
      "Mean J cost Results:\n",
      "         Architecture, Lambda  Mean J Cost\n",
      "0       ([13, 2, 2, 3], 0.01)     0.018753\n",
      "1        ([13, 2, 2, 3], 0.1)     0.017366\n",
      "2        ([13, 2, 2, 3], 1.0)     0.015965\n",
      "3       ([13, 5, 4, 3], 0.01)     0.004992\n",
      "4        ([13, 5, 4, 3], 0.1)     0.004990\n",
      "5        ([13, 5, 4, 3], 1.0)     0.004991\n",
      "6      ([13, 10, 8, 3], 0.01)     0.004993\n",
      "7       ([13, 10, 8, 3], 0.1)     0.004988\n",
      "8       ([13, 10, 8, 3], 1.0)     0.004987\n",
      "9          ([13, 5, 3], 0.01)     0.004994\n",
      "10          ([13, 5, 3], 0.1)     0.004993\n",
      "11          ([13, 5, 3], 1.0)     0.004992\n",
      "12      ([13, 5, 5, 3], 0.01)     0.004994\n",
      "13       ([13, 5, 5, 3], 0.1)     0.004992\n",
      "14       ([13, 5, 5, 3], 1.0)     0.004987\n",
      "15  ([13, 10, 8, 5, 3], 0.01)     0.004988\n",
      "16   ([13, 10, 8, 5, 3], 0.1)     0.004991\n",
      "17   ([13, 10, 8, 5, 3], 1.0)     0.004983\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        self.weights = [np.random.randn(layers[i], layers[i+1]) for i in range(len(layers)-1)]\n",
    "        self.biases = [np.zeros((1, layers[i+1])) for i in range(len(layers)-1)]\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    def forward_pass(self, X):\n",
    "        activations = [X]\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(activations[-1], w) + b\n",
    "            activations.append(self.sigmoid(z))\n",
    "        return activations\n",
    "    \n",
    "    def backward_pass(self, X, Y, activations):\n",
    "        deltas = [(activations[-1] - Y) * self.sigmoid_derivative(activations[-1])]\n",
    "        for i in range(len(self.layers) - 2, 0, -1):\n",
    "            delta = np.dot(deltas[0], self.weights[i].T) * self.sigmoid_derivative(activations[i])\n",
    "            deltas.insert(0, delta)\n",
    "        return deltas\n",
    "    \n",
    "    def compute_gradients(self, activations, deltas):\n",
    "        gradients_weights = [np.dot(activations[i].T, deltas[i]) for i in range(len(self.layers) - 1)]\n",
    "        gradients_biases = [np.sum(deltas[i], axis=0) for i in range(len(self.layers) - 1)]\n",
    "        return gradients_weights, gradients_biases\n",
    "    \n",
    "    def update_weights(self, gradients_weights, gradients_biases, learning_rate):\n",
    "        self.weights = [w - learning_rate * gw for w, gw in zip(self.weights, gradients_weights)]\n",
    "        self.biases = [b - learning_rate * gb for b, gb in zip(self.biases, gradients_biases)]\n",
    "    \n",
    "    def train(self, X, Y, learning_rate, lam, max_iterations, epsilon):\n",
    "        for iteration in range(max_iterations):\n",
    "            activations = self.forward_pass(X)\n",
    "            deltas = self.backward_pass(X, Y, activations)\n",
    "            gradients_weights, gradients_biases = self.compute_gradients(activations, deltas)\n",
    "            self.update_weights(gradients_weights, gradients_biases, learning_rate)\n",
    "            # Compute cost function\n",
    "            J = np.mean(np.square(activations[-1] - Y))\n",
    "            #print(f\"Iteration {iteration+1}, Cost: {J}\")\n",
    "            # Check for convergence\n",
    "            if J < epsilon:\n",
    "                print(f\"Converged at cost :{J} while Epsilon:{epsilon} \")\n",
    "                return J\n",
    "        return J\n",
    "            \n",
    "\n",
    "    def accuracy(self, y_true, y_pred):\n",
    "        correct = np.sum(np.all(y_true == y_pred, axis=1))\n",
    "        return correct / len(y_true)\n",
    "\n",
    "\n",
    "    def f1_score(self, y_true, y_pred):\n",
    "        tp = np.sum(np.logical_and(y_true, y_pred))\n",
    "        fp = np.sum(np.logical_and(np.logical_not(y_true), y_pred))\n",
    "        fn = np.sum(np.logical_and(y_true, np.logical_not(y_pred)))\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        return f1\n",
    "\n",
    "\n",
    "    def evaluate(self, X_test, y_test, J):\n",
    "        activations = self.forward_pass(X_test)[-1]\n",
    "        y_pred = (activations > 0.5).astype(int)\n",
    "        acc = self.accuracy(y_test, y_pred)\n",
    "        f1 = self.f1_score(y_test, y_pred)\n",
    "        return J, acc, f1\n",
    "    \n",
    "    def k_fold_cross_validation(X, y, architectures, regularization_params, learning_rate, max_iterations, epsilon):\n",
    "        skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "        results_accuracy = {}\n",
    "        results_f1_score = {}\n",
    "\n",
    "        for arch in architectures:\n",
    "            for lam in regularization_params:\n",
    "                accuracy_list = []\n",
    "                f1_score_list = []\n",
    "                for train_index, test_index in skf.split(X, y):\n",
    "                    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "                    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "                    mean = np.mean(X_train, axis=0)\n",
    "                    std = np.std(X_train, axis=0)\n",
    "                    X_train_normalized = (X_train - mean) / std\n",
    "                    X_test_normalized = (X_test - mean) / std\n",
    "\n",
    "                    model = NeuralNetwork(arch)\n",
    "                    model.train(X_train_normalized, y_train, learning_rate=learning_rate, lam=lam, max_iterations=max_iterations, epsilon=epsilon)\n",
    "                    accuracy, f1_score = model.evaluate(X_test_normalized, y_test)\n",
    "                    accuracy_list.append(accuracy)\n",
    "                    f1_score_list.append(f1_score)\n",
    "\n",
    "                mean_accuracy = np.mean(accuracy_list)\n",
    "                mean_f1_score = np.mean(f1_score_list)\n",
    "\n",
    "                results_accuracy[(str(arch), lam)] = mean_accuracy\n",
    "                results_f1_score[(str(arch), lam)] = mean_f1_score\n",
    "\n",
    "        return results_accuracy, results_f1_score\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "df_wine = pd.read_csv(\"/Users/noshitha/Downloads/hw3/datasets/hw3_wine.csv\", delimiter=\"\\t\")\n",
    "\n",
    "# Preprocess data\n",
    "X_wine = pd.get_dummies(df_wine.iloc[:, 1:])\n",
    "y_wine = df_wine.iloc[:, 0]\n",
    "\n",
    "# Normalize data\n",
    "y_wine_resized = y_wine.values.reshape(-1, 1)\n",
    "\n",
    "# Initialize the OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Fit and transform the target variable\n",
    "y_encoded = encoder.fit_transform(y_wine_resized)\n",
    "\n",
    "# Define model architectures and regularization parameters\n",
    "architectures = [\n",
    "    [X_wine.shape[1], 2, 2, 3],  # Architecture with fewer neurons\n",
    "    [X_wine.shape[1], 5, 4, 3],  # Architecture with a moderate number of neurons\n",
    "    [X_wine.shape[1], 10, 8, 3],  # Architecture with a higher number of neurons\n",
    "    [X_wine.shape[1], 5, 3],  # Architecture with fewer layers\n",
    "    [X_wine.shape[1], 5, 5, 3],  # Architecture with more layers\n",
    "    [X_wine.shape[1], 10, 8, 5, 3]  # Architecture with more layers and neurons\n",
    "]\n",
    "\n",
    "regularization_params = [0.01, 0.1, 1.0]  # Example regularization parameters\n",
    "\n",
    "# Initialize lists to store results\n",
    "results_accuracy = {}\n",
    "results_f1_score = {}\n",
    "results_J_cost = {}\n",
    "\n",
    "# Perform stratified k-fold cross-validation\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "for arch in architectures:\n",
    "        for lam in regularization_params:\n",
    "            accuracy_list = []\n",
    "            f1_score_list = []\n",
    "            J_list = []\n",
    "            for train_index, test_index in skf.split(X_wine, y_wine):\n",
    "                X_train, X_test = X_wine.iloc[train_index], X_wine.iloc[test_index]\n",
    "                y_train, y_test = y_encoded[train_index], y_encoded[test_index]\n",
    "\n",
    "                mean = np.mean(X_train, axis=0)\n",
    "                std = np.std(X_train, axis=0)\n",
    "                X_train_normalized = (X_train - mean) / std\n",
    "                X_test_normalized = (X_test - mean) / std\n",
    "\n",
    "                model = NeuralNetwork(arch)\n",
    "                J = model.train(X_train_normalized, y_train, learning_rate=0.01, lam=lam, max_iterations=1000, epsilon=0.005)\n",
    "                J, accuracy, f1_score = model.evaluate(X_test_normalized, y_test, J)\n",
    "                accuracy_list.append(accuracy)\n",
    "                f1_score_list.append(f1_score)\n",
    "                J_list.append(J)\n",
    "\n",
    "            mean_accuracy = np.mean(accuracy_list)\n",
    "            mean_f1_score = np.mean(f1_score_list)\n",
    "            mean_J_cost   = np.mean(J_list)\n",
    "            \n",
    "            results_accuracy[(str(arch), lam)] = mean_accuracy\n",
    "            results_f1_score[(str(arch), lam)] = mean_f1_score\n",
    "            results_J_cost[(str(arch), lam)] = mean_J_cost\n",
    "\n",
    "\n",
    "# Convert the results into a DataFrame for tabular representation\n",
    "accuracy_df = pd.DataFrame(list(results_accuracy.items()), columns=['Architecture, Lambda', 'Mean Accuracy'])\n",
    "f1_score_df = pd.DataFrame(list(results_f1_score.items()), columns=['Architecture, Lambda', 'Mean F1 Score'])\n",
    "J_cost_df = pd.DataFrame(list(results_J_cost.items()), columns=['Architecture, Lambda', 'Mean J Cost'])\n",
    "\n",
    "print(\"Mean Accuracy Results:\")\n",
    "print(accuracy_df)\n",
    "print(\"\\nMean F1 Score Results:\")\n",
    "print(f1_score_df)\n",
    "print(\"\\nMean J cost Results:\")\n",
    "print(J_cost_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d4d1a5e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Results:\n",
      "         Architecture, Lambda  Mean Accuracy\n",
      "0       ([13, 2, 2, 3], 0.01)       0.949673\n",
      "1        ([13, 2, 2, 3], 0.1)       0.938562\n",
      "2        ([13, 2, 2, 3], 1.0)       0.950000\n",
      "3       ([13, 5, 4, 3], 0.01)       0.971895\n",
      "4        ([13, 5, 4, 3], 0.1)       0.972222\n",
      "5        ([13, 5, 4, 3], 1.0)       0.972222\n",
      "6      ([13, 10, 8, 3], 0.01)       0.983333\n",
      "7       ([13, 10, 8, 3], 0.1)       0.972222\n",
      "8       ([13, 10, 8, 3], 1.0)       0.977778\n",
      "9          ([13, 5, 3], 0.01)       0.960784\n",
      "10          ([13, 5, 3], 0.1)       0.977778\n",
      "11          ([13, 5, 3], 1.0)       0.977778\n",
      "12      ([13, 5, 5, 3], 0.01)       0.972222\n",
      "13       ([13, 5, 5, 3], 0.1)       0.961111\n",
      "14       ([13, 5, 5, 3], 1.0)       0.977451\n",
      "15  ([13, 10, 8, 5, 3], 0.01)       0.971895\n",
      "16   ([13, 10, 8, 5, 3], 0.1)       0.977778\n",
      "17   ([13, 10, 8, 5, 3], 1.0)       0.972222\n"
     ]
    }
   ],
   "source": [
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "print(\"Accuracy Results:\")\n",
    "print(accuracy_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "197f644f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "F1 Score Results:\n",
      "         Architecture, Lambda  Mean F1 Score\n",
      "0       ([13, 2, 2, 3], 0.01)       0.949673\n",
      "1        ([13, 2, 2, 3], 0.1)       0.938562\n",
      "2        ([13, 2, 2, 3], 1.0)       0.956061\n",
      "3       ([13, 5, 4, 3], 0.01)       0.974747\n",
      "4        ([13, 5, 4, 3], 0.1)       0.972222\n",
      "5        ([13, 5, 4, 3], 1.0)       0.974921\n",
      "6      ([13, 10, 8, 3], 0.01)       0.988885\n",
      "7       ([13, 10, 8, 3], 0.1)       0.977773\n",
      "8       ([13, 10, 8, 3], 1.0)       0.986178\n",
      "9          ([13, 5, 3], 0.01)       0.971887\n",
      "10          ([13, 5, 3], 0.1)       0.983016\n",
      "11          ([13, 5, 3], 1.0)       0.989451\n",
      "12      ([13, 5, 5, 3], 0.01)       0.977924\n",
      "13       ([13, 5, 5, 3], 0.1)       0.969507\n",
      "14       ([13, 5, 5, 3], 1.0)       0.985859\n",
      "15  ([13, 10, 8, 5, 3], 0.01)       0.974747\n",
      "16   ([13, 10, 8, 5, 3], 0.1)       0.983626\n",
      "17   ([13, 10, 8, 5, 3], 1.0)       0.974921\n"
     ]
    }
   ],
   "source": [
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(\"\\nF1 Score Results:\")\n",
    "print(f1_score_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8c679a",
   "metadata": {},
   "source": [
    "Regularization Parameter:\n",
    "Changing the regularization parameter had a significant impact on performance. Lower regularization parameters generally led to better performance, as they allow the model to fit the training data more closely. However, extremely low regularization might result in overfitting, reducing performance on unseen data.\n",
    "Adding More Layers:\n",
    "Adding more layers sometimes improved performance, especially when the added layers captured important patterns in the data. However, excessively deep networks might suffer from vanishing gradients or overfitting, leading to poorer performance. It's important to find the right balance between depth and complexity.\n",
    "Deeper Networks with Many Layers but Few Neurons per Layer:\n",
    "Deeper networks with few neurons per layer might help capture hierarchical features in the data. However, if each layer has too few neurons, the network might struggle to learn complex patterns, leading to underfitting.\n",
    "Designing Networks with Few Layers but Many Neurons per Layer:\n",
    "Networks with few layers but many neurons per layer might quickly learn complex patterns due to the high representational capacity of each layer. However, they might also overfit the training data if not regularized properly.\n",
    "Patterns:\n",
    "The performance improvement due to increasing complexity (more layers or neurons) is not linear. There's a point of diminishing returns where adding more complexity no longer improves performance and might even degrade it due to overfitting.\n",
    "Regularization plays a crucial role in preventing overfitting, especially as models become more complex. It helps generalize better to unseen data by penalizing overly complex models.\n",
    "Selection of Neural Network Architecture:\n",
    "Given the observations, a neural network architecture that strikes a balance between depth and width, along with moderate regularization, would be preferred for deployment in real life.\n",
    "I would select an architecture that has demonstrated consistently high performance across different regularization parameters, such as [13, 5, 4, 3] with a moderate regularization parameter around 0.01.\n",
    "This architecture shows good generalization performance without overly complicating the model, making it more robust for real-world applications.\n",
    "Regarding constructing larger networks, there's indeed a point where increasing complexity no longer improves performance. Beyond this point, larger networks might suffer from overfitting or computational inefficiency without providing significant performance gains. It's essential to balance model complexity with generalization performance and computational resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086ac69f",
   "metadata": {},
   "source": [
    "### MINI BATCH NEURAL NETS:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e16bf043",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Function to generate mini-batches\n",
    "def generate_mini_batches(X, y, batch_size):\n",
    "    num_samples = X.shape[0]\n",
    "    num_batches = num_samples // batch_size\n",
    "    mini_batches = []\n",
    "    shuffled_indices = np.random.permutation(num_samples)\n",
    "    X_shuffled = X[shuffled_indices]\n",
    "    y_shuffled = y[shuffled_indices]\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        mini_batches.append((X_shuffled[start_idx:end_idx], y_shuffled[start_idx:end_idx]))\n",
    "    if num_samples % batch_size != 0:\n",
    "        mini_batches.append((X_shuffled[num_batches*batch_size:], y_shuffled[num_batches*batch_size:]))\n",
    "    return mini_batches\n",
    "\n",
    "\n",
    "def train_mini_batch(X_train, y_train, X_test, y_test, model, learning_rate, batch_size, max_iterations, epsilon):\n",
    "    training_errors = []\n",
    "    testing_errors = []\n",
    "    for iteration in range(max_iterations):\n",
    "        mini_batches = generate_mini_batches(X_train, y_train, batch_size)\n",
    "        for mini_batch in mini_batches:\n",
    "            X_mini_batch, y_mini_batch = mini_batch\n",
    "            J= model.train(X_mini_batch, y_mini_batch, learning_rate=learning_rate, lam=lam, max_iterations=1, epsilon=epsilon)\n",
    "        training_cost,train_accuracy,train_f1_Score = model.evaluate(X_train, y_train,J)  # Compute training cost\n",
    "        testing_cost,test_accuracy,test_f1_Score = model.evaluate(X_test, y_test, J)  # Compute testing cost\n",
    "        training_errors.append(training_cost)\n",
    "        testing_errors.append(testing_cost)\n",
    "        print(f\"Iteration {iteration+1}, Training Cost: {training_cost}, Testing Cost: {testing_cost}\")\n",
    "        # Check for convergence\n",
    "        if training_cost < epsilon:\n",
    "            print(f\"Converged at training cost :{training_cost} while Epsilon:{epsilon} \")\n",
    "            break\n",
    "    return training_errors, testing_errors\n",
    "\n",
    "# Plot learning curve\n",
    "def plot_learning_curve(training_errors, testing_errors, step_size):\n",
    "    iterations = range(1, len(training_errors) + 1)\n",
    "    plt.plot(iterations, training_errors, label='Training Error')\n",
    "    plt.plot(iterations, testing_errors, label='Testing Error')\n",
    "    plt.title('Learning Curve')\n",
    "    plt.xlabel('Number of Training Examples')\n",
    "    plt.ylabel('Error (J)')\n",
    "    plt.xticks(np.arange(1, len(training_errors) + 1, step=step_size))\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5dd2baab",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, Training Cost: 0.23100717469946455, Testing Cost: 0.23100717469946455\n",
      "Iteration 2, Training Cost: 0.25096612972952104, Testing Cost: 0.25096612972952104\n",
      "Iteration 3, Training Cost: 0.22585240167468304, Testing Cost: 0.22585240167468304\n",
      "Iteration 4, Training Cost: 0.21661595802336214, Testing Cost: 0.21661595802336214\n",
      "Iteration 5, Training Cost: 0.21069799458847788, Testing Cost: 0.21069799458847788\n",
      "Iteration 6, Training Cost: 0.23284091140729496, Testing Cost: 0.23284091140729496\n",
      "Iteration 7, Training Cost: 0.2003974943186537, Testing Cost: 0.2003974943186537\n",
      "Iteration 8, Training Cost: 0.20729116551739535, Testing Cost: 0.20729116551739535\n",
      "Iteration 9, Training Cost: 0.21207291223696795, Testing Cost: 0.21207291223696795\n",
      "Iteration 10, Training Cost: 0.19107267749077206, Testing Cost: 0.19107267749077206\n",
      "Iteration 11, Training Cost: 0.19536862470877722, Testing Cost: 0.19536862470877722\n",
      "Iteration 12, Training Cost: 0.20048521712001366, Testing Cost: 0.20048521712001366\n",
      "Iteration 13, Training Cost: 0.20135867132062413, Testing Cost: 0.20135867132062413\n",
      "Iteration 14, Training Cost: 0.1972068016118833, Testing Cost: 0.1972068016118833\n",
      "Iteration 15, Training Cost: 0.2145355662793713, Testing Cost: 0.2145355662793713\n",
      "Iteration 16, Training Cost: 0.21359483904169435, Testing Cost: 0.21359483904169435\n",
      "Iteration 17, Training Cost: 0.20592072740271233, Testing Cost: 0.20592072740271233\n",
      "Iteration 18, Training Cost: 0.204693546514499, Testing Cost: 0.204693546514499\n",
      "Iteration 19, Training Cost: 0.22210332320160805, Testing Cost: 0.22210332320160805\n",
      "Iteration 20, Training Cost: 0.21550896264434483, Testing Cost: 0.21550896264434483\n",
      "Iteration 21, Training Cost: 0.19283266209331312, Testing Cost: 0.19283266209331312\n",
      "Iteration 22, Training Cost: 0.21441539775643387, Testing Cost: 0.21441539775643387\n",
      "Iteration 23, Training Cost: 0.20974600487625855, Testing Cost: 0.20974600487625855\n",
      "Iteration 24, Training Cost: 0.21301365544735382, Testing Cost: 0.21301365544735382\n",
      "Iteration 25, Training Cost: 0.19674231596180464, Testing Cost: 0.19674231596180464\n",
      "Iteration 26, Training Cost: 0.20439772970974757, Testing Cost: 0.20439772970974757\n",
      "Iteration 27, Training Cost: 0.20897437016123624, Testing Cost: 0.20897437016123624\n",
      "Iteration 28, Training Cost: 0.18708729982769493, Testing Cost: 0.18708729982769493\n",
      "Iteration 29, Training Cost: 0.20232886050070475, Testing Cost: 0.20232886050070475\n",
      "Iteration 30, Training Cost: 0.1890773386287916, Testing Cost: 0.1890773386287916\n",
      "Iteration 31, Training Cost: 0.17525945958857625, Testing Cost: 0.17525945958857625\n",
      "Iteration 32, Training Cost: 0.21270200929471975, Testing Cost: 0.21270200929471975\n",
      "Iteration 33, Training Cost: 0.18803360712673486, Testing Cost: 0.18803360712673486\n",
      "Iteration 34, Training Cost: 0.19083750284610992, Testing Cost: 0.19083750284610992\n",
      "Iteration 35, Training Cost: 0.18985736531087238, Testing Cost: 0.18985736531087238\n",
      "Iteration 36, Training Cost: 0.19266751966781034, Testing Cost: 0.19266751966781034\n",
      "Iteration 37, Training Cost: 0.17884861560225923, Testing Cost: 0.17884861560225923\n",
      "Iteration 38, Training Cost: 0.19178192949259762, Testing Cost: 0.19178192949259762\n",
      "Iteration 39, Training Cost: 0.1902413444177998, Testing Cost: 0.1902413444177998\n",
      "Iteration 40, Training Cost: 0.20161562456297094, Testing Cost: 0.20161562456297094\n",
      "Iteration 41, Training Cost: 0.19655016346195797, Testing Cost: 0.19655016346195797\n",
      "Iteration 42, Training Cost: 0.19349772897428355, Testing Cost: 0.19349772897428355\n",
      "Iteration 43, Training Cost: 0.2087933565665811, Testing Cost: 0.2087933565665811\n",
      "Iteration 44, Training Cost: 0.21223542608475962, Testing Cost: 0.21223542608475962\n",
      "Iteration 45, Training Cost: 0.20391492975216688, Testing Cost: 0.20391492975216688\n",
      "Iteration 46, Training Cost: 0.17862896300280415, Testing Cost: 0.17862896300280415\n",
      "Iteration 47, Training Cost: 0.1725533660690814, Testing Cost: 0.1725533660690814\n",
      "Iteration 48, Training Cost: 0.1895875370906181, Testing Cost: 0.1895875370906181\n",
      "Iteration 49, Training Cost: 0.18702626483907364, Testing Cost: 0.18702626483907364\n",
      "Iteration 50, Training Cost: 0.16558724371572645, Testing Cost: 0.16558724371572645\n",
      "Iteration 51, Training Cost: 0.18036504473361575, Testing Cost: 0.18036504473361575\n",
      "Iteration 52, Training Cost: 0.19669285252774443, Testing Cost: 0.19669285252774443\n",
      "Iteration 53, Training Cost: 0.15960684164752628, Testing Cost: 0.15960684164752628\n",
      "Iteration 54, Training Cost: 0.17607160224449328, Testing Cost: 0.17607160224449328\n",
      "Iteration 55, Training Cost: 0.15113307334977624, Testing Cost: 0.15113307334977624\n",
      "Iteration 56, Training Cost: 0.16037765454999425, Testing Cost: 0.16037765454999425\n",
      "Iteration 57, Training Cost: 0.1758917863036132, Testing Cost: 0.1758917863036132\n",
      "Iteration 58, Training Cost: 0.15617829233077116, Testing Cost: 0.15617829233077116\n",
      "Iteration 59, Training Cost: 0.13800475074755236, Testing Cost: 0.13800475074755236\n",
      "Iteration 60, Training Cost: 0.11559405785732377, Testing Cost: 0.11559405785732377\n",
      "Iteration 61, Training Cost: 0.15524063898619614, Testing Cost: 0.15524063898619614\n",
      "Iteration 62, Training Cost: 0.1653683531909542, Testing Cost: 0.1653683531909542\n",
      "Iteration 63, Training Cost: 0.1323573660195471, Testing Cost: 0.1323573660195471\n",
      "Iteration 64, Training Cost: 0.13211232705995282, Testing Cost: 0.13211232705995282\n",
      "Iteration 65, Training Cost: 0.1630408409105613, Testing Cost: 0.1630408409105613\n",
      "Iteration 66, Training Cost: 0.14591614122335508, Testing Cost: 0.14591614122335508\n",
      "Iteration 67, Training Cost: 0.1369815513476014, Testing Cost: 0.1369815513476014\n",
      "Iteration 68, Training Cost: 0.1667281548890523, Testing Cost: 0.1667281548890523\n",
      "Iteration 69, Training Cost: 0.1333004562071189, Testing Cost: 0.1333004562071189\n",
      "Iteration 70, Training Cost: 0.15153524998926032, Testing Cost: 0.15153524998926032\n",
      "Iteration 71, Training Cost: 0.13960727505253034, Testing Cost: 0.13960727505253034\n",
      "Iteration 72, Training Cost: 0.1522791101393729, Testing Cost: 0.1522791101393729\n",
      "Iteration 73, Training Cost: 0.11891894525025816, Testing Cost: 0.11891894525025816\n",
      "Iteration 74, Training Cost: 0.1509145854123634, Testing Cost: 0.1509145854123634\n",
      "Iteration 75, Training Cost: 0.11128134649040099, Testing Cost: 0.11128134649040099\n",
      "Iteration 76, Training Cost: 0.15185652801064736, Testing Cost: 0.15185652801064736\n",
      "Iteration 77, Training Cost: 0.1326973740161806, Testing Cost: 0.1326973740161806\n",
      "Iteration 78, Training Cost: 0.1336456633020465, Testing Cost: 0.1336456633020465\n",
      "Iteration 79, Training Cost: 0.11872228461044441, Testing Cost: 0.11872228461044441\n",
      "Iteration 80, Training Cost: 0.138771855188181, Testing Cost: 0.138771855188181\n",
      "Iteration 81, Training Cost: 0.11422118182883362, Testing Cost: 0.11422118182883362\n",
      "Iteration 82, Training Cost: 0.1364905209741068, Testing Cost: 0.1364905209741068\n",
      "Iteration 83, Training Cost: 0.15263380923000025, Testing Cost: 0.15263380923000025\n",
      "Iteration 84, Training Cost: 0.10908022154610221, Testing Cost: 0.10908022154610221\n",
      "Iteration 85, Training Cost: 0.09839496301542427, Testing Cost: 0.09839496301542427\n",
      "Iteration 86, Training Cost: 0.12076534993402468, Testing Cost: 0.12076534993402468\n",
      "Iteration 87, Training Cost: 0.10432835640770795, Testing Cost: 0.10432835640770795\n",
      "Iteration 88, Training Cost: 0.11716777429460884, Testing Cost: 0.11716777429460884\n",
      "Iteration 89, Training Cost: 0.12012411724344092, Testing Cost: 0.12012411724344092\n",
      "Iteration 90, Training Cost: 0.09126917582172969, Testing Cost: 0.09126917582172969\n",
      "Iteration 91, Training Cost: 0.1146934764873195, Testing Cost: 0.1146934764873195\n",
      "Iteration 92, Training Cost: 0.09950697396749429, Testing Cost: 0.09950697396749429\n",
      "Iteration 93, Training Cost: 0.08306780744686917, Testing Cost: 0.08306780744686917\n",
      "Iteration 94, Training Cost: 0.0794240160706378, Testing Cost: 0.0794240160706378\n",
      "Iteration 95, Training Cost: 0.09865889775842167, Testing Cost: 0.09865889775842167\n",
      "Iteration 96, Training Cost: 0.10678948520375846, Testing Cost: 0.10678948520375846\n",
      "Iteration 97, Training Cost: 0.11433314995974636, Testing Cost: 0.11433314995974636\n",
      "Iteration 98, Training Cost: 0.07860328971291942, Testing Cost: 0.07860328971291942\n",
      "Iteration 99, Training Cost: 0.10311516050054918, Testing Cost: 0.10311516050054918\n",
      "Iteration 100, Training Cost: 0.07498046215312253, Testing Cost: 0.07498046215312253\n",
      "Iteration 101, Training Cost: 0.1109943021825092, Testing Cost: 0.1109943021825092\n",
      "Iteration 102, Training Cost: 0.0892547968887169, Testing Cost: 0.0892547968887169\n",
      "Iteration 103, Training Cost: 0.07184581550536671, Testing Cost: 0.07184581550536671\n",
      "Iteration 104, Training Cost: 0.08963895971460933, Testing Cost: 0.08963895971460933\n",
      "Iteration 105, Training Cost: 0.08251186105524681, Testing Cost: 0.08251186105524681\n",
      "Iteration 106, Training Cost: 0.09329399209059883, Testing Cost: 0.09329399209059883\n",
      "Iteration 107, Training Cost: 0.07719104294530177, Testing Cost: 0.07719104294530177\n",
      "Iteration 108, Training Cost: 0.07326683075222482, Testing Cost: 0.07326683075222482\n",
      "Iteration 109, Training Cost: 0.0772025711061772, Testing Cost: 0.0772025711061772\n",
      "Iteration 110, Training Cost: 0.08016732458245565, Testing Cost: 0.08016732458245565\n",
      "Iteration 111, Training Cost: 0.07992406394530725, Testing Cost: 0.07992406394530725\n",
      "Iteration 112, Training Cost: 0.07154284850472285, Testing Cost: 0.07154284850472285\n",
      "Iteration 113, Training Cost: 0.08279341251544313, Testing Cost: 0.08279341251544313\n",
      "Iteration 114, Training Cost: 0.08094045586041847, Testing Cost: 0.08094045586041847\n",
      "Iteration 115, Training Cost: 0.08311272345026963, Testing Cost: 0.08311272345026963\n",
      "Iteration 116, Training Cost: 0.07316966488805283, Testing Cost: 0.07316966488805283\n",
      "Iteration 117, Training Cost: 0.08096512522782683, Testing Cost: 0.08096512522782683\n",
      "Iteration 118, Training Cost: 0.054956960793643095, Testing Cost: 0.054956960793643095\n",
      "Iteration 119, Training Cost: 0.07112333448293377, Testing Cost: 0.07112333448293377\n",
      "Iteration 120, Training Cost: 0.05778527356305422, Testing Cost: 0.05778527356305422\n",
      "Iteration 121, Training Cost: 0.07195924431827645, Testing Cost: 0.07195924431827645\n",
      "Iteration 122, Training Cost: 0.05580628684521062, Testing Cost: 0.05580628684521062\n",
      "Iteration 123, Training Cost: 0.0527293692249314, Testing Cost: 0.0527293692249314\n",
      "Iteration 124, Training Cost: 0.07093341422464598, Testing Cost: 0.07093341422464598\n",
      "Iteration 125, Training Cost: 0.05312015774088833, Testing Cost: 0.05312015774088833\n",
      "Iteration 126, Training Cost: 0.0670927230275421, Testing Cost: 0.0670927230275421\n",
      "Iteration 127, Training Cost: 0.06549241831245096, Testing Cost: 0.06549241831245096\n",
      "Iteration 128, Training Cost: 0.056277401069978576, Testing Cost: 0.056277401069978576\n",
      "Iteration 129, Training Cost: 0.056946697441877366, Testing Cost: 0.056946697441877366\n",
      "Iteration 130, Training Cost: 0.04637986251909447, Testing Cost: 0.04637986251909447\n",
      "Iteration 131, Training Cost: 0.05130472832427272, Testing Cost: 0.05130472832427272\n",
      "Iteration 132, Training Cost: 0.04126385321924451, Testing Cost: 0.04126385321924451\n",
      "Iteration 133, Training Cost: 0.06402510198539135, Testing Cost: 0.06402510198539135\n",
      "Iteration 134, Training Cost: 0.07044992939975425, Testing Cost: 0.07044992939975425\n",
      "Iteration 135, Training Cost: 0.06343960768978775, Testing Cost: 0.06343960768978775\n",
      "Iteration 136, Training Cost: 0.06157091989400949, Testing Cost: 0.06157091989400949\n",
      "Iteration 137, Training Cost: 0.050537654331082316, Testing Cost: 0.050537654331082316\n",
      "Iteration 138, Training Cost: 0.0793377943450514, Testing Cost: 0.0793377943450514\n",
      "Iteration 139, Training Cost: 0.03714386489044956, Testing Cost: 0.03714386489044956\n",
      "Iteration 140, Training Cost: 0.04100869898907355, Testing Cost: 0.04100869898907355\n",
      "Iteration 141, Training Cost: 0.08156379452696243, Testing Cost: 0.08156379452696243\n",
      "Iteration 142, Training Cost: 0.039301532555428155, Testing Cost: 0.039301532555428155\n",
      "Iteration 143, Training Cost: 0.0810124452863192, Testing Cost: 0.0810124452863192\n",
      "Iteration 144, Training Cost: 0.04061874002964082, Testing Cost: 0.04061874002964082\n",
      "Iteration 145, Training Cost: 0.031024115077215645, Testing Cost: 0.031024115077215645\n",
      "Iteration 146, Training Cost: 0.03342795348619444, Testing Cost: 0.03342795348619444\n",
      "Iteration 147, Training Cost: 0.06348705140842585, Testing Cost: 0.06348705140842585\n",
      "Iteration 148, Training Cost: 0.03848582035268424, Testing Cost: 0.03848582035268424\n",
      "Iteration 149, Training Cost: 0.03745753737516396, Testing Cost: 0.03745753737516396\n",
      "Iteration 150, Training Cost: 0.03785931578108586, Testing Cost: 0.03785931578108586\n",
      "Iteration 151, Training Cost: 0.05918125682252617, Testing Cost: 0.05918125682252617\n",
      "Iteration 152, Training Cost: 0.0412675916827065, Testing Cost: 0.0412675916827065\n",
      "Iteration 153, Training Cost: 0.0365457411856425, Testing Cost: 0.0365457411856425\n",
      "Iteration 154, Training Cost: 0.030036870568370434, Testing Cost: 0.030036870568370434\n",
      "Iteration 155, Training Cost: 0.04744758713652767, Testing Cost: 0.04744758713652767\n",
      "Iteration 156, Training Cost: 0.04222621724225047, Testing Cost: 0.04222621724225047\n",
      "Iteration 157, Training Cost: 0.0454948100817743, Testing Cost: 0.0454948100817743\n",
      "Iteration 158, Training Cost: 0.05964789815286855, Testing Cost: 0.05964789815286855\n",
      "Iteration 159, Training Cost: 0.043226346399846356, Testing Cost: 0.043226346399846356\n",
      "Iteration 160, Training Cost: 0.02947802369291011, Testing Cost: 0.02947802369291011\n",
      "Iteration 161, Training Cost: 0.029683223435110113, Testing Cost: 0.029683223435110113\n",
      "Iteration 162, Training Cost: 0.04759568927062865, Testing Cost: 0.04759568927062865\n",
      "Iteration 163, Training Cost: 0.03630639593692416, Testing Cost: 0.03630639593692416\n",
      "Iteration 164, Training Cost: 0.03936567753555806, Testing Cost: 0.03936567753555806\n",
      "Iteration 165, Training Cost: 0.04316692810616361, Testing Cost: 0.04316692810616361\n",
      "Iteration 166, Training Cost: 0.03361027356000369, Testing Cost: 0.03361027356000369\n",
      "Iteration 167, Training Cost: 0.030241343199708642, Testing Cost: 0.030241343199708642\n",
      "Iteration 168, Training Cost: 0.026240918386676082, Testing Cost: 0.026240918386676082\n",
      "Iteration 169, Training Cost: 0.03413680732929122, Testing Cost: 0.03413680732929122\n",
      "Iteration 170, Training Cost: 0.03121469346793365, Testing Cost: 0.03121469346793365\n",
      "Iteration 171, Training Cost: 0.020475185005824056, Testing Cost: 0.020475185005824056\n",
      "Iteration 172, Training Cost: 0.04043558421750203, Testing Cost: 0.04043558421750203\n",
      "Iteration 173, Training Cost: 0.032641255382696494, Testing Cost: 0.032641255382696494\n",
      "Iteration 174, Training Cost: 0.022563065327055286, Testing Cost: 0.022563065327055286\n",
      "Iteration 175, Training Cost: 0.029835896028468905, Testing Cost: 0.029835896028468905\n",
      "Iteration 176, Training Cost: 0.0278796762266073, Testing Cost: 0.0278796762266073\n",
      "Iteration 177, Training Cost: 0.023974870697787195, Testing Cost: 0.023974870697787195\n",
      "Iteration 178, Training Cost: 0.06643369820499521, Testing Cost: 0.06643369820499521\n",
      "Iteration 179, Training Cost: 0.036004248556627066, Testing Cost: 0.036004248556627066\n",
      "Iteration 180, Training Cost: 0.020796212927212517, Testing Cost: 0.020796212927212517\n",
      "Iteration 181, Training Cost: 0.02469568523328982, Testing Cost: 0.02469568523328982\n",
      "Iteration 182, Training Cost: 0.039489870719908594, Testing Cost: 0.039489870719908594\n",
      "Iteration 183, Training Cost: 0.039435871179609845, Testing Cost: 0.039435871179609845\n",
      "Iteration 184, Training Cost: 0.03098444020616533, Testing Cost: 0.03098444020616533\n",
      "Iteration 185, Training Cost: 0.027915363199293147, Testing Cost: 0.027915363199293147\n",
      "Iteration 186, Training Cost: 0.01699918813772303, Testing Cost: 0.01699918813772303\n",
      "Iteration 187, Training Cost: 0.0278147169314569, Testing Cost: 0.0278147169314569\n",
      "Iteration 188, Training Cost: 0.019691047407754254, Testing Cost: 0.019691047407754254\n",
      "Iteration 189, Training Cost: 0.031815392052417533, Testing Cost: 0.031815392052417533\n",
      "Iteration 190, Training Cost: 0.020202762286504798, Testing Cost: 0.020202762286504798\n",
      "Iteration 191, Training Cost: 0.01699562886545775, Testing Cost: 0.01699562886545775\n",
      "Iteration 192, Training Cost: 0.024483622818071177, Testing Cost: 0.024483622818071177\n",
      "Iteration 193, Training Cost: 0.025285605072680732, Testing Cost: 0.025285605072680732\n",
      "Iteration 194, Training Cost: 0.01742372565810818, Testing Cost: 0.01742372565810818\n",
      "Iteration 195, Training Cost: 0.03126055654070975, Testing Cost: 0.03126055654070975\n",
      "Iteration 196, Training Cost: 0.022123547988993846, Testing Cost: 0.022123547988993846\n",
      "Iteration 197, Training Cost: 0.01949699707752651, Testing Cost: 0.01949699707752651\n",
      "Iteration 198, Training Cost: 0.02791391775860294, Testing Cost: 0.02791391775860294\n",
      "Iteration 199, Training Cost: 0.017363913732511654, Testing Cost: 0.017363913732511654\n",
      "Iteration 200, Training Cost: 0.026400698481008893, Testing Cost: 0.026400698481008893\n",
      "Iteration 201, Training Cost: 0.017893130749893037, Testing Cost: 0.017893130749893037\n",
      "Iteration 202, Training Cost: 0.018331692904221077, Testing Cost: 0.018331692904221077\n",
      "Iteration 203, Training Cost: 0.0207692173346352, Testing Cost: 0.0207692173346352\n",
      "Iteration 204, Training Cost: 0.01804931174024593, Testing Cost: 0.01804931174024593\n",
      "Iteration 205, Training Cost: 0.026346269034502788, Testing Cost: 0.026346269034502788\n",
      "Iteration 206, Training Cost: 0.017572802965703658, Testing Cost: 0.017572802965703658\n",
      "Iteration 207, Training Cost: 0.021158728439151786, Testing Cost: 0.021158728439151786\n",
      "Iteration 208, Training Cost: 0.030471680769924896, Testing Cost: 0.030471680769924896\n",
      "Iteration 209, Training Cost: 0.0253363834353001, Testing Cost: 0.0253363834353001\n",
      "Iteration 210, Training Cost: 0.014661795192902555, Testing Cost: 0.014661795192902555\n",
      "Iteration 211, Training Cost: 0.03419247430066388, Testing Cost: 0.03419247430066388\n",
      "Iteration 212, Training Cost: 0.03224777536190357, Testing Cost: 0.03224777536190357\n",
      "Iteration 213, Training Cost: 0.014423367600235958, Testing Cost: 0.014423367600235958\n",
      "Iteration 214, Training Cost: 0.025371157746153575, Testing Cost: 0.025371157746153575\n",
      "Iteration 215, Training Cost: 0.020272650123214726, Testing Cost: 0.020272650123214726\n",
      "Iteration 216, Training Cost: 0.018176297136029942, Testing Cost: 0.018176297136029942\n",
      "Iteration 217, Training Cost: 0.03305478680707833, Testing Cost: 0.03305478680707833\n",
      "Iteration 218, Training Cost: 0.01466645412653265, Testing Cost: 0.01466645412653265\n",
      "Iteration 219, Training Cost: 0.026832233105026766, Testing Cost: 0.026832233105026766\n",
      "Iteration 220, Training Cost: 0.025275547707334603, Testing Cost: 0.025275547707334603\n",
      "Iteration 221, Training Cost: 0.03646767693278696, Testing Cost: 0.03646767693278696\n",
      "Iteration 222, Training Cost: 0.02287160154929505, Testing Cost: 0.02287160154929505\n",
      "Iteration 223, Training Cost: 0.013778267980264032, Testing Cost: 0.013778267980264032\n",
      "Iteration 224, Training Cost: 0.017161138350728335, Testing Cost: 0.017161138350728335\n",
      "Iteration 225, Training Cost: 0.015273815611114929, Testing Cost: 0.015273815611114929\n",
      "Iteration 226, Training Cost: 0.024364550618060705, Testing Cost: 0.024364550618060705\n",
      "Iteration 227, Training Cost: 0.024981102458229154, Testing Cost: 0.024981102458229154\n",
      "Iteration 228, Training Cost: 0.01871492317272082, Testing Cost: 0.01871492317272082\n",
      "Iteration 229, Training Cost: 0.0278623955288689, Testing Cost: 0.0278623955288689\n",
      "Iteration 230, Training Cost: 0.02303817475161864, Testing Cost: 0.02303817475161864\n",
      "Iteration 231, Training Cost: 0.01933717090883071, Testing Cost: 0.01933717090883071\n",
      "Iteration 232, Training Cost: 0.013626870295587123, Testing Cost: 0.013626870295587123\n",
      "Iteration 233, Training Cost: 0.01710657376690404, Testing Cost: 0.01710657376690404\n",
      "Iteration 234, Training Cost: 0.019662554481514067, Testing Cost: 0.019662554481514067\n",
      "Iteration 235, Training Cost: 0.014061386714103449, Testing Cost: 0.014061386714103449\n",
      "Iteration 236, Training Cost: 0.018226091323530227, Testing Cost: 0.018226091323530227\n",
      "Iteration 237, Training Cost: 0.011896373936648542, Testing Cost: 0.011896373936648542\n",
      "Iteration 238, Training Cost: 0.014784056344376637, Testing Cost: 0.014784056344376637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 239, Training Cost: 0.01270895824703249, Testing Cost: 0.01270895824703249\n",
      "Iteration 240, Training Cost: 0.012747131438297683, Testing Cost: 0.012747131438297683\n",
      "Iteration 241, Training Cost: 0.015083651202174126, Testing Cost: 0.015083651202174126\n",
      "Iteration 242, Training Cost: 0.014644851245828452, Testing Cost: 0.014644851245828452\n",
      "Iteration 243, Training Cost: 0.014256642586930853, Testing Cost: 0.014256642586930853\n",
      "Iteration 244, Training Cost: 0.012257951586833373, Testing Cost: 0.012257951586833373\n",
      "Iteration 245, Training Cost: 0.014365747889977167, Testing Cost: 0.014365747889977167\n",
      "Iteration 246, Training Cost: 0.012445621150390638, Testing Cost: 0.012445621150390638\n",
      "Iteration 247, Training Cost: 0.010689353258930131, Testing Cost: 0.010689353258930131\n",
      "Iteration 248, Training Cost: 0.00905636542323626, Testing Cost: 0.00905636542323626\n",
      "Iteration 249, Training Cost: 0.02032371285026825, Testing Cost: 0.02032371285026825\n",
      "Iteration 250, Training Cost: 0.018110692960799705, Testing Cost: 0.018110692960799705\n",
      "Iteration 251, Training Cost: 0.008149274624313323, Testing Cost: 0.008149274624313323\n",
      "Iteration 252, Training Cost: 0.010142252339590482, Testing Cost: 0.010142252339590482\n",
      "Iteration 253, Training Cost: 0.00983106401025001, Testing Cost: 0.00983106401025001\n",
      "Iteration 254, Training Cost: 0.01565352769540513, Testing Cost: 0.01565352769540513\n",
      "Iteration 255, Training Cost: 0.014634554710544999, Testing Cost: 0.014634554710544999\n",
      "Iteration 256, Training Cost: 0.01622266365807972, Testing Cost: 0.01622266365807972\n",
      "Iteration 257, Training Cost: 0.010429403484158998, Testing Cost: 0.010429403484158998\n",
      "Iteration 258, Training Cost: 0.019250432665266243, Testing Cost: 0.019250432665266243\n",
      "Iteration 259, Training Cost: 0.011327643833006672, Testing Cost: 0.011327643833006672\n",
      "Iteration 260, Training Cost: 0.018823093894866565, Testing Cost: 0.018823093894866565\n",
      "Iteration 261, Training Cost: 0.01230163507743834, Testing Cost: 0.01230163507743834\n",
      "Iteration 262, Training Cost: 0.01067895004402802, Testing Cost: 0.01067895004402802\n",
      "Iteration 263, Training Cost: 0.01196805729481785, Testing Cost: 0.01196805729481785\n",
      "Iteration 264, Training Cost: 0.010014276313815055, Testing Cost: 0.010014276313815055\n",
      "Iteration 265, Training Cost: 0.011930827470173174, Testing Cost: 0.011930827470173174\n",
      "Iteration 266, Training Cost: 0.00841656549858778, Testing Cost: 0.00841656549858778\n",
      "Iteration 267, Training Cost: 0.013641137755925884, Testing Cost: 0.013641137755925884\n",
      "Iteration 268, Training Cost: 0.009569705495256145, Testing Cost: 0.009569705495256145\n",
      "Iteration 269, Training Cost: 0.00957073980222683, Testing Cost: 0.00957073980222683\n",
      "Iteration 270, Training Cost: 0.013827623431267169, Testing Cost: 0.013827623431267169\n",
      "Iteration 271, Training Cost: 0.015918840064810195, Testing Cost: 0.015918840064810195\n",
      "Iteration 272, Training Cost: 0.008887399734624329, Testing Cost: 0.008887399734624329\n",
      "Iteration 273, Training Cost: 0.028764987298015115, Testing Cost: 0.028764987298015115\n",
      "Iteration 274, Training Cost: 0.013425136283005129, Testing Cost: 0.013425136283005129\n",
      "Iteration 275, Training Cost: 0.016887687549653006, Testing Cost: 0.016887687549653006\n",
      "Iteration 276, Training Cost: 0.013791081767497862, Testing Cost: 0.013791081767497862\n",
      "Iteration 277, Training Cost: 0.015921633762391044, Testing Cost: 0.015921633762391044\n",
      "Iteration 278, Training Cost: 0.021888304187393702, Testing Cost: 0.021888304187393702\n",
      "Iteration 279, Training Cost: 0.011357816961105667, Testing Cost: 0.011357816961105667\n",
      "Iteration 280, Training Cost: 0.017053875182966232, Testing Cost: 0.017053875182966232\n",
      "Iteration 281, Training Cost: 0.01920957607448172, Testing Cost: 0.01920957607448172\n",
      "Iteration 282, Training Cost: 0.02881375340785615, Testing Cost: 0.02881375340785615\n",
      "Iteration 283, Training Cost: 0.008038389166770259, Testing Cost: 0.008038389166770259\n",
      "Iteration 284, Training Cost: 0.01184106632968767, Testing Cost: 0.01184106632968767\n",
      "Iteration 285, Training Cost: 0.009037052941552243, Testing Cost: 0.009037052941552243\n",
      "Iteration 286, Training Cost: 0.01089580757339284, Testing Cost: 0.01089580757339284\n",
      "Iteration 287, Training Cost: 0.011075389081328425, Testing Cost: 0.011075389081328425\n",
      "Iteration 288, Training Cost: 0.013630904652299679, Testing Cost: 0.013630904652299679\n",
      "Iteration 289, Training Cost: 0.008928090972726678, Testing Cost: 0.008928090972726678\n",
      "Iteration 290, Training Cost: 0.012380628398222938, Testing Cost: 0.012380628398222938\n",
      "Iteration 291, Training Cost: 0.005398603795339623, Testing Cost: 0.005398603795339623\n",
      "Iteration 292, Training Cost: 0.011376618866934432, Testing Cost: 0.011376618866934432\n",
      "Iteration 293, Training Cost: 0.008641531451396631, Testing Cost: 0.008641531451396631\n",
      "Iteration 294, Training Cost: 0.011697873540162591, Testing Cost: 0.011697873540162591\n",
      "Iteration 295, Training Cost: 0.007125117054884512, Testing Cost: 0.007125117054884512\n",
      "Iteration 296, Training Cost: 0.010589087872685336, Testing Cost: 0.010589087872685336\n",
      "Iteration 297, Training Cost: 0.011589712846164493, Testing Cost: 0.011589712846164493\n",
      "Iteration 298, Training Cost: 0.014275524715564616, Testing Cost: 0.014275524715564616\n",
      "Iteration 299, Training Cost: 0.011834478447814774, Testing Cost: 0.011834478447814774\n",
      "Iteration 300, Training Cost: 0.014068628876633616, Testing Cost: 0.014068628876633616\n",
      "Iteration 301, Training Cost: 0.007952848282272272, Testing Cost: 0.007952848282272272\n",
      "Iteration 302, Training Cost: 0.012128108593778034, Testing Cost: 0.012128108593778034\n",
      "Iteration 303, Training Cost: 0.006208641823213752, Testing Cost: 0.006208641823213752\n",
      "Iteration 304, Training Cost: 0.01608554434093607, Testing Cost: 0.01608554434093607\n",
      "Iteration 305, Training Cost: 0.007190219542872734, Testing Cost: 0.007190219542872734\n",
      "Iteration 306, Training Cost: 0.0060957361327247765, Testing Cost: 0.0060957361327247765\n",
      "Iteration 307, Training Cost: 0.014306551645120244, Testing Cost: 0.014306551645120244\n",
      "Iteration 308, Training Cost: 0.012783396708359148, Testing Cost: 0.012783396708359148\n",
      "Iteration 309, Training Cost: 0.016975183014253173, Testing Cost: 0.016975183014253173\n",
      "Iteration 310, Training Cost: 0.011647048350587028, Testing Cost: 0.011647048350587028\n",
      "Iteration 311, Training Cost: 0.00607592330515956, Testing Cost: 0.00607592330515956\n",
      "Iteration 312, Training Cost: 0.012436906061991186, Testing Cost: 0.012436906061991186\n",
      "Iteration 313, Training Cost: 0.006316081117000961, Testing Cost: 0.006316081117000961\n",
      "Iteration 314, Training Cost: 0.016135842338500454, Testing Cost: 0.016135842338500454\n",
      "Iteration 315, Training Cost: 0.013245507889336958, Testing Cost: 0.013245507889336958\n",
      "Iteration 316, Training Cost: 0.014668495375763514, Testing Cost: 0.014668495375763514\n",
      "Iteration 317, Training Cost: 0.0057213285637161505, Testing Cost: 0.0057213285637161505\n",
      "Iteration 318, Training Cost: 0.0070720009354584175, Testing Cost: 0.0070720009354584175\n",
      "Iteration 319, Training Cost: 0.0075855828194021264, Testing Cost: 0.0075855828194021264\n",
      "Iteration 320, Training Cost: 0.0115803464947527, Testing Cost: 0.0115803464947527\n",
      "Iteration 321, Training Cost: 0.011054123505576151, Testing Cost: 0.011054123505576151\n",
      "Iteration 322, Training Cost: 0.008618624033391052, Testing Cost: 0.008618624033391052\n",
      "Iteration 323, Training Cost: 0.00564383273646995, Testing Cost: 0.00564383273646995\n",
      "Iteration 324, Training Cost: 0.006883759652763587, Testing Cost: 0.006883759652763587\n",
      "Iteration 325, Training Cost: 0.006169872585967535, Testing Cost: 0.006169872585967535\n",
      "Iteration 326, Training Cost: 0.00993307920073277, Testing Cost: 0.00993307920073277\n",
      "Iteration 327, Training Cost: 0.0063958017598459175, Testing Cost: 0.0063958017598459175\n",
      "Iteration 328, Training Cost: 0.008479352944445257, Testing Cost: 0.008479352944445257\n",
      "Iteration 329, Training Cost: 0.007240532698304083, Testing Cost: 0.007240532698304083\n",
      "Iteration 330, Training Cost: 0.0067660021101839845, Testing Cost: 0.0067660021101839845\n",
      "Iteration 331, Training Cost: 0.009820712955143043, Testing Cost: 0.009820712955143043\n",
      "Iteration 332, Training Cost: 0.005860757888273541, Testing Cost: 0.005860757888273541\n",
      "Iteration 333, Training Cost: 0.007983344700287016, Testing Cost: 0.007983344700287016\n",
      "Iteration 334, Training Cost: 0.009512347496860288, Testing Cost: 0.009512347496860288\n",
      "Iteration 335, Training Cost: 0.009857768883684085, Testing Cost: 0.009857768883684085\n",
      "Iteration 336, Training Cost: 0.013921315665078824, Testing Cost: 0.013921315665078824\n",
      "Iteration 337, Training Cost: 0.008543460163538404, Testing Cost: 0.008543460163538404\n",
      "Iteration 338, Training Cost: 0.013442919616881696, Testing Cost: 0.013442919616881696\n",
      "Iteration 339, Training Cost: 0.009037171171394628, Testing Cost: 0.009037171171394628\n",
      "Iteration 340, Training Cost: 0.007495673841348522, Testing Cost: 0.007495673841348522\n",
      "Iteration 341, Training Cost: 0.011529858508726287, Testing Cost: 0.011529858508726287\n",
      "Iteration 342, Training Cost: 0.00525918670522216, Testing Cost: 0.00525918670522216\n",
      "Iteration 343, Training Cost: 0.016828401289982194, Testing Cost: 0.016828401289982194\n",
      "Iteration 344, Training Cost: 0.00879946303673026, Testing Cost: 0.00879946303673026\n",
      "Iteration 345, Training Cost: 0.006393636293198286, Testing Cost: 0.006393636293198286\n",
      "Iteration 346, Training Cost: 0.005675094690102563, Testing Cost: 0.005675094690102563\n",
      "Iteration 347, Training Cost: 0.010151675327114465, Testing Cost: 0.010151675327114465\n",
      "Iteration 348, Training Cost: 0.006879526908031621, Testing Cost: 0.006879526908031621\n",
      "Iteration 349, Training Cost: 0.012277901048012805, Testing Cost: 0.012277901048012805\n",
      "Iteration 350, Training Cost: 0.00545446253890808, Testing Cost: 0.00545446253890808\n",
      "Iteration 351, Training Cost: 0.006297445871559888, Testing Cost: 0.006297445871559888\n",
      "Iteration 352, Training Cost: 0.006585561268009894, Testing Cost: 0.006585561268009894\n",
      "Converged at cost :0.00497435560715745 while Epsilon:0.005 \n",
      "Iteration 353, Training Cost: 0.00497435560715745, Testing Cost: 0.00497435560715745\n",
      "Converged at training cost :0.00497435560715745 while Epsilon:0.005 \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define your neural network model and parameters\n",
    "model = NeuralNetwork([X_wine.shape[1], 5, 4, 3])  # Your desired architecture\n",
    "learning_rate = 0.01\n",
    "batch_size = 32\n",
    "max_iterations = 1000\n",
    "epsilon = 0.005\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_wine, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train)\n",
    "X_test_normalized = scaler.transform(X_test)\n",
    "\n",
    "# Train the model using mini-batch gradient descent\n",
    "training_errors, testing_errors = train_mini_batch(X_train_normalized, y_train, X_test_normalized, y_test, model, learning_rate, batch_size, max_iterations, epsilon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "904d7e66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the learning curve\n",
    "step_size = 50  # Adjust as needed\n",
    "plot_learning_curve(training_errors, testing_errors, step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "481da476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.23100717469946455,\n",
       " 0.25096612972952104,\n",
       " 0.22585240167468304,\n",
       " 0.21661595802336214,\n",
       " 0.21069799458847788,\n",
       " 0.23284091140729496,\n",
       " 0.2003974943186537,\n",
       " 0.20729116551739535,\n",
       " 0.21207291223696795,\n",
       " 0.19107267749077206,\n",
       " 0.19536862470877722,\n",
       " 0.20048521712001366,\n",
       " 0.20135867132062413,\n",
       " 0.1972068016118833,\n",
       " 0.2145355662793713,\n",
       " 0.21359483904169435,\n",
       " 0.20592072740271233,\n",
       " 0.204693546514499,\n",
       " 0.22210332320160805,\n",
       " 0.21550896264434483,\n",
       " 0.19283266209331312,\n",
       " 0.21441539775643387,\n",
       " 0.20974600487625855,\n",
       " 0.21301365544735382,\n",
       " 0.19674231596180464,\n",
       " 0.20439772970974757,\n",
       " 0.20897437016123624,\n",
       " 0.18708729982769493,\n",
       " 0.20232886050070475,\n",
       " 0.1890773386287916,\n",
       " 0.17525945958857625,\n",
       " 0.21270200929471975,\n",
       " 0.18803360712673486,\n",
       " 0.19083750284610992,\n",
       " 0.18985736531087238,\n",
       " 0.19266751966781034,\n",
       " 0.17884861560225923,\n",
       " 0.19178192949259762,\n",
       " 0.1902413444177998,\n",
       " 0.20161562456297094,\n",
       " 0.19655016346195797,\n",
       " 0.19349772897428355,\n",
       " 0.2087933565665811,\n",
       " 0.21223542608475962,\n",
       " 0.20391492975216688,\n",
       " 0.17862896300280415,\n",
       " 0.1725533660690814,\n",
       " 0.1895875370906181,\n",
       " 0.18702626483907364,\n",
       " 0.16558724371572645,\n",
       " 0.18036504473361575,\n",
       " 0.19669285252774443,\n",
       " 0.15960684164752628,\n",
       " 0.17607160224449328,\n",
       " 0.15113307334977624,\n",
       " 0.16037765454999425,\n",
       " 0.1758917863036132,\n",
       " 0.15617829233077116,\n",
       " 0.13800475074755236,\n",
       " 0.11559405785732377,\n",
       " 0.15524063898619614,\n",
       " 0.1653683531909542,\n",
       " 0.1323573660195471,\n",
       " 0.13211232705995282,\n",
       " 0.1630408409105613,\n",
       " 0.14591614122335508,\n",
       " 0.1369815513476014,\n",
       " 0.1667281548890523,\n",
       " 0.1333004562071189,\n",
       " 0.15153524998926032,\n",
       " 0.13960727505253034,\n",
       " 0.1522791101393729,\n",
       " 0.11891894525025816,\n",
       " 0.1509145854123634,\n",
       " 0.11128134649040099,\n",
       " 0.15185652801064736,\n",
       " 0.1326973740161806,\n",
       " 0.1336456633020465,\n",
       " 0.11872228461044441,\n",
       " 0.138771855188181,\n",
       " 0.11422118182883362,\n",
       " 0.1364905209741068,\n",
       " 0.15263380923000025,\n",
       " 0.10908022154610221,\n",
       " 0.09839496301542427,\n",
       " 0.12076534993402468,\n",
       " 0.10432835640770795,\n",
       " 0.11716777429460884,\n",
       " 0.12012411724344092,\n",
       " 0.09126917582172969,\n",
       " 0.1146934764873195,\n",
       " 0.09950697396749429,\n",
       " 0.08306780744686917,\n",
       " 0.0794240160706378,\n",
       " 0.09865889775842167,\n",
       " 0.10678948520375846,\n",
       " 0.11433314995974636,\n",
       " 0.07860328971291942,\n",
       " 0.10311516050054918,\n",
       " 0.07498046215312253,\n",
       " 0.1109943021825092,\n",
       " 0.0892547968887169,\n",
       " 0.07184581550536671,\n",
       " 0.08963895971460933,\n",
       " 0.08251186105524681,\n",
       " 0.09329399209059883,\n",
       " 0.07719104294530177,\n",
       " 0.07326683075222482,\n",
       " 0.0772025711061772,\n",
       " 0.08016732458245565,\n",
       " 0.07992406394530725,\n",
       " 0.07154284850472285,\n",
       " 0.08279341251544313,\n",
       " 0.08094045586041847,\n",
       " 0.08311272345026963,\n",
       " 0.07316966488805283,\n",
       " 0.08096512522782683,\n",
       " 0.054956960793643095,\n",
       " 0.07112333448293377,\n",
       " 0.05778527356305422,\n",
       " 0.07195924431827645,\n",
       " 0.05580628684521062,\n",
       " 0.0527293692249314,\n",
       " 0.07093341422464598,\n",
       " 0.05312015774088833,\n",
       " 0.0670927230275421,\n",
       " 0.06549241831245096,\n",
       " 0.056277401069978576,\n",
       " 0.056946697441877366,\n",
       " 0.04637986251909447,\n",
       " 0.05130472832427272,\n",
       " 0.04126385321924451,\n",
       " 0.06402510198539135,\n",
       " 0.07044992939975425,\n",
       " 0.06343960768978775,\n",
       " 0.06157091989400949,\n",
       " 0.050537654331082316,\n",
       " 0.0793377943450514,\n",
       " 0.03714386489044956,\n",
       " 0.04100869898907355,\n",
       " 0.08156379452696243,\n",
       " 0.039301532555428155,\n",
       " 0.0810124452863192,\n",
       " 0.04061874002964082,\n",
       " 0.031024115077215645,\n",
       " 0.03342795348619444,\n",
       " 0.06348705140842585,\n",
       " 0.03848582035268424,\n",
       " 0.03745753737516396,\n",
       " 0.03785931578108586,\n",
       " 0.05918125682252617,\n",
       " 0.0412675916827065,\n",
       " 0.0365457411856425,\n",
       " 0.030036870568370434,\n",
       " 0.04744758713652767,\n",
       " 0.04222621724225047,\n",
       " 0.0454948100817743,\n",
       " 0.05964789815286855,\n",
       " 0.043226346399846356,\n",
       " 0.02947802369291011,\n",
       " 0.029683223435110113,\n",
       " 0.04759568927062865,\n",
       " 0.03630639593692416,\n",
       " 0.03936567753555806,\n",
       " 0.04316692810616361,\n",
       " 0.03361027356000369,\n",
       " 0.030241343199708642,\n",
       " 0.026240918386676082,\n",
       " 0.03413680732929122,\n",
       " 0.03121469346793365,\n",
       " 0.020475185005824056,\n",
       " 0.04043558421750203,\n",
       " 0.032641255382696494,\n",
       " 0.022563065327055286,\n",
       " 0.029835896028468905,\n",
       " 0.0278796762266073,\n",
       " 0.023974870697787195,\n",
       " 0.06643369820499521,\n",
       " 0.036004248556627066,\n",
       " 0.020796212927212517,\n",
       " 0.02469568523328982,\n",
       " 0.039489870719908594,\n",
       " 0.039435871179609845,\n",
       " 0.03098444020616533,\n",
       " 0.027915363199293147,\n",
       " 0.01699918813772303,\n",
       " 0.0278147169314569,\n",
       " 0.019691047407754254,\n",
       " 0.031815392052417533,\n",
       " 0.020202762286504798,\n",
       " 0.01699562886545775,\n",
       " 0.024483622818071177,\n",
       " 0.025285605072680732,\n",
       " 0.01742372565810818,\n",
       " 0.03126055654070975,\n",
       " 0.022123547988993846,\n",
       " 0.01949699707752651,\n",
       " 0.02791391775860294,\n",
       " 0.017363913732511654,\n",
       " 0.026400698481008893,\n",
       " 0.017893130749893037,\n",
       " 0.018331692904221077,\n",
       " 0.0207692173346352,\n",
       " 0.01804931174024593,\n",
       " 0.026346269034502788,\n",
       " 0.017572802965703658,\n",
       " 0.021158728439151786,\n",
       " 0.030471680769924896,\n",
       " 0.0253363834353001,\n",
       " 0.014661795192902555,\n",
       " 0.03419247430066388,\n",
       " 0.03224777536190357,\n",
       " 0.014423367600235958,\n",
       " 0.025371157746153575,\n",
       " 0.020272650123214726,\n",
       " 0.018176297136029942,\n",
       " 0.03305478680707833,\n",
       " 0.01466645412653265,\n",
       " 0.026832233105026766,\n",
       " 0.025275547707334603,\n",
       " 0.03646767693278696,\n",
       " 0.02287160154929505,\n",
       " 0.013778267980264032,\n",
       " 0.017161138350728335,\n",
       " 0.015273815611114929,\n",
       " 0.024364550618060705,\n",
       " 0.024981102458229154,\n",
       " 0.01871492317272082,\n",
       " 0.0278623955288689,\n",
       " 0.02303817475161864,\n",
       " 0.01933717090883071,\n",
       " 0.013626870295587123,\n",
       " 0.01710657376690404,\n",
       " 0.019662554481514067,\n",
       " 0.014061386714103449,\n",
       " 0.018226091323530227,\n",
       " 0.011896373936648542,\n",
       " 0.014784056344376637,\n",
       " 0.01270895824703249,\n",
       " 0.012747131438297683,\n",
       " 0.015083651202174126,\n",
       " 0.014644851245828452,\n",
       " 0.014256642586930853,\n",
       " 0.012257951586833373,\n",
       " 0.014365747889977167,\n",
       " 0.012445621150390638,\n",
       " 0.010689353258930131,\n",
       " 0.00905636542323626,\n",
       " 0.02032371285026825,\n",
       " 0.018110692960799705,\n",
       " 0.008149274624313323,\n",
       " 0.010142252339590482,\n",
       " 0.00983106401025001,\n",
       " 0.01565352769540513,\n",
       " 0.014634554710544999,\n",
       " 0.01622266365807972,\n",
       " 0.010429403484158998,\n",
       " 0.019250432665266243,\n",
       " 0.011327643833006672,\n",
       " 0.018823093894866565,\n",
       " 0.01230163507743834,\n",
       " 0.01067895004402802,\n",
       " 0.01196805729481785,\n",
       " 0.010014276313815055,\n",
       " 0.011930827470173174,\n",
       " 0.00841656549858778,\n",
       " 0.013641137755925884,\n",
       " 0.009569705495256145,\n",
       " 0.00957073980222683,\n",
       " 0.013827623431267169,\n",
       " 0.015918840064810195,\n",
       " 0.008887399734624329,\n",
       " 0.028764987298015115,\n",
       " 0.013425136283005129,\n",
       " 0.016887687549653006,\n",
       " 0.013791081767497862,\n",
       " 0.015921633762391044,\n",
       " 0.021888304187393702,\n",
       " 0.011357816961105667,\n",
       " 0.017053875182966232,\n",
       " 0.01920957607448172,\n",
       " 0.02881375340785615,\n",
       " 0.008038389166770259,\n",
       " 0.01184106632968767,\n",
       " 0.009037052941552243,\n",
       " 0.01089580757339284,\n",
       " 0.011075389081328425,\n",
       " 0.013630904652299679,\n",
       " 0.008928090972726678,\n",
       " 0.012380628398222938,\n",
       " 0.005398603795339623,\n",
       " 0.011376618866934432,\n",
       " 0.008641531451396631,\n",
       " 0.011697873540162591,\n",
       " 0.007125117054884512,\n",
       " 0.010589087872685336,\n",
       " 0.011589712846164493,\n",
       " 0.014275524715564616,\n",
       " 0.011834478447814774,\n",
       " 0.014068628876633616,\n",
       " 0.007952848282272272,\n",
       " 0.012128108593778034,\n",
       " 0.006208641823213752,\n",
       " 0.01608554434093607,\n",
       " 0.007190219542872734,\n",
       " 0.0060957361327247765,\n",
       " 0.014306551645120244,\n",
       " 0.012783396708359148,\n",
       " 0.016975183014253173,\n",
       " 0.011647048350587028,\n",
       " 0.00607592330515956,\n",
       " 0.012436906061991186,\n",
       " 0.006316081117000961,\n",
       " 0.016135842338500454,\n",
       " 0.013245507889336958,\n",
       " 0.014668495375763514,\n",
       " 0.0057213285637161505,\n",
       " 0.0070720009354584175,\n",
       " 0.0075855828194021264,\n",
       " 0.0115803464947527,\n",
       " 0.011054123505576151,\n",
       " 0.008618624033391052,\n",
       " 0.00564383273646995,\n",
       " 0.006883759652763587,\n",
       " 0.006169872585967535,\n",
       " 0.00993307920073277,\n",
       " 0.0063958017598459175,\n",
       " 0.008479352944445257,\n",
       " 0.007240532698304083,\n",
       " 0.0067660021101839845,\n",
       " 0.009820712955143043,\n",
       " 0.005860757888273541,\n",
       " 0.007983344700287016,\n",
       " 0.009512347496860288,\n",
       " 0.009857768883684085,\n",
       " 0.013921315665078824,\n",
       " 0.008543460163538404,\n",
       " 0.013442919616881696,\n",
       " 0.009037171171394628,\n",
       " 0.007495673841348522,\n",
       " 0.011529858508726287,\n",
       " 0.00525918670522216,\n",
       " 0.016828401289982194,\n",
       " 0.00879946303673026,\n",
       " 0.006393636293198286,\n",
       " 0.005675094690102563,\n",
       " 0.010151675327114465,\n",
       " 0.006879526908031621,\n",
       " 0.012277901048012805,\n",
       " 0.00545446253890808,\n",
       " 0.006297445871559888,\n",
       " 0.006585561268009894,\n",
       " 0.00497435560715745]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b4d09f81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.23100717469946455,\n",
       " 0.25096612972952104,\n",
       " 0.22585240167468304,\n",
       " 0.21661595802336214,\n",
       " 0.21069799458847788,\n",
       " 0.23284091140729496,\n",
       " 0.2003974943186537,\n",
       " 0.20729116551739535,\n",
       " 0.21207291223696795,\n",
       " 0.19107267749077206,\n",
       " 0.19536862470877722,\n",
       " 0.20048521712001366,\n",
       " 0.20135867132062413,\n",
       " 0.1972068016118833,\n",
       " 0.2145355662793713,\n",
       " 0.21359483904169435,\n",
       " 0.20592072740271233,\n",
       " 0.204693546514499,\n",
       " 0.22210332320160805,\n",
       " 0.21550896264434483,\n",
       " 0.19283266209331312,\n",
       " 0.21441539775643387,\n",
       " 0.20974600487625855,\n",
       " 0.21301365544735382,\n",
       " 0.19674231596180464,\n",
       " 0.20439772970974757,\n",
       " 0.20897437016123624,\n",
       " 0.18708729982769493,\n",
       " 0.20232886050070475,\n",
       " 0.1890773386287916,\n",
       " 0.17525945958857625,\n",
       " 0.21270200929471975,\n",
       " 0.18803360712673486,\n",
       " 0.19083750284610992,\n",
       " 0.18985736531087238,\n",
       " 0.19266751966781034,\n",
       " 0.17884861560225923,\n",
       " 0.19178192949259762,\n",
       " 0.1902413444177998,\n",
       " 0.20161562456297094,\n",
       " 0.19655016346195797,\n",
       " 0.19349772897428355,\n",
       " 0.2087933565665811,\n",
       " 0.21223542608475962,\n",
       " 0.20391492975216688,\n",
       " 0.17862896300280415,\n",
       " 0.1725533660690814,\n",
       " 0.1895875370906181,\n",
       " 0.18702626483907364,\n",
       " 0.16558724371572645,\n",
       " 0.18036504473361575,\n",
       " 0.19669285252774443,\n",
       " 0.15960684164752628,\n",
       " 0.17607160224449328,\n",
       " 0.15113307334977624,\n",
       " 0.16037765454999425,\n",
       " 0.1758917863036132,\n",
       " 0.15617829233077116,\n",
       " 0.13800475074755236,\n",
       " 0.11559405785732377,\n",
       " 0.15524063898619614,\n",
       " 0.1653683531909542,\n",
       " 0.1323573660195471,\n",
       " 0.13211232705995282,\n",
       " 0.1630408409105613,\n",
       " 0.14591614122335508,\n",
       " 0.1369815513476014,\n",
       " 0.1667281548890523,\n",
       " 0.1333004562071189,\n",
       " 0.15153524998926032,\n",
       " 0.13960727505253034,\n",
       " 0.1522791101393729,\n",
       " 0.11891894525025816,\n",
       " 0.1509145854123634,\n",
       " 0.11128134649040099,\n",
       " 0.15185652801064736,\n",
       " 0.1326973740161806,\n",
       " 0.1336456633020465,\n",
       " 0.11872228461044441,\n",
       " 0.138771855188181,\n",
       " 0.11422118182883362,\n",
       " 0.1364905209741068,\n",
       " 0.15263380923000025,\n",
       " 0.10908022154610221,\n",
       " 0.09839496301542427,\n",
       " 0.12076534993402468,\n",
       " 0.10432835640770795,\n",
       " 0.11716777429460884,\n",
       " 0.12012411724344092,\n",
       " 0.09126917582172969,\n",
       " 0.1146934764873195,\n",
       " 0.09950697396749429,\n",
       " 0.08306780744686917,\n",
       " 0.0794240160706378,\n",
       " 0.09865889775842167,\n",
       " 0.10678948520375846,\n",
       " 0.11433314995974636,\n",
       " 0.07860328971291942,\n",
       " 0.10311516050054918,\n",
       " 0.07498046215312253,\n",
       " 0.1109943021825092,\n",
       " 0.0892547968887169,\n",
       " 0.07184581550536671,\n",
       " 0.08963895971460933,\n",
       " 0.08251186105524681,\n",
       " 0.09329399209059883,\n",
       " 0.07719104294530177,\n",
       " 0.07326683075222482,\n",
       " 0.0772025711061772,\n",
       " 0.08016732458245565,\n",
       " 0.07992406394530725,\n",
       " 0.07154284850472285,\n",
       " 0.08279341251544313,\n",
       " 0.08094045586041847,\n",
       " 0.08311272345026963,\n",
       " 0.07316966488805283,\n",
       " 0.08096512522782683,\n",
       " 0.054956960793643095,\n",
       " 0.07112333448293377,\n",
       " 0.05778527356305422,\n",
       " 0.07195924431827645,\n",
       " 0.05580628684521062,\n",
       " 0.0527293692249314,\n",
       " 0.07093341422464598,\n",
       " 0.05312015774088833,\n",
       " 0.0670927230275421,\n",
       " 0.06549241831245096,\n",
       " 0.056277401069978576,\n",
       " 0.056946697441877366,\n",
       " 0.04637986251909447,\n",
       " 0.05130472832427272,\n",
       " 0.04126385321924451,\n",
       " 0.06402510198539135,\n",
       " 0.07044992939975425,\n",
       " 0.06343960768978775,\n",
       " 0.06157091989400949,\n",
       " 0.050537654331082316,\n",
       " 0.0793377943450514,\n",
       " 0.03714386489044956,\n",
       " 0.04100869898907355,\n",
       " 0.08156379452696243,\n",
       " 0.039301532555428155,\n",
       " 0.0810124452863192,\n",
       " 0.04061874002964082,\n",
       " 0.031024115077215645,\n",
       " 0.03342795348619444,\n",
       " 0.06348705140842585,\n",
       " 0.03848582035268424,\n",
       " 0.03745753737516396,\n",
       " 0.03785931578108586,\n",
       " 0.05918125682252617,\n",
       " 0.0412675916827065,\n",
       " 0.0365457411856425,\n",
       " 0.030036870568370434,\n",
       " 0.04744758713652767,\n",
       " 0.04222621724225047,\n",
       " 0.0454948100817743,\n",
       " 0.05964789815286855,\n",
       " 0.043226346399846356,\n",
       " 0.02947802369291011,\n",
       " 0.029683223435110113,\n",
       " 0.04759568927062865,\n",
       " 0.03630639593692416,\n",
       " 0.03936567753555806,\n",
       " 0.04316692810616361,\n",
       " 0.03361027356000369,\n",
       " 0.030241343199708642,\n",
       " 0.026240918386676082,\n",
       " 0.03413680732929122,\n",
       " 0.03121469346793365,\n",
       " 0.020475185005824056,\n",
       " 0.04043558421750203,\n",
       " 0.032641255382696494,\n",
       " 0.022563065327055286,\n",
       " 0.029835896028468905,\n",
       " 0.0278796762266073,\n",
       " 0.023974870697787195,\n",
       " 0.06643369820499521,\n",
       " 0.036004248556627066,\n",
       " 0.020796212927212517,\n",
       " 0.02469568523328982,\n",
       " 0.039489870719908594,\n",
       " 0.039435871179609845,\n",
       " 0.03098444020616533,\n",
       " 0.027915363199293147,\n",
       " 0.01699918813772303,\n",
       " 0.0278147169314569,\n",
       " 0.019691047407754254,\n",
       " 0.031815392052417533,\n",
       " 0.020202762286504798,\n",
       " 0.01699562886545775,\n",
       " 0.024483622818071177,\n",
       " 0.025285605072680732,\n",
       " 0.01742372565810818,\n",
       " 0.03126055654070975,\n",
       " 0.022123547988993846,\n",
       " 0.01949699707752651,\n",
       " 0.02791391775860294,\n",
       " 0.017363913732511654,\n",
       " 0.026400698481008893,\n",
       " 0.017893130749893037,\n",
       " 0.018331692904221077,\n",
       " 0.0207692173346352,\n",
       " 0.01804931174024593,\n",
       " 0.026346269034502788,\n",
       " 0.017572802965703658,\n",
       " 0.021158728439151786,\n",
       " 0.030471680769924896,\n",
       " 0.0253363834353001,\n",
       " 0.014661795192902555,\n",
       " 0.03419247430066388,\n",
       " 0.03224777536190357,\n",
       " 0.014423367600235958,\n",
       " 0.025371157746153575,\n",
       " 0.020272650123214726,\n",
       " 0.018176297136029942,\n",
       " 0.03305478680707833,\n",
       " 0.01466645412653265,\n",
       " 0.026832233105026766,\n",
       " 0.025275547707334603,\n",
       " 0.03646767693278696,\n",
       " 0.02287160154929505,\n",
       " 0.013778267980264032,\n",
       " 0.017161138350728335,\n",
       " 0.015273815611114929,\n",
       " 0.024364550618060705,\n",
       " 0.024981102458229154,\n",
       " 0.01871492317272082,\n",
       " 0.0278623955288689,\n",
       " 0.02303817475161864,\n",
       " 0.01933717090883071,\n",
       " 0.013626870295587123,\n",
       " 0.01710657376690404,\n",
       " 0.019662554481514067,\n",
       " 0.014061386714103449,\n",
       " 0.018226091323530227,\n",
       " 0.011896373936648542,\n",
       " 0.014784056344376637,\n",
       " 0.01270895824703249,\n",
       " 0.012747131438297683,\n",
       " 0.015083651202174126,\n",
       " 0.014644851245828452,\n",
       " 0.014256642586930853,\n",
       " 0.012257951586833373,\n",
       " 0.014365747889977167,\n",
       " 0.012445621150390638,\n",
       " 0.010689353258930131,\n",
       " 0.00905636542323626,\n",
       " 0.02032371285026825,\n",
       " 0.018110692960799705,\n",
       " 0.008149274624313323,\n",
       " 0.010142252339590482,\n",
       " 0.00983106401025001,\n",
       " 0.01565352769540513,\n",
       " 0.014634554710544999,\n",
       " 0.01622266365807972,\n",
       " 0.010429403484158998,\n",
       " 0.019250432665266243,\n",
       " 0.011327643833006672,\n",
       " 0.018823093894866565,\n",
       " 0.01230163507743834,\n",
       " 0.01067895004402802,\n",
       " 0.01196805729481785,\n",
       " 0.010014276313815055,\n",
       " 0.011930827470173174,\n",
       " 0.00841656549858778,\n",
       " 0.013641137755925884,\n",
       " 0.009569705495256145,\n",
       " 0.00957073980222683,\n",
       " 0.013827623431267169,\n",
       " 0.015918840064810195,\n",
       " 0.008887399734624329,\n",
       " 0.028764987298015115,\n",
       " 0.013425136283005129,\n",
       " 0.016887687549653006,\n",
       " 0.013791081767497862,\n",
       " 0.015921633762391044,\n",
       " 0.021888304187393702,\n",
       " 0.011357816961105667,\n",
       " 0.017053875182966232,\n",
       " 0.01920957607448172,\n",
       " 0.02881375340785615,\n",
       " 0.008038389166770259,\n",
       " 0.01184106632968767,\n",
       " 0.009037052941552243,\n",
       " 0.01089580757339284,\n",
       " 0.011075389081328425,\n",
       " 0.013630904652299679,\n",
       " 0.008928090972726678,\n",
       " 0.012380628398222938,\n",
       " 0.005398603795339623,\n",
       " 0.011376618866934432,\n",
       " 0.008641531451396631,\n",
       " 0.011697873540162591,\n",
       " 0.007125117054884512,\n",
       " 0.010589087872685336,\n",
       " 0.011589712846164493,\n",
       " 0.014275524715564616,\n",
       " 0.011834478447814774,\n",
       " 0.014068628876633616,\n",
       " 0.007952848282272272,\n",
       " 0.012128108593778034,\n",
       " 0.006208641823213752,\n",
       " 0.01608554434093607,\n",
       " 0.007190219542872734,\n",
       " 0.0060957361327247765,\n",
       " 0.014306551645120244,\n",
       " 0.012783396708359148,\n",
       " 0.016975183014253173,\n",
       " 0.011647048350587028,\n",
       " 0.00607592330515956,\n",
       " 0.012436906061991186,\n",
       " 0.006316081117000961,\n",
       " 0.016135842338500454,\n",
       " 0.013245507889336958,\n",
       " 0.014668495375763514,\n",
       " 0.0057213285637161505,\n",
       " 0.0070720009354584175,\n",
       " 0.0075855828194021264,\n",
       " 0.0115803464947527,\n",
       " 0.011054123505576151,\n",
       " 0.008618624033391052,\n",
       " 0.00564383273646995,\n",
       " 0.006883759652763587,\n",
       " 0.006169872585967535,\n",
       " 0.00993307920073277,\n",
       " 0.0063958017598459175,\n",
       " 0.008479352944445257,\n",
       " 0.007240532698304083,\n",
       " 0.0067660021101839845,\n",
       " 0.009820712955143043,\n",
       " 0.005860757888273541,\n",
       " 0.007983344700287016,\n",
       " 0.009512347496860288,\n",
       " 0.009857768883684085,\n",
       " 0.013921315665078824,\n",
       " 0.008543460163538404,\n",
       " 0.013442919616881696,\n",
       " 0.009037171171394628,\n",
       " 0.007495673841348522,\n",
       " 0.011529858508726287,\n",
       " 0.00525918670522216,\n",
       " 0.016828401289982194,\n",
       " 0.00879946303673026,\n",
       " 0.006393636293198286,\n",
       " 0.005675094690102563,\n",
       " 0.010151675327114465,\n",
       " 0.006879526908031621,\n",
       " 0.012277901048012805,\n",
       " 0.00545446253890808,\n",
       " 0.006297445871559888,\n",
       " 0.006585561268009894,\n",
       " 0.00497435560715745]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e34029",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
