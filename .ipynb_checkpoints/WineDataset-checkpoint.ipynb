{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "154dfb7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy Results:\n",
      "                  Architecture, Lambda  Mean Accuracy\n",
      "0                  ([13, 2, 3], 1e-05)       0.935294\n",
      "1                  ([13, 2, 3], 0.001)       0.947059\n",
      "2                    ([13, 2, 3], 1.0)       0.958824\n",
      "3               ([13, 6, 5, 3], 1e-05)       0.947059\n",
      "4               ([13, 6, 5, 3], 0.001)       0.964706\n",
      "5                 ([13, 6, 5, 3], 1.0)       0.923529\n",
      "6            ([13, 8, 6, 4, 3], 1e-05)       0.941176\n",
      "7            ([13, 8, 6, 4, 3], 0.001)       0.917647\n",
      "8              ([13, 8, 6, 4, 3], 1.0)       0.964706\n",
      "9       ([13, 12, 10, 8, 6, 3], 1e-05)       0.947059\n",
      "10      ([13, 12, 10, 8, 6, 3], 0.001)       0.917647\n",
      "11        ([13, 12, 10, 8, 6, 3], 1.0)       0.952941\n",
      "12     ([13, 6, 6, 6, 6, 6, 3], 1e-05)       0.929412\n",
      "13     ([13, 6, 6, 6, 6, 6, 3], 0.001)       0.917647\n",
      "14       ([13, 6, 6, 6, 6, 6, 3], 1.0)       0.823529\n",
      "15  ([13, 5, 5, 5, 5, 5, 5, 3], 1e-05)       0.558824\n",
      "16  ([13, 5, 5, 5, 5, 5, 5, 3], 0.001)       0.517647\n",
      "17    ([13, 5, 5, 5, 5, 5, 5, 3], 1.0)       0.447059\n",
      "\n",
      "Mean F1 Score Results:\n",
      "                  Architecture, Lambda  Mean F1 Score\n",
      "0                  ([13, 2, 3], 1e-05)       0.953944\n",
      "1                  ([13, 2, 3], 0.001)       0.965196\n",
      "2                    ([13, 2, 3], 1.0)       0.969316\n",
      "3               ([13, 6, 5, 3], 1e-05)       0.956287\n",
      "4               ([13, 6, 5, 3], 0.001)       0.976104\n",
      "5                 ([13, 6, 5, 3], 1.0)       0.951298\n",
      "6            ([13, 8, 6, 4, 3], 1e-05)       0.946524\n",
      "7            ([13, 8, 6, 4, 3], 0.001)       0.933262\n",
      "8              ([13, 8, 6, 4, 3], 1.0)       0.972727\n",
      "9       ([13, 12, 10, 8, 6, 3], 1e-05)       0.956975\n",
      "10      ([13, 12, 10, 8, 6, 3], 0.001)       0.936641\n",
      "11        ([13, 12, 10, 8, 6, 3], 1.0)       0.958462\n",
      "12     ([13, 6, 6, 6, 6, 6, 3], 1e-05)       0.935457\n",
      "13     ([13, 6, 6, 6, 6, 6, 3], 0.001)       0.923524\n",
      "14       ([13, 6, 6, 6, 6, 6, 3], 1.0)       0.834166\n",
      "15  ([13, 5, 5, 5, 5, 5, 5, 3], 1e-05)       0.590558\n",
      "16  ([13, 5, 5, 5, 5, 5, 5, 3], 0.001)       0.540319\n",
      "17    ([13, 5, 5, 5, 5, 5, 5, 3], 1.0)       0.521876\n",
      "\n",
      "Mean J cost Results:\n",
      "                  Architecture, Lambda  Mean J Cost\n",
      "0                  ([13, 2, 3], 1e-05)     0.005154\n",
      "1                  ([13, 2, 3], 0.001)     0.005009\n",
      "2                    ([13, 2, 3], 1.0)     0.005086\n",
      "3               ([13, 6, 5, 3], 1e-05)     0.004986\n",
      "4               ([13, 6, 5, 3], 0.001)     0.004989\n",
      "5                 ([13, 6, 5, 3], 1.0)     0.004993\n",
      "6            ([13, 8, 6, 4, 3], 1e-05)     0.004989\n",
      "7            ([13, 8, 6, 4, 3], 0.001)     0.005005\n",
      "8              ([13, 8, 6, 4, 3], 1.0)     0.005024\n",
      "9       ([13, 12, 10, 8, 6, 3], 1e-05)     0.004986\n",
      "10      ([13, 12, 10, 8, 6, 3], 0.001)     0.004983\n",
      "11        ([13, 12, 10, 8, 6, 3], 1.0)     0.004992\n",
      "12     ([13, 6, 6, 6, 6, 6, 3], 1e-05)     0.004988\n",
      "13     ([13, 6, 6, 6, 6, 6, 3], 0.001)     0.005128\n",
      "14       ([13, 6, 6, 6, 6, 6, 3], 1.0)     0.020987\n",
      "15  ([13, 5, 5, 5, 5, 5, 5, 3], 1e-05)     0.073405\n",
      "16  ([13, 5, 5, 5, 5, 5, 5, 3], 0.001)     0.085235\n",
      "17    ([13, 5, 5, 5, 5, 5, 5, 3], 1.0)     0.075098\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        self.weights = [np.random.randn(layers[i], layers[i+1]) for i in range(len(layers)-1)]\n",
    "        self.biases = [np.zeros((1, layers[i+1])) for i in range(len(layers)-1)]\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    def forward_pass(self, X):\n",
    "        activations = [X]\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(activations[-1], w) + b\n",
    "            activations.append(self.sigmoid(z))\n",
    "        return activations\n",
    "    \n",
    "    def backward_pass(self, X, Y, activations):\n",
    "        deltas = [(activations[-1] - Y) * self.sigmoid_derivative(activations[-1])]\n",
    "        for i in range(len(self.layers) - 2, 0, -1):\n",
    "            delta = np.dot(deltas[0], self.weights[i].T) * self.sigmoid_derivative(activations[i])\n",
    "            deltas.insert(0, delta)\n",
    "        return deltas\n",
    "    \n",
    "    \n",
    "    def compute_gradients(self, activations, deltas):\n",
    "        gradients_weights = [np.dot(activations[i].T, deltas[i]) for i in range(len(self.layers) - 1)]\n",
    "        gradients_biases = [np.sum(deltas[i], axis=0) for i in range(len(self.layers) - 1)]\n",
    "        return gradients_weights, gradients_biases\n",
    "    \n",
    "    def update_weights(self, gradients_weights, gradients_biases, learning_rate):\n",
    "        self.weights = [w - learning_rate * gw for w, gw in zip(self.weights, gradients_weights)]\n",
    "        self.biases = [b - learning_rate * gb for b, gb in zip(self.biases, gradients_biases)]\n",
    "    \n",
    "    def train(self, X, Y, learning_rate, lam, max_iterations, epsilon):\n",
    "        for iteration in range(max_iterations):\n",
    "            activations = self.forward_pass(X)\n",
    "            deltas = self.backward_pass(X, Y, activations)\n",
    "            gradients_weights, gradients_biases = self.compute_gradients(activations, deltas)\n",
    "            self.update_weights(gradients_weights, gradients_biases, learning_rate)\n",
    "            # Compute cost function\n",
    "            J = np.mean(np.square(activations[-1] - Y))\n",
    "            #print(f\"Iteration {iteration+1}, Cost: {J}\")\n",
    "            # Check for convergence\n",
    "            if J < epsilon:\n",
    "                #print(f\"Converged at cost :{J} while Epsilon:{epsilon} \")\n",
    "                return J\n",
    "        return J\n",
    "            \n",
    "\n",
    "    def accuracy(self, y_true, y_pred):\n",
    "        correct = np.sum(np.all(y_true == y_pred, axis=1))\n",
    "        return correct / len(y_true)\n",
    "\n",
    "\n",
    "    def f1_score(self, y_true, y_pred):\n",
    "        tp = np.sum(np.logical_and(y_true, y_pred))\n",
    "        fp = np.sum(np.logical_and(np.logical_not(y_true), y_pred))\n",
    "        fn = np.sum(np.logical_and(y_true, np.logical_not(y_pred)))\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        return f1\n",
    "\n",
    "\n",
    "    def evaluate(self, X_test, y_test, J):\n",
    "        activations = self.forward_pass(X_test)[-1]\n",
    "        y_pred = (activations > 0.5).astype(int)\n",
    "        acc = self.accuracy(y_test, y_pred)\n",
    "        f1 = self.f1_score(y_test, y_pred)\n",
    "        return J, acc, f1\n",
    "    \n",
    "    def k_fold_cross_validation(X, y, architectures, regularization_params, learning_rate, max_iterations, epsilon):\n",
    "        results_accuracy = {}\n",
    "        results_f1_score = {}\n",
    "        results_J_cost = {}\n",
    "        \n",
    "        num_splits = 10\n",
    "        fold_size = len(X) // num_splits\n",
    "\n",
    "        for arch in architectures:\n",
    "            for lam in regularization_params:\n",
    "                accuracy_list = []\n",
    "                f1_score_list = []\n",
    "                J_list = []\n",
    "                \n",
    "                for i in range(num_splits):\n",
    "                    start = i * fold_size\n",
    "                    end = (i + 1) * fold_size\n",
    "                    \n",
    "                    X_train = pd.concat([X[:start], X[end:]])\n",
    "                    y_train = np.concatenate([y[:start], y[end:]])\n",
    "                    X_test = X[start:end]\n",
    "                    y_test = y[start:end]\n",
    "\n",
    "                    mean = np.mean(X_train, axis=0)\n",
    "                    std = np.std(X_train, axis=0)\n",
    "                    X_train_normalized = (X_train - mean) / std\n",
    "                    X_test_normalized = (X_test - mean) / std\n",
    "\n",
    "                    model = NeuralNetwork(arch)\n",
    "                    J = model.train(X_train_normalized, y_train, learning_rate=learning_rate, lam=lam, max_iterations=max_iterations, epsilon=epsilon)\n",
    "                    J, accuracy, f1_score = model.evaluate(X_test_normalized, y_test, J)\n",
    "                    accuracy_list.append(accuracy)\n",
    "                    f1_score_list.append(f1_score)\n",
    "                    J_list.append(J)\n",
    "\n",
    "                mean_accuracy = np.mean(accuracy_list)\n",
    "                mean_f1_score = np.mean(f1_score_list)\n",
    "                mean_J_cost   = np.mean(J_list)\n",
    "\n",
    "                results_accuracy[(str(arch), lam)] = mean_accuracy\n",
    "                results_f1_score[(str(arch), lam)] = mean_f1_score\n",
    "                results_J_cost[(str(arch), lam)] = mean_J_cost\n",
    "\n",
    "        return results_accuracy, results_f1_score, results_J_cost\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "df_wine = pd.read_csv(\"/Users/noshitha/Downloads/hw3/datasets/hw3_wine.csv\", delimiter=\"\\t\")\n",
    "\n",
    "# Preprocess data\n",
    "X_wine = pd.get_dummies(df_wine.iloc[:, 1:])\n",
    "y_wine = df_wine.iloc[:, 0]\n",
    "\n",
    "# Normalize data\n",
    "y_wine_resized = y_wine.values.reshape(-1, 1)\n",
    "\n",
    "# Initialize the OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Fit and transform the target variable\n",
    "y_encoded = encoder.fit_transform(y_wine_resized)\n",
    "\n",
    "architectures = [\n",
    "    [X_wine.shape[1], 2, 3],\n",
    "    [X_wine.shape[1], 6, 5, 3],\n",
    "    [X_wine.shape[1], 8, 6, 4, 3],\n",
    "    [X_wine.shape[1], 12, 10, 8, 6, 3],\n",
    "    [X_wine.shape[1], 6, 6, 6, 6, 6, 3],\n",
    "    [X_wine.shape[1], 5, 5, 5, 5, 5, 5, 3]\n",
    "]\n",
    "\n",
    "\n",
    "regularization_params = [0.00001, 0.001, 1.0]  # Example regularization parameters\n",
    "\n",
    "# Initialize lists to store results\n",
    "results_accuracy = {}\n",
    "results_f1_score = {}\n",
    "results_J_cost = {}\n",
    "\n",
    "# Perform stratified k-fold cross-validation\n",
    "results_accuracy, results_f1_score, results_J_cost = NeuralNetwork.k_fold_cross_validation(X_wine, y_encoded, architectures, regularization_params, learning_rate=0.01, max_iterations=1000, epsilon=0.005)\n",
    "\n",
    "# Convert the results into a DataFrame for tabular representation\n",
    "accuracy_df = pd.DataFrame(list(results_accuracy.items()), columns=['Architecture, Lambda', 'Mean Accuracy'])\n",
    "f1_score_df = pd.DataFrame(list(results_f1_score.items()), columns=['Architecture, Lambda', 'Mean F1 Score'])\n",
    "J_cost_df = pd.DataFrame(list(results_J_cost.items()), columns=['Architecture, Lambda', 'Mean J Cost'])\n",
    "\n",
    "print(\"Mean Accuracy Results:\")\n",
    "print(accuracy_df)\n",
    "print(\"\\nMean F1 Score Results:\")\n",
    "print(f1_score_df)\n",
    "print(\"\\nMean J cost Results:\")\n",
    "print(J_cost_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "172f6e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Architecture, Lambda</th>\n",
       "      <th>Mean Accuracy</th>\n",
       "      <th>Mean F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>([13, 2, 3], 1e-05)</td>\n",
       "      <td>0.935294</td>\n",
       "      <td>0.953944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>([13, 2, 3], 0.001)</td>\n",
       "      <td>0.947059</td>\n",
       "      <td>0.965196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>([13, 2, 3], 1.0)</td>\n",
       "      <td>0.958824</td>\n",
       "      <td>0.969316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>([13, 6, 5, 3], 1e-05)</td>\n",
       "      <td>0.947059</td>\n",
       "      <td>0.956287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>([13, 6, 5, 3], 0.001)</td>\n",
       "      <td>0.964706</td>\n",
       "      <td>0.976104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>([13, 6, 5, 3], 1.0)</td>\n",
       "      <td>0.923529</td>\n",
       "      <td>0.951298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>([13, 8, 6, 4, 3], 1e-05)</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.946524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>([13, 8, 6, 4, 3], 0.001)</td>\n",
       "      <td>0.917647</td>\n",
       "      <td>0.933262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>([13, 8, 6, 4, 3], 1.0)</td>\n",
       "      <td>0.964706</td>\n",
       "      <td>0.972727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>([13, 12, 10, 8, 6, 3], 1e-05)</td>\n",
       "      <td>0.947059</td>\n",
       "      <td>0.956975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>([13, 12, 10, 8, 6, 3], 0.001)</td>\n",
       "      <td>0.917647</td>\n",
       "      <td>0.936641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>([13, 12, 10, 8, 6, 3], 1.0)</td>\n",
       "      <td>0.952941</td>\n",
       "      <td>0.958462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>([13, 6, 6, 6, 6, 6, 3], 1e-05)</td>\n",
       "      <td>0.929412</td>\n",
       "      <td>0.935457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>([13, 6, 6, 6, 6, 6, 3], 0.001)</td>\n",
       "      <td>0.917647</td>\n",
       "      <td>0.923524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>([13, 6, 6, 6, 6, 6, 3], 1.0)</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.834166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>([13, 5, 5, 5, 5, 5, 5, 3], 1e-05)</td>\n",
       "      <td>0.558824</td>\n",
       "      <td>0.590558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>([13, 5, 5, 5, 5, 5, 5, 3], 0.001)</td>\n",
       "      <td>0.517647</td>\n",
       "      <td>0.540319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>([13, 5, 5, 5, 5, 5, 5, 3], 1.0)</td>\n",
       "      <td>0.447059</td>\n",
       "      <td>0.521876</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Architecture, Lambda  Mean Accuracy  Mean F1 Score\n",
       "0                  ([13, 2, 3], 1e-05)       0.935294       0.953944\n",
       "1                  ([13, 2, 3], 0.001)       0.947059       0.965196\n",
       "2                    ([13, 2, 3], 1.0)       0.958824       0.969316\n",
       "3               ([13, 6, 5, 3], 1e-05)       0.947059       0.956287\n",
       "4               ([13, 6, 5, 3], 0.001)       0.964706       0.976104\n",
       "5                 ([13, 6, 5, 3], 1.0)       0.923529       0.951298\n",
       "6            ([13, 8, 6, 4, 3], 1e-05)       0.941176       0.946524\n",
       "7            ([13, 8, 6, 4, 3], 0.001)       0.917647       0.933262\n",
       "8              ([13, 8, 6, 4, 3], 1.0)       0.964706       0.972727\n",
       "9       ([13, 12, 10, 8, 6, 3], 1e-05)       0.947059       0.956975\n",
       "10      ([13, 12, 10, 8, 6, 3], 0.001)       0.917647       0.936641\n",
       "11        ([13, 12, 10, 8, 6, 3], 1.0)       0.952941       0.958462\n",
       "12     ([13, 6, 6, 6, 6, 6, 3], 1e-05)       0.929412       0.935457\n",
       "13     ([13, 6, 6, 6, 6, 6, 3], 0.001)       0.917647       0.923524\n",
       "14       ([13, 6, 6, 6, 6, 6, 3], 1.0)       0.823529       0.834166\n",
       "15  ([13, 5, 5, 5, 5, 5, 5, 3], 1e-05)       0.558824       0.590558\n",
       "16  ([13, 5, 5, 5, 5, 5, 5, 3], 0.001)       0.517647       0.540319\n",
       "17    ([13, 5, 5, 5, 5, 5, 5, 3], 1.0)       0.447059       0.521876"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge accuracy and f1_score DataFrames on 'Architecture, Lambda'\n",
    "merged_df = pd.merge(accuracy_df, f1_score_df, on='Architecture, Lambda')\n",
    "\n",
    "# Merge the merged DataFrame with J_cost_df on 'Architecture, Lambda'\n",
    "final_df = pd.merge(merged_df, J_cost_df, on='Architecture, Lambda')\n",
    "\n",
    "# Rename columns for clarity\n",
    "merged_df.columns = ['Architecture, Lambda', 'Mean Accuracy', 'Mean F1 Score']\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c904f756",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab645a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Function to generate mini-batches\n",
    "def generate_mini_batches(X, y, batch_size):\n",
    "    num_samples = X.shape[0]\n",
    "    num_batches = num_samples // batch_size\n",
    "    mini_batches = []\n",
    "    shuffled_indices = np.random.permutation(num_samples)\n",
    "    X_shuffled = X[shuffled_indices]\n",
    "    y_shuffled = y[shuffled_indices]\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        mini_batches.append((X_shuffled[start_idx:end_idx], y_shuffled[start_idx:end_idx]))\n",
    "    if num_samples % batch_size != 0:\n",
    "        mini_batches.append((X_shuffled[num_batches*batch_size:], y_shuffled[num_batches*batch_size:]))\n",
    "    return mini_batches\n",
    "\n",
    "\n",
    "def train_mini_batch(X_train, y_train, X_test, y_test, model, learning_rate, batch_size, max_iterations, epsilon):\n",
    "    training_errors = []\n",
    "    testing_errors = []\n",
    "    for iteration in range(max_iterations):\n",
    "        mini_batches = generate_mini_batches(X_train, y_train, batch_size)\n",
    "        for mini_batch in mini_batches:\n",
    "            X_mini_batch, y_mini_batch = mini_batch\n",
    "            J = model.train(X_mini_batch, y_mini_batch, learning_rate=learning_rate, lam=0.001, max_iterations=1, epsilon=epsilon)\n",
    "        training_cost = np.mean(np.square(model.forward_pass(X_train)[-1] - y_train))  # Compute training cost\n",
    "        testing_cost = np.mean(np.square(model.forward_pass(X_test)[-1] - y_test))  # Compute testing cost\n",
    "        training_errors.append(training_cost)\n",
    "        testing_errors.append(testing_cost)\n",
    "        print(f\"Iteration {iteration+1}, Training Cost: {training_cost}, Testing Cost: {testing_cost}\")\n",
    "        # Check for convergence\n",
    "        if training_cost < epsilon:\n",
    "            #print(f\"Converged at training cost :{training_cost} while Epsilon:{epsilon} \")\n",
    "            break\n",
    "    return training_errors, testing_errors\n",
    "\n",
    "# Plot learning curve\n",
    "def plot_learning_curve(training_errors, testing_errors, step_size):\n",
    "    iterations = range(1, len(training_errors) + 1)\n",
    "    plt.plot(iterations, training_errors, label='Training Error')\n",
    "    plt.plot(iterations, testing_errors, label='Testing Error')\n",
    "    plt.title('Learning Curve')\n",
    "    plt.xlabel('Number of Training Samples')\n",
    "    plt.ylabel('J values')\n",
    "    plt.xticks(np.arange(1, len(training_errors) + 1, step=step_size))\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02dbef2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, Training Cost: 0.24871881775139004, Testing Cost: 0.25696359948268277\n",
      "Iteration 2, Training Cost: 0.23980567042405632, Testing Cost: 0.2461976559695185\n",
      "Iteration 3, Training Cost: 0.23329693864874068, Testing Cost: 0.2381493419259178\n",
      "Iteration 4, Training Cost: 0.22854459129982574, Testing Cost: 0.23213160779892328\n",
      "Iteration 5, Training Cost: 0.22501451185114318, Testing Cost: 0.22756009250373527\n",
      "Iteration 6, Training Cost: 0.2223011721269296, Testing Cost: 0.22395355860414676\n",
      "Iteration 7, Training Cost: 0.22017798364207447, Testing Cost: 0.22108479965370342\n",
      "Iteration 8, Training Cost: 0.21846270197037862, Testing Cost: 0.21873899639388636\n",
      "Iteration 9, Training Cost: 0.21702919347375782, Testing Cost: 0.21676598581812156\n",
      "Iteration 10, Training Cost: 0.21578943056976702, Testing Cost: 0.21506265468697933\n",
      "Iteration 11, Training Cost: 0.21467786759268925, Testing Cost: 0.21352944716204134\n",
      "Iteration 12, Training Cost: 0.2136694515634836, Testing Cost: 0.21216254929861758\n",
      "Iteration 13, Training Cost: 0.2127267405667419, Testing Cost: 0.2109058898882649\n",
      "Iteration 14, Training Cost: 0.2118289707945297, Testing Cost: 0.2097274003520024\n",
      "Iteration 15, Training Cost: 0.21095800169644596, Testing Cost: 0.20859783286367664\n",
      "Iteration 16, Training Cost: 0.2101051532547038, Testing Cost: 0.20750251198958072\n",
      "Iteration 17, Training Cost: 0.20926437814727733, Testing Cost: 0.20644959447244815\n",
      "Iteration 18, Training Cost: 0.20842593786776079, Testing Cost: 0.20540276951509545\n",
      "Iteration 19, Training Cost: 0.20758748018003573, Testing Cost: 0.2043662265435069\n",
      "Iteration 20, Training Cost: 0.20674390732452147, Testing Cost: 0.20331801162319404\n",
      "Iteration 21, Training Cost: 0.20589533131999035, Testing Cost: 0.2022895085557754\n",
      "Iteration 22, Training Cost: 0.2050396295544413, Testing Cost: 0.2012689931751528\n",
      "Iteration 23, Training Cost: 0.20417143285948514, Testing Cost: 0.20021989736128643\n",
      "Iteration 24, Training Cost: 0.20329436740072102, Testing Cost: 0.1992033052920396\n",
      "Iteration 25, Training Cost: 0.20240264720831005, Testing Cost: 0.19815755829514703\n",
      "Iteration 26, Training Cost: 0.20149758313646024, Testing Cost: 0.19708476235846528\n",
      "Iteration 27, Training Cost: 0.20057892412194905, Testing Cost: 0.19602608395282511\n",
      "Iteration 28, Training Cost: 0.19964493728490543, Testing Cost: 0.19494703483214645\n",
      "Iteration 29, Training Cost: 0.1986964460774474, Testing Cost: 0.19383549402031683\n",
      "Iteration 30, Training Cost: 0.19773096464721227, Testing Cost: 0.19271156170752932\n",
      "Iteration 31, Training Cost: 0.19674857636020604, Testing Cost: 0.1915893895810966\n",
      "Iteration 32, Training Cost: 0.19574970535679861, Testing Cost: 0.19046503925022146\n",
      "Iteration 33, Training Cost: 0.19473329442555823, Testing Cost: 0.1893003067706383\n",
      "Iteration 34, Training Cost: 0.19370040036467004, Testing Cost: 0.18811193707769835\n",
      "Iteration 35, Training Cost: 0.19264924873964942, Testing Cost: 0.18691731087397961\n",
      "Iteration 36, Training Cost: 0.19158022848633918, Testing Cost: 0.18569621158777827\n",
      "Iteration 37, Training Cost: 0.19049292127350448, Testing Cost: 0.18447250748542082\n",
      "Iteration 38, Training Cost: 0.18938612842839542, Testing Cost: 0.18324319729808952\n",
      "Iteration 39, Training Cost: 0.18826364608018478, Testing Cost: 0.18196655437577539\n",
      "Iteration 40, Training Cost: 0.1871226539068113, Testing Cost: 0.18065691472362358\n",
      "Iteration 41, Training Cost: 0.18596174603962956, Testing Cost: 0.1793320143331957\n",
      "Iteration 42, Training Cost: 0.18478356021178957, Testing Cost: 0.17798799841703425\n",
      "Iteration 43, Training Cost: 0.18358436769944694, Testing Cost: 0.17665223161836816\n",
      "Iteration 44, Training Cost: 0.18236624692310444, Testing Cost: 0.17528666963163317\n",
      "Iteration 45, Training Cost: 0.1811303828471596, Testing Cost: 0.17388929600639658\n",
      "Iteration 46, Training Cost: 0.1798762427502228, Testing Cost: 0.1724764084286502\n",
      "Iteration 47, Training Cost: 0.17860170843609657, Testing Cost: 0.17103048313646937\n",
      "Iteration 48, Training Cost: 0.17730952872529096, Testing Cost: 0.16957411932905758\n",
      "Iteration 49, Training Cost: 0.1759980048122157, Testing Cost: 0.16810135055902006\n",
      "Iteration 50, Training Cost: 0.17466976272034734, Testing Cost: 0.16660502776610162\n",
      "Iteration 51, Training Cost: 0.17332219737302737, Testing Cost: 0.1651069173138579\n",
      "Iteration 52, Training Cost: 0.17195685509644323, Testing Cost: 0.16358230210395042\n",
      "Iteration 53, Training Cost: 0.17057254168286726, Testing Cost: 0.1620495471386745\n",
      "Iteration 54, Training Cost: 0.1691731096403032, Testing Cost: 0.16048630188233415\n",
      "Iteration 55, Training Cost: 0.16775764990788855, Testing Cost: 0.15889482093870685\n",
      "Iteration 56, Training Cost: 0.16632616067436198, Testing Cost: 0.15728667520123527\n",
      "Iteration 57, Training Cost: 0.1648774987348063, Testing Cost: 0.1556863698366959\n",
      "Iteration 58, Training Cost: 0.16341582116841225, Testing Cost: 0.1540397883131302\n",
      "Iteration 59, Training Cost: 0.16193609423132888, Testing Cost: 0.1524234160064675\n",
      "Iteration 60, Training Cost: 0.16044398299254306, Testing Cost: 0.15079442794092734\n",
      "Iteration 61, Training Cost: 0.15893939664611392, Testing Cost: 0.14914402419406095\n",
      "Iteration 62, Training Cost: 0.1574220523893067, Testing Cost: 0.14750579365719416\n",
      "Iteration 63, Training Cost: 0.15589339727476142, Testing Cost: 0.14584039721896394\n",
      "Iteration 64, Training Cost: 0.15435545720068064, Testing Cost: 0.14414618953921146\n",
      "Iteration 65, Training Cost: 0.15280718708894928, Testing Cost: 0.14246960042437276\n",
      "Iteration 66, Training Cost: 0.15125186745510505, Testing Cost: 0.14076158711041356\n",
      "Iteration 67, Training Cost: 0.1496879718400837, Testing Cost: 0.13906433070331248\n",
      "Iteration 68, Training Cost: 0.14811894222335786, Testing Cost: 0.1373739603056293\n",
      "Iteration 69, Training Cost: 0.1465423514840775, Testing Cost: 0.1356604617699309\n",
      "Iteration 70, Training Cost: 0.1449633339353723, Testing Cost: 0.13394906271394583\n",
      "Iteration 71, Training Cost: 0.14338070531405267, Testing Cost: 0.1322410617555206\n",
      "Iteration 72, Training Cost: 0.14179518741958796, Testing Cost: 0.13053824579762574\n",
      "Iteration 73, Training Cost: 0.14020955546977878, Testing Cost: 0.12882969446426237\n",
      "Iteration 74, Training Cost: 0.13862445681682098, Testing Cost: 0.12713226947952713\n",
      "Iteration 75, Training Cost: 0.13704169605358457, Testing Cost: 0.12544341369784184\n",
      "Iteration 76, Training Cost: 0.13546017978901806, Testing Cost: 0.12375805310130764\n",
      "Iteration 77, Training Cost: 0.13388317231122754, Testing Cost: 0.12208029348632791\n",
      "Iteration 78, Training Cost: 0.13230935452600376, Testing Cost: 0.12041660660468978\n",
      "Iteration 79, Training Cost: 0.13074230634212283, Testing Cost: 0.11874896092194727\n",
      "Iteration 80, Training Cost: 0.12918245546465681, Testing Cost: 0.11708695986950264\n",
      "Iteration 81, Training Cost: 0.12763041963389699, Testing Cost: 0.11545976258481673\n",
      "Iteration 82, Training Cost: 0.12608640454336845, Testing Cost: 0.11381942756031059\n",
      "Iteration 83, Training Cost: 0.1245520732253043, Testing Cost: 0.11223477960476579\n",
      "Iteration 84, Training Cost: 0.12302897711666243, Testing Cost: 0.11063628538125284\n",
      "Iteration 85, Training Cost: 0.12151755653239205, Testing Cost: 0.10905198962418253\n",
      "Iteration 86, Training Cost: 0.12001913641480816, Testing Cost: 0.10746275028213643\n",
      "Iteration 87, Training Cost: 0.11853379038283561, Testing Cost: 0.10588592202803662\n",
      "Iteration 88, Training Cost: 0.11706104634485974, Testing Cost: 0.10435566334867179\n",
      "Iteration 89, Training Cost: 0.11560153637218118, Testing Cost: 0.10284844540314257\n",
      "Iteration 90, Training Cost: 0.11415932017995907, Testing Cost: 0.1013667937387202\n",
      "Iteration 91, Training Cost: 0.11273226401780526, Testing Cost: 0.09988201750376124\n",
      "Iteration 92, Training Cost: 0.11132102304369236, Testing Cost: 0.09842015579857548\n",
      "Iteration 93, Training Cost: 0.10992781810437746, Testing Cost: 0.09696980268204579\n",
      "Iteration 94, Training Cost: 0.10855127405190525, Testing Cost: 0.09553971226902382\n",
      "Iteration 95, Training Cost: 0.10719201531456204, Testing Cost: 0.09413592926051668\n",
      "Iteration 96, Training Cost: 0.10585044948015367, Testing Cost: 0.09275418026228896\n",
      "Iteration 97, Training Cost: 0.10452550356563811, Testing Cost: 0.0914049061211042\n",
      "Iteration 98, Training Cost: 0.10321922835713347, Testing Cost: 0.09006016112981864\n",
      "Iteration 99, Training Cost: 0.10193097707312286, Testing Cost: 0.08874546511526173\n",
      "Iteration 100, Training Cost: 0.10065971369088056, Testing Cost: 0.08743614116634475\n",
      "Iteration 101, Training Cost: 0.0994088936148522, Testing Cost: 0.08617476139566954\n",
      "Iteration 102, Training Cost: 0.09817523883317807, Testing Cost: 0.0849197144381189\n",
      "Iteration 103, Training Cost: 0.09696096658678485, Testing Cost: 0.083686955813549\n",
      "Iteration 104, Training Cost: 0.09576442887898934, Testing Cost: 0.0824592883047072\n",
      "Iteration 105, Training Cost: 0.09458585459972631, Testing Cost: 0.08126096159567199\n",
      "Iteration 106, Training Cost: 0.09342522489209772, Testing Cost: 0.08008156105987023\n",
      "Iteration 107, Training Cost: 0.09228222641417372, Testing Cost: 0.07891945219678803\n",
      "Iteration 108, Training Cost: 0.0911583078519682, Testing Cost: 0.07777974820075269\n",
      "Iteration 109, Training Cost: 0.0900513002411991, Testing Cost: 0.07665074019444112\n",
      "Iteration 110, Training Cost: 0.08896281177655264, Testing Cost: 0.07556429248759065\n",
      "Iteration 111, Training Cost: 0.08789165851131364, Testing Cost: 0.0744952090711136\n",
      "Iteration 112, Training Cost: 0.08683778143058767, Testing Cost: 0.07345357846776521\n",
      "Iteration 113, Training Cost: 0.0858016220645937, Testing Cost: 0.07242703449767553\n",
      "Iteration 114, Training Cost: 0.08478309479543096, Testing Cost: 0.07139448946393293\n",
      "Iteration 115, Training Cost: 0.08378059887236698, Testing Cost: 0.07038661029673429\n",
      "Iteration 116, Training Cost: 0.08279652362875842, Testing Cost: 0.06941783094610351\n",
      "Iteration 117, Training Cost: 0.08183029872668246, Testing Cost: 0.06844811491908691\n",
      "Iteration 118, Training Cost: 0.0808799495145988, Testing Cost: 0.06750598176329928\n",
      "Iteration 119, Training Cost: 0.07994624241530049, Testing Cost: 0.0665658052273023\n",
      "Iteration 120, Training Cost: 0.0790290216448851, Testing Cost: 0.0656436786442215\n",
      "Iteration 121, Training Cost: 0.07812833867559739, Testing Cost: 0.0647242461352704\n",
      "Iteration 122, Training Cost: 0.0772434820087763, Testing Cost: 0.06383467586617508\n",
      "Iteration 123, Training Cost: 0.07637477564141547, Testing Cost: 0.06298168402142583\n",
      "Iteration 124, Training Cost: 0.07552205721241907, Testing Cost: 0.06213277338969303\n",
      "Iteration 125, Training Cost: 0.07468393021901461, Testing Cost: 0.06131018431901808\n",
      "Iteration 126, Training Cost: 0.0738611521716837, Testing Cost: 0.060486187153080524\n",
      "Iteration 127, Training Cost: 0.07305246424550751, Testing Cost: 0.05967457593628175\n",
      "Iteration 128, Training Cost: 0.07225807897205407, Testing Cost: 0.058891053950661756\n",
      "Iteration 129, Training Cost: 0.07147912358627088, Testing Cost: 0.058097519470698196\n",
      "Iteration 130, Training Cost: 0.07071339163318874, Testing Cost: 0.057332051474163655\n",
      "Iteration 131, Training Cost: 0.06996034122939457, Testing Cost: 0.05658641870145481\n",
      "Iteration 132, Training Cost: 0.06922084839486395, Testing Cost: 0.0558525505459084\n",
      "Iteration 133, Training Cost: 0.06849419590351435, Testing Cost: 0.05511772347477987\n",
      "Iteration 134, Training Cost: 0.0677795422348141, Testing Cost: 0.05441153498795156\n",
      "Iteration 135, Training Cost: 0.06707620994692272, Testing Cost: 0.053708814490909\n",
      "Iteration 136, Training Cost: 0.06638481893087185, Testing Cost: 0.05301468026105699\n",
      "Iteration 137, Training Cost: 0.06570457648513947, Testing Cost: 0.052342321551097366\n",
      "Iteration 138, Training Cost: 0.06503490088218022, Testing Cost: 0.05168264123230058\n",
      "Iteration 139, Training Cost: 0.06437525145780834, Testing Cost: 0.05101373741576742\n",
      "Iteration 140, Training Cost: 0.06372542362828183, Testing Cost: 0.05038175341149585\n",
      "Iteration 141, Training Cost: 0.06308458520163227, Testing Cost: 0.04976365173291291\n",
      "Iteration 142, Training Cost: 0.06245250494463138, Testing Cost: 0.04916898908966443\n",
      "Iteration 143, Training Cost: 0.06182873886653643, Testing Cost: 0.04856503407065507\n",
      "Iteration 144, Training Cost: 0.06121246499717031, Testing Cost: 0.047964171224393476\n",
      "Iteration 145, Training Cost: 0.06060395827833955, Testing Cost: 0.04736352185209283\n",
      "Iteration 146, Training Cost: 0.06000284000630538, Testing Cost: 0.04678984826878673\n",
      "Iteration 147, Training Cost: 0.059408339851713766, Testing Cost: 0.04620857321513325\n",
      "Iteration 148, Training Cost: 0.058820370975591715, Testing Cost: 0.04567005794333136\n",
      "Iteration 149, Training Cost: 0.05823711213715889, Testing Cost: 0.04512938070740069\n",
      "Iteration 150, Training Cost: 0.05765959835547193, Testing Cost: 0.04458583249023554\n",
      "Iteration 151, Training Cost: 0.05708609882218224, Testing Cost: 0.044059857993194444\n",
      "Iteration 152, Training Cost: 0.05651731991801054, Testing Cost: 0.043526099175028796\n",
      "Iteration 153, Training Cost: 0.05595152264624218, Testing Cost: 0.04302079351664688\n",
      "Iteration 154, Training Cost: 0.05538842197250663, Testing Cost: 0.04249863813491116\n",
      "Iteration 155, Training Cost: 0.05482854486742745, Testing Cost: 0.04199971934993793\n",
      "Iteration 156, Training Cost: 0.05427038653013288, Testing Cost: 0.041511919250708476\n",
      "Iteration 157, Training Cost: 0.05371252921513339, Testing Cost: 0.04103066822676077\n",
      "Iteration 158, Training Cost: 0.05315719770113119, Testing Cost: 0.0405471649427994\n",
      "Iteration 159, Training Cost: 0.05260089321991284, Testing Cost: 0.04008167042576794\n",
      "Iteration 160, Training Cost: 0.05204279950361488, Testing Cost: 0.03961945084547453\n",
      "Iteration 161, Training Cost: 0.05148453628573587, Testing Cost: 0.039179169910366136\n",
      "Iteration 162, Training Cost: 0.05092499569866191, Testing Cost: 0.038739154374887536\n",
      "Iteration 163, Training Cost: 0.05036326179742405, Testing Cost: 0.038305898480345796\n",
      "Iteration 164, Training Cost: 0.04980009871600607, Testing Cost: 0.03785781958653348\n",
      "Iteration 165, Training Cost: 0.049234703046691954, Testing Cost: 0.03743084557861582\n",
      "Iteration 166, Training Cost: 0.04866592603232625, Testing Cost: 0.036999062126885356\n",
      "Iteration 167, Training Cost: 0.04809757720581834, Testing Cost: 0.03658921263952175\n",
      "Iteration 168, Training Cost: 0.047527700258005345, Testing Cost: 0.036178059700590015\n",
      "Iteration 169, Training Cost: 0.04695661096986813, Testing Cost: 0.03576124060219477\n",
      "Iteration 170, Training Cost: 0.046385615390202105, Testing Cost: 0.03536124117489722\n",
      "Iteration 171, Training Cost: 0.045813572593211865, Testing Cost: 0.034963410885926416\n",
      "Iteration 172, Training Cost: 0.04524401935373718, Testing Cost: 0.034548703879041985\n",
      "Iteration 173, Training Cost: 0.04467813456367806, Testing Cost: 0.03415414081082953\n",
      "Iteration 174, Training Cost: 0.044116582529183844, Testing Cost: 0.03375879491800857\n",
      "Iteration 175, Training Cost: 0.043556014109211644, Testing Cost: 0.0333651773942056\n",
      "Iteration 176, Training Cost: 0.04300154462610884, Testing Cost: 0.03299025543750096\n",
      "Iteration 177, Training Cost: 0.042446054673887214, Testing Cost: 0.03261641654722758\n",
      "Iteration 178, Training Cost: 0.04189935150787029, Testing Cost: 0.032238140686927576\n",
      "Iteration 179, Training Cost: 0.041352205331494425, Testing Cost: 0.031872460305496904\n",
      "Iteration 180, Training Cost: 0.040806608561320144, Testing Cost: 0.031500231396970856\n",
      "Iteration 181, Training Cost: 0.040263937212975096, Testing Cost: 0.031139687223916048\n",
      "Iteration 182, Training Cost: 0.039727184779058324, Testing Cost: 0.03077981998745213\n",
      "Iteration 183, Training Cost: 0.03919731226867155, Testing Cost: 0.03042014173950029\n",
      "Iteration 184, Training Cost: 0.03866913535058113, Testing Cost: 0.03007051336960918\n",
      "Iteration 185, Training Cost: 0.03814688354418936, Testing Cost: 0.029737360831464728\n",
      "Iteration 186, Training Cost: 0.03762873440220846, Testing Cost: 0.029398889330850272\n",
      "Iteration 187, Training Cost: 0.03711957244182172, Testing Cost: 0.029060052353271294\n",
      "Iteration 188, Training Cost: 0.03661658523084354, Testing Cost: 0.028712263158663973\n",
      "Iteration 189, Training Cost: 0.03612375179595793, Testing Cost: 0.028381965447330113\n",
      "Iteration 190, Training Cost: 0.0356391974744726, Testing Cost: 0.028046883251114263\n",
      "Iteration 191, Training Cost: 0.03516721342140662, Testing Cost: 0.027725411250452577\n",
      "Iteration 192, Training Cost: 0.034705217452335325, Testing Cost: 0.02739942738134479\n",
      "Iteration 193, Training Cost: 0.03425373026985789, Testing Cost: 0.027088044083123194\n",
      "Iteration 194, Training Cost: 0.03381428633326776, Testing Cost: 0.02677950790767663\n",
      "Iteration 195, Training Cost: 0.0333853214910793, Testing Cost: 0.02647269439970152\n",
      "Iteration 196, Training Cost: 0.03296692540689944, Testing Cost: 0.02617437246679906\n",
      "Iteration 197, Training Cost: 0.03255987440299594, Testing Cost: 0.02587964876927469\n",
      "Iteration 198, Training Cost: 0.032162162346922755, Testing Cost: 0.025583274663345992\n",
      "Iteration 199, Training Cost: 0.031774857755892456, Testing Cost: 0.025286840965778518\n",
      "Iteration 200, Training Cost: 0.03139783003492859, Testing Cost: 0.02499723085912788\n",
      "Iteration 201, Training Cost: 0.031030060927910542, Testing Cost: 0.024717562693744952\n",
      "Iteration 202, Training Cost: 0.030671422655998707, Testing Cost: 0.02444518626205933\n",
      "Iteration 203, Training Cost: 0.03032204653657993, Testing Cost: 0.024161381781955555\n",
      "Iteration 204, Training Cost: 0.029981133237535405, Testing Cost: 0.02388555775639371\n",
      "Iteration 205, Training Cost: 0.02964863842749701, Testing Cost: 0.023615126729797473\n",
      "Iteration 206, Training Cost: 0.02932418996795933, Testing Cost: 0.023349709246084392\n",
      "Iteration 207, Training Cost: 0.02900726070222874, Testing Cost: 0.023089961158685533\n",
      "Iteration 208, Training Cost: 0.02869773562448418, Testing Cost: 0.022833205083400407\n",
      "Iteration 209, Training Cost: 0.02839542606051129, Testing Cost: 0.022580897735264796\n",
      "Iteration 210, Training Cost: 0.028099921763251996, Testing Cost: 0.022334577031201782\n",
      "Iteration 211, Training Cost: 0.02781118079299075, Testing Cost: 0.02209262119841014\n",
      "Iteration 212, Training Cost: 0.027528761882552325, Testing Cost: 0.021854172913555407\n",
      "Iteration 213, Training Cost: 0.027252973857039384, Testing Cost: 0.021614507397243334\n",
      "Iteration 214, Training Cost: 0.026982309458843636, Testing Cost: 0.02138606794302182\n",
      "Iteration 215, Training Cost: 0.026717986750166012, Testing Cost: 0.021160165137397005\n",
      "Iteration 216, Training Cost: 0.026459307644826072, Testing Cost: 0.02093731041682949\n",
      "Iteration 217, Training Cost: 0.026205843506788985, Testing Cost: 0.020714031113255357\n",
      "Iteration 218, Training Cost: 0.02595739725985677, Testing Cost: 0.020496996476216486\n",
      "Iteration 219, Training Cost: 0.025713500209444472, Testing Cost: 0.02028464927474177\n",
      "Iteration 220, Training Cost: 0.025474526638361816, Testing Cost: 0.020077430937088796\n",
      "Iteration 221, Training Cost: 0.025239944098797783, Testing Cost: 0.019874794063253317\n",
      "Iteration 222, Training Cost: 0.025009789422118418, Testing Cost: 0.019674822769807934\n",
      "Iteration 223, Training Cost: 0.02478370749592494, Testing Cost: 0.0194752325395982\n",
      "Iteration 224, Training Cost: 0.024561834753180526, Testing Cost: 0.019281273394371586\n",
      "Iteration 225, Training Cost: 0.024343985423674774, Testing Cost: 0.019083907442752967\n",
      "Iteration 226, Training Cost: 0.024129867722195237, Testing Cost: 0.018894576471014734\n",
      "Iteration 227, Training Cost: 0.02391949954908288, Testing Cost: 0.018703928316331896\n",
      "Iteration 228, Training Cost: 0.023712833595301023, Testing Cost: 0.018523462575864767\n",
      "Iteration 229, Training Cost: 0.023509520624519564, Testing Cost: 0.01833956561525494\n",
      "Iteration 230, Training Cost: 0.023309491383753118, Testing Cost: 0.018162411964777518\n",
      "Iteration 231, Training Cost: 0.023112767869379294, Testing Cost: 0.01798460524209013\n",
      "Iteration 232, Training Cost: 0.02291931303638124, Testing Cost: 0.017813834043563508\n",
      "Iteration 233, Training Cost: 0.02272875744892698, Testing Cost: 0.017641278972302197\n",
      "Iteration 234, Training Cost: 0.022541132382194774, Testing Cost: 0.01747634750691324\n",
      "Iteration 235, Training Cost: 0.02235652587937516, Testing Cost: 0.017309720354654654\n",
      "Iteration 236, Training Cost: 0.02217486884650174, Testing Cost: 0.017144784452662234\n",
      "Iteration 237, Training Cost: 0.021995777125680564, Testing Cost: 0.016985960545467124\n",
      "Iteration 238, Training Cost: 0.021819251077983274, Testing Cost: 0.016827454315599684\n",
      "Iteration 239, Training Cost: 0.02164561136027472, Testing Cost: 0.016670662346452973\n",
      "Iteration 240, Training Cost: 0.02147436413393148, Testing Cost: 0.016517525248522225\n",
      "Iteration 241, Training Cost: 0.021305633623146605, Testing Cost: 0.01636583657938569\n",
      "Iteration 242, Training Cost: 0.021139350279893988, Testing Cost: 0.016217683351984886\n",
      "Iteration 243, Training Cost: 0.020975354339468802, Testing Cost: 0.016070019052555822\n",
      "Iteration 244, Training Cost: 0.020813721764011067, Testing Cost: 0.01592710998302294\n",
      "Iteration 245, Training Cost: 0.020654336714404677, Testing Cost: 0.01578736939548433\n",
      "Iteration 246, Training Cost: 0.02049693239498759, Testing Cost: 0.01564666403138346\n",
      "Iteration 247, Training Cost: 0.02034185781785533, Testing Cost: 0.015505732928803658\n",
      "Iteration 248, Training Cost: 0.020188858300160574, Testing Cost: 0.01536730496628511\n",
      "Iteration 249, Training Cost: 0.0200378111920915, Testing Cost: 0.015233212949534398\n",
      "Iteration 250, Training Cost: 0.019888809935790466, Testing Cost: 0.015101710038109116\n",
      "Iteration 251, Training Cost: 0.01974178593412906, Testing Cost: 0.014974501854221603\n",
      "Iteration 252, Training Cost: 0.019596777184203596, Testing Cost: 0.01484435425248816\n",
      "Iteration 253, Training Cost: 0.01945334562231423, Testing Cost: 0.014715926434704738\n",
      "Iteration 254, Training Cost: 0.019311898582987035, Testing Cost: 0.014591482744398232\n",
      "Iteration 255, Training Cost: 0.019172250002559583, Testing Cost: 0.014470080485621066\n",
      "Iteration 256, Training Cost: 0.019034452032646976, Testing Cost: 0.014348831867658585\n",
      "Iteration 257, Training Cost: 0.018898224187173355, Testing Cost: 0.014228868588602157\n",
      "Iteration 258, Training Cost: 0.018763813608710683, Testing Cost: 0.014111426701900095\n",
      "Iteration 259, Training Cost: 0.01863097354064159, Testing Cost: 0.013992772049289707\n",
      "Iteration 260, Training Cost: 0.018499814900412713, Testing Cost: 0.013879163948697006\n",
      "Iteration 261, Training Cost: 0.01837022902046435, Testing Cost: 0.013767703508304509\n",
      "Iteration 262, Training Cost: 0.018242227312085888, Testing Cost: 0.013656580844346315\n",
      "Iteration 263, Training Cost: 0.018115729751813126, Testing Cost: 0.01354740153589991\n",
      "Iteration 264, Training Cost: 0.017990678054500765, Testing Cost: 0.01343728124470733\n",
      "Iteration 265, Training Cost: 0.017867144264204354, Testing Cost: 0.013328753414091033\n",
      "Iteration 266, Training Cost: 0.017745191479820288, Testing Cost: 0.0132232915917975\n",
      "Iteration 267, Training Cost: 0.01762452700500153, Testing Cost: 0.013118906537092514\n",
      "Iteration 268, Training Cost: 0.017505226605571404, Testing Cost: 0.013015820932980435\n",
      "Iteration 269, Training Cost: 0.017387366540931468, Testing Cost: 0.012916987928865522\n",
      "Iteration 270, Training Cost: 0.017270850396332534, Testing Cost: 0.012816552772156299\n",
      "Iteration 271, Training Cost: 0.017155522423912307, Testing Cost: 0.012715252479661501\n",
      "Iteration 272, Training Cost: 0.017041633273685533, Testing Cost: 0.01261702400241544\n",
      "Iteration 273, Training Cost: 0.016929028378919842, Testing Cost: 0.012521569146100626\n",
      "Iteration 274, Training Cost: 0.01681774106099924, Testing Cost: 0.012426368298959989\n",
      "Iteration 275, Training Cost: 0.016707619272307503, Testing Cost: 0.012330320137869685\n",
      "Iteration 276, Training Cost: 0.016598686583628758, Testing Cost: 0.012238138267990771\n",
      "Iteration 277, Training Cost: 0.016490990431981704, Testing Cost: 0.012146634632636008\n",
      "Iteration 278, Training Cost: 0.016384435142496243, Testing Cost: 0.01205886276490016\n",
      "Iteration 279, Training Cost: 0.016279066797220004, Testing Cost: 0.011971216316192903\n",
      "Iteration 280, Training Cost: 0.016174851327105326, Testing Cost: 0.011883993260397996\n",
      "Iteration 281, Training Cost: 0.016071761087550356, Testing Cost: 0.011797906891183511\n",
      "Iteration 282, Training Cost: 0.015969813484380733, Testing Cost: 0.011711020267819764\n",
      "Iteration 283, Training Cost: 0.015868981961184362, Testing Cost: 0.011626168149642178\n",
      "Iteration 284, Training Cost: 0.01576917950307023, Testing Cost: 0.011542709899264402\n",
      "Iteration 285, Training Cost: 0.015670412700421087, Testing Cost: 0.011457285536745345\n",
      "Iteration 286, Training Cost: 0.015572679433329779, Testing Cost: 0.011376363425900118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 287, Training Cost: 0.015476015648858544, Testing Cost: 0.011294600757960111\n",
      "Iteration 288, Training Cost: 0.015380355305891727, Testing Cost: 0.011214790011464908\n",
      "Iteration 289, Training Cost: 0.015285637689300203, Testing Cost: 0.011136890738667634\n",
      "Iteration 290, Training Cost: 0.015191897662933506, Testing Cost: 0.01105852875063616\n",
      "Iteration 291, Training Cost: 0.015099185257422739, Testing Cost: 0.010980876920192947\n",
      "Iteration 292, Training Cost: 0.015007282201097432, Testing Cost: 0.010903871277798238\n",
      "Iteration 293, Training Cost: 0.014916370947353529, Testing Cost: 0.010827137804347929\n",
      "Iteration 294, Training Cost: 0.014826290587823466, Testing Cost: 0.010751635057721103\n",
      "Iteration 295, Training Cost: 0.014737203511715974, Testing Cost: 0.010678182083924966\n",
      "Iteration 296, Training Cost: 0.014648943574477477, Testing Cost: 0.0106053113287965\n",
      "Iteration 297, Training Cost: 0.014561615304321459, Testing Cost: 0.010534145536637134\n",
      "Iteration 298, Training Cost: 0.014475156451320276, Testing Cost: 0.010462730509009373\n",
      "Iteration 299, Training Cost: 0.014389611530945087, Testing Cost: 0.010390716767194384\n",
      "Iteration 300, Training Cost: 0.014304876982105133, Testing Cost: 0.0103204213883104\n",
      "Iteration 301, Training Cost: 0.014220977041873814, Testing Cost: 0.01025288788322147\n",
      "Iteration 302, Training Cost: 0.014137914149451416, Testing Cost: 0.010185738194301339\n",
      "Iteration 303, Training Cost: 0.014055634488577747, Testing Cost: 0.010119931604903539\n",
      "Iteration 304, Training Cost: 0.013974196540973812, Testing Cost: 0.010052664489989031\n",
      "Iteration 305, Training Cost: 0.013893447531050332, Testing Cost: 0.009986743790386735\n",
      "Iteration 306, Training Cost: 0.013813467284107437, Testing Cost: 0.00992142497909875\n",
      "Iteration 307, Training Cost: 0.013734328830197344, Testing Cost: 0.009856717641948265\n",
      "Iteration 308, Training Cost: 0.013655998385177755, Testing Cost: 0.00979292960863031\n",
      "Iteration 309, Training Cost: 0.013578389082057307, Testing Cost: 0.009730003381854343\n",
      "Iteration 310, Training Cost: 0.013501460825924667, Testing Cost: 0.009668803456735297\n",
      "Iteration 311, Training Cost: 0.013425299476515218, Testing Cost: 0.009607346436946237\n",
      "Iteration 312, Training Cost: 0.013349842476774565, Testing Cost: 0.009545829249389832\n",
      "Iteration 313, Training Cost: 0.013275120711164197, Testing Cost: 0.009486233326629028\n",
      "Iteration 314, Training Cost: 0.013201050798429328, Testing Cost: 0.009426540394138693\n",
      "Iteration 315, Training Cost: 0.013127629358583066, Testing Cost: 0.00936776355368223\n",
      "Iteration 316, Training Cost: 0.013054924436937568, Testing Cost: 0.009308573895600648\n",
      "Iteration 317, Training Cost: 0.0129829010370333, Testing Cost: 0.00925010674296501\n",
      "Iteration 318, Training Cost: 0.012911594100037308, Testing Cost: 0.009193519171405632\n",
      "Iteration 319, Training Cost: 0.012840918556486301, Testing Cost: 0.009136237983833857\n",
      "Iteration 320, Training Cost: 0.012770880822087813, Testing Cost: 0.009081038364581882\n",
      "Iteration 321, Training Cost: 0.012701399872687985, Testing Cost: 0.009025923971592794\n",
      "Iteration 322, Training Cost: 0.012632604568457847, Testing Cost: 0.00897157677214207\n",
      "Iteration 323, Training Cost: 0.012564463779173208, Testing Cost: 0.00891681399023769\n",
      "Iteration 324, Training Cost: 0.012496940037883101, Testing Cost: 0.008863653938092665\n",
      "Iteration 325, Training Cost: 0.012429977657239015, Testing Cost: 0.00881049306806708\n",
      "Iteration 326, Training Cost: 0.012363675158760272, Testing Cost: 0.008758181891362361\n",
      "Iteration 327, Training Cost: 0.012297839074663341, Testing Cost: 0.008706212525437521\n",
      "Iteration 328, Training Cost: 0.01223265147012371, Testing Cost: 0.00865539880015824\n",
      "Iteration 329, Training Cost: 0.012168049595406063, Testing Cost: 0.008604721442302623\n",
      "Iteration 330, Training Cost: 0.0121040728622545, Testing Cost: 0.00855359171848004\n",
      "Iteration 331, Training Cost: 0.012040600148181643, Testing Cost: 0.008503183245804104\n",
      "Iteration 332, Training Cost: 0.011977665255983882, Testing Cost: 0.008453838080323357\n",
      "Iteration 333, Training Cost: 0.01191535419472578, Testing Cost: 0.00840520832114714\n",
      "Iteration 334, Training Cost: 0.011853548312048011, Testing Cost: 0.008357200782063975\n",
      "Iteration 335, Training Cost: 0.01179220487597161, Testing Cost: 0.00830885955701779\n",
      "Iteration 336, Training Cost: 0.011731500588049849, Testing Cost: 0.008260359955184546\n",
      "Iteration 337, Training Cost: 0.011671300233088343, Testing Cost: 0.008213075578018918\n",
      "Iteration 338, Training Cost: 0.011611584814025511, Testing Cost: 0.008166424447519284\n",
      "Iteration 339, Training Cost: 0.011552403832721506, Testing Cost: 0.008120373442344718\n",
      "Iteration 340, Training Cost: 0.011493713939766203, Testing Cost: 0.008074714653395468\n",
      "Iteration 341, Training Cost: 0.01143548729726599, Testing Cost: 0.008029491261259687\n",
      "Iteration 342, Training Cost: 0.01137776633150275, Testing Cost: 0.007983970135146465\n",
      "Iteration 343, Training Cost: 0.011320540637717763, Testing Cost: 0.007938861063299794\n",
      "Iteration 344, Training Cost: 0.011263827489348777, Testing Cost: 0.007895030933423253\n",
      "Iteration 345, Training Cost: 0.011207537847996534, Testing Cost: 0.007850625063219903\n",
      "Iteration 346, Training Cost: 0.011151743574773188, Testing Cost: 0.007806916972412915\n",
      "Iteration 347, Training Cost: 0.011096445019584879, Testing Cost: 0.0077645099526219225\n",
      "Iteration 348, Training Cost: 0.011041540091181894, Testing Cost: 0.007722499524992581\n",
      "Iteration 349, Training Cost: 0.010987136982815702, Testing Cost: 0.007679684490435369\n",
      "Iteration 350, Training Cost: 0.010933202772367607, Testing Cost: 0.007638087708505329\n",
      "Iteration 351, Training Cost: 0.010879626387402946, Testing Cost: 0.007595494405950369\n",
      "Iteration 352, Training Cost: 0.010826516716794469, Testing Cost: 0.007555163257060995\n",
      "Iteration 353, Training Cost: 0.010773817589319421, Testing Cost: 0.007515501581098525\n",
      "Iteration 354, Training Cost: 0.010721556913980662, Testing Cost: 0.00747548069900792\n",
      "Iteration 355, Training Cost: 0.010669737657795369, Testing Cost: 0.007434659540029724\n",
      "Iteration 356, Training Cost: 0.01061830903539017, Testing Cost: 0.007394896729442054\n",
      "Iteration 357, Training Cost: 0.010567352134865152, Testing Cost: 0.007355699693705612\n",
      "Iteration 358, Training Cost: 0.010516752176065486, Testing Cost: 0.007317399984051098\n",
      "Iteration 359, Training Cost: 0.010466571415290138, Testing Cost: 0.007279755329384201\n",
      "Iteration 360, Training Cost: 0.010416778965125282, Testing Cost: 0.007241308547289794\n",
      "Iteration 361, Training Cost: 0.010367397509060565, Testing Cost: 0.007203983231837184\n",
      "Iteration 362, Training Cost: 0.010318428139855777, Testing Cost: 0.007167594760954891\n",
      "Iteration 363, Training Cost: 0.010269797506917186, Testing Cost: 0.0071307793225654295\n",
      "Iteration 364, Training Cost: 0.010221558029455175, Testing Cost: 0.007093802695350453\n",
      "Iteration 365, Training Cost: 0.01017375669841073, Testing Cost: 0.007057306083937294\n",
      "Iteration 366, Training Cost: 0.010126317091743192, Testing Cost: 0.007021179430708048\n",
      "Iteration 367, Training Cost: 0.010079241286340848, Testing Cost: 0.006985287955662845\n",
      "Iteration 368, Training Cost: 0.010032517583812794, Testing Cost: 0.006949963964365047\n",
      "Iteration 369, Training Cost: 0.009986147379784765, Testing Cost: 0.006914975456580723\n",
      "Iteration 370, Training Cost: 0.009940096277368986, Testing Cost: 0.006880801773086239\n",
      "Iteration 371, Training Cost: 0.009894436459777431, Testing Cost: 0.006846683327745528\n",
      "Iteration 372, Training Cost: 0.00984910981687286, Testing Cost: 0.006813535068033878\n",
      "Iteration 373, Training Cost: 0.00980413248166635, Testing Cost: 0.006780128598202722\n",
      "Iteration 374, Training Cost: 0.009759545320972996, Testing Cost: 0.006746964769657414\n",
      "Iteration 375, Training Cost: 0.0097152108919282, Testing Cost: 0.006712970364040085\n",
      "Iteration 376, Training Cost: 0.009671268095894, Testing Cost: 0.006679944602573977\n",
      "Iteration 377, Training Cost: 0.009627655876318813, Testing Cost: 0.006647046219405018\n",
      "Iteration 378, Training Cost: 0.009584335411713897, Testing Cost: 0.006614564130548047\n",
      "Iteration 379, Training Cost: 0.009541340562086983, Testing Cost: 0.00658210383585015\n",
      "Iteration 380, Training Cost: 0.009498656946125002, Testing Cost: 0.006550240876463508\n",
      "Iteration 381, Training Cost: 0.00945632529802847, Testing Cost: 0.006518001019970665\n",
      "Iteration 382, Training Cost: 0.00941430970080499, Testing Cost: 0.006486307904737274\n",
      "Iteration 383, Training Cost: 0.009372604663267668, Testing Cost: 0.006455202172856044\n",
      "Iteration 384, Training Cost: 0.009331194617643165, Testing Cost: 0.006423734217731765\n",
      "Iteration 385, Training Cost: 0.00929009818656185, Testing Cost: 0.006393331118477867\n",
      "Iteration 386, Training Cost: 0.009249287879049098, Testing Cost: 0.00636272577643045\n",
      "Iteration 387, Training Cost: 0.009208801778259276, Testing Cost: 0.006332884339303424\n",
      "Iteration 388, Training Cost: 0.00916855290627474, Testing Cost: 0.006302436202120375\n",
      "Iteration 389, Training Cost: 0.00912865328012561, Testing Cost: 0.006273197197203956\n",
      "Iteration 390, Training Cost: 0.009089036032316379, Testing Cost: 0.006244129141963599\n",
      "Iteration 391, Training Cost: 0.009049675968448826, Testing Cost: 0.006214304754784179\n",
      "Iteration 392, Training Cost: 0.00901060604827022, Testing Cost: 0.006185398507533513\n",
      "Iteration 393, Training Cost: 0.008971831640299185, Testing Cost: 0.006157076605593289\n",
      "Iteration 394, Training Cost: 0.008933337697456705, Testing Cost: 0.00612869717480983\n",
      "Iteration 395, Training Cost: 0.008895137510397352, Testing Cost: 0.006101103551474392\n",
      "Iteration 396, Training Cost: 0.008857166210801721, Testing Cost: 0.00607255829236197\n",
      "Iteration 397, Training Cost: 0.008819473927692202, Testing Cost: 0.006044642356977321\n",
      "Iteration 398, Training Cost: 0.008782035007689133, Testing Cost: 0.006017311386951081\n",
      "Iteration 399, Training Cost: 0.008744850909103161, Testing Cost: 0.005990062345740782\n",
      "Iteration 400, Training Cost: 0.008707941756168727, Testing Cost: 0.005962700599647878\n",
      "Iteration 401, Training Cost: 0.00867131996546888, Testing Cost: 0.005935678011025586\n",
      "Iteration 402, Training Cost: 0.008634913416159523, Testing Cost: 0.005909377896281482\n",
      "Iteration 403, Training Cost: 0.008598760367158207, Testing Cost: 0.005883170090338654\n",
      "Iteration 404, Training Cost: 0.008562905452892134, Testing Cost: 0.005857009075583632\n",
      "Iteration 405, Training Cost: 0.008527279240564171, Testing Cost: 0.005831202704929767\n",
      "Iteration 406, Training Cost: 0.008491876752077954, Testing Cost: 0.005805335292485218\n",
      "Iteration 407, Training Cost: 0.008456713091366612, Testing Cost: 0.005779625239869727\n",
      "Iteration 408, Training Cost: 0.008421813696422639, Testing Cost: 0.005754610737578063\n",
      "Iteration 409, Training Cost: 0.008387150709300179, Testing Cost: 0.00572911853920029\n",
      "Iteration 410, Training Cost: 0.008352716300726511, Testing Cost: 0.005704107578357372\n",
      "Iteration 411, Training Cost: 0.00831851210036602, Testing Cost: 0.00567892698482667\n",
      "Iteration 412, Training Cost: 0.008284537593509157, Testing Cost: 0.0056545575539879355\n",
      "Iteration 413, Training Cost: 0.008250788088592767, Testing Cost: 0.005629964204240517\n",
      "Iteration 414, Training Cost: 0.008217270859252985, Testing Cost: 0.0056056516998547745\n",
      "Iteration 415, Training Cost: 0.008183960041749757, Testing Cost: 0.005581527015343339\n",
      "Iteration 416, Training Cost: 0.008150884266386636, Testing Cost: 0.005557175958591004\n",
      "Iteration 417, Training Cost: 0.008118034154567586, Testing Cost: 0.005533159341173717\n",
      "Iteration 418, Training Cost: 0.008085395100871145, Testing Cost: 0.0055094632964976745\n",
      "Iteration 419, Training Cost: 0.008052955167735623, Testing Cost: 0.005486348001856639\n",
      "Iteration 420, Training Cost: 0.008020765898942385, Testing Cost: 0.00546324593327826\n",
      "Iteration 421, Training Cost: 0.007988766375175118, Testing Cost: 0.005440342515580189\n",
      "Iteration 422, Training Cost: 0.007956989012105443, Testing Cost: 0.005417499683268073\n",
      "Iteration 423, Training Cost: 0.007925405423409438, Testing Cost: 0.005395033306380418\n",
      "Iteration 424, Training Cost: 0.007894027224352123, Testing Cost: 0.005371797922721748\n",
      "Iteration 425, Training Cost: 0.007862854757324022, Testing Cost: 0.00534959366009133\n",
      "Iteration 426, Training Cost: 0.00783188132920363, Testing Cost: 0.005327389531154255\n",
      "Iteration 427, Training Cost: 0.00780113557944304, Testing Cost: 0.005305348630078259\n",
      "Iteration 428, Training Cost: 0.007770540542481493, Testing Cost: 0.005283456912317614\n",
      "Iteration 429, Training Cost: 0.0077401814047424345, Testing Cost: 0.005261482131078858\n",
      "Iteration 430, Training Cost: 0.0077100220908360405, Testing Cost: 0.005240140538934081\n",
      "Iteration 431, Training Cost: 0.007680058572257344, Testing Cost: 0.005218892614764422\n",
      "Iteration 432, Training Cost: 0.007650268680271533, Testing Cost: 0.0051977685206193325\n",
      "Iteration 433, Training Cost: 0.007620681024448814, Testing Cost: 0.005176444770948148\n",
      "Iteration 434, Training Cost: 0.007591230277892457, Testing Cost: 0.005155433215457526\n",
      "Iteration 435, Training Cost: 0.007562032033689433, Testing Cost: 0.005134874010121471\n",
      "Iteration 436, Training Cost: 0.007532983047147805, Testing Cost: 0.005113771162989604\n",
      "Iteration 437, Training Cost: 0.007504118697325484, Testing Cost: 0.005092985237200988\n",
      "Iteration 438, Training Cost: 0.007475448414604531, Testing Cost: 0.005072637449186968\n",
      "Iteration 439, Training Cost: 0.007446961541299255, Testing Cost: 0.005052913569125925\n",
      "Iteration 440, Training Cost: 0.007418649158813295, Testing Cost: 0.005032819865648601\n",
      "Iteration 441, Training Cost: 0.007390532115645549, Testing Cost: 0.0050129375254124205\n",
      "Iteration 442, Training Cost: 0.007362546542409677, Testing Cost: 0.004993354954072371\n",
      "Iteration 443, Training Cost: 0.007334735901373336, Testing Cost: 0.004973493128863593\n",
      "Iteration 444, Training Cost: 0.007307087691879569, Testing Cost: 0.004953691347622118\n",
      "Iteration 445, Training Cost: 0.007279615032756605, Testing Cost: 0.004934369688200662\n",
      "Iteration 446, Training Cost: 0.0072523393717971955, Testing Cost: 0.004915227185823692\n",
      "Iteration 447, Training Cost: 0.007225240708086499, Testing Cost: 0.004896105880987849\n",
      "Iteration 448, Training Cost: 0.007198291905069356, Testing Cost: 0.004876982799297557\n",
      "Iteration 449, Training Cost: 0.0071714778732624935, Testing Cost: 0.004858061465708713\n",
      "Iteration 450, Training Cost: 0.007144865949495224, Testing Cost: 0.004839533512810534\n",
      "Iteration 451, Training Cost: 0.007118389615595568, Testing Cost: 0.004821338935948109\n",
      "Iteration 452, Training Cost: 0.007092073349426946, Testing Cost: 0.004802747340715389\n",
      "Iteration 453, Training Cost: 0.007065922798103771, Testing Cost: 0.004784794363724598\n",
      "Iteration 454, Training Cost: 0.007039955065610284, Testing Cost: 0.0047666787763797\n",
      "Iteration 455, Training Cost: 0.007014148516845313, Testing Cost: 0.004748665568473198\n",
      "Iteration 456, Training Cost: 0.00698847302265203, Testing Cost: 0.004730968702127429\n",
      "Iteration 457, Training Cost: 0.006962945110800304, Testing Cost: 0.004713178660929468\n",
      "Iteration 458, Training Cost: 0.0069375495633368315, Testing Cost: 0.004695490747824697\n",
      "Iteration 459, Training Cost: 0.0069123214521592475, Testing Cost: 0.004677674136605559\n",
      "Iteration 460, Training Cost: 0.006887225880077692, Testing Cost: 0.004660213832982823\n",
      "Iteration 461, Training Cost: 0.006862312114401828, Testing Cost: 0.004642619683054517\n",
      "Iteration 462, Training Cost: 0.006837503203825997, Testing Cost: 0.004624999063107661\n",
      "Iteration 463, Training Cost: 0.006812875396271889, Testing Cost: 0.0046078667939190516\n",
      "Iteration 464, Training Cost: 0.006788363048791108, Testing Cost: 0.004591049349579868\n",
      "Iteration 465, Training Cost: 0.00676402432490151, Testing Cost: 0.004574109434906871\n",
      "Iteration 466, Training Cost: 0.006739810677671243, Testing Cost: 0.004557225471442207\n",
      "Iteration 467, Training Cost: 0.00671573167310815, Testing Cost: 0.004540371477543169\n",
      "Iteration 468, Training Cost: 0.006691824230760559, Testing Cost: 0.004523814233564486\n",
      "Iteration 469, Training Cost: 0.006668021296008667, Testing Cost: 0.004507570757445449\n",
      "Iteration 470, Training Cost: 0.0066443823535500945, Testing Cost: 0.004491082074434555\n",
      "Iteration 471, Training Cost: 0.0066208594311325, Testing Cost: 0.0044751179806700484\n",
      "Iteration 472, Training Cost: 0.006597478290902651, Testing Cost: 0.0044592484840356526\n",
      "Iteration 473, Training Cost: 0.0065742472242877005, Testing Cost: 0.004443309759438544\n",
      "Iteration 474, Training Cost: 0.006551128380927142, Testing Cost: 0.00442739766663252\n",
      "Iteration 475, Training Cost: 0.006528127502473105, Testing Cost: 0.0044113550584656555\n",
      "Iteration 476, Training Cost: 0.006505276189033341, Testing Cost: 0.004395818053853935\n",
      "Iteration 477, Training Cost: 0.006482528297532596, Testing Cost: 0.004380631000466666\n",
      "Iteration 478, Training Cost: 0.006459944205889574, Testing Cost: 0.004365005123011792\n",
      "Iteration 479, Training Cost: 0.006437495123810912, Testing Cost: 0.0043496940664001504\n",
      "Iteration 480, Training Cost: 0.0064151429580930055, Testing Cost: 0.00433440972643812\n",
      "Iteration 481, Training Cost: 0.0063929206537095065, Testing Cost: 0.004319155198133738\n",
      "Iteration 482, Training Cost: 0.006370823718987068, Testing Cost: 0.004304047551192072\n",
      "Iteration 483, Training Cost: 0.006348861595635634, Testing Cost: 0.0042891969373932935\n",
      "Iteration 484, Training Cost: 0.006327016566672522, Testing Cost: 0.004274249876114141\n",
      "Iteration 485, Training Cost: 0.00630530336061898, Testing Cost: 0.0042593829601473236\n",
      "Iteration 486, Training Cost: 0.006283702237274283, Testing Cost: 0.00424474587016377\n",
      "Iteration 487, Training Cost: 0.0062621902415914005, Testing Cost: 0.004230193154316613\n",
      "Iteration 488, Training Cost: 0.006240829185468758, Testing Cost: 0.0042155287406361285\n",
      "Iteration 489, Training Cost: 0.006219590990227925, Testing Cost: 0.0042008733879366125\n",
      "Iteration 490, Training Cost: 0.006198459293523911, Testing Cost: 0.004186454641508534\n",
      "Iteration 491, Training Cost: 0.006177459409120817, Testing Cost: 0.004172351055296998\n",
      "Iteration 492, Training Cost: 0.006156540563989101, Testing Cost: 0.004158450736594896\n",
      "Iteration 493, Training Cost: 0.006135756873640983, Testing Cost: 0.004144192848864392\n",
      "Iteration 494, Training Cost: 0.006115076213884269, Testing Cost: 0.004130099262653957\n",
      "Iteration 495, Training Cost: 0.006094528132004391, Testing Cost: 0.00411614777086089\n",
      "Iteration 496, Training Cost: 0.006074060515444966, Testing Cost: 0.004102354714617535\n",
      "Iteration 497, Training Cost: 0.00605371933320535, Testing Cost: 0.0040887192074431635\n",
      "Iteration 498, Training Cost: 0.006033486148991679, Testing Cost: 0.004075125252290639\n",
      "Iteration 499, Training Cost: 0.006013353367196728, Testing Cost: 0.004061637201451662\n",
      "Iteration 500, Training Cost: 0.005993328102094763, Testing Cost: 0.004047970910676759\n",
      "Iteration 501, Training Cost: 0.005973430298386414, Testing Cost: 0.004034551326030238\n",
      "Iteration 502, Training Cost: 0.005953628448957587, Testing Cost: 0.004021126369398124\n",
      "Iteration 503, Training Cost: 0.005933923454902631, Testing Cost: 0.004007857114937474\n",
      "Iteration 504, Training Cost: 0.005914329016092442, Testing Cost: 0.003994902957437266\n",
      "Iteration 505, Training Cost: 0.005894847002501361, Testing Cost: 0.003981716403723505\n",
      "Iteration 506, Training Cost: 0.005875465536357591, Testing Cost: 0.003968726141929397\n",
      "Iteration 507, Training Cost: 0.005856185681488516, Testing Cost: 0.00395585095357239\n",
      "Iteration 508, Training Cost: 0.0058369949875455825, Testing Cost: 0.003942870564842621\n",
      "Iteration 509, Training Cost: 0.005817911263088475, Testing Cost: 0.003930007160214687\n",
      "Iteration 510, Training Cost: 0.0057989189393581695, Testing Cost: 0.003917464188182936\n",
      "Iteration 511, Training Cost: 0.005780028807975172, Testing Cost: 0.0039046977645782054\n",
      "Iteration 512, Training Cost: 0.005761243559207287, Testing Cost: 0.0038921202429756025\n",
      "Iteration 513, Training Cost: 0.005742543975437033, Testing Cost: 0.0038796292883734314\n",
      "Iteration 514, Training Cost: 0.005723941656564191, Testing Cost: 0.003867358357391623\n",
      "Iteration 515, Training Cost: 0.0057054512140847155, Testing Cost: 0.0038551040735307035\n",
      "Iteration 516, Training Cost: 0.005687054472080215, Testing Cost: 0.003842979821057655\n",
      "Iteration 517, Training Cost: 0.00566873719884381, Testing Cost: 0.0038307926181229976\n",
      "Iteration 518, Training Cost: 0.005650519994912832, Testing Cost: 0.0038186706565375046\n",
      "Iteration 519, Training Cost: 0.005632406011748144, Testing Cost: 0.0038066692672478492\n",
      "Iteration 520, Training Cost: 0.005614378617255358, Testing Cost: 0.003794597330250518\n",
      "Iteration 521, Training Cost: 0.005596426353711927, Testing Cost: 0.0037826095674470196\n",
      "Iteration 522, Training Cost: 0.005578567200722712, Testing Cost: 0.0037706808814451753\n",
      "Iteration 523, Training Cost: 0.005560824080277694, Testing Cost: 0.003758983570782664\n",
      "Iteration 524, Training Cost: 0.005543165553059824, Testing Cost: 0.0037471992259533434\n",
      "Iteration 525, Training Cost: 0.005525591079824771, Testing Cost: 0.0037355320716227487\n",
      "Iteration 526, Training Cost: 0.005508091770955322, Testing Cost: 0.003723758312236704\n",
      "Iteration 527, Training Cost: 0.005490682565866892, Testing Cost: 0.0037123363422336447\n",
      "Iteration 528, Training Cost: 0.005473367177844236, Testing Cost: 0.003700952567300924\n",
      "Iteration 529, Training Cost: 0.005456137250797634, Testing Cost: 0.00368954939657806\n",
      "Iteration 530, Training Cost: 0.005438995614995677, Testing Cost: 0.0036782827848208977\n",
      "Iteration 531, Training Cost: 0.005421925323016097, Testing Cost: 0.003667088892956646\n",
      "Iteration 532, Training Cost: 0.005404947861302811, Testing Cost: 0.0036559949135563617\n",
      "Iteration 533, Training Cost: 0.005388062734283463, Testing Cost: 0.003644903984141707\n",
      "Iteration 534, Training Cost: 0.0053712396360321235, Testing Cost: 0.0036340449523611317\n",
      "Iteration 535, Training Cost: 0.005354511599912825, Testing Cost: 0.003623302517967205\n",
      "Iteration 536, Training Cost: 0.0053378704727633335, Testing Cost: 0.003612321381339473\n",
      "Iteration 537, Training Cost: 0.00532131056414901, Testing Cost: 0.003601504068771288\n",
      "Iteration 538, Training Cost: 0.005304817395422888, Testing Cost: 0.003590766742778489\n",
      "Iteration 539, Training Cost: 0.005288413538055522, Testing Cost: 0.0035801613324120537\n",
      "Iteration 540, Training Cost: 0.005272097576893074, Testing Cost: 0.0035694475250670184\n",
      "Iteration 541, Training Cost: 0.005255845402448053, Testing Cost: 0.0035588275126401834\n",
      "Iteration 542, Training Cost: 0.005239684762932619, Testing Cost: 0.0035481470000826676\n",
      "Iteration 543, Training Cost: 0.0052235939817514634, Testing Cost: 0.0035377771562498934\n",
      "Iteration 544, Training Cost: 0.005207598498469592, Testing Cost: 0.0035273429639530235\n",
      "Iteration 545, Training Cost: 0.005191679053217406, Testing Cost: 0.0035170518806561116\n",
      "Iteration 546, Training Cost: 0.005175838557503696, Testing Cost: 0.003506734852440124\n",
      "Iteration 547, Training Cost: 0.0051600714553891765, Testing Cost: 0.0034966708938985284\n",
      "Iteration 548, Training Cost: 0.005144368420502579, Testing Cost: 0.003486292188786304\n",
      "Iteration 549, Training Cost: 0.005128757279411884, Testing Cost: 0.003476154957855631\n",
      "Iteration 550, Training Cost: 0.00511321271415081, Testing Cost: 0.003466046962146781\n",
      "Iteration 551, Training Cost: 0.005097743583646977, Testing Cost: 0.0034561678435365726\n",
      "Iteration 552, Training Cost: 0.005082338567627877, Testing Cost: 0.0034462917722609685\n",
      "Iteration 553, Training Cost: 0.005067017968780987, Testing Cost: 0.003436403744058567\n",
      "Iteration 554, Training Cost: 0.005051767154486025, Testing Cost: 0.0034266808935959126\n",
      "Iteration 555, Training Cost: 0.0050365960425572875, Testing Cost: 0.0034168070216173455\n",
      "Iteration 556, Training Cost: 0.005021494276872395, Testing Cost: 0.003407043547571273\n",
      "Iteration 557, Training Cost: 0.005006463653154797, Testing Cost: 0.003397328407363251\n",
      "Iteration 558, Training Cost: 0.004991480364065094, Testing Cost: 0.003387588627515483\n",
      "Converged at training cost :0.004991480364065094 while Epsilon:0.005 \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define your neural network model and parameters\n",
    "model = NeuralNetwork([X_wine.shape[1], 6, 5, y_encoded.shape[1]])  # Your desired architecture\n",
    "learning_rate = 0.01\n",
    "batch_size = 32\n",
    "max_iterations = 1000\n",
    "epsilon = 0.005\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_wine, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "mean = np.mean(X_train, axis=0)\n",
    "std = np.std(X_train, axis=0)\n",
    "X_train_normalized = (X_train - mean) / std\n",
    "X_test_normalized = (X_test - mean) / std\n",
    "\n",
    "# Train the model using mini-batch gradient descent\n",
    "training_errors, testing_errors = train_mini_batch(X_train_normalized.to_numpy(), y_train, X_test_normalized.to_numpy(), y_test, model, learning_rate, batch_size, max_iterations, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c6a6e80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the learning curve\n",
    "step_size = 50\n",
    "plot_learning_curve(training_errors, testing_errors, step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931c0700",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
