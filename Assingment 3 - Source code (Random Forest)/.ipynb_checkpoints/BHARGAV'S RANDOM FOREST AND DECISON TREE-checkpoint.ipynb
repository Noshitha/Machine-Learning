{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8148872",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import time\n",
    "import operator\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "class DecisionNode:\n",
    "    \"\"\"Class to represent a single node in a decision tree.\"\"\"\n",
    "\n",
    "    def __init__(self, left, right, decision_function, class_label=None):\n",
    "        \"\"\"Create a decision function to select between left and right nodes.\n",
    "        Note: In this representation 'True' values for a decision take us to\n",
    "        the left. This is arbitrary but is important for this assignment.\n",
    "        Args:\n",
    "            left (DecisionNode): left child node.\n",
    "            right (DecisionNode): right child node.\n",
    "            decision_function (func): function to decide left or right node.\n",
    "            class_label (int): label for leaf node. Default is None.\n",
    "        \"\"\"\n",
    "\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.decision_function = decision_function\n",
    "        self.class_label = class_label\n",
    "\n",
    "    def decide(self, feature):\n",
    "        \"\"\"Get a child node based on the decision function.\n",
    "        Args:\n",
    "            feature (list(int)): vector for feature.\n",
    "        Return:\n",
    "            Class label if a leaf node, otherwise a child node.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.class_label is not None:\n",
    "            return self.class_label\n",
    "\n",
    "        elif self.decision_function(feature):\n",
    "            return self.left.decide(feature)\n",
    "\n",
    "        else:\n",
    "            return self.right.decide(feature)\n",
    "\n",
    "\n",
    "def load_csv(data_file_path, class_index=-1):\n",
    "    \"\"\"Load csv data in a numpy array.\n",
    "    Args:\n",
    "        data_file_path (str): path to data file.\n",
    "        class_index (int): slice output by index.\n",
    "    Returns:\n",
    "        features, classes as numpy arrays if class_index is specified,\n",
    "            otherwise all as nump array.\n",
    "    \"\"\"\n",
    "\n",
    "    handle = open(data_file_path, 'r')\n",
    "    contents = handle.read()\n",
    "    handle.close()\n",
    "    rows = contents.split('\\n')\n",
    "    out = np.array([[float(i) for i in r.split(',')] for r in rows if r])\n",
    "\n",
    "    if(class_index == -1):\n",
    "        classes = out[:, class_index]\n",
    "        features = out[:, :class_index]\n",
    "        return features, classes\n",
    "    elif(class_index == 0):\n",
    "        classes = out[:, class_index]\n",
    "        features = out[:, 1:]\n",
    "        return features, classes\n",
    "\n",
    "    else:\n",
    "        return out\n",
    "\n",
    "\n",
    "def build_decision_tree():\n",
    "    \"\"\"Create a decision tree capable of handling the sample data.\n",
    "    Tree is built fully starting from the root.\n",
    "    Returns:\n",
    "        The root node of the decision tree.\n",
    "    \"\"\"\n",
    "\n",
    "    decision_tree_root = DecisionNode(\n",
    "        None, None, lambda feature: feature[0] == 0)\n",
    "    decision_tree_root.right = DecisionNode(None, None, None, 1)\n",
    "    decision_tree_root.left = DecisionNode(\n",
    "        None, None, lambda feature: feature[3] == 0)\n",
    "    decision_tree_root.left.left = DecisionNode(\n",
    "        None, None, lambda feature: feature[2] == 0)\n",
    "    decision_tree_root.left.left.left = DecisionNode(None, None, None, 1)\n",
    "    decision_tree_root.left.left.right = DecisionNode(None, None, None, 0)\n",
    "    decision_tree_root.left.right = DecisionNode(\n",
    "        None, None, lambda feature: feature[1] == 0)\n",
    "    decision_tree_root.left.right.left = DecisionNode(None, None, None, 1)\n",
    "    decision_tree_root.left.right.right = DecisionNode(None, None, None, 0)\n",
    "    return decision_tree_root\n",
    "\n",
    "\n",
    "def confusion_matrix(classifier_output, true_labels):\n",
    "    \"\"\"Create a confusion matrix to measure classifier performance.\n",
    "    Output will in the format:\n",
    "        [[true_positive, false_negative],\n",
    "         [false_positive, true_negative]]\n",
    "    Args:\n",
    "        classifier_output (list(int)): output from classifier.\n",
    "        true_labels: (list(int): correct classified labels.\n",
    "    Returns:\n",
    "        A two dimensional array representing the confusion matrix.\n",
    "    \"\"\"\n",
    "    x = list(zip(true_labels, classifier_output))\n",
    "    true_positive = len([ele for ele in x if ele == (1, 1)])\n",
    "    false_positive = len([ele for ele in x if ele == (0, 1)])\n",
    "    false_negative = len([ele for ele in x if ele == (1, 0)])\n",
    "    true_negative = len([ele for ele in x if ele == (0, 0)])\n",
    "    return np.array([[true_positive, false_negative],\n",
    "                     [false_positive, true_negative]])\n",
    "\n",
    "\n",
    "def precision(classifier_output, true_labels):\n",
    "    \"\"\"Get the precision of a classifier compared to the correct values.\n",
    "    Precision is measured as:\n",
    "        true_positive/ (true_positive + false_positive)\n",
    "    Args:\n",
    "        classifier_output (list(int)): output from classifier.\n",
    "        true_labels: (list(int): correct classified labels.\n",
    "    Returns:\n",
    "        The precision of the classifier output.\n",
    "    \"\"\"\n",
    "    x = list(zip(true_labels, classifier_output))\n",
    "    true_positive = len([ele for ele in x if ele == (1, 1)])\n",
    "    false_positive = len([ele for ele in x if ele == (0, 1)])\n",
    "    return true_positive / (true_positive + false_positive)\n",
    "\n",
    "\n",
    "def recall(classifier_output, true_labels):\n",
    "    \"\"\"Get the recall of a classifier compared to the correct values.\n",
    "    Recall is measured as:\n",
    "        true_positive/ (true_positive + false_negative)\n",
    "    Args:\n",
    "        classifier_output (list(int)): output from classifier.\n",
    "        true_labels: (list(int): correct classified labels.\n",
    "    Returns:\n",
    "        The recall of the classifier output.\n",
    "    \"\"\"\n",
    "    x = list(zip(true_labels, classifier_output))\n",
    "    true_positive = len([ele for ele in x if ele == (1, 1)])\n",
    "    false_negative = len([ele for ele in x if ele == (1, 0)])\n",
    "    return true_positive / (true_positive + false_negative)\n",
    "\n",
    "\n",
    "def accuracy(classifier_output, true_labels):\n",
    "    \"\"\"Get the accuracy of a classifier compared to the correct values.\n",
    "    Accuracy is measured as:\n",
    "        correct_classifications / total_number_examples\n",
    "    Args:\n",
    "        classifier_output (list(int)): output from classifier.\n",
    "        true_labels: (list(int): correct classified labels.\n",
    "    Returns:\n",
    "        The accuracy of the classifier output.\n",
    "    \"\"\"\n",
    "    return (np.array(classifier_output) == np.array(true_labels)).sum()/len(true_labels)\n",
    "\n",
    "\n",
    "def gini_impurity(class_vector):\n",
    "    \"\"\"Compute the gini impurity for a list of classes.\n",
    "    This is a measure of how often a randomly chosen element\n",
    "    drawn from the class_vector would be incorrectly labeled\n",
    "    if it was randomly labeled according to the distribution\n",
    "    of the labels in the class_vector.\n",
    "    It reaches its minimum at zero when all elements of class_vector\n",
    "    belong to the same class.\n",
    "    Args:\n",
    "        class_vector (list(int)): Vector of classes given as 0 or 1.\n",
    "    Returns:\n",
    "        Floating point number representing the gini impurity.\n",
    "    \"\"\"\n",
    "    return 1-np.square(np.array(list(Counter(class_vector).values()))/len(class_vector)).sum()\n",
    "\n",
    "\n",
    "def gini_gain(previous_classes, current_classes):\n",
    "    \"\"\"Compute the gini impurity gain between the previous and current classes.\n",
    "    Args:\n",
    "        previous_classes (list(int)): Vector of classes given as 0 or 1.\n",
    "        current_classes (list(list(int): A list of lists where each list has\n",
    "            0 and 1 values).\n",
    "    Returns:\n",
    "        Floating point number representing the information gain.\n",
    "    \"\"\"\n",
    "    current_classes_length = np.hstack(current_classes).flatten().shape[0]\n",
    "    return gini_impurity(previous_classes) - np.array([(len(ele)/current_classes_length)*gini_impurity(ele)\n",
    "                                                       for ele in current_classes]).sum()\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "    \"\"\"Class for automatic tree-building and classification.\"\"\"\n",
    "\n",
    "    def __init__(self, depth_limit=float('inf')):\n",
    "        \"\"\"Create a decision tree with a set depth limit.\n",
    "        Starts with an empty root.\n",
    "        Args:\n",
    "            depth_limit (float): The maximum depth to build the tree.\n",
    "        \"\"\"\n",
    "\n",
    "        self.root = None\n",
    "        self.depth_limit = depth_limit\n",
    "\n",
    "    def fit(self, features, classes):\n",
    "        \"\"\"Build the tree from root using __build_tree__().\n",
    "        Args:\n",
    "            features (m x n): m examples with n features.\n",
    "            classes (m x 1): Array of Classes.\n",
    "        \"\"\"\n",
    "\n",
    "        self.root = self.__build_tree__(features, classes)\n",
    "\n",
    "    def __build_tree__(self, features, classes, depth=0):\n",
    "        \"\"\"Build tree that automatically finds the decision functions.\n",
    "        Args:\n",
    "            features (m x n): m examples with n features.\n",
    "            classes (m x 1): Array of Classes.\n",
    "            depth (int): depth to build tree to.\n",
    "        Returns:\n",
    "            Root node of decision tree.\n",
    "        Check for base cases:\n",
    "            If all elements of a list are of the same class, return a leaf node with the appropriate class label.\n",
    "            If a specified depth limit is reached, return a leaf labeled with the most frequent class.\n",
    "        For each attribute alpha: evaluate the normalized gini gain gained by splitting on attribute alpha.\n",
    "        Let alpha_best be the attribute with the highest normalized gini gain.\n",
    "        Create a decision node that splits on alpha_best.\n",
    "        Repeat on the sublists obtained by splitting on alpha_best, and add those nodes as children of this node\n",
    "        \"\"\"\n",
    "        # Check for base cases:\n",
    "        classes_counter = dict(Counter(classes))\n",
    "        # If all elements of a list are of the same class, return a leaf node with the appropriate class label.\n",
    "        if len(classes_counter) == 1:\n",
    "            return DecisionNode(None, None, None, list(classes_counter.keys())[0])\n",
    "        # If a specified depth limit is reached, return a leaf labeled with the most frequent class.\n",
    "        if depth == self.depth_limit:\n",
    "            return DecisionNode(None, None, None, max(classes_counter.items(), key=operator.itemgetter(1))[0])\n",
    "\n",
    "        # For each attribute alpha: evaluate the normalized gini gain gained by splitting on attribute alpha.\n",
    "        # Let alpha_best be the attribute with the highest normalized gini gain.\n",
    "        feature_list = [ele for ele in features.transpose()]\n",
    "        feature_mean_list = [ele.mean() for ele in feature_list]\n",
    "        gini_gain_list = [gini_gain(classes, (np.array(classes[np.argwhere(selected_features < selected_features_mean).flatten()]).tolist(\n",
    "        ), np.array(classes[np.argwhere(selected_features >= selected_features_mean).flatten()]).tolist())) for selected_features, selected_features_mean in zip(feature_list, feature_mean_list)]\n",
    "        normalized_gini_gain = gini_gain_list/sum(gini_gain_list)\n",
    "        alpha_best_index = normalized_gini_gain.argmax()\n",
    "\n",
    "        right = np.argwhere(\n",
    "            feature_list[alpha_best_index] < feature_mean_list[alpha_best_index]).flatten().tolist()\n",
    "        left = np.argwhere(feature_list[alpha_best_index] >=\n",
    "                           feature_mean_list[alpha_best_index]).flatten().tolist()\n",
    "\n",
    "        return DecisionNode(self.__build_tree__(features[left, :], classes[left], depth+1),\n",
    "                            self.__build_tree__(\n",
    "                                features[right, :], classes[right], depth+1),\n",
    "                            lambda feature: feature[alpha_best_index] >= feature_mean_list[alpha_best_index])\n",
    "\n",
    "    def classify(self, features):\n",
    "        \"\"\"Use the fitted tree to classify a list of example features.\n",
    "        Args:\n",
    "            features (m x n): m examples with n features.\n",
    "        Return:\n",
    "            A list of class labels.\n",
    "        \"\"\"\n",
    "\n",
    "        class_labels = []\n",
    "        for feature in features:\n",
    "            class_labels.append(self.root.decide(feature))\n",
    "        return class_labels\n",
    "\n",
    "\n",
    "def generate_k_folds(dataset, k):\n",
    "    \"\"\"Split dataset into folds.\n",
    "    Randomly split data into k equal subsets.\n",
    "    Fold is a tuple (training_set, test_set).\n",
    "    Set is a tuple (features, classes).\n",
    "    Args:\n",
    "        dataset: dataset to be split.\n",
    "        k (int): number of subsections to create.\n",
    "    Returns:\n",
    "        List of folds.\n",
    "        => Each fold is a tuple of sets.\n",
    "        => Each Set is a tuple of numpy arrays.\n",
    "    \"\"\"\n",
    "\n",
    "    k_folds = []\n",
    "    features, classes = dataset\n",
    "    for _ in range(k):\n",
    "        indexes = np.random.choice(\n",
    "            range(len(features)), int(len(features)/k), replace=False)\n",
    "        invert_indexes = np.array(\n",
    "            [ele for ele in range(len(features)) if ele not in indexes])\n",
    "        k_folds.append(((features[invert_indexes, :], classes[invert_indexes]),\n",
    "                        (features[indexes, :], classes[indexes])))\n",
    "    return k_folds\n",
    "\n",
    "\n",
    "class RandomForest:\n",
    "    \"\"\"Random forest classification.\"\"\"\n",
    "\n",
    "    def __init__(self, num_trees, depth_limit, example_subsample_rate,\n",
    "                 attr_subsample_rate):\n",
    "        \"\"\"Create a random forest.\n",
    "         Args:\n",
    "             num_trees (int): fixed number of trees.\n",
    "             depth_limit (int): max depth limit of tree.\n",
    "             example_subsample_rate (float): percentage of example samples.\n",
    "             attr_subsample_rate (float): percentage of attribute samples.\n",
    "\n",
    "        The decision boundaries drawn by decision trees are very sharp, and fitting a decision tree of unbounded depth to a list of training examples almost inevitably leads to overfitting. In an attempt to decrease the variance of our classifier we're going to use a technique called 'Bootstrap Aggregating' (often abbreviated as 'bagging').\n",
    "\n",
    "        A Random Forest is a collection of decision trees, built as follows:\n",
    "\n",
    "        For every tree we're going to build:\n",
    "        Subsample the examples provided us (with replacement) in accordance with a provided example subsampling rate.\n",
    "        From the sample in the first step, choose attributes at random to learn on (in accordance with a provided attribute subsampling rate). (Without replacement)\n",
    "        Fit a decision tree to the subsample of data we've chosen (to a certain depth).\n",
    "        Classification for a random forest is then done by taking a majority vote of the classifications yielded by each tree in the forest after it classifies an example.\n",
    "\n",
    "        Fill in RandomForest.fit() to fit the decision tree as we describe above, and fill in RandomForest.classify() to classify a given list of examples.\n",
    "\n",
    "        Your features and classify should be in numpy arrays where if the dataset is (m_ x _n) then the features is (m_ x _n-1) and classify is (n_ x _1).\n",
    "\n",
    "        To test, we will be using a forest with 5 trees, with a depth limit of 5, example subsample rate of 0.5 and attribute subsample rate of 0.5\n",
    "        \"\"\"\n",
    "\n",
    "        self.trees = []\n",
    "        self.num_trees = num_trees\n",
    "        self.depth_limit = depth_limit\n",
    "        self.example_subsample_rate = example_subsample_rate\n",
    "        self.attr_subsample_rate = attr_subsample_rate\n",
    "        self.subsampled_attributes = []\n",
    "\n",
    "    def fit(self, features, classes):\n",
    "        \"\"\"Build a random forest of decision trees using Bootstrap Aggregation.\n",
    "            features (m x n): m examples with n features.\n",
    "            classes (m x 1): Array of Classes.\n",
    "        \"\"\"\n",
    "\n",
    "        # Subsample the examples provided us (with replacement) in accordance with a provided example subsampling rate.\n",
    "        subsampled_indexes = [np.random.choice(\n",
    "            range(features.shape[0]), int(features.shape[0]*self.example_subsample_rate), replace=True) for _ in range(self.num_trees)]\n",
    "\n",
    "        subsampled_features = [features[indices, :]\n",
    "                               for indices in subsampled_indexes]\n",
    "        subsampled_classes = [classes[indices]\n",
    "                              for indices in subsampled_indexes]\n",
    "\n",
    "        # From the sample in the first step, choose attributes at random to learn on (in accordance with a provided attribute subsampling rate). (Without replacement)\n",
    "\n",
    "        self.subsampled_attributes = [np.random.choice(\n",
    "            range(features.shape[1]), int(features.shape[1]*self.attr_subsample_rate), replace=False).tolist() for _ in range(self.num_trees)]\n",
    "\n",
    "        for sub_features, sub_classes, sub_attributes in zip(subsampled_features, subsampled_classes, self.subsampled_attributes):\n",
    "            dt = DecisionTree(depth_limit=self.depth_limit)\n",
    "            dt.fit(sub_features[:, sub_attributes], sub_classes)\n",
    "            self.trees.append(dt)\n",
    "\n",
    "    def classify(self, features):\n",
    "        \"\"\"Classify a list of features based on the trained random forest.\n",
    "        Args:\n",
    "            features (m x n): m examples with n features.\n",
    "        \"\"\"\n",
    "        labels = np.array([dt.classify(features[:, sub_attributes])\n",
    "                           for dt, sub_attributes in zip(self.trees, self.subsampled_attributes)]).transpose()\n",
    "        return [int(max(Counter(label).items(), key=operator.itemgetter(1))[0]) for label in labels]\n",
    "\n",
    "\n",
    "class ChallengeClassifier:\n",
    "    \"\"\"Challenge Classifier used on Challenge Training Data.\"\"\"\n",
    "\n",
    "    def __init__(self, num_trees=8, depth_limit=8, example_subsample_rate=0.6,\n",
    "                 attr_subsample_rate=0.8):\n",
    "        \"\"\"Create a random forest.\n",
    "         Args:\n",
    "             num_trees (int): fixed number of trees.\n",
    "             depth_limit (int): max depth limit of tree.\n",
    "             example_subsample_rate (float): percentage of example samples.\n",
    "             attr_subsample_rate (float): percentage of attribute samples.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.trees = []\n",
    "        self.num_trees = num_trees\n",
    "        self.depth_limit = depth_limit\n",
    "        self.example_subsample_rate = example_subsample_rate\n",
    "        self.attr_subsample_rate = attr_subsample_rate\n",
    "        self.subsampled_attributes = []\n",
    "\n",
    "    def fit(self, features, classes):\n",
    "        \"\"\"Build a random forest of decision trees using Bootstrap Aggregation.\n",
    "            features (m x n): m examples with n features.\n",
    "            classes (m x 1): Array of Classes.\n",
    "        \"\"\"\n",
    "        self.num_trees = int(features.shape[1]/3)\n",
    "        #self.depth_limit = int(features.shape[1]/3)\n",
    "        # Subsample the examples provided us (with replacement) in accordance with a provided example subsampling rate.\n",
    "        subsampled_indexes = [np.random.choice(\n",
    "            range(features.shape[0]), int(features.shape[0]*self.example_subsample_rate), replace=True) for _ in range(self.num_trees)]\n",
    "\n",
    "        subsampled_features = [features[indices, :]\n",
    "                               for indices in subsampled_indexes]\n",
    "        subsampled_classes = [classes[indices]\n",
    "                              for indices in subsampled_indexes]\n",
    "\n",
    "        # From the sample in the first step, choose attributes at random to learn on (in accordance with a provided attribute subsampling rate). (Without replacement)\n",
    "\n",
    "        self.subsampled_attributes = [np.random.choice(\n",
    "            range(features.shape[1]), int(features.shape[1]*self.attr_subsample_rate), replace=False).tolist() for _ in range(self.num_trees)]\n",
    "\n",
    "        for sub_features, sub_classes, sub_attributes in zip(subsampled_features, subsampled_classes, self.subsampled_attributes):\n",
    "            try:\n",
    "                dt = DecisionTree(depth_limit=self.depth_limit)\n",
    "                dt.fit(sub_features[:, sub_attributes], sub_classes)\n",
    "                self.trees.append(dt)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    def classify(self, features):\n",
    "        \"\"\"Classify a list of features based on the trained random forest.\n",
    "        Args:\n",
    "            features (m x n): m examples with n features.\n",
    "        \"\"\"\n",
    "        labels = np.array([dt.classify(features[:, sub_attributes])\n",
    "                           for dt, sub_attributes in zip(self.trees, self.subsampled_attributes)]).transpose()\n",
    "        return [int(max(Counter(label).items(), key=operator.itemgetter(1))[0]) for label in labels]\n",
    "\n",
    "\n",
    "class Vectorization:\n",
    "    \"\"\"Vectorization preparation for Assignment 5.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def non_vectorized_loops(self, data):\n",
    "        \"\"\"Element wise array arithmetic with loops.\n",
    "        This function takes one matrix, multiplies by itself and then adds to\n",
    "        itself.\n",
    "        Args:\n",
    "            data: data to be added to array.\n",
    "        Returns:\n",
    "            Numpy array of data.\n",
    "        \"\"\"\n",
    "\n",
    "        non_vectorized = np.zeros(data.shape)\n",
    "        for row in range(data.shape[0]):\n",
    "            for col in range(data.shape[1]):\n",
    "                non_vectorized[row][col] = (data[row][col] * data[row][col] +\n",
    "                                            data[row][col])\n",
    "        return non_vectorized\n",
    "\n",
    "    def vectorized_loops(self, data):\n",
    "        \"\"\"Element wise array arithmetic using vectorization.\n",
    "        This function takes one matrix, multiplies by itself and then adds to\n",
    "        itself.\n",
    "        Args:\n",
    "            data: data to be sliced and summed.\n",
    "        Returns:\n",
    "            Numpy array of data.\n",
    "        \"\"\"\n",
    "        return (data*data)+data\n",
    "\n",
    "    def non_vectorized_slice(self, data):\n",
    "        \"\"\"Find row with max sum using loops.\n",
    "        This function searches through the first 100 rows, looking for the row\n",
    "        with the max sum. (ie, add all the values in that row together).\n",
    "        Args:\n",
    "            data: data to be added to array.\n",
    "        Returns:\n",
    "            Tuple (Max row sum, index of row with max sum)\n",
    "        \"\"\"\n",
    "\n",
    "        max_sum = 0\n",
    "        max_sum_index = 0\n",
    "        for row in range(100):\n",
    "            temp_sum = 0\n",
    "            for col in range(data.shape[1]):\n",
    "                temp_sum += data[row][col]\n",
    "\n",
    "            if temp_sum > max_sum:\n",
    "                max_sum = temp_sum\n",
    "                max_sum_index = row\n",
    "\n",
    "        return max_sum, max_sum_index\n",
    "\n",
    "    def vectorized_slice(self, data):\n",
    "        \"\"\"Find row with max sum using vectorization.\n",
    "        This function searches through the first 100 rows, looking for the row\n",
    "        with the max sum. (ie, add all the values in that row together).\n",
    "        Args:\n",
    "            data: data to be sliced and summed.\n",
    "        Returns:\n",
    "            Tuple (Max row sum, index of row with max sum)\n",
    "        \"\"\"\n",
    "\n",
    "        sum_of_100_rows = np.sum(data[:100], axis=1)\n",
    "        max_arg = sum_of_100_rows.argmax()\n",
    "        return sum_of_100_rows[max_arg], max_arg\n",
    "\n",
    "    def non_vectorized_flatten(self, data):\n",
    "        \"\"\"Display occurrences of positive numbers using loops.\n",
    "         Flattens down data into a 1d array, then creates a dictionary of how\n",
    "         often a positive number appears in the data and displays that value.\n",
    "         ie, [(1203,3)] = integer 1203 appeared 3 times in data.\n",
    "         Args:\n",
    "            data: data to be added to array.\n",
    "        Returns:\n",
    "            List of occurrences [(integer, number of occurrences), ...]\n",
    "        \"\"\"\n",
    "\n",
    "        unique_dict = {}\n",
    "        flattened = np.hstack(data)\n",
    "        for item in range(len(flattened)):\n",
    "            if flattened[item] > 0:\n",
    "                if flattened[item] in unique_dict:\n",
    "                    unique_dict[flattened[item]] += 1\n",
    "                else:\n",
    "                    unique_dict[flattened[item]] = 1\n",
    "\n",
    "        return unique_dict.items()\n",
    "\n",
    "    def vectorized_flatten(self, data):\n",
    "        \"\"\"Display occurrences of positive numbers using vectorization.\n",
    "         Flattens down data into a 1d array, then creates a dictionary of how\n",
    "         often a positive number appears in the data and displays that value.\n",
    "         ie, [(1203,3)] = integer 1203 appeared 3 times in data.\n",
    "         Args:\n",
    "            data: data to be added to array.\n",
    "        Returns:\n",
    "            List of occurrences [(integer, number of occurrences), ...]\n",
    "        \"\"\"\n",
    "        flattened = data.flatten()\n",
    "        flattened = flattened[flattened > 0]\n",
    "        unique, counts = np.unique(flattened, return_counts=True)\n",
    "        return list(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8bfd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "def entropy(labels):\n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "    probabilities = counts / len(labels)\n",
    "    entropy_value = -np.sum(probabilities * np.log2(probabilities))\n",
    "    return entropy_value\n",
    "\n",
    "def information_gain(y, x):\n",
    "    parent_entropy = entropy(y)\n",
    "    info_a = 0\n",
    "    for value in set(x):\n",
    "        partition_indices = x[x == value].index\n",
    "        partition_entropy = entropy(y[partition_indices])\n",
    "        info_a += len(partition_indices) / len(x) * partition_entropy\n",
    "    gain_a = parent_entropy - info_a\n",
    "    return gain_a\n",
    "\n",
    "def decision_tree(X_train, y_train, max_depth, current_depth=0):\n",
    "    if len(set(y_train)) == 1 or current_depth == max_depth or len(X_train.columns) == 0:\n",
    "        class_counts = Counter(y_train)\n",
    "        majority_class = class_counts.most_common(1)[0][0]\n",
    "        return {\"class_label\": majority_class}\n",
    "    \n",
    "    gains = {attr: information_gain(y_train, X_train[attr]) for attr in X_train.columns}\n",
    "    best_attr = max(gains, key=gains.get)\n",
    "    node = {\"attribute\": best_attr, \"leaf\": {}}\n",
    "    unique_values = X_train[best_attr].unique()\n",
    "    for value in unique_values:\n",
    "        partition_indices = X_train[X_train[best_attr] == value].index\n",
    "        node[\"leaf\"][value] = decision_tree(X_train.loc[partition_indices], y_train.loc[partition_indices], max_depth, current_depth + 1)\n",
    "    return node\n",
    "\n",
    "\n",
    "def classify_random_forest(trees, subsampled_attributes, X_test):\n",
    "    class_labels = []\n",
    "    for _, test_row in X_test.iterrows():\n",
    "        tree_votes = []\n",
    "        for tree, sub_attributes in zip(trees, subsampled_attributes):\n",
    "            # Convert test_row[sub_attributes] to DataFrame\n",
    "            test_features = pd.DataFrame(test_row[sub_attributes]).T\n",
    "            predicted_label = classify(tree, test_features)\n",
    "            tree_votes.append(predicted_label[0])  # Append predicted label\n",
    "        class_labels.append(max(set(tree_votes), key=tree_votes.count))  # Perform majority voting\n",
    "    return class_label\n",
    "\n",
    "\n",
    "def classify(tree, features):\n",
    "    class_labels = []\n",
    "    for _, feature in features.iterrows():\n",
    "        node = tree\n",
    "        while \"class_label\" not in node:\n",
    "            split_attr = node[\"attribute\"]\n",
    "            feature_value = feature[split_attr]\n",
    "            if feature_value in node[\"leaf\"]:\n",
    "                node = node[\"leaf\"][feature_value]\n",
    "            else:\n",
    "                class_labels.append(max(node[\"leaf\"].items(), key=lambda x: len(x[1]))[0])\n",
    "                break\n",
    "        else:\n",
    "            class_labels.append(node[\"class_label\"])\n",
    "    return class_labels\n",
    "\n",
    "def bootstrap_sampling(X, y):\n",
    "    indices = np.random.choice(len(X), size=len(X), replace=True)\n",
    "    return X.iloc[indices], y.iloc[indices]\n",
    "\n",
    "\n",
    "def fit_random_forest(num_trees, max_depth, example_subsample_rate, attr_subsample_rate, X_train, y_train):\n",
    "    trees = []\n",
    "    subsampled_attributes = []\n",
    "\n",
    "    for i in range(num_trees):\n",
    "        # Bootstrap sampling to create a bootstrapped dataset\n",
    "        bootstrapped_X, bootstrapped_y = bootstrap_sampling(X_train, y_train)\n",
    "\n",
    "        # Subsample attributes\n",
    "        subsampled_attr_indexes = np.random.choice(range(X_train.shape[1]), int(X_train.shape[1] * attr_subsample_rate), replace=False)\n",
    "        subsampled_attributes.append(subsampled_attr_indexes.tolist())\n",
    "        subsampled_X = bootstrapped_X.iloc[:, subsampled_attr_indexes]\n",
    "\n",
    "        # Build decision tree using the bootstrapped and subsampled dataset\n",
    "        tree = decision_tree(subsampled_X, bootstrapped_y, max_depth)\n",
    "        trees.append(tree)\n",
    "\n",
    "    return trees, subsampled_attributes\n",
    "\n",
    "\n",
    "# Main\n",
    "if __name__ == \"__main__\":\n",
    "    # Read the dataset\n",
    "    df_voting = pd.read_csv(\"/Users/noshitha/Downloads/hw3/datasets/hw3_house_votes_84.csv\")\n",
    "\n",
    "    # Shuffle the dataset\n",
    "    df_voting_shuffle = shuffle(df_voting)\n",
    "\n",
    "    # Split the dataset into training and testing sets\n",
    "    train_df, test_df = train_test_split(df_voting_shuffle, test_size=0.2, random_state=34)\n",
    "\n",
    "    X_train = train_df.iloc[:, :-1]  # Features\n",
    "    y_train = train_df.iloc[:, -1]   # Target variable\n",
    "    X_test = test_df.iloc[:, :-1]\n",
    "    y_test = test_df.iloc[:, -1]\n",
    "\n",
    "    num_trees = 5\n",
    "    max_depth = 3\n",
    "    example_subsample_rate = 0.5\n",
    "    attr_subsample_rate = 0.5\n",
    "\n",
    "    trees, subsampled_attributes = fit_random_forest(num_trees, max_depth, example_subsample_rate, attr_subsample_rate, X_train, y_train)\n",
    "    predictions = classify_random_forest(trees, subsampled_attributes, X_test)\n",
    "    print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0866460d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2578f525",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d59271a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494f20b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7684b1ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ba136f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5560eb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2030a5be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8960a1c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966b885f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90665a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0eec57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b70199a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "def entropy(y):\n",
    "    class_counts = Counter(y)\n",
    "    total_instances = len(y)\n",
    "    entropy_val = 0\n",
    "    for count in class_counts.values():\n",
    "        probability = count / total_instances\n",
    "        entropy_val -= probability * np.log2(probability)\n",
    "    return entropy_val\n",
    "\n",
    "def information_gain(X, y, feature):\n",
    "    pivot = X[feature].mean()\n",
    "    left_mask = X[feature] < pivot\n",
    "    right_mask = ~left_mask\n",
    "    left_entropy = entropy(y[left_mask])\n",
    "    right_entropy = entropy(y[right_mask])\n",
    "    return entropy(y) - (left_entropy * np.mean(left_mask) + right_entropy * np.mean(right_mask))\n",
    "\n",
    "def decision_tree(X_train, y_train, max_depth, current_depth=0):\n",
    "    # If all instances belong to the same class or max depth is reached\n",
    "    if len(set(y_train)) == 1 or current_depth == max_depth or len(X_train.columns) == 0:\n",
    "        class_counts = Counter(y_train)\n",
    "        majority_class = class_counts.most_common(1)[0][0]\n",
    "        return {\"class_label\": majority_class}\n",
    "    \n",
    "    # For each attribute: evaluate the information gain gained by splitting on the attribute\n",
    "    gains = {attr: information_gain(X_train, y_train, attr) for attr in X_train.columns}\n",
    "    \n",
    "    # Find the attribute with the highest information gain\n",
    "    best_attr = max(gains, key=gains.get)\n",
    "    \n",
    "    # Create a decision node that splits on the best attribute\n",
    "    node = {\"attribute\": best_attr, \"leaf\": {}}\n",
    "\n",
    "    # Partition the data based on the best attribute\n",
    "    feature_mean = X_train[best_attr].mean()\n",
    "    left_child_indices = X_train[X_train[best_attr] < feature_mean].index\n",
    "    right_child_indices = X_train[X_train[best_attr] >= feature_mean].index\n",
    "    \n",
    "    # Recursively create left and right subtrees\n",
    "    node[\"leaf\"][\"<= \" + str(feature_mean)] = decision_tree(X_train.loc[left_child_indices], y_train.loc[left_child_indices], max_depth, current_depth + 1)\n",
    "    node[\"leaf\"][\"> \" + str(feature_mean)] = decision_tree(X_train.loc[right_child_indices], y_train.loc[right_child_indices], max_depth, current_depth + 1)\n",
    "    \n",
    "    return node\n",
    "\n",
    "def classify(tree, features):\n",
    "    class_labels = []\n",
    "    for _, feature in features.iterrows():  # Iterate over rows and treat each row as a dictionary\n",
    "        node = tree\n",
    "        while \"class_label\" not in node:\n",
    "            split_attr = node[\"attribute\"]\n",
    "            feature_value = feature[split_attr]\n",
    "            if feature_value in node[\"leaf\"]:\n",
    "                node = node[\"leaf\"][feature_value]\n",
    "            else:\n",
    "                # In case of missing values or unseen feature values, predict the majority class\n",
    "                class_labels.append(max(node[\"leaf\"].items(), key=lambda x: len(x[1]))[0])\n",
    "                break\n",
    "        else:\n",
    "            class_labels.append(node[\"class_label\"])\n",
    "    return class_labels\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "X_train = pd.DataFrame({'feature1': [1, 2, 3, 4, 5], 'feature2': [5, 4, 3, 2, 1]})\n",
    "y_train = pd.Series([0, 0, 1, 1, 1])\n",
    "\n",
    "tree = decision_tree(X_train, y_train, max_depth=3)\n",
    "print(classify(tree, X_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c027e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def entropy(labels):\n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "    probabilities = counts / len(labels)\n",
    "    entropy_value = -np.sum(probabilities * np.log2(probabilities))\n",
    "    return entropy_value\n",
    "\n",
    "def information_gain(y, x):\n",
    "    parent_entropy = entropy(y)\n",
    "    info_a = 0\n",
    "    for value in set(x):\n",
    "        partition_indices = x[x == value].index\n",
    "        partition_entropy = entropy(y[partition_indices])\n",
    "        info_a += len(partition_indices) / len(x) * partition_entropy\n",
    "    gain_a = parent_entropy - info_a\n",
    "    return gain_a\n",
    "\n",
    "def decision_tree(X_train, y_train, max_depth, current_depth=0):\n",
    "    # If all instances belong to the same class or max depth is reached\n",
    "    if len(set(y_train)) == 1 or current_depth == max_depth or len(X_train.columns) == 0:\n",
    "        class_counts = Counter(y_train)\n",
    "        majority_class = class_counts.most_common(1)[0][0]\n",
    "        return {\"class_label\": majority_class}\n",
    "    \n",
    "    # For each attribute: evaluate the information gain gained by splitting on the attribute\n",
    "    gains = {attr: information_gain(y_train, X_train[attr]) for attr in X_train.columns}\n",
    "    \n",
    "    # Find the attribute with the highest information gain\n",
    "    best_attr = max(gains, key=gains.get)\n",
    "    \n",
    "    # Create a decision node that splits on the best attribute\n",
    "    node = {\"attribute\": best_attr, \"leaf\": {}}\n",
    "\n",
    "    # Partition the data based on the best attribute\n",
    "    unique_values = X_train[best_attr].unique()\n",
    "    for value in unique_values:\n",
    "        partition_indices = X_train[X_train[best_attr] == value].index\n",
    "        node[\"leaf\"][value] = decision_tree(X_train.loc[partition_indices], y_train.loc[partition_indices], max_depth, current_depth + 1)\n",
    "    \n",
    "    return node\n",
    "\n",
    "def classify(tree, features):\n",
    "    class_labels = []\n",
    "    for _, feature in features.iterrows():  # Iterate over rows and treat each row as a dictionary\n",
    "        node = tree\n",
    "        while \"class_label\" not in node:\n",
    "            split_attr = node[\"attribute\"]\n",
    "            feature_value = feature[split_attr]\n",
    "            if feature_value in node[\"leaf\"]:\n",
    "                node = node[\"leaf\"][feature_value]\n",
    "            else:\n",
    "                # In case of missing values or unseen feature values, predict the majority class\n",
    "                class_labels.append(max(node[\"leaf\"].items(), key=lambda x: len(x[1]))[0])\n",
    "                break\n",
    "        else:\n",
    "            class_labels.append(node[\"class_label\"])\n",
    "    return class_labels\n",
    "\n",
    "\n",
    "\n",
    "# Main\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Read the dataset\n",
    "    df_voting = pd.read_csv(\"/Users/noshitha/Downloads/hw3/datasets/hw3_house_votes_84.csv\")\n",
    "\n",
    "    # Shuffle the dataset\n",
    "    df_voting_shuffle = shuffle(df_voting)\n",
    "\n",
    "    # Split the dataset into training and testing sets\n",
    "    train_df, test_df = train_test_split(df_voting_shuffle, test_size=0.2, random_state=34)\n",
    "\n",
    "    # Assuming df_voting contains your dataset with features and target variable\n",
    "    # Features are all columns except the last one, target variable is the last column\n",
    "    X = df_voting.iloc[:, :-1]  # Features\n",
    "    y = df_voting.iloc[:, -1]   # Target variable\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=34)\n",
    "    \n",
    "    tree = decision_tree(X_train, y_train, max_depth=3)\n",
    "    print(classify(tree, X_train))\n",
    "    # Perform stratified cross-validation\n",
    "#     n_folds = 10\n",
    "#     n_trees = 100\n",
    "#     max_depth = 5\n",
    "#     avg_accuracy = stratified_cross_validation(X, y, n_folds, n_trees, max_depth)\n",
    "#     print(\"Average cross-validation accuracy:\", avg_accuracy)\n",
    "\n",
    "#     # Evaluate impact of different numbers of trees\n",
    "#     n_trees_list = [10, 50, 100, 200]\n",
    "#     results = evaluate_n_trees(X_train, X_test, y_train, y_test, n_trees_list, max_depth)\n",
    "#     print(\"Evaluation results for different numbers of trees:\", results)\n",
    "\n",
    "# # Example usage:\n",
    "# X_train = pd.DataFrame({'feature1': [1, 2, 3, 4, 5], 'feature2': [5, 4, 3, 2, 1]})\n",
    "# y_train = pd.Series([0, 0, 1, 1, 1])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
