{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e414df55",
   "metadata": {},
   "source": [
    "## RANDOM FOREST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dd98ec80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df_voting=pd.read_csv(\"/Users/noshitha/Downloads/hw3/datasets/hw3_house_votes_84.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5bb7b432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#handicapped-infants</th>\n",
       "      <th>water-project-cost-sharing</th>\n",
       "      <th>adoption-of-the-budget-resolution</th>\n",
       "      <th>physician-fee-freeze</th>\n",
       "      <th>el-salvador-adi</th>\n",
       "      <th>religious-groups-in-schools</th>\n",
       "      <th>anti-satellite-test-ban</th>\n",
       "      <th>aid-to-nicaraguan-contras</th>\n",
       "      <th>mx-missile</th>\n",
       "      <th>immigration</th>\n",
       "      <th>synfuels-corporation-cutback</th>\n",
       "      <th>education-spending</th>\n",
       "      <th>superfund-right-to-sue</th>\n",
       "      <th>crime</th>\n",
       "      <th>duty-free-exports</th>\n",
       "      <th>export-administration-act-south-africa</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   #handicapped-infants  water-project-cost-sharing  \\\n",
       "0                     1                           2   \n",
       "1                     1                           2   \n",
       "2                     0                           2   \n",
       "3                     1                           2   \n",
       "4                     2                           2   \n",
       "\n",
       "   adoption-of-the-budget-resolution  physician-fee-freeze  el-salvador-adi  \\\n",
       "0                                  1                     2                2   \n",
       "1                                  1                     2                2   \n",
       "2                                  2                     0                2   \n",
       "3                                  2                     1                0   \n",
       "4                                  2                     1                2   \n",
       "\n",
       "   religious-groups-in-schools  anti-satellite-test-ban  \\\n",
       "0                            2                        1   \n",
       "1                            2                        1   \n",
       "2                            2                        1   \n",
       "3                            2                        1   \n",
       "4                            2                        1   \n",
       "\n",
       "   aid-to-nicaraguan-contras  mx-missile  immigration  \\\n",
       "0                          1           1            2   \n",
       "1                          1           1            1   \n",
       "2                          1           1            1   \n",
       "3                          1           1            1   \n",
       "4                          1           1            1   \n",
       "\n",
       "   synfuels-corporation-cutback  education-spending  superfund-right-to-sue  \\\n",
       "0                             0                   2                       2   \n",
       "1                             1                   2                       2   \n",
       "2                             2                   1                       2   \n",
       "3                             2                   1                       2   \n",
       "4                             2                   0                       2   \n",
       "\n",
       "   crime  duty-free-exports  export-administration-act-south-africa  class  \n",
       "0      2                  1                                       2      1  \n",
       "1      2                  1                                       0      1  \n",
       "2      2                  1                                       1      0  \n",
       "3      1                  1                                       2      0  \n",
       "4      2                  2                                       2      0  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_voting.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0ffa8797",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "df_voting_shuffle = shuffle(df_voting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a9de7332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Set Shape: (348, 17)\n",
      "Testing Set Shape: (87, 17)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(df_voting_shuffle, test_size=0.2, random_state=34)\n",
    "\n",
    "# Shapes of the training and testing sets\n",
    "print(\"\\nTraining Set Shape:\", train_df.shape)\n",
    "print(\"Testing Set Shape:\", test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9080a2d2",
   "metadata": {},
   "source": [
    "## Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "67edbcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def decision_tree(X_train, y_train, max_depth, current_depth=0):\n",
    "    # If all instances belong to the same class or max depth is reached\n",
    "    if len(set(y_train)) == 1 or current_depth == max_depth:\n",
    "        class_counts = y_train.value_counts()\n",
    "        majority_class = class_counts.idxmax()\n",
    "        return {\"class_label\": majority_class}\n",
    "    \n",
    "    # Calculate information gain for all attributes\n",
    "    info_gain_values = [information_gain(y_train, X_train[attr]) for attr in X_train.columns]\n",
    "    best_attribute_index = np.argmax(info_gain_values) # Index of attribute with highest information gain\n",
    "    best_attribute = X_train.columns[best_attribute_index]\n",
    "    \n",
    "    # Create a dictionary for the current decision node\n",
    "    node = {\"attribute\": best_attribute, \"leaf\": {}}\n",
    "    \n",
    "    print(\"Depth: \",current_depth)\n",
    "    \n",
    "    # Find the average of the attribute's values\n",
    "    threshold = np.mean(X_train[best_attribute])\n",
    "\n",
    "    # Partition the data based on the threshold\n",
    "    left_partition_indices = X_train[X_train[best_attribute] <= threshold].index\n",
    "    right_partition_indices = X_train[X_train[best_attribute] > threshold].index\n",
    "\n",
    "    # Recursively create left and right subtrees\n",
    "    left_subtree = decision_tree(X_train.loc[left_partition_indices], y_train.loc[left_partition_indices], max_depth, current_depth + 1)\n",
    "    right_subtree = decision_tree(X_train.loc[right_partition_indices], y_train.loc[right_partition_indices], max_depth, current_depth + 1)\n",
    "\n",
    "    node[\"leaf\"][\"<= \" + str(threshold)] = left_subtree\n",
    "    node[\"leaf\"][\"> \" + str(threshold)] = right_subtree\n",
    "\n",
    "    \n",
    "    return node\n",
    "\n",
    "def entropy(labels):\n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "    probabilities = counts / len(labels)\n",
    "    entropy_value = -np.sum(probabilities * np.log2(probabilities))\n",
    "    return entropy_value\n",
    "\n",
    "def information_gain(y, x):\n",
    "    parent_entropy = entropy(y)\n",
    "    info_a = 0\n",
    "    for value in set(x):\n",
    "        partition_indices = x[x == value].index\n",
    "        partition_entropy = entropy(y[partition_indices])\n",
    "        info_a += len(partition_indices) / len(x) * partition_entropy\n",
    "    gain_a = parent_entropy - info_a\n",
    "    return gain_a\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c2a5e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f14eb2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_instance(tree, instance):\n",
    "    attribute = next(iter(tree.keys()))  # Get the attribute of the current node\n",
    "    if attribute not in instance or instance[attribute] not in tree[attribute]:\n",
    "        # Handle unknown attribute values or missing branches\n",
    "        if 'default' in tree[\"leaf\"]:\n",
    "            return tree[\"leaf\"][\"default\"]\n",
    "        else:\n",
    "            # Handle this case according to your application requirements\n",
    "            # For example, you can return None or raise an exception\n",
    "            return None\n",
    "    \n",
    "    if isinstance(instance[attribute], (int, float)):\n",
    "        # If the attribute is numeric, check for a threshold\n",
    "        if instance[attribute] <= list(tree[attribute].keys())[0]:\n",
    "            # Recur for the left subtree\n",
    "            return predict_instance(tree[attribute][list(tree[attribute].keys())[0]], instance)\n",
    "        else:\n",
    "            # Recur for the right subtree\n",
    "            return predict_instance(tree[attribute][list(tree[attribute].keys())[1]], instance)\n",
    "    else:\n",
    "        # Recur for the subtree corresponding to the attribute value\n",
    "        return predict_instance(tree[attribute][instance[attribute]], instance)\n",
    "\n",
    "        \n",
    "def predict(tree, X):\n",
    "    predictions = []\n",
    "    for i in range(len(X)):\n",
    "        predictions.append(predict_instance(tree, X.iloc[i]))\n",
    "    return predictions\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    correct = sum(y_t == y_p for y_t, y_p in zip(y_true, y_pred))\n",
    "    return correct / len(y_true) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "786aa399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth:  0\n",
      "Depth:  1\n",
      "Depth:  2\n",
      "Depth:  3\n",
      "Depth:  4\n",
      "Depth:  4\n",
      "Depth:  1\n",
      "Depth:  2\n",
      "Depth:  3\n",
      "Depth:  4\n",
      "Depth:  3\n",
      "Depth:  4\n",
      "Depth:  2\n",
      "Depth:  3\n",
      "Depth:  4\n",
      "Depth:  4\n",
      "Depth:  3\n",
      "Depth:  4\n"
     ]
    }
   ],
   "source": [
    "tree = decision_tree(X_train, y_train, 5,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f2769737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Predict Labels\n",
    "y_pred = predict(tree, X_test)\n",
    "\n",
    "# Calculate Accuracy\n",
    "acc = accuracy(y_test, y_pred)\n",
    "print(\"Decision Tree Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b43f43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9f498e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319b6521",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "54d5abab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap Dataset Generation\n",
    "def generate_bootstrap_dataset(X_train, y_train):\n",
    "    n_samples = len(X_train)\n",
    "    indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "    X_bootstrap = X_train.iloc[indices]\n",
    "    y_bootstrap = y_train.iloc[indices]\n",
    "    return X_bootstrap, y_bootstrap\n",
    "\n",
    "# Random Attribute Selection\n",
    "def select_random_attributes(X_train, m):\n",
    "    n_attributes = X_train.shape[1]\n",
    "    selected_indices = np.random.choice(n_attributes, m, replace=False)\n",
    "    return selected_indices.tolist()\n",
    "\n",
    "# Majority Voting Mechanism\n",
    "def majority_voting(predictions):\n",
    "    vote_counts = Counter(predictions)\n",
    "    predicted_class = vote_counts.most_common(1)[0][0]\n",
    "    return predicted_class\n",
    "\n",
    "# Stratified Cross-Validation\n",
    "def stratified_cross_validation(X, y, n_splits, n_trees, max_depth):\n",
    "    n_samples = X.shape[0]\n",
    "    fold_size = n_samples // n_splits\n",
    "    \n",
    "    accuracies = []\n",
    "    for i in range(n_splits):\n",
    "        start_idx = i * fold_size\n",
    "        end_idx = (i + 1) * fold_size if i < n_splits - 1 else n_samples\n",
    "        \n",
    "        X_test_fold = X.iloc[start_idx:end_idx]\n",
    "        y_test_fold = y.iloc[start_idx:end_idx]\n",
    "        \n",
    "        X_train_fold = pd.concat([X.iloc[:start_idx], X.iloc[end_idx:]])\n",
    "        y_train_fold = pd.concat([y.iloc[:start_idx], y.iloc[end_idx:]])\n",
    "        \n",
    "        # Construct Random Forest\n",
    "        forest = []\n",
    "        for i in range(n_trees):\n",
    "            print(\"Tree:\",i)\n",
    "            X_bootstrap, y_bootstrap = generate_bootstrap_dataset(X_train_fold, y_train_fold)\n",
    "            tree = decision_tree(X_bootstrap, y_bootstrap, max_depth)\n",
    "            forest.append(tree)\n",
    "        \n",
    "        # Predict Labels\n",
    "        y_pred = [predict(tree, X_test_fold) for tree in forest]\n",
    "        y_pred_combined = [majority_voting(predictions) for predictions in zip(*y_pred)]\n",
    "        \n",
    "        # Calculate Accuracy\n",
    "        acc = accuracy(y_test_fold, y_pred_combined)\n",
    "        accuracies.append(acc)\n",
    "    \n",
    "    avg_accuracy = np.mean(accuracies)\n",
    "    return avg_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "38c8bffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Set Shape: (348, 16)\n",
      "Testing Set Shape: (87, 16)\n",
      "Depth:  0\n",
      "Depth:  1\n",
      "Depth:  2\n",
      "Depth:  3\n",
      "Depth:  4\n",
      "Depth:  4\n",
      "Depth:  1\n",
      "Depth:  2\n",
      "Depth:  3\n",
      "Depth:  4\n",
      "Depth:  3\n",
      "Depth:  4\n",
      "Depth:  2\n",
      "Depth:  3\n",
      "Depth:  4\n",
      "Depth:  4\n",
      "Depth:  3\n",
      "Depth:  4\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'default'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/bm/l2zpyjxn2gqg_1d20xgx5z0w0000gn/T/ipykernel_22844/2805325513.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecision_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;31m# Calculate Accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/bm/l2zpyjxn2gqg_1d20xgx5z0w0000gn/T/ipykernel_22844/1872000679.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(tree, X)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/bm/l2zpyjxn2gqg_1d20xgx5z0w0000gn/T/ipykernel_22844/1872000679.py\u001b[0m in \u001b[0;36mpredict_instance\u001b[0;34m(tree, instance)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattribute\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minstance\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mattribute\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"leaf\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0;31m# Handle unknown attribute values or missing branches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"leaf\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"default\"\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Assuming you have a default branch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mattribute\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'default'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming df_voting contains your dataset with features and target variable\n",
    "# Features are all columns except the last one, target variable is the last column\n",
    "X = df_voting.iloc[:, :-1]  # Features\n",
    "y = df_voting.iloc[:, -1]   # Target variable\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=34)\n",
    "\n",
    "# Shapes of the training and testing sets\n",
    "print(\"\\nTraining Set Shape:\", X_train.shape)\n",
    "print(\"Testing Set Shape:\", X_test.shape)\n",
    "\n",
    "tree = decision_tree(X_train, y_train, max_depth=5)\n",
    "y_pred= predict(tree, X_test)\n",
    "# Calculate Accuracy\n",
    "acc = accuracy(y_test, y_pred)\n",
    "print(\"Decision Tree Accuracy:\", acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e3a997",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Evaluate Impact of Number of Trees\n",
    "n_trees_values = [5, 10, 15, 20]  # Example values for number of trees\n",
    "avg_accuracies = []\n",
    "\n",
    "for n_trees in n_trees_values:\n",
    "    avg_accuracy = stratified_cross_validation(X_train, y_train, n_splits=10, n_trees=n_trees, max_depth=5)\n",
    "    avg_accuracies.append(avg_accuracy)\n",
    "\n",
    "# Print Average Accuracies\n",
    "for n_trees, avg_accuracy in zip(n_trees_values, avg_accuracies):\n",
    "    print(\"n_tree_value:\",n_trees)\n",
    "    print(f\"Number of Trees: {n_trees}, Average Accuracy: {avg_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa10416",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80c5de1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056e2f35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac99b257",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming df_voting contains your dataset with features and target variable\n",
    "# Features are all columns except the last one, target variable is the last column\n",
    "X = df_voting.iloc[:, :-1]  # Features\n",
    "y = df_voting.iloc[:, -1]   # Target variable\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=34)\n",
    "\n",
    "# Shapes of the training and testing sets\n",
    "print(\"\\nTraining Set Shape:\", X_train.shape)\n",
    "print(\"Testing Set Shape:\", X_test.shape)\n",
    "# Decision Tree Construction\n",
    "tree = decision_tree(X_train, y_train, max_depth=5)\n",
    "\n",
    "# Predict Labels\n",
    "y_pred = predict(tree, X_test)\n",
    "\n",
    "# Calculate Accuracy\n",
    "acc = accuracy(y_test, y_pred)\n",
    "print(\"Decision Tree Accuracy:\", acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982b4fc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caef99f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e74718",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93fff23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147c3aad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c0f102",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff8c4a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d063fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5746ea8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c77037",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74703065",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afa9b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def decision_tree(X_train, y_train, max_depth=None):\n",
    "    if max_depth is not None and max_depth <= 0:\n",
    "        class_counts = y_train.value_counts()\n",
    "        majority_class = class_counts.idxmax()\n",
    "        print(\"Reached maximum depth or no more depth left. Returning majority class:\", majority_class)\n",
    "        return {\"class_label\": majority_class}\n",
    "\n",
    "    # Randomly select attributes for splitting\n",
    "    selected_attributes = np.random.choice(X_train.columns, len(X_train.columns), replace=False)\n",
    "    #print(\"Selected attributes:\", selected_attributes)\n",
    "    info_gain_values = [information_gain(y_train, X_train[attr]) for attr in selected_attributes]\n",
    "    best_attribute = selected_attributes[np.argmax(info_gain_values)]\n",
    "    print(\"Best attribute:\", best_attribute)\n",
    "    node = {\"attribute\": best_attribute, \"leaf\": {}}\n",
    "    attribute_values = sorted(set(X_train[best_attribute]))\n",
    "\n",
    "    thresholds = [(attribute_values[i] + attribute_values[i+1]) / 2 for i in range(len(attribute_values) - 1)]\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        partition_left = X_train[X_train[best_attribute] <= threshold]\n",
    "        partition_right = X_train[X_train[best_attribute] > threshold]\n",
    "\n",
    "        if partition_left.empty or partition_right.empty:\n",
    "            majority_class = y_train.value_counts().idxmax()\n",
    "            print(\"Partitioned data has empty partitions. Returning majority class:\", majority_class)\n",
    "            node[\"leaf\"][threshold] = {\"class_label\": majority_class}\n",
    "        else:\n",
    "            print(\"Partitioning data with threshold:\", threshold)\n",
    "            subtree_left = decision_tree(partition_left, y_train[partition_left.index], max_depth - 1)\n",
    "            subtree_right = decision_tree(partition_right, y_train[partition_right.index], max_depth - 1)\n",
    "            node[\"leaf\"][threshold] = {\"<= threshold\": subtree_left, \"> threshold\": subtree_right}\n",
    "    return node\n",
    "\n",
    "def entropy(labels):\n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "    probabilities = counts / len(labels)\n",
    "    entropy_value = -np.sum(probabilities * np.log2(probabilities))\n",
    "    return entropy_value\n",
    "\n",
    "def information_gain(y, x):\n",
    "    parent_entropy = entropy(y)\n",
    "    info_a = 0\n",
    "    for value in set(x):\n",
    "        partition_indices = x[x == value].index\n",
    "        partition_entropy = entropy(y[partition_indices])\n",
    "        info_a += len(partition_indices) / len(x) * partition_entropy\n",
    "    gain_a = parent_entropy - info_a\n",
    "    return gain_a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2236cfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bootstrap_datasets(train_df, num_datasets):\n",
    "    bootstrap_datasets = []\n",
    "    num_instances = len(train_df)\n",
    "\n",
    "    for _ in range(num_datasets):\n",
    "        # Sampling with replacement\n",
    "        bootstrap_sample_indices = np.random.choice(num_instances, num_instances, replace=True)\n",
    "        bootstrap_sample = train_df.iloc[bootstrap_sample_indices]\n",
    "        bootstrap_datasets.append(bootstrap_sample)\n",
    "\n",
    "    return bootstrap_datasets\n",
    "\n",
    "def fit_random_forest(bootstrap_datasets, num_trees=10, max_features=None):\n",
    "    random_forest = []\n",
    "    num_instances = len(bootstrap_datasets[0])  # Assuming all datasets have the same length\n",
    "    num_features = len(bootstrap_datasets[0].columns) - 1  # Excluding the class column\n",
    "\n",
    "    for tree_index in range(num_trees):\n",
    "        print(\"Training tree:\", tree_index + 1)\n",
    "        # Randomly select features if max_features is specified\n",
    "        selected_features = list(np.random.choice(num_features, max_features, replace=False)) if max_features else list(range(num_features))\n",
    "        print(\"Selected features for tree:\", selected_features)\n",
    "        \n",
    "        # Train a decision tree on each bootstrap dataset\n",
    "        trees = []\n",
    "        for dataset_index, dataset in enumerate(bootstrap_datasets):\n",
    "            X_train = dataset.drop(columns=[\"class\"])\n",
    "            y_train = dataset[\"class\"]\n",
    "            print(\"Training tree\", tree_index + 1, \"on dataset\", dataset_index + 1)\n",
    "            tree = decision_tree(X_train, y_train, max_depth=2)\n",
    "            trees.append(tree)\n",
    "        \n",
    "        random_forest.append(trees)\n",
    "\n",
    "    return random_forest\n",
    "\n",
    "def predict_random_forest(X_test, random_forest):\n",
    "    predictions = []\n",
    "    for tree in random_forest:\n",
    "        predictions.append(predict(X_test, tree))\n",
    "    \n",
    "    # Perform majority voting\n",
    "    majority_votes = np.apply_along_axis(lambda x: np.argmax(np.bincount(x)), axis=0, arr=np.array(predictions))\n",
    "    return majority_votes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372fa358",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage:\n",
    "num_datasets = 10  # Number of bootstrap datasets to create\n",
    "bootstrap_datasets = create_bootstrap_datasets(train_df, num_datasets)\n",
    "print(train_df.columns)\n",
    "random_forest = fit_random_forest(bootstrap_datasets)\n",
    "\n",
    "# Predictions\n",
    "test_X = test_df.drop(columns=[\"class\"])\n",
    "predictions = predict_random_forest(test_X, random_forest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf286833",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1df80de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f2bcd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7875a2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680bbc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def check_same_class(y_train):\n",
    "    # If all instances belong to the same class\n",
    "    if len(set(y_train['class'])) == 1:\n",
    "        return {\"class_label\": y_train['class'].iloc[0]}\n",
    "    else:\n",
    "        return {\"not_same_class\": True}\n",
    "\n",
    "# Example usage\n",
    "y_train = pd.DataFrame({'class': [0, 0, 0, 0, 1, 1, 1, 1]})\n",
    "check=len(set(y_train['class']))\n",
    "print(check)\n",
    "print(set(y_train['class']))\n",
    "result = check_same_class(y_train)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9f7bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def decision_tree(X_train, y_train, remaining_attributes, m=None):\n",
    "    if len(set(y_train)) == 1:\n",
    "        return {\"class_label\": y_train.iloc[0]}\n",
    "    \n",
    "    if not remaining_attributes:\n",
    "        class_counts = y_train.value_counts()\n",
    "        majority_class = class_counts.idxmax()\n",
    "        return {\"class_label\": majority_class}\n",
    "    \n",
    "    if m is None:\n",
    "        m = int(np.sqrt(len(remaining_attributes)))\n",
    "    \n",
    "    # Randomly select 'm' attributes from the remaining attributes\n",
    "    selected_attributes = np.random.choice(remaining_attributes, m, replace=False)\n",
    "    \n",
    "    info_gain_values = [information_gain(y_train, X_train[attr]) for attr in selected_attributes]\n",
    "    best_attribute = selected_attributes[np.argmax(info_gain_values)]\n",
    "    remaining_attributes_copy = remaining_attributes.copy()\n",
    "    remaining_attributes_copy.remove(best_attribute)\n",
    "    remaining_attributes = remaining_attributes_copy.copy()\n",
    "    \n",
    "    node = {\"attribute\": best_attribute, \"leaf\": {}}\n",
    "    attribute_values = sorted(set(X_train[best_attribute]))\n",
    "    \n",
    "    thresholds = [(attribute_values[i] + attribute_values[i+1]) / 2 for i in range(len(attribute_values) - 1)]\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        partition_left = X_train[X_train[best_attribute] <= threshold]\n",
    "        partition_right = X_train[X_train[best_attribute] > threshold]\n",
    "\n",
    "        if partition_left.empty or partition_right.empty:\n",
    "            majority_class = y_train.value_counts().idxmax()\n",
    "            node[\"leaf\"][threshold] = {\"class_label\": majority_class}\n",
    "        else:\n",
    "            subtree_left = decision_tree(partition_left, y_train[partition_left.index], remaining_attributes, m)\n",
    "            subtree_right = decision_tree(partition_right, y_train[partition_right.index], remaining_attributes, m)\n",
    "            node[\"leaf\"][threshold] = {\"<= threshold\": subtree_left, \"> threshold\": subtree_right}\n",
    "            \n",
    "    return node\n",
    "\n",
    "def entropy(labels):\n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "    probabilities = counts / len(labels)\n",
    "    entropy_value = -np.sum(probabilities * np.log2(probabilities))\n",
    "    return entropy_value\n",
    "\n",
    "def information_gain(y, x):\n",
    "    parent_entropy = entropy(y)\n",
    "    info_a = 0\n",
    "    for value in set(x):\n",
    "        partition_indices = x[x == value].index\n",
    "        partition_entropy = entropy(y[partition_indices])\n",
    "        info_a += len(partition_indices) / len(x) * partition_entropy\n",
    "    gain_a = parent_entropy - info_a\n",
    "    return gain_a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a96f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single(x, tree):\n",
    "    if 'class_label' in tree:\n",
    "        # Single data point\n",
    "        return tree['class_label']\n",
    "    else:\n",
    "        value = x[tree['attribute']]\n",
    "        if value in tree['leaf']:\n",
    "            # leaf has only class label or a nested dictionary\n",
    "            next_node = tree['leaf'][value]\n",
    "            if 'class_label' in next_node:\n",
    "                return next_node['class_label']\n",
    "            else:\n",
    "                return predict_single(x, next_node)\n",
    "        else:\n",
    "            return list(tree['leaf'].values())[0]\n",
    "\n",
    "\n",
    "def predict(X, tree):\n",
    "    return [predict_single(x, tree) for i, x in X.iterrows()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7a900d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_bootstrap_datasets(train_df, num_datasets):\n",
    "    bootstrap_datasets = []\n",
    "    num_instances = len(train_df)\n",
    "\n",
    "    for _ in range(num_datasets):\n",
    "        # Sampling with replacement\n",
    "        bootstrap_sample_indices = np.random.choice(num_instances, num_instances, replace=True)\n",
    "        bootstrap_sample = train_df.iloc[bootstrap_sample_indices]\n",
    "        bootstrap_datasets.append(bootstrap_sample)\n",
    "\n",
    "    return bootstrap_datasets\n",
    "\n",
    "# Example usage:\n",
    "num_datasets = 10  # Number of bootstrap datasets to create\n",
    "bootstrap_datasets = create_bootstrap_datasets(train_df, num_datasets)\n",
    "\n",
    "# Check the shapes of the bootstrap datasets\n",
    "for i, dataset in enumerate(bootstrap_datasets):\n",
    "    print(f\"Bootstrap Dataset {i+1} Shape:\", dataset.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9ddab4",
   "metadata": {},
   "source": [
    "## Decision Tree using info-gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c96db0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7bd8e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911b4219",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e99fb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def decision_tree(X_train, y_train, remaining_attributes, m):\n",
    "    # If all instances belong to the same class\n",
    "    if len(set(y_train)) == 1:\n",
    "        return {\"class_label\": y_train.iloc[0]}\n",
    "    if not remaining_attributes:\n",
    "        # If no more attributes left\n",
    "        class_counts = y_train.value_counts()\n",
    "        majority_class = class_counts.idxmax()\n",
    "        return {\"class_label\": majority_class}\n",
    "    \n",
    "    # Randomly select m attributes\n",
    "    selected_attributes = np.random.choice(remaining_attributes, size=min(m, len(remaining_attributes)), replace=False)\n",
    "    \n",
    "    info_gain_values = [information_gain(y_train, X_train[attr]) for attr in selected_attributes]\n",
    "    best_attribute = selected_attributes[np.argmax(info_gain_values)] # Highest info gain attribute\n",
    "    remaining_attributes_copy = remaining_attributes.copy()  # Make a copy\n",
    "    remaining_attributes_copy.remove(best_attribute) # L(attr)-Selected(attr)\n",
    "    remaining_attributes = remaining_attributes_copy.copy()  # Update remaining_attributes\n",
    "    \n",
    "    # Create a dictionary for the current decision node\n",
    "    node = {\"attribute\": best_attribute, \"leaf\": {}}\n",
    "    # Create a list of values for the selected attribute\n",
    "    attribute_values = set(X_train[best_attribute])\n",
    "\n",
    "    # For each attribute value, create sub-trees and edges\n",
    "    for value in attribute_values:\n",
    "        # Partition - based on the attribute value\n",
    "        partition = X_train[X_train[best_attribute] == value]\n",
    "\n",
    "        if partition.empty:\n",
    "            # If partition is empty, leaf node = node with majority class\n",
    "            majority_class = y_train.value_counts().idxmax()\n",
    "            node[\"leaf\"][value] = {\"class_label\": majority_class}\n",
    "        else:\n",
    "            # Recursively create a sub-tree\n",
    "            subtree = decision_tree(partition, y_train[partition.index], remaining_attributes, m)\n",
    "            node[\"leaf\"][value] = subtree\n",
    "    return node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8753c2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree(X_train, y_train, attributes, m):\n",
    "    if len(set(y_train)) == 1:\n",
    "        return {\"class_label\": y_train.iloc[0]}\n",
    "    if not attributes:\n",
    "        class_counts = y_train.value_counts()\n",
    "        majority_class = class_counts.idxmax()\n",
    "        return {\"class_label\": majority_class}\n",
    "\n",
    "        # Ensure that 'attributes' is a list of attribute names\n",
    "    attributes = list(train_df.columns[:-1])\n",
    "\n",
    "    # Call the function with the corrected 'attributes'\n",
    "    tree = decision_tree(bootstrap_df, attributes, target, m)\n",
    "\n",
    "    \n",
    "    selected_attributes = np.random.choice(attributes, size=min(m, len(attributes)), replace=False)\n",
    "    info_gain_values = [information_gain(y_train, X_train[attr]) for attr in selected_attributes]\n",
    "    best_attribute = selected_attributes[np.argmax(info_gain_values)]\n",
    "    \n",
    "    remaining_attributes = [attr for attr in attributes if attr != best_attribute]\n",
    "    \n",
    "    node = {\"attribute\": best_attribute, \"leaf\": {}}\n",
    "    attribute_values = set(X_train[best_attribute])\n",
    "    for value in attribute_values:\n",
    "        partition = X_train[X_train[best_attribute] == value]\n",
    "        if partition.empty:\n",
    "            majority_class = y_train.value_counts().idxmax()\n",
    "            node[\"leaf\"][value] = {\"class_label\": majority_class}\n",
    "        else:\n",
    "            subtree = decision_tree(partition, y_train[partition.index], remaining_attributes, m)\n",
    "            node[\"leaf\"][value] = subtree\n",
    "    return node\n",
    "\n",
    "def entropy(labels):\n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "    probabilities = counts / len(labels)\n",
    "    entropy_value = -np.sum(probabilities * np.log2(probabilities))\n",
    "    return entropy_value\n",
    "\n",
    "def information_gain(y, x):\n",
    "    parent_entropy = entropy(y)\n",
    "    # InfoA(D)\n",
    "    info_a = 0\n",
    "    for value in set(x):\n",
    "        partition_indices = x[x == value].index\n",
    "        partition_entropy = entropy(y[partition_indices])\n",
    "        info_a += len(partition_indices) / len(x) * partition_entropy\n",
    "    # GainA(D)\n",
    "    gain_a = parent_entropy - info_a\n",
    "    return gain_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c828c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single(x, tree):\n",
    "    if 'class_label' in tree:\n",
    "        # Single data point\n",
    "        return tree['class_label']\n",
    "    else:\n",
    "        value = x[tree['attribute']]\n",
    "        if value in tree['leaf']:\n",
    "            # leaf has only class label or a nested dictionary\n",
    "            next_node = tree['leaf'][value]\n",
    "            if 'class_label' in next_node:\n",
    "                return next_node['class_label']\n",
    "            else:\n",
    "                return predict_single(x, next_node)\n",
    "        else:\n",
    "            return list(tree['leaf'].values())[0]\n",
    "\n",
    "def predict(X, tree):\n",
    "    return [predict_single(x, tree) for i, x in X.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfcf4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_predict(X, forest):\n",
    "    predictions = []\n",
    "    for tree in forest:\n",
    "        predictions.append(predict(X, tree))\n",
    "    # Transpose predictions to get each tree's prediction for each instance\n",
    "    predictions = np.array(predictions).T\n",
    "    # Use majority voting to get final prediction for each instance\n",
    "    final_predictions = [np.bincount(row).argmax() for row in predictions]\n",
    "    return final_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611389c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Function to calculate accuracy\n",
    "def calculate_accuracy(predictions, actual):\n",
    "    return np.sum(predictions == actual) / len(actual)\n",
    "\n",
    "def create_bootstrap_dataset(df):\n",
    "    return df.sample(n=len(df), replace=True)\n",
    "\n",
    "def run_random_forest(df, attributes, target, num_trees=100, num_iterations=100):\n",
    "    forest = []\n",
    "    for i in range(num_trees):\n",
    "        bootstrap_df = create_bootstrap_dataset(df)\n",
    "        tree = decision_tree(bootstrap_df, attributes, target,m)\n",
    "        forest.append(tree)\n",
    "    return forest\n",
    "\n",
    "def stratified_cross_validation(df, attributes, target, num_trees=100, num_folds=10):\n",
    "    fold_size = len(df) // num_folds\n",
    "    accuracies = []\n",
    "    for i in range(num_folds):\n",
    "        start_index = i * fold_size\n",
    "        end_index = start_index + fold_size if i < num_folds - 1 else len(df)\n",
    "        test_df = df.iloc[start_index:end_index]\n",
    "        train_df = pd.concat([df.iloc[:start_index], df.iloc[end_index:]])\n",
    "        \n",
    "        # Train random forest on training set\n",
    "        forest = run_random_forest(train_df, attributes, target, num_trees)\n",
    "        \n",
    "        # Make predictions on test set\n",
    "        X_test, y_test = test_df[attributes], test_df[target]\n",
    "        predictions = random_forest_predict(X_test, forest)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        accuracy = calculate_accuracy(predictions, y_test)\n",
    "        accuracies.append(accuracy)\n",
    "    return np.mean(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb75ab0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8c02bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_voting_shuffle.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b04a49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = df_voting_shuffle.columns[:-1].tolist()\n",
    "target = df_voting_shuffle['class']\n",
    "train_df, test_df = train_test_split(df_voting_shuffle, test_size=0.2, random_state=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0921b252",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f432e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Split the DataFrame into features and target for training and testing sets\n",
    "X_train, y_train = train_df[attributes], train_df['class']\n",
    "X_test, y_test = test_df[attributes], test_df['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66d61e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the decision tree using the training set\n",
    "\n",
    "m = int(np.sqrt(len(attributes)))\n",
    "\n",
    "root_node = decision_tree(X_train, y_train, attributes,m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adefcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate accuracy\n",
    "def calculate_accuracy(predictions, actual):\n",
    "    return np.sum(predictions == actual) / len(actual)\n",
    "\n",
    "# Make predictions on the training set\n",
    "predictions_train = predict(X_train, root_node)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions_test = predict(X_test, root_node)\n",
    "\n",
    "# Evaluate the accuracy on the training set\n",
    "accuracy_train = calculate_accuracy(predictions_train, y_train)\n",
    "print(\"Accuracy on Training Set:\", accuracy_train)\n",
    "\n",
    "# Evaluate the accuracy on the test set\n",
    "accuracy_test = calculate_accuracy(predictions_test, y_test)\n",
    "print(\"Accuracy on Test Set:\", accuracy_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8822291a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate the impact of the number of trees\n",
    "num_trees_list = [10, 50, 100, 200]\n",
    "for num_trees in num_trees_list:\n",
    "    mean_accuracy = stratified_cross_validation(df_voting_shuffle, attributes, 'class', num_trees)\n",
    "    print(\"Number of Trees:\", num_trees)\n",
    "    print(\"Mean Accuracy:\", mean_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463188d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1e37fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0ad26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the experiment\n",
    "attributes = df_voting_shuffle.columns[:-1].tolist()\n",
    "target = 'class'\n",
    "df_voting_shuffle = shuffle(df_voting_shuffle, random_state=np.random.randint(1, 1000))  # Shuffle once initially\n",
    "\n",
    "accuracy_train, accuracy_test = run_decision_tree(df_voting_shuffle, attributes, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7307a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_accuracy_train = np.mean(accuracy_train)\n",
    "mean_accuracy_test = np.mean(accuracy_test)\n",
    "\n",
    "# Print or use the values as needed\n",
    "print(\"Mean Training Accuracy:\", mean_accuracy_train)\n",
    "print(\"Mean Testing Accuracy:\", mean_accuracy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5691c4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "std_dev_accuracy_train = np.std(accuracy_train)\n",
    "std_dev_accuracy_test = np.std(accuracy_test)\n",
    "\n",
    "print(\"Standard Deviation Training Accuracy:\", std_dev_accuracy_train)\n",
    "print(\"Standard Deviation Testing Accuracy:\", std_dev_accuracy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e846c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot histograms\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(accuracy_train, bins=20, edgecolor='black')\n",
    "plt.title('Accuracy Distribution on Training Set on info gain')\n",
    "plt.xlabel('Accuracy')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc0da0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(accuracy_test, bins=20, edgecolor='black')\n",
    "plt.title('Accuracy Distribution on Testing Set on info gain')\n",
    "plt.xlabel('Accuracy')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca31187b",
   "metadata": {},
   "source": [
    "## Decision Trees using Gini impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e403a4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree(X_train, y_train, remaining_attributes):\n",
    "    # If all instances belong to the same class\n",
    "    if len(set(y_train)) == 1:\n",
    "        return {\"class_label\": y_train.iloc[0]}\n",
    "    if not remaining_attributes:\n",
    "        # If no more attributes left\n",
    "        class_counts = y_train.value_counts()\n",
    "        majority_class = class_counts.idxmax()\n",
    "        return {\"class_label\": majority_class}\n",
    "\n",
    "\n",
    "def calculate_gini_index(labels):\n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "    probabilities = counts / len(labels)\n",
    "    gini_index_value = 1 - np.square(probabilities).sum()\n",
    "    return gini_index_value\n",
    "\n",
    "def gini_attribute(y_train, attribute_values):\n",
    "    gini = 0.0\n",
    "    for value in attribute_values:\n",
    "        partition_indices = y_train[attribute_values == value].index\n",
    "        partition_gini = calculate_gini_index(y_train[partition_indices])\n",
    "        gini += len(partition_indices) / len(y_train) * partition_gini\n",
    "    return gini\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c88ed5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single(x, tree):\n",
    "    if 'class_label' in tree:\n",
    "        # Single data point\n",
    "        return tree['class_label']\n",
    "    else:\n",
    "        value = x[tree['attribute']]\n",
    "        if value in tree['leaf']:\n",
    "            # leaf has only class label or a nested dictionary\n",
    "            next_node = tree['leaf'][value]\n",
    "            if 'class_label' in next_node:\n",
    "                return next_node['class_label']\n",
    "            else:\n",
    "                return predict_single(x, next_node)\n",
    "        else:\n",
    "            return list(tree['leaf'].values())[0]\n",
    "\n",
    "\n",
    "def predict(X, tree):\n",
    "    return [predict_single(x, tree) for i, x in X.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe92fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = df_voting_shuffle.columns[:-1].tolist()\n",
    "target = df_voting_shuffle['target']\n",
    "train_df, test_df = train_test_split(df_voting_shuffle, test_size=0.2, random_state=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc92ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Split the DataFrame into features and target for training and testing sets\n",
    "X_train, y_train = train_df[attributes], train_df['target']\n",
    "X_test, y_test = test_df[attributes], test_df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3947858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the decision tree using the training set\n",
    "root_node = decision_tree(X_train, y_train, attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2355507b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Function to calculate accuracy\n",
    "def calculate_accuracy(predictions, actual):\n",
    "    return np.sum(predictions == actual) / len(actual)\n",
    "\n",
    "# Function to train and evaluate Decision Tree 100 times\n",
    "def run_decision_tree(df, attributes, target, num_iterations=100):\n",
    "    accuracy_train_list = []\n",
    "    accuracy_test_list = []\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        # Shuffle the dataset\n",
    "        df_shuffled = shuffle(df)\n",
    "\n",
    "        # Split the DataFrame into features and target for training and testing sets\n",
    "        train_df, test_df = train_test_split(df_shuffled, test_size=0.2, random_state=np.random.randint(1, 1000))\n",
    "\n",
    "        X_train, y_train = train_df[attributes], train_df[target]\n",
    "        X_test, y_test = test_df[attributes], test_df[target]\n",
    "\n",
    "        # Build the decision tree using the training set\n",
    "        root_node = decision_tree(X_train, y_train, attributes)\n",
    "\n",
    "        # Make predictions on the training set\n",
    "        predictions_train = predict(X_train, root_node)\n",
    "        accuracy_train = calculate_accuracy(predictions_train, y_train)\n",
    "        accuracy_train_list.append(accuracy_train)\n",
    "\n",
    "        # Make predictions on the test set\n",
    "        predictions_test = predict(X_test, root_node)\n",
    "        accuracy_test = calculate_accuracy(predictions_test, y_test)\n",
    "        accuracy_test_list.append(accuracy_test)\n",
    "\n",
    "    return accuracy_train_list, accuracy_test_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55693368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the experiment\n",
    "attributes = df_voting_shuffle.columns[:-1].tolist()\n",
    "target = 'target'\n",
    "df_voting_shuffle = shuffle(df_voting_shuffle, random_state=np.random.randint(1, 1000)) \n",
    "accuracy_train, accuracy_test = run_decision_tree(df_voting_shuffle, attributes, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14b3015",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_accuracy_train = np.mean(accuracy_train)\n",
    "mean_accuracy_test = np.mean(accuracy_test)\n",
    "\n",
    "# Print or use the values as needed\n",
    "print(\"Mean Training Accuracy:\", mean_accuracy_train)\n",
    "print(\"Mean Testing Accuracy:\", mean_accuracy_test)\n",
    "\n",
    "\n",
    "std_dev_accuracy_train = np.std(accuracy_train)\n",
    "std_dev_accuracy_test = np.std(accuracy_test)\n",
    "\n",
    "print(\"Standard Deviation Training Accuracy:\", std_dev_accuracy_train)\n",
    "print(\"Standard Deviation Testing Accuracy:\", std_dev_accuracy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914c621a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot histograms\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(accuracy_train, bins=20, edgecolor='black')\n",
    "plt.title('Accuracy Distribution on Training Set on Gini')\n",
    "plt.xlabel('Accuracy')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0614522",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(accuracy_test, bins=20, edgecolor='black')\n",
    "plt.title('Accuracy Distribution on Testing Set on Gini')\n",
    "plt.xlabel('Accuracy')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c146ba35",
   "metadata": {},
   "source": [
    "## Decision Trees with 0.85 threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af08c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree(X_train, y_train, remaining_attributes):\n",
    "    # If all instances belong to the same class\n",
    "    if len(set(y_train)) == 1:\n",
    "        return {\"class_label\": y_train.iloc[0]}\n",
    "    if not remaining_attributes or len(y_train) <= 0.85 * len(X_train):\n",
    "        # If no more attributes left or instances threshold reached\n",
    "        class_counts = y_train.value_counts()\n",
    "        majority_class = class_counts.idxmax()\n",
    "        return {\"class_label\": majority_class}\n",
    "\n",
    "    info_gain_values = [information_gain(y_train, X_train[attr]) for attr in remaining_attributes]\n",
    "    best_attribute = remaining_attributes[np.argmax(info_gain_values)] # Highest info gain attribute\n",
    "    remaining_attributes_copy = remaining_attributes.copy()  # Make a copy\n",
    "    remaining_attributes_copy.remove(best_attribute) # L(attr)-Selected(attr)\n",
    "    remaining_attributes = remaining_attributes_copy.copy()  # Update remaining_attributes\n",
    "    \n",
    "    # Create a dictionary for the current decision node\n",
    "    node = {\"attribute\": best_attribute, \"leaf\": {}}\n",
    "    # Create a list of values for the selected attribute\n",
    "    attribute_values = set(X_train[best_attribute])\n",
    "\n",
    "    # For each attribute value, create sub-trees and edges\n",
    "    for value in attribute_values:\n",
    "        # Partition - based on the attribute value\n",
    "        partition = X_train[X_train[best_attribute] == value]\n",
    "\n",
    "        if partition.empty:\n",
    "            # If partition is empty, leaf node = node with majority class\n",
    "            majority_class = y_train.value_counts().idxmax()\n",
    "            node[\"leaf\"][value] = {\"class_label\": majority_class}\n",
    "        else:\n",
    "            # Recursively create a sub-tree\n",
    "            subtree = decision_tree(partition, y_train[partition.index], remaining_attributes)\n",
    "            node[\"leaf\"][value] = subtree\n",
    "    return node\n",
    "\n",
    "def entropy(labels):\n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "    probabilities = counts / len(labels)\n",
    "    entropy_value = -np.sum(probabilities * np.log2(probabilities))\n",
    "    return entropy_value\n",
    "\n",
    "def information_gain(y, x):\n",
    "    parent_entropy = entropy(y)\n",
    "    # InfoA(D)\n",
    "    info_a = 0\n",
    "    for value in set(x):\n",
    "        partition_indices = x[x == value].index\n",
    "        partition_entropy = entropy(y[partition_indices])\n",
    "        info_a += len(partition_indices) / len(x) * partition_entropy\n",
    "    # GainA(D)\n",
    "    gain_a = parent_entropy - info_a\n",
    "    return gain_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae8794d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single(x, tree):\n",
    "    if 'class_label' in tree:\n",
    "        # Single data point\n",
    "        return tree['class_label']\n",
    "    else:\n",
    "        value = x[tree['attribute']]\n",
    "        if value in tree['leaf']:\n",
    "            # leaf has only class label or a nested dictionary\n",
    "            next_node = tree['leaf'][value]\n",
    "            if 'class_label' in next_node:\n",
    "                return next_node['class_label']\n",
    "            else:\n",
    "                return predict_single(x, next_node)\n",
    "        else:\n",
    "            return list(tree['leaf'].values())[0]\n",
    "\n",
    "def predict(X, tree):\n",
    "    return [predict_single(x, tree) for i, x in X.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40282ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = df_voting_shuffle.columns[:-1].tolist()\n",
    "target = df_voting_shuffle['target']\n",
    "train_df, test_df = train_test_split(df_voting_shuffle, test_size=0.2, random_state=34)\n",
    "\n",
    " # Split the DataFrame into features and target for training and testing sets\n",
    "X_train, y_train = train_df[attributes], train_df['target']\n",
    "X_test, y_test = test_df[attributes], test_df['target']\n",
    "\n",
    "# Build the decision tree using the training set\n",
    "root_node = decision_tree(X_train, y_train, attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495cd2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate accuracy\n",
    "def calculate_accuracy(predictions, actual):\n",
    "    return np.sum(predictions == actual) / len(actual)\n",
    "\n",
    "# Function to train and evaluate Decision Tree 100 times\n",
    "def run_decision_tree(df, attributes, target, num_iterations=100):\n",
    "    accuracy_train_list = []\n",
    "    accuracy_test_list = []\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        # Shuffle the dataset\n",
    "        df_shuffled = shuffle(df)\n",
    "\n",
    "        # Split the DataFrame into features and target for training and testing sets\n",
    "        train_df, test_df = train_test_split(df_shuffled, test_size=0.2, random_state=np.random.randint(1, 1000))\n",
    "\n",
    "        X_train, y_train = train_df[attributes], train_df[target]\n",
    "        X_test, y_test = test_df[attributes], test_df[target]\n",
    "\n",
    "        # Build the decision tree using the training set\n",
    "        root_node = decision_tree(X_train, y_train, attributes)\n",
    "\n",
    "        # Make predictions on the training set\n",
    "        predictions_train = predict(X_train, root_node)\n",
    "        accuracy_train = calculate_accuracy(predictions_train, y_train)\n",
    "        accuracy_train_list.append(accuracy_train)\n",
    "\n",
    "        # Make predictions on the test set\n",
    "        predictions_test = predict(X_test, root_node)\n",
    "        accuracy_test = calculate_accuracy(predictions_test, y_test)\n",
    "        accuracy_test_list.append(accuracy_test)\n",
    "        \n",
    "    return accuracy_train_list, accuracy_test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3fa93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the experiment\n",
    "attributes = df_voting_shuffle.columns[:-1].tolist()\n",
    "target = 'class'\n",
    "df_voting_shuffle = shuffle(df_voting_shuffle, random_state=np.random.randint(1, 1000)) \n",
    "accuracy_train, accuracy_test = run_decision_tree(df_voting_shuffle, attributes, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6f4640",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_accuracy_train = np.mean(accuracy_train)\n",
    "mean_accuracy_test = np.mean(accuracy_test)\n",
    "\n",
    "# Print or use the values as needed\n",
    "print(\"Mean Training Accuracy:\", mean_accuracy_train)\n",
    "print(\"Mean Testing Accuracy:\", mean_accuracy_test)\n",
    "\n",
    "\n",
    "std_dev_accuracy_train = np.std(accuracy_train)\n",
    "std_dev_accuracy_test = np.std(accuracy_test)\n",
    "\n",
    "print(\"Standard Deviation Training Accuracy:\", std_dev_accuracy_train)\n",
    "print(\"Standard Deviation Testing Accuracy:\", std_dev_accuracy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d37c976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(accuracy_train, bins=20, edgecolor='black')\n",
    "plt.title('Accuracy Distribution on Training Set on 85% threshold')\n",
    "plt.xlabel('Accuracy')\n",
    "plt.ylabel('Frequency')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bfca62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(accuracy_test, bins=20, edgecolor='black')\n",
    "plt.title('Accuracy Distribution on Testing Set on 85% threshold')\n",
    "plt.xlabel('Accuracy')\n",
    "plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a24557b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba875b2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5276f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
