{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08cc1fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy Results:\n",
      "                            Architecture, Lambda  Mean Accuracy\n",
      "0                              ([9, 5, 2], 0.01)       0.950725\n",
      "1                               ([9, 5, 2], 0.1)       0.959420\n",
      "2                               ([9, 5, 2], 1.0)       0.955072\n",
      "3                           ([9, 6, 4, 2], 0.01)       0.947826\n",
      "4                            ([9, 6, 4, 2], 0.1)       0.953623\n",
      "5                            ([9, 6, 4, 2], 1.0)       0.955072\n",
      "6                        ([9, 7, 5, 3, 2], 0.01)       0.946377\n",
      "7                         ([9, 7, 5, 3, 2], 0.1)       0.950725\n",
      "8                         ([9, 7, 5, 3, 2], 1.0)       0.950725\n",
      "9                     ([9, 8, 6, 4, 3, 2], 0.01)       0.962319\n",
      "10                     ([9, 8, 6, 4, 3, 2], 0.1)       0.952174\n",
      "11                     ([9, 8, 6, 4, 3, 2], 1.0)       0.946377\n",
      "12  ([9, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], 0.01)       0.656522\n",
      "13   ([9, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], 0.1)       0.656522\n",
      "14   ([9, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], 1.0)       0.656522\n",
      "15                            ([9, 20, 2], 0.01)       0.949275\n",
      "16                             ([9, 20, 2], 0.1)       0.952174\n",
      "17                             ([9, 20, 2], 1.0)       0.944928\n",
      "\n",
      "Mean F1 Score Results:\n",
      "                            Architecture, Lambda  Mean F1 Score\n",
      "0                              ([9, 5, 2], 0.01)       0.952174\n",
      "1                               ([9, 5, 2], 0.1)       0.960161\n",
      "2                               ([9, 5, 2], 1.0)       0.956584\n",
      "3                           ([9, 6, 4, 2], 0.01)       0.949170\n",
      "4                            ([9, 6, 4, 2], 0.1)       0.954374\n",
      "5                            ([9, 6, 4, 2], 1.0)       0.955875\n",
      "6                        ([9, 7, 5, 3, 2], 0.01)       0.946377\n",
      "7                         ([9, 7, 5, 3, 2], 0.1)       0.950725\n",
      "8                         ([9, 7, 5, 3, 2], 1.0)       0.951475\n",
      "9                     ([9, 8, 6, 4, 3, 2], 0.01)       0.962319\n",
      "10                     ([9, 8, 6, 4, 3, 2], 0.1)       0.952174\n",
      "11                     ([9, 8, 6, 4, 3, 2], 1.0)       0.946377\n",
      "12  ([9, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], 0.01)       0.656522\n",
      "13   ([9, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], 0.1)       0.656522\n",
      "14   ([9, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], 1.0)       0.656522\n",
      "15                            ([9, 20, 2], 0.01)       0.949974\n",
      "16                             ([9, 20, 2], 0.1)       0.956860\n",
      "17                             ([9, 20, 2], 1.0)       0.947064\n",
      "\n",
      "Mean J cost Results:\n",
      "                            Architecture, Lambda  Mean J Cost\n",
      "0                              ([9, 5, 2], 0.01)     0.020187\n",
      "1                               ([9, 5, 2], 0.1)     0.019890\n",
      "2                               ([9, 5, 2], 1.0)     0.019870\n",
      "3                           ([9, 6, 4, 2], 0.01)     0.017505\n",
      "4                            ([9, 6, 4, 2], 0.1)     0.018805\n",
      "5                            ([9, 6, 4, 2], 1.0)     0.017943\n",
      "6                        ([9, 7, 5, 3, 2], 0.01)     0.018010\n",
      "7                         ([9, 7, 5, 3, 2], 0.1)     0.017273\n",
      "8                         ([9, 7, 5, 3, 2], 1.0)     0.017197\n",
      "9                     ([9, 8, 6, 4, 3, 2], 0.01)     0.017531\n",
      "10                     ([9, 8, 6, 4, 3, 2], 0.1)     0.017051\n",
      "11                     ([9, 8, 6, 4, 3, 2], 1.0)     0.015862\n",
      "12  ([9, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], 0.01)     0.225761\n",
      "13   ([9, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], 0.1)     0.225761\n",
      "14   ([9, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], 1.0)     0.225761\n",
      "15                            ([9, 20, 2], 0.01)     0.017869\n",
      "16                             ([9, 20, 2], 0.1)     0.016457\n",
      "17                             ([9, 20, 2], 1.0)     0.016857\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        self.weights = [np.random.randn(layers[i], layers[i+1]) for i in range(len(layers)-1)]\n",
    "        self.biases = [np.zeros((1, layers[i+1])) for i in range(len(layers)-1)]\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    def forward_pass(self, X):\n",
    "        activations = [X]\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(activations[-1], w) + b\n",
    "            activations.append(self.sigmoid(z))\n",
    "        return activations\n",
    "    \n",
    "    def backward_pass(self, X, Y, activations):\n",
    "        deltas = [(activations[-1] - Y) * self.sigmoid_derivative(activations[-1])]\n",
    "        for i in range(len(self.layers) - 2, 0, -1):\n",
    "            delta = np.dot(deltas[0], self.weights[i].T) * self.sigmoid_derivative(activations[i])\n",
    "            deltas.insert(0, delta)\n",
    "        return deltas\n",
    "    \n",
    "    \n",
    "    def compute_gradients(self, activations, deltas):\n",
    "        gradients_weights = [np.dot(activations[i].T, deltas[i]) for i in range(len(self.layers) - 1)]\n",
    "        gradients_biases = [np.sum(deltas[i], axis=0) for i in range(len(self.layers) - 1)]\n",
    "        return gradients_weights, gradients_biases\n",
    "    \n",
    "    def update_weights(self, gradients_weights, gradients_biases, learning_rate):\n",
    "        self.weights = [w - learning_rate * gw for w, gw in zip(self.weights, gradients_weights)]\n",
    "        self.biases = [b - learning_rate * gb for b, gb in zip(self.biases, gradients_biases)]\n",
    "    \n",
    "    def train(self, X, Y, learning_rate, lam, max_iterations, epsilon):\n",
    "        for iteration in range(max_iterations):\n",
    "            activations = self.forward_pass(X)\n",
    "            deltas = self.backward_pass(X, Y, activations)\n",
    "            gradients_weights, gradients_biases = self.compute_gradients(activations, deltas)\n",
    "            self.update_weights(gradients_weights, gradients_biases, learning_rate)\n",
    "            # Compute cost function\n",
    "            J = np.mean(np.square(activations[-1] - Y))\n",
    "            #print(f\"Iteration {iteration+1}, Cost: {J}\")\n",
    "            # Check for convergence\n",
    "            if J < epsilon:\n",
    "                #print(f\"Converged at cost :{J} while Epsilon:{epsilon} \")\n",
    "                return J\n",
    "        return J\n",
    "            \n",
    "\n",
    "    def accuracy(self, y_true, y_pred):\n",
    "        correct = np.sum(np.all(y_true == y_pred, axis=1))\n",
    "        return correct / len(y_true)\n",
    "\n",
    "\n",
    "    def f1_score(self, y_true, y_pred):\n",
    "        tp = np.sum(np.logical_and(y_true, y_pred))\n",
    "        fp = np.sum(np.logical_and(np.logical_not(y_true), y_pred))\n",
    "        fn = np.sum(np.logical_and(y_true, np.logical_not(y_pred)))\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        return f1\n",
    "\n",
    "\n",
    "    def evaluate(self, X_test, y_test, J):\n",
    "        activations = self.forward_pass(X_test)[-1]\n",
    "        y_pred = (activations > 0.5).astype(int)\n",
    "        acc = self.accuracy(y_test, y_pred)\n",
    "        f1 = self.f1_score(y_test, y_pred)\n",
    "        return J, acc, f1\n",
    "    \n",
    "    def k_fold_cross_validation(X, y, architectures, regularization_params, learning_rate, max_iterations, epsilon):\n",
    "        results_accuracy = {}\n",
    "        results_f1_score = {}\n",
    "        results_J_cost = {}\n",
    "        \n",
    "        num_splits = 10\n",
    "        fold_size = len(X) // num_splits\n",
    "\n",
    "        for arch in architectures:\n",
    "            for lam in regularization_params:\n",
    "                accuracy_list = []\n",
    "                f1_score_list = []\n",
    "                J_list = []\n",
    "                \n",
    "                for i in range(num_splits):\n",
    "                    start = i * fold_size\n",
    "                    end = (i + 1) * fold_size\n",
    "                    \n",
    "                    X_train = pd.concat([X[:start], X[end:]])\n",
    "                    y_train = np.concatenate([y[:start], y[end:]])\n",
    "                    X_test = X[start:end]\n",
    "                    y_test = y[start:end]\n",
    "\n",
    "                    mean = np.mean(X_train, axis=0)\n",
    "                    std = np.std(X_train, axis=0)\n",
    "                    X_train_normalized = (X_train - mean) / std\n",
    "                    X_test_normalized = (X_test - mean) / std\n",
    "\n",
    "                    model = NeuralNetwork(arch)\n",
    "                    J = model.train(X_train_normalized, y_train, learning_rate=learning_rate, lam=lam, max_iterations=max_iterations, epsilon=epsilon)\n",
    "                    J, accuracy, f1_score = model.evaluate(X_test_normalized, y_test, J)\n",
    "                    accuracy_list.append(accuracy)\n",
    "                    f1_score_list.append(f1_score)\n",
    "                    J_list.append(J)\n",
    "\n",
    "                mean_accuracy = np.mean(accuracy_list)\n",
    "                mean_f1_score = np.mean(f1_score_list)\n",
    "                mean_J_cost   = np.mean(J_list)\n",
    "\n",
    "                results_accuracy[(str(arch), lam)] = mean_accuracy\n",
    "                results_f1_score[(str(arch), lam)] = mean_f1_score\n",
    "                results_J_cost[(str(arch), lam)] = mean_J_cost\n",
    "\n",
    "        return results_accuracy, results_f1_score, results_J_cost\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "df_cancer = pd.read_csv(\"/Users/noshitha/Downloads/hw4/datasets/hw3_cancer.csv\", delimiter=\"\\t\")\n",
    "\n",
    "# Extract features and target variable\n",
    "X_cancer = pd.get_dummies(df_cancer.drop(columns=['Class']))  # Features\n",
    "y_cancer = df_cancer['Class']  \n",
    "\n",
    "\n",
    "# Re-size data\n",
    "y_cancer_resized = y_cancer.values.reshape(-1, 1)\n",
    "\n",
    "# Initialize the OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Fit and transform the target variable\n",
    "y_encoded = encoder.fit_transform(y_cancer_resized)\n",
    "\n",
    "# Define model architectures and regularization parameters\n",
    "architectures = [\n",
    "    [X_cancer.shape[1], 5, y_encoded.shape[1]] , \n",
    "    [X_cancer.shape[1], 6, 4, y_encoded.shape[1]],  \n",
    "    [X_cancer.shape[1],7, 5, 3, y_encoded.shape[1]],  \n",
    "    [X_cancer.shape[1], 8, 6, 4, 3, y_encoded.shape[1]], \n",
    "    [X_cancer.shape[1],1,2,2,2,2,2,2,2,2,2, y_encoded.shape[1]], \n",
    "    [X_cancer.shape[1], 20, y_encoded.shape[1]]  \n",
    "]\n",
    "\n",
    "regularization_params = [0.01, 0.1, 1.0]  # Example regularization parameters\n",
    "\n",
    "# Initialize lists to store results\n",
    "results_accuracy = {}\n",
    "results_f1_score = {}\n",
    "results_J_cost = {}\n",
    "# Perform stratified k-fold cross-validation\n",
    "results_accuracy, results_f1_score, results_J_cost = NeuralNetwork.k_fold_cross_validation(X_cancer, y_encoded, architectures, regularization_params, learning_rate=0.01, max_iterations=1000, epsilon=0.005)\n",
    "\n",
    "# Convert the results into a DataFrame for tabular representation\n",
    "accuracy_df = pd.DataFrame(list(results_accuracy.items()), columns=['Architecture, Lambda', 'Mean Accuracy'])\n",
    "f1_score_df = pd.DataFrame(list(results_f1_score.items()), columns=['Architecture, Lambda', 'Mean F1 Score'])\n",
    "J_cost_df = pd.DataFrame(list(results_J_cost.items()), columns=['Architecture, Lambda', 'Mean J Cost'])\n",
    "\n",
    "print(\"Mean Accuracy Results:\")\n",
    "print(accuracy_df)\n",
    "print(\"\\nMean F1 Score Results:\")\n",
    "print(f1_score_df)\n",
    "print(\"\\nMean J cost Results:\")\n",
    "print(J_cost_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5719625a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Architecture, Lambda</th>\n",
       "      <th>Mean Accuracy</th>\n",
       "      <th>Mean F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>([9, 5, 2], 0.01)</td>\n",
       "      <td>0.950725</td>\n",
       "      <td>0.952174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>([9, 5, 2], 0.1)</td>\n",
       "      <td>0.959420</td>\n",
       "      <td>0.960161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>([9, 5, 2], 1.0)</td>\n",
       "      <td>0.955072</td>\n",
       "      <td>0.956584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>([9, 6, 4, 2], 0.01)</td>\n",
       "      <td>0.947826</td>\n",
       "      <td>0.949170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>([9, 6, 4, 2], 0.1)</td>\n",
       "      <td>0.953623</td>\n",
       "      <td>0.954374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>([9, 6, 4, 2], 1.0)</td>\n",
       "      <td>0.955072</td>\n",
       "      <td>0.955875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>([9, 7, 5, 3, 2], 0.01)</td>\n",
       "      <td>0.946377</td>\n",
       "      <td>0.946377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>([9, 7, 5, 3, 2], 0.1)</td>\n",
       "      <td>0.950725</td>\n",
       "      <td>0.950725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>([9, 7, 5, 3, 2], 1.0)</td>\n",
       "      <td>0.950725</td>\n",
       "      <td>0.951475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>([9, 8, 6, 4, 3, 2], 0.01)</td>\n",
       "      <td>0.962319</td>\n",
       "      <td>0.962319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>([9, 8, 6, 4, 3, 2], 0.1)</td>\n",
       "      <td>0.952174</td>\n",
       "      <td>0.952174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>([9, 8, 6, 4, 3, 2], 1.0)</td>\n",
       "      <td>0.946377</td>\n",
       "      <td>0.946377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>([9, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], 0.01)</td>\n",
       "      <td>0.656522</td>\n",
       "      <td>0.656522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>([9, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], 0.1)</td>\n",
       "      <td>0.656522</td>\n",
       "      <td>0.656522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>([9, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], 1.0)</td>\n",
       "      <td>0.656522</td>\n",
       "      <td>0.656522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>([9, 20, 2], 0.01)</td>\n",
       "      <td>0.949275</td>\n",
       "      <td>0.949974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>([9, 20, 2], 0.1)</td>\n",
       "      <td>0.952174</td>\n",
       "      <td>0.956860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>([9, 20, 2], 1.0)</td>\n",
       "      <td>0.944928</td>\n",
       "      <td>0.947064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Architecture, Lambda  Mean Accuracy  Mean F1 Score\n",
       "0                              ([9, 5, 2], 0.01)       0.950725       0.952174\n",
       "1                               ([9, 5, 2], 0.1)       0.959420       0.960161\n",
       "2                               ([9, 5, 2], 1.0)       0.955072       0.956584\n",
       "3                           ([9, 6, 4, 2], 0.01)       0.947826       0.949170\n",
       "4                            ([9, 6, 4, 2], 0.1)       0.953623       0.954374\n",
       "5                            ([9, 6, 4, 2], 1.0)       0.955072       0.955875\n",
       "6                        ([9, 7, 5, 3, 2], 0.01)       0.946377       0.946377\n",
       "7                         ([9, 7, 5, 3, 2], 0.1)       0.950725       0.950725\n",
       "8                         ([9, 7, 5, 3, 2], 1.0)       0.950725       0.951475\n",
       "9                     ([9, 8, 6, 4, 3, 2], 0.01)       0.962319       0.962319\n",
       "10                     ([9, 8, 6, 4, 3, 2], 0.1)       0.952174       0.952174\n",
       "11                     ([9, 8, 6, 4, 3, 2], 1.0)       0.946377       0.946377\n",
       "12  ([9, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], 0.01)       0.656522       0.656522\n",
       "13   ([9, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], 0.1)       0.656522       0.656522\n",
       "14   ([9, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], 1.0)       0.656522       0.656522\n",
       "15                            ([9, 20, 2], 0.01)       0.949275       0.949974\n",
       "16                             ([9, 20, 2], 0.1)       0.952174       0.956860\n",
       "17                             ([9, 20, 2], 1.0)       0.944928       0.947064"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge accuracy and f1_score DataFrames on 'Architecture, Lambda'\n",
    "merged_df = pd.merge(accuracy_df, f1_score_df, on='Architecture, Lambda')\n",
    "\n",
    "# Merge the merged DataFrame with J_cost_df on 'Architecture, Lambda'\n",
    "final_df = pd.merge(merged_df, J_cost_df, on='Architecture, Lambda')\n",
    "\n",
    "# Rename columns for clarity\n",
    "merged_df.columns = ['Architecture, Lambda', 'Mean Accuracy', 'Mean F1 Score']\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52bf6c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "825417a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to generate mini-batches\n",
    "def generate_mini_batches(X, y, batch_size):\n",
    "    num_samples = X.shape[0]\n",
    "    num_batches = num_samples // batch_size\n",
    "    mini_batches = []\n",
    "    shuffled_indices = np.random.permutation(num_samples)\n",
    "    X_shuffled = X[shuffled_indices]\n",
    "    y_shuffled = y[shuffled_indices]\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        mini_batches.append((X_shuffled[start_idx:end_idx], y_shuffled[start_idx:end_idx]))\n",
    "    if num_samples % batch_size != 0:\n",
    "        mini_batches.append((X_shuffled[num_batches*batch_size:], y_shuffled[num_batches*batch_size:]))\n",
    "    return mini_batches\n",
    "\n",
    "def manual_train_test_split(X, y, test_size=0.2):\n",
    "    indices = np.random.permutation(X.shape[0])\n",
    "    test_indices = indices[:int(test_size * X.shape[0])]\n",
    "    train_indices = indices[int(test_size * X.shape[0]):]\n",
    "    X_train, X_test = X[train_indices], X[test_indices]\n",
    "    y_train, y_test = y[train_indices], y[test_indices]\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def train_mini_batch(X_train, y_train, X_test, y_test, model, learning_rate, batch_size, max_iterations, epsilon):\n",
    "    training_errors = []\n",
    "    testing_errors = []\n",
    "    for iteration in range(max_iterations):\n",
    "        mini_batches = generate_mini_batches(X_train, y_train, batch_size)\n",
    "        for mini_batch in mini_batches:\n",
    "            X_mini_batch, y_mini_batch = mini_batch\n",
    "            J = model.train(X_mini_batch, y_mini_batch, learning_rate=learning_rate, lam=0.001, max_iterations=1, epsilon=epsilon)\n",
    "        training_cost = np.mean(np.square(model.forward_pass(X_train)[-1] - y_train))  # Compute training cost\n",
    "        testing_cost = np.mean(np.square(model.forward_pass(X_test)[-1] - y_test))  # Compute testing cost\n",
    "        training_errors.append(training_cost)\n",
    "        testing_errors.append(testing_cost)\n",
    "        print(f\"Iteration {iteration+1}, Training Cost: {training_cost}, Testing Cost: {testing_cost}\")\n",
    "        # Check for convergence\n",
    "        if training_cost < epsilon:\n",
    "            #print(f\"Converged at training cost :{training_cost} while Epsilon:{epsilon} \")\n",
    "            break\n",
    "    return training_errors, testing_errors\n",
    "\n",
    "# Plot learning curve\n",
    "def plot_learning_curve(training_errors, testing_errors, step_size):\n",
    "    iterations = range(1, len(training_errors) + 1)\n",
    "    plt.plot(iterations, training_errors, label='Training Error')\n",
    "    plt.plot(iterations, testing_errors, label='Testing Error')\n",
    "    plt.title('Learning Curve')\n",
    "    plt.xlabel('Number of Training Samples')\n",
    "    plt.ylabel('J values')\n",
    "    plt.xticks(np.arange(1, len(training_errors) + 1, step=step_size))\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49048726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, Training Cost: 0.2752389140520499, Testing Cost: 0.27552875711963665\n",
      "Iteration 2, Training Cost: 0.18556871232435868, Testing Cost: 0.18481552810857205\n",
      "Iteration 3, Training Cost: 0.15824526507486647, Testing Cost: 0.15708772521175685\n",
      "Iteration 4, Training Cost: 0.14027958173004298, Testing Cost: 0.13974485604398926\n",
      "Iteration 5, Training Cost: 0.12457676231233172, Testing Cost: 0.1252184060497719\n",
      "Iteration 6, Training Cost: 0.11014663990640974, Testing Cost: 0.11196150284540905\n",
      "Iteration 7, Training Cost: 0.09723147524780275, Testing Cost: 0.10015256253491203\n",
      "Iteration 8, Training Cost: 0.08599109454974233, Testing Cost: 0.0896937612499988\n",
      "Iteration 9, Training Cost: 0.07651720441537353, Testing Cost: 0.08088659184182058\n",
      "Iteration 10, Training Cost: 0.06869023696168922, Testing Cost: 0.07356160749303065\n",
      "Iteration 11, Training Cost: 0.06223864816543884, Testing Cost: 0.06758363663329288\n",
      "Iteration 12, Training Cost: 0.057022231027541684, Testing Cost: 0.06267654557268675\n",
      "Iteration 13, Training Cost: 0.05277466731717364, Testing Cost: 0.058674183775677645\n",
      "Iteration 14, Training Cost: 0.04932410209164041, Testing Cost: 0.055407498371486154\n",
      "Iteration 15, Training Cost: 0.04648820484731968, Testing Cost: 0.0527091566091691\n",
      "Iteration 16, Training Cost: 0.044134117584614954, Testing Cost: 0.05042902627992498\n",
      "Iteration 17, Training Cost: 0.04216048353009923, Testing Cost: 0.04850649749206759\n",
      "Iteration 18, Training Cost: 0.040480213203740366, Testing Cost: 0.046870518829333685\n",
      "Iteration 19, Training Cost: 0.03904484793899829, Testing Cost: 0.04546568202519232\n",
      "Iteration 20, Training Cost: 0.03780245156506424, Testing Cost: 0.044252396554780485\n",
      "Iteration 21, Training Cost: 0.03672003924272055, Testing Cost: 0.04319647241830901\n",
      "Iteration 22, Training Cost: 0.03577124107737291, Testing Cost: 0.042271736931682034\n",
      "Iteration 23, Training Cost: 0.03493361786816513, Testing Cost: 0.04146031643682651\n",
      "Iteration 24, Training Cost: 0.0341893899815363, Testing Cost: 0.040746178442940435\n",
      "Iteration 25, Training Cost: 0.03352341232491774, Testing Cost: 0.04011002817020371\n",
      "Iteration 26, Training Cost: 0.03292696779391306, Testing Cost: 0.039544020929912604\n",
      "Iteration 27, Training Cost: 0.03238821149495686, Testing Cost: 0.03903899304694397\n",
      "Iteration 28, Training Cost: 0.031900174288342416, Testing Cost: 0.03858461095648117\n",
      "Iteration 29, Training Cost: 0.031457745504066284, Testing Cost: 0.038177481982058746\n",
      "Iteration 30, Training Cost: 0.031054459988977944, Testing Cost: 0.03781144690259706\n",
      "Iteration 31, Training Cost: 0.030684867710892457, Testing Cost: 0.03748148393091627\n",
      "Iteration 32, Training Cost: 0.03034557910972952, Testing Cost: 0.037181929068504\n",
      "Iteration 33, Training Cost: 0.030032737113148543, Testing Cost: 0.03691032492493781\n",
      "Iteration 34, Training Cost: 0.029744770844922892, Testing Cost: 0.03666433008124701\n",
      "Iteration 35, Training Cost: 0.02947778239661188, Testing Cost: 0.036441383816400635\n",
      "Iteration 36, Training Cost: 0.029229611432794792, Testing Cost: 0.03623878014589799\n",
      "Iteration 37, Training Cost: 0.028999059333186104, Testing Cost: 0.03605391407371419\n",
      "Iteration 38, Training Cost: 0.0287841475677225, Testing Cost: 0.035885608664346196\n",
      "Iteration 39, Training Cost: 0.028583242695609207, Testing Cost: 0.0357311218190521\n",
      "Iteration 40, Training Cost: 0.02839543042484632, Testing Cost: 0.03559056191877468\n",
      "Iteration 41, Training Cost: 0.02821899926192753, Testing Cost: 0.035462178104901196\n",
      "Iteration 42, Training Cost: 0.028052974750354488, Testing Cost: 0.035344612471795066\n",
      "Iteration 43, Training Cost: 0.027896667149261502, Testing Cost: 0.035237142208409386\n",
      "Iteration 44, Training Cost: 0.027749358688369974, Testing Cost: 0.03513627269633042\n",
      "Iteration 45, Training Cost: 0.027610102704625293, Testing Cost: 0.03504595561387975\n",
      "Iteration 46, Training Cost: 0.027478476293517237, Testing Cost: 0.03496301488839294\n",
      "Iteration 47, Training Cost: 0.027353742788220437, Testing Cost: 0.034885892188917564\n",
      "Iteration 48, Training Cost: 0.0272353085294366, Testing Cost: 0.03481669591109076\n",
      "Iteration 49, Training Cost: 0.027123132792565133, Testing Cost: 0.03475157729684115\n",
      "Iteration 50, Training Cost: 0.027015990198905437, Testing Cost: 0.03469499284009365\n",
      "Iteration 51, Training Cost: 0.026914253261111208, Testing Cost: 0.034640218604936115\n",
      "Iteration 52, Training Cost: 0.02681726118833114, Testing Cost: 0.03458850186624064\n",
      "Iteration 53, Training Cost: 0.026724737390111594, Testing Cost: 0.03454265698747847\n",
      "Iteration 54, Training Cost: 0.02663601025226453, Testing Cost: 0.03450159427126952\n",
      "Iteration 55, Training Cost: 0.026551374279035575, Testing Cost: 0.03446217720366815\n",
      "Iteration 56, Training Cost: 0.026470414228205844, Testing Cost: 0.034425450122093096\n",
      "Iteration 57, Training Cost: 0.026392596156653823, Testing Cost: 0.034392653429267575\n",
      "Iteration 58, Training Cost: 0.026318102645740506, Testing Cost: 0.03436195620893786\n",
      "Iteration 59, Training Cost: 0.026246409528033105, Testing Cost: 0.034337098117310734\n",
      "Iteration 60, Training Cost: 0.02617776593280948, Testing Cost: 0.0343124332310352\n",
      "Iteration 61, Training Cost: 0.02611179623824887, Testing Cost: 0.03428757045981799\n",
      "Iteration 62, Training Cost: 0.026048290206992116, Testing Cost: 0.0342658682733441\n",
      "Iteration 63, Training Cost: 0.025986882813427016, Testing Cost: 0.034247196947458584\n",
      "Iteration 64, Training Cost: 0.025927735952336283, Testing Cost: 0.03423011466610746\n",
      "Iteration 65, Training Cost: 0.025870682963681276, Testing Cost: 0.03421764301893421\n",
      "Iteration 66, Training Cost: 0.02581567206607374, Testing Cost: 0.03420542423037115\n",
      "Iteration 67, Training Cost: 0.025762589874783443, Testing Cost: 0.034191652606290944\n",
      "Iteration 68, Training Cost: 0.025711314218392435, Testing Cost: 0.034180997411432\n",
      "Iteration 69, Training Cost: 0.025661514718136425, Testing Cost: 0.034173522550832884\n",
      "Iteration 70, Training Cost: 0.02561356910209622, Testing Cost: 0.03416553009522895\n",
      "Iteration 71, Training Cost: 0.025566946080467307, Testing Cost: 0.03416203767162905\n",
      "Iteration 72, Training Cost: 0.02552180927037272, Testing Cost: 0.034155335395793886\n",
      "Iteration 73, Training Cost: 0.02547802805786245, Testing Cost: 0.034151484173276826\n",
      "Iteration 74, Training Cost: 0.02543571513582787, Testing Cost: 0.03414890075322641\n",
      "Iteration 75, Training Cost: 0.02539449767896611, Testing Cost: 0.03414567697546476\n",
      "Iteration 76, Training Cost: 0.025354606463386598, Testing Cost: 0.03414349424603954\n",
      "Iteration 77, Training Cost: 0.025315780632397294, Testing Cost: 0.03414152664114823\n",
      "Iteration 78, Training Cost: 0.025278115179436136, Testing Cost: 0.0341423308846075\n",
      "Iteration 79, Training Cost: 0.025241364906755957, Testing Cost: 0.034141237416273146\n",
      "Iteration 80, Training Cost: 0.025205604001726188, Testing Cost: 0.03414462295640903\n",
      "Iteration 81, Training Cost: 0.025170849623760677, Testing Cost: 0.03414489462179749\n",
      "Iteration 82, Training Cost: 0.02513702513835997, Testing Cost: 0.03414398853718039\n",
      "Iteration 83, Training Cost: 0.0251040440755973, Testing Cost: 0.03414379764045433\n",
      "Iteration 84, Training Cost: 0.025071836034713933, Testing Cost: 0.03414427331285206\n",
      "Iteration 85, Training Cost: 0.025040484984028284, Testing Cost: 0.03414848863067476\n",
      "Iteration 86, Training Cost: 0.025009805774461564, Testing Cost: 0.03415275396273513\n",
      "Iteration 87, Training Cost: 0.024979860627916937, Testing Cost: 0.03415651861658018\n",
      "Iteration 88, Training Cost: 0.02495062181983424, Testing Cost: 0.03416214128800493\n",
      "Iteration 89, Training Cost: 0.024922136428756056, Testing Cost: 0.03416677031969384\n",
      "Iteration 90, Training Cost: 0.024894300498103324, Testing Cost: 0.03417296277361198\n",
      "Iteration 91, Training Cost: 0.024867098059486614, Testing Cost: 0.03417765810808434\n",
      "Iteration 92, Training Cost: 0.02484049881054328, Testing Cost: 0.03418608927061151\n",
      "Iteration 93, Training Cost: 0.024814512690786977, Testing Cost: 0.03419035492703886\n",
      "Iteration 94, Training Cost: 0.024789028086883342, Testing Cost: 0.034197403307289424\n",
      "Iteration 95, Training Cost: 0.02476408441207519, Testing Cost: 0.03420538593204642\n",
      "Iteration 96, Training Cost: 0.024739694040697084, Testing Cost: 0.034211972455352964\n",
      "Iteration 97, Training Cost: 0.024715841840583376, Testing Cost: 0.03422074270381886\n",
      "Iteration 98, Training Cost: 0.024692457047449455, Testing Cost: 0.034230342241450974\n",
      "Iteration 99, Training Cost: 0.0246695247282294, Testing Cost: 0.034240190494346866\n",
      "Iteration 100, Training Cost: 0.024646977965459255, Testing Cost: 0.034245407886322014\n",
      "Iteration 101, Training Cost: 0.024624936233876796, Testing Cost: 0.03425245182805089\n",
      "Iteration 102, Training Cost: 0.024603333956551347, Testing Cost: 0.034257958095394975\n",
      "Iteration 103, Training Cost: 0.02458210092571292, Testing Cost: 0.03426690758193216\n",
      "Iteration 104, Training Cost: 0.024561320919768235, Testing Cost: 0.03427497645367072\n",
      "Iteration 105, Training Cost: 0.024540882618366698, Testing Cost: 0.0342835043659363\n",
      "Iteration 106, Training Cost: 0.02452085388484498, Testing Cost: 0.034292483387617964\n",
      "Iteration 107, Training Cost: 0.024501084474805538, Testing Cost: 0.0342992816248021\n",
      "Iteration 108, Training Cost: 0.024481695546292398, Testing Cost: 0.0343095594152722\n",
      "Iteration 109, Training Cost: 0.02446274231223272, Testing Cost: 0.034322181752864514\n",
      "Iteration 110, Training Cost: 0.02444406381325898, Testing Cost: 0.034331488481200455\n",
      "Iteration 111, Training Cost: 0.02442573647158242, Testing Cost: 0.0343415414901481\n",
      "Iteration 112, Training Cost: 0.024407666873521625, Testing Cost: 0.03435271565360025\n",
      "Iteration 113, Training Cost: 0.02438991585003385, Testing Cost: 0.034363091370985144\n",
      "Iteration 114, Training Cost: 0.024372446702317627, Testing Cost: 0.034374361299202856\n",
      "Iteration 115, Training Cost: 0.0243552480482752, Testing Cost: 0.034383184790431155\n",
      "Iteration 116, Training Cost: 0.024338341455103742, Testing Cost: 0.03439401161540125\n",
      "Iteration 117, Training Cost: 0.02432167392765172, Testing Cost: 0.034406295903504795\n",
      "Iteration 118, Training Cost: 0.024305285031148992, Testing Cost: 0.034417791252628914\n",
      "Iteration 119, Training Cost: 0.024289112060007827, Testing Cost: 0.03442775558244694\n",
      "Iteration 120, Training Cost: 0.024273206292009516, Testing Cost: 0.034441153246119856\n",
      "Iteration 121, Training Cost: 0.0242575307125053, Testing Cost: 0.03445107442202703\n",
      "Iteration 122, Training Cost: 0.024242088071446476, Testing Cost: 0.034461210522312935\n",
      "Iteration 123, Training Cost: 0.024226820917656267, Testing Cost: 0.03447266932918951\n",
      "Iteration 124, Training Cost: 0.024211790649171682, Testing Cost: 0.034484084176058485\n",
      "Iteration 125, Training Cost: 0.024196928845241595, Testing Cost: 0.034494871817442686\n",
      "Iteration 126, Training Cost: 0.024182321123699894, Testing Cost: 0.03450563588673149\n",
      "Iteration 127, Training Cost: 0.02416790503757242, Testing Cost: 0.03451748938567429\n",
      "Iteration 128, Training Cost: 0.02415368814509205, Testing Cost: 0.0345321893176029\n",
      "Iteration 129, Training Cost: 0.024139661056854115, Testing Cost: 0.034546318254243334\n",
      "Iteration 130, Training Cost: 0.024125792562238697, Testing Cost: 0.03455729637449974\n",
      "Iteration 131, Training Cost: 0.02411211792247514, Testing Cost: 0.034568857687173334\n",
      "Iteration 132, Training Cost: 0.02409860308446094, Testing Cost: 0.03458252025372175\n",
      "Iteration 133, Training Cost: 0.024085288261270956, Testing Cost: 0.03459555275126295\n",
      "Iteration 134, Training Cost: 0.024072120124752263, Testing Cost: 0.03460626634933192\n",
      "Iteration 135, Training Cost: 0.024059120496910457, Testing Cost: 0.03461970965447554\n",
      "Iteration 136, Training Cost: 0.024046285695690964, Testing Cost: 0.03463395817623638\n",
      "Iteration 137, Training Cost: 0.024033590034085537, Testing Cost: 0.03464855433110099\n",
      "Iteration 138, Training Cost: 0.02402101839108475, Testing Cost: 0.03466073460891553\n",
      "Iteration 139, Training Cost: 0.02400863672556886, Testing Cost: 0.03467398230525611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 140, Training Cost: 0.023996353740462707, Testing Cost: 0.03468381563176268\n",
      "Iteration 141, Training Cost: 0.023984302051123684, Testing Cost: 0.03469979448744668\n",
      "Iteration 142, Training Cost: 0.02397230907987636, Testing Cost: 0.03471299425907947\n",
      "Iteration 143, Training Cost: 0.02396044272771331, Testing Cost: 0.03472523177290365\n",
      "Iteration 144, Training Cost: 0.02394873876490882, Testing Cost: 0.03474005531655352\n",
      "Iteration 145, Training Cost: 0.02393712906098366, Testing Cost: 0.03475332783381036\n",
      "Iteration 146, Training Cost: 0.02392566984816799, Testing Cost: 0.03476591942065356\n",
      "Iteration 147, Training Cost: 0.023914329393801594, Testing Cost: 0.03477993127724836\n",
      "Iteration 148, Training Cost: 0.023903089000304585, Testing Cost: 0.03479258381573211\n",
      "Iteration 149, Training Cost: 0.0238919375591713, Testing Cost: 0.03480456060391753\n",
      "Iteration 150, Training Cost: 0.023880927749342327, Testing Cost: 0.03481772860836108\n",
      "Iteration 151, Training Cost: 0.023870006436920346, Testing Cost: 0.03483007044420118\n",
      "Iteration 152, Training Cost: 0.023859195251760043, Testing Cost: 0.034842714251849635\n",
      "Iteration 153, Training Cost: 0.023848509334248858, Testing Cost: 0.0348593147476682\n",
      "Iteration 154, Training Cost: 0.02383794853735836, Testing Cost: 0.034876570758441514\n",
      "Iteration 155, Training Cost: 0.023827499185556236, Testing Cost: 0.03489359149245941\n",
      "Iteration 156, Training Cost: 0.02381714540551669, Testing Cost: 0.03491013563168636\n",
      "Iteration 157, Training Cost: 0.02380681624699729, Testing Cost: 0.03492264225519761\n",
      "Iteration 158, Training Cost: 0.023796625656109495, Testing Cost: 0.034939319631123275\n",
      "Iteration 159, Training Cost: 0.023786502939977153, Testing Cost: 0.03495215207746437\n",
      "Iteration 160, Training Cost: 0.02377648522570309, Testing Cost: 0.034961914532410245\n",
      "Iteration 161, Training Cost: 0.02376650918790378, Testing Cost: 0.034974632358758244\n",
      "Iteration 162, Training Cost: 0.02375668526990365, Testing Cost: 0.034992674704449424\n",
      "Iteration 163, Training Cost: 0.02374692636468828, Testing Cost: 0.035007694564005085\n",
      "Iteration 164, Training Cost: 0.02373723263157522, Testing Cost: 0.03502443282541261\n",
      "Iteration 165, Training Cost: 0.02372760527816075, Testing Cost: 0.03503862378259729\n",
      "Iteration 166, Training Cost: 0.02371806074307926, Testing Cost: 0.03505092156153667\n",
      "Iteration 167, Training Cost: 0.023708600456218143, Testing Cost: 0.03506497580191862\n",
      "Iteration 168, Training Cost: 0.023699206905097428, Testing Cost: 0.03507848297418894\n",
      "Iteration 169, Training Cost: 0.023689934819725544, Testing Cost: 0.035094169241476715\n",
      "Iteration 170, Training Cost: 0.02368067900396199, Testing Cost: 0.03510486404011382\n",
      "Iteration 171, Training Cost: 0.02367151414303355, Testing Cost: 0.03511951510051073\n",
      "Iteration 172, Training Cost: 0.02366242131348832, Testing Cost: 0.03513128635099999\n",
      "Iteration 173, Training Cost: 0.023653428837469317, Testing Cost: 0.035145423358445706\n",
      "Iteration 174, Training Cost: 0.023644458534529622, Testing Cost: 0.03516093872297588\n",
      "Iteration 175, Training Cost: 0.023635561518202345, Testing Cost: 0.03517664647582939\n",
      "Iteration 176, Training Cost: 0.023626750545131338, Testing Cost: 0.03519261609225311\n",
      "Iteration 177, Training Cost: 0.02361801683712766, Testing Cost: 0.03520818271168676\n",
      "Iteration 178, Training Cost: 0.02360931186845936, Testing Cost: 0.03522611099652012\n",
      "Iteration 179, Training Cost: 0.0236006784636144, Testing Cost: 0.035240468464632815\n",
      "Iteration 180, Training Cost: 0.023592118054196684, Testing Cost: 0.03525709884440795\n",
      "Iteration 181, Training Cost: 0.023583607801486692, Testing Cost: 0.035274460371622685\n",
      "Iteration 182, Training Cost: 0.023575119616210206, Testing Cost: 0.03528706583856788\n",
      "Iteration 183, Training Cost: 0.023566730231992676, Testing Cost: 0.035305280623232314\n",
      "Iteration 184, Training Cost: 0.023558388354323487, Testing Cost: 0.03532119802395759\n",
      "Iteration 185, Training Cost: 0.02355007914669824, Testing Cost: 0.03533783497544967\n",
      "Iteration 186, Training Cost: 0.023541793654738973, Testing Cost: 0.035352424439151306\n",
      "Iteration 187, Training Cost: 0.023533599155470843, Testing Cost: 0.03536701920901665\n",
      "Iteration 188, Training Cost: 0.023525456853417743, Testing Cost: 0.03538543830352988\n",
      "Iteration 189, Training Cost: 0.023517337401802476, Testing Cost: 0.03540042640679518\n",
      "Iteration 190, Training Cost: 0.023509258207845742, Testing Cost: 0.03541681108774027\n",
      "Iteration 191, Training Cost: 0.023501255581376293, Testing Cost: 0.035430550452806554\n",
      "Iteration 192, Training Cost: 0.0234932903070314, Testing Cost: 0.03544576902984239\n",
      "Iteration 193, Training Cost: 0.023485372380737446, Testing Cost: 0.03546097418569669\n",
      "Iteration 194, Training Cost: 0.023477520392648263, Testing Cost: 0.035478606074692415\n",
      "Iteration 195, Training Cost: 0.02346968152139121, Testing Cost: 0.03549639361447383\n",
      "Iteration 196, Training Cost: 0.023461897589036655, Testing Cost: 0.03551315602081559\n",
      "Iteration 197, Training Cost: 0.02345415094025086, Testing Cost: 0.03553054241791678\n",
      "Iteration 198, Training Cost: 0.023446430251199434, Testing Cost: 0.03554740464223736\n",
      "Iteration 199, Training Cost: 0.023438752870666813, Testing Cost: 0.0355619150515426\n",
      "Iteration 200, Training Cost: 0.02343112288395093, Testing Cost: 0.035578792966483266\n",
      "Iteration 201, Training Cost: 0.023423554408390532, Testing Cost: 0.03559551778874781\n",
      "Iteration 202, Training Cost: 0.023416020883623507, Testing Cost: 0.03560859062638091\n",
      "Iteration 203, Training Cost: 0.023408506931654225, Testing Cost: 0.035628787929031784\n",
      "Iteration 204, Training Cost: 0.023401045252809026, Testing Cost: 0.03565003421945378\n",
      "Iteration 205, Training Cost: 0.023393579009988934, Testing Cost: 0.03566914196532178\n",
      "Iteration 206, Training Cost: 0.023386151014866796, Testing Cost: 0.035687019757011496\n",
      "Iteration 207, Training Cost: 0.02337880695187189, Testing Cost: 0.035703821155887744\n",
      "Iteration 208, Training Cost: 0.02337148440677412, Testing Cost: 0.035720823924938264\n",
      "Iteration 209, Training Cost: 0.02336416183873694, Testing Cost: 0.03573550508794598\n",
      "Iteration 210, Training Cost: 0.023356872285080342, Testing Cost: 0.03575165890058676\n",
      "Iteration 211, Training Cost: 0.023349600937551385, Testing Cost: 0.03577169142784869\n",
      "Iteration 212, Training Cost: 0.023342376519624158, Testing Cost: 0.0357903237501573\n",
      "Iteration 213, Training Cost: 0.023335172842245767, Testing Cost: 0.035807234337606866\n",
      "Iteration 214, Training Cost: 0.02332801968287448, Testing Cost: 0.03582680140056531\n",
      "Iteration 215, Training Cost: 0.0233208825030629, Testing Cost: 0.0358425803026761\n",
      "Iteration 216, Training Cost: 0.023313746737824992, Testing Cost: 0.0358565117615233\n",
      "Iteration 217, Training Cost: 0.023306628868625542, Testing Cost: 0.03586900688315944\n",
      "Iteration 218, Training Cost: 0.023299573634566535, Testing Cost: 0.035887895739586624\n",
      "Iteration 219, Training Cost: 0.023292538161296488, Testing Cost: 0.03590340105032787\n",
      "Iteration 220, Training Cost: 0.02328552398077609, Testing Cost: 0.0359188982576628\n",
      "Iteration 221, Training Cost: 0.023278501531120373, Testing Cost: 0.03593355611825894\n",
      "Iteration 222, Training Cost: 0.023271539847392723, Testing Cost: 0.03594992209339903\n",
      "Iteration 223, Training Cost: 0.023264609563788372, Testing Cost: 0.035971322829827526\n",
      "Iteration 224, Training Cost: 0.023257698770779937, Testing Cost: 0.035995722183434685\n",
      "Iteration 225, Training Cost: 0.02325079929713036, Testing Cost: 0.03601466643264409\n",
      "Iteration 226, Training Cost: 0.023243961542269955, Testing Cost: 0.036035406979061414\n",
      "Iteration 227, Training Cost: 0.023237092438174657, Testing Cost: 0.03605151299130025\n",
      "Iteration 228, Training Cost: 0.023230254311096057, Testing Cost: 0.036068670905529246\n",
      "Iteration 229, Training Cost: 0.023223426384780588, Testing Cost: 0.036086959695106495\n",
      "Iteration 230, Training Cost: 0.02321663153173361, Testing Cost: 0.03610792838516667\n",
      "Iteration 231, Training Cost: 0.02320985428893724, Testing Cost: 0.0361227995014075\n",
      "Iteration 232, Training Cost: 0.023203102312995803, Testing Cost: 0.036142044386710016\n",
      "Iteration 233, Training Cost: 0.023196368051931086, Testing Cost: 0.03615901093244907\n",
      "Iteration 234, Training Cost: 0.02318964382625982, Testing Cost: 0.03618123109146965\n",
      "Iteration 235, Training Cost: 0.023182947266525627, Testing Cost: 0.03619873817229261\n",
      "Iteration 236, Training Cost: 0.023176274166872613, Testing Cost: 0.0362169337546514\n",
      "Iteration 237, Training Cost: 0.023169604211166803, Testing Cost: 0.0362311300929558\n",
      "Iteration 238, Training Cost: 0.02316294866621391, Testing Cost: 0.03624878198547587\n",
      "Iteration 239, Training Cost: 0.023156322304987438, Testing Cost: 0.036267894865551396\n",
      "Iteration 240, Training Cost: 0.0231496955726979, Testing Cost: 0.03628789472360394\n",
      "Iteration 241, Training Cost: 0.0231431084265464, Testing Cost: 0.036308422061846625\n",
      "Iteration 242, Training Cost: 0.02313651976579037, Testing Cost: 0.03632734025632177\n",
      "Iteration 243, Training Cost: 0.023129928030522492, Testing Cost: 0.03634590131331291\n",
      "Iteration 244, Training Cost: 0.023123357121757903, Testing Cost: 0.03636259857463489\n",
      "Iteration 245, Training Cost: 0.023116828385059584, Testing Cost: 0.036382890939294935\n",
      "Iteration 246, Training Cost: 0.02311029846742808, Testing Cost: 0.0364026575386268\n",
      "Iteration 247, Training Cost: 0.023103751097286344, Testing Cost: 0.036421672413578264\n",
      "Iteration 248, Training Cost: 0.02309723058640681, Testing Cost: 0.03643773673491388\n",
      "Iteration 249, Training Cost: 0.02309074160954457, Testing Cost: 0.036458233663237166\n",
      "Iteration 250, Training Cost: 0.02308423131136941, Testing Cost: 0.03648021564077929\n",
      "Iteration 251, Training Cost: 0.023077729243113992, Testing Cost: 0.036496345661749856\n",
      "Iteration 252, Training Cost: 0.023071220069885615, Testing Cost: 0.036514232507059964\n",
      "Iteration 253, Training Cost: 0.02306474642960197, Testing Cost: 0.03653129536835497\n",
      "Iteration 254, Training Cost: 0.023058273850315652, Testing Cost: 0.036553868243750766\n",
      "Iteration 255, Training Cost: 0.023051814331886685, Testing Cost: 0.03657469162871552\n",
      "Iteration 256, Training Cost: 0.02304538964958545, Testing Cost: 0.03659489187706336\n",
      "Iteration 257, Training Cost: 0.02303897169783368, Testing Cost: 0.0366181668073284\n",
      "Iteration 258, Training Cost: 0.02303254791244517, Testing Cost: 0.03664056876979439\n",
      "Iteration 259, Training Cost: 0.023026128328503755, Testing Cost: 0.03665846077088529\n",
      "Iteration 260, Training Cost: 0.0230197105490746, Testing Cost: 0.03667834783439779\n",
      "Iteration 261, Training Cost: 0.023013312697563612, Testing Cost: 0.03669896140432064\n",
      "Iteration 262, Training Cost: 0.02300688222935668, Testing Cost: 0.03671530824335367\n",
      "Iteration 263, Training Cost: 0.023000501941142636, Testing Cost: 0.03673087478456372\n",
      "Iteration 264, Training Cost: 0.022994091423762977, Testing Cost: 0.036748925790131386\n",
      "Iteration 265, Training Cost: 0.0229877126494018, Testing Cost: 0.036770520661857514\n",
      "Iteration 266, Training Cost: 0.022981340864099975, Testing Cost: 0.03679124835862482\n",
      "Iteration 267, Training Cost: 0.022974962157544073, Testing Cost: 0.036810637179015886\n",
      "Iteration 268, Training Cost: 0.022968598036048664, Testing Cost: 0.03683219429217811\n",
      "Iteration 269, Training Cost: 0.022962223031756844, Testing Cost: 0.03684906256420408\n",
      "Iteration 270, Training Cost: 0.022955862730257362, Testing Cost: 0.03686904704398204\n",
      "Iteration 271, Training Cost: 0.022949514125609725, Testing Cost: 0.03688972111807117\n",
      "Iteration 272, Training Cost: 0.02294315503864723, Testing Cost: 0.03691200027738467\n",
      "Iteration 273, Training Cost: 0.02293679863271372, Testing Cost: 0.036931021518375524\n",
      "Iteration 274, Training Cost: 0.02293041850501503, Testing Cost: 0.03695497396608042\n",
      "Iteration 275, Training Cost: 0.022924042517003522, Testing Cost: 0.03697419400050026\n",
      "Iteration 276, Training Cost: 0.022917708505840694, Testing Cost: 0.036994979186677525\n",
      "Iteration 277, Training Cost: 0.02291136184632453, Testing Cost: 0.037018199572205514\n",
      "Iteration 278, Training Cost: 0.022905009732674918, Testing Cost: 0.03703656397483335\n",
      "Iteration 279, Training Cost: 0.022898667163616018, Testing Cost: 0.03705667156162495\n",
      "Iteration 280, Training Cost: 0.02289232706494818, Testing Cost: 0.037077281564711\n",
      "Iteration 281, Training Cost: 0.022885982328480107, Testing Cost: 0.03709961430035946\n",
      "Iteration 282, Training Cost: 0.02287966331856634, Testing Cost: 0.03711948931735033\n",
      "Iteration 283, Training Cost: 0.022873332024968083, Testing Cost: 0.03714114757846345\n",
      "Iteration 284, Training Cost: 0.022866973485565583, Testing Cost: 0.037157098692189604\n",
      "Iteration 285, Training Cost: 0.022860609638522277, Testing Cost: 0.03717712112930951\n",
      "Iteration 286, Training Cost: 0.022854267451713938, Testing Cost: 0.0371973821203062\n",
      "Iteration 287, Training Cost: 0.022847933478344525, Testing Cost: 0.037216951149050916\n",
      "Iteration 288, Training Cost: 0.022841584768781523, Testing Cost: 0.037241486666419285\n",
      "Iteration 289, Training Cost: 0.02283527675766581, Testing Cost: 0.03726540263030424\n",
      "Iteration 290, Training Cost: 0.022828929652571054, Testing Cost: 0.037285661664259784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 291, Training Cost: 0.02282259845171054, Testing Cost: 0.03730607051182948\n",
      "Iteration 292, Training Cost: 0.022816239947185638, Testing Cost: 0.037327704028706964\n",
      "Iteration 293, Training Cost: 0.02280987359351212, Testing Cost: 0.03734870782038591\n",
      "Iteration 294, Training Cost: 0.02280350452912171, Testing Cost: 0.03737166024565443\n",
      "Iteration 295, Training Cost: 0.022797167174250902, Testing Cost: 0.037395008573549306\n",
      "Iteration 296, Training Cost: 0.02279078565817117, Testing Cost: 0.037418199766962146\n",
      "Iteration 297, Training Cost: 0.02278440465875423, Testing Cost: 0.03743949548388445\n",
      "Iteration 298, Training Cost: 0.022778057877608743, Testing Cost: 0.037461013165329037\n",
      "Iteration 299, Training Cost: 0.02277170257746406, Testing Cost: 0.03748212705942996\n",
      "Iteration 300, Training Cost: 0.022765349942801046, Testing Cost: 0.0375045958125206\n",
      "Iteration 301, Training Cost: 0.02275898520319203, Testing Cost: 0.03752762533251481\n",
      "Iteration 302, Training Cost: 0.022752608223834486, Testing Cost: 0.037545615852063514\n",
      "Iteration 303, Training Cost: 0.022746231562700996, Testing Cost: 0.037567146511711315\n",
      "Iteration 304, Training Cost: 0.022739864600135617, Testing Cost: 0.03758820005508249\n",
      "Iteration 305, Training Cost: 0.022733503968862166, Testing Cost: 0.03761171279882133\n",
      "Iteration 306, Training Cost: 0.022727126865942283, Testing Cost: 0.037635114993984586\n",
      "Iteration 307, Training Cost: 0.02272074296085601, Testing Cost: 0.037657934857569485\n",
      "Iteration 308, Training Cost: 0.022714362408763778, Testing Cost: 0.0376783962579883\n",
      "Iteration 309, Training Cost: 0.022707962653712943, Testing Cost: 0.03769714950206617\n",
      "Iteration 310, Training Cost: 0.022701589102425344, Testing Cost: 0.03772388727261398\n",
      "Iteration 311, Training Cost: 0.022695179646113308, Testing Cost: 0.03774380614393372\n",
      "Iteration 312, Training Cost: 0.02268878471218883, Testing Cost: 0.03776751069991817\n",
      "Iteration 313, Training Cost: 0.02268235526594224, Testing Cost: 0.03779123905364092\n",
      "Iteration 314, Training Cost: 0.022675975483071046, Testing Cost: 0.037815521069119375\n",
      "Iteration 315, Training Cost: 0.022669561131979626, Testing Cost: 0.037839766063848916\n",
      "Iteration 316, Training Cost: 0.022663119882117423, Testing Cost: 0.03786351562147152\n",
      "Iteration 317, Training Cost: 0.022656694155943884, Testing Cost: 0.03788640632957138\n",
      "Iteration 318, Training Cost: 0.02265023563725115, Testing Cost: 0.03791097520872251\n",
      "Iteration 319, Training Cost: 0.022643784736905467, Testing Cost: 0.03793334431682504\n",
      "Iteration 320, Training Cost: 0.02263735115507804, Testing Cost: 0.03795307866377569\n",
      "Iteration 321, Training Cost: 0.02263088204106724, Testing Cost: 0.03797359456111136\n",
      "Iteration 322, Training Cost: 0.022624466071976948, Testing Cost: 0.03800072269123213\n",
      "Iteration 323, Training Cost: 0.022617988095951697, Testing Cost: 0.038019266656434826\n",
      "Iteration 324, Training Cost: 0.022611523236285996, Testing Cost: 0.03804075820923869\n",
      "Iteration 325, Training Cost: 0.02260504158759576, Testing Cost: 0.03806038258421462\n",
      "Iteration 326, Training Cost: 0.022598589102201076, Testing Cost: 0.03808540038751652\n",
      "Iteration 327, Training Cost: 0.022592090321320826, Testing Cost: 0.03810585136054177\n",
      "Iteration 328, Training Cost: 0.02258560575539078, Testing Cost: 0.03812986785986854\n",
      "Iteration 329, Training Cost: 0.022579132826128105, Testing Cost: 0.03814831016707392\n",
      "Iteration 330, Training Cost: 0.022572652579228032, Testing Cost: 0.03817062411191229\n",
      "Iteration 331, Training Cost: 0.022566161518358222, Testing Cost: 0.038196493713389804\n",
      "Iteration 332, Training Cost: 0.022559660450626427, Testing Cost: 0.03822042470897805\n",
      "Iteration 333, Training Cost: 0.022553157195853116, Testing Cost: 0.03824333149670763\n",
      "Iteration 334, Training Cost: 0.02254665391851385, Testing Cost: 0.03826993476979141\n",
      "Iteration 335, Training Cost: 0.022540168275621372, Testing Cost: 0.038291047359140656\n",
      "Iteration 336, Training Cost: 0.022533634095816622, Testing Cost: 0.0383165398141156\n",
      "Iteration 337, Training Cost: 0.02252711044586305, Testing Cost: 0.0383404105173907\n",
      "Iteration 338, Training Cost: 0.02252060185126735, Testing Cost: 0.038362274300063365\n",
      "Iteration 339, Training Cost: 0.02251406439734527, Testing Cost: 0.038388488076897424\n",
      "Iteration 340, Training Cost: 0.022507532386024872, Testing Cost: 0.03841259122827814\n",
      "Iteration 341, Training Cost: 0.02250096467637939, Testing Cost: 0.03843270209730533\n",
      "Iteration 342, Training Cost: 0.022494441183948354, Testing Cost: 0.03845517041756187\n",
      "Iteration 343, Training Cost: 0.022487880457810864, Testing Cost: 0.03847798301697196\n",
      "Iteration 344, Training Cost: 0.02248133681316685, Testing Cost: 0.038498928665780925\n",
      "Iteration 345, Training Cost: 0.022474776178933103, Testing Cost: 0.03852330704715883\n",
      "Iteration 346, Training Cost: 0.022468204563930692, Testing Cost: 0.03854622633836663\n",
      "Iteration 347, Training Cost: 0.022461653481747706, Testing Cost: 0.038572903278293776\n",
      "Iteration 348, Training Cost: 0.02245510324929266, Testing Cost: 0.038599923618017805\n",
      "Iteration 349, Training Cost: 0.02244851401030554, Testing Cost: 0.03862175293015249\n",
      "Iteration 350, Training Cost: 0.022441921719664817, Testing Cost: 0.038645156230660914\n",
      "Iteration 351, Training Cost: 0.02243532687925769, Testing Cost: 0.0386660460582112\n",
      "Iteration 352, Training Cost: 0.022428725394893594, Testing Cost: 0.03868801158097868\n",
      "Iteration 353, Training Cost: 0.022422141157316385, Testing Cost: 0.03871131953777644\n",
      "Iteration 354, Training Cost: 0.02241555662910788, Testing Cost: 0.03873384586740658\n",
      "Iteration 355, Training Cost: 0.022408981597219668, Testing Cost: 0.038757630977607996\n",
      "Iteration 356, Training Cost: 0.0224024175567256, Testing Cost: 0.0387825930075905\n",
      "Iteration 357, Training Cost: 0.022395826767028323, Testing Cost: 0.038802818897169696\n",
      "Iteration 358, Training Cost: 0.022389192867062018, Testing Cost: 0.038832126672021475\n",
      "Iteration 359, Training Cost: 0.02238260723575018, Testing Cost: 0.03885507987910491\n",
      "Iteration 360, Training Cost: 0.022376026348135642, Testing Cost: 0.038881817341926646\n",
      "Iteration 361, Training Cost: 0.022369432699180677, Testing Cost: 0.03890708504046309\n",
      "Iteration 362, Training Cost: 0.02236283617177503, Testing Cost: 0.03893222091670021\n",
      "Iteration 363, Training Cost: 0.022356250354252732, Testing Cost: 0.038956049818275315\n",
      "Iteration 364, Training Cost: 0.022349662254864136, Testing Cost: 0.03898177499617683\n",
      "Iteration 365, Training Cost: 0.022343060547639036, Testing Cost: 0.03900804206317153\n",
      "Iteration 366, Training Cost: 0.022336443924898308, Testing Cost: 0.03902854430172421\n",
      "Iteration 367, Training Cost: 0.022329832148122064, Testing Cost: 0.039049384483890004\n",
      "Iteration 368, Training Cost: 0.022323208105564128, Testing Cost: 0.03907080664008089\n",
      "Iteration 369, Training Cost: 0.022316622772507663, Testing Cost: 0.03909495811972854\n",
      "Iteration 370, Training Cost: 0.022310002358986458, Testing Cost: 0.039119989170199536\n",
      "Iteration 371, Training Cost: 0.02230337415356555, Testing Cost: 0.039143736738358276\n",
      "Iteration 372, Training Cost: 0.022296752448324367, Testing Cost: 0.03916649415486384\n",
      "Iteration 373, Training Cost: 0.022290142997001203, Testing Cost: 0.039188109279526956\n",
      "Iteration 374, Training Cost: 0.022283516587487045, Testing Cost: 0.0392134378485872\n",
      "Iteration 375, Training Cost: 0.022276907701476625, Testing Cost: 0.03923981199650808\n",
      "Iteration 376, Training Cost: 0.022270298730494457, Testing Cost: 0.039259344682038\n",
      "Iteration 377, Training Cost: 0.022263688614888148, Testing Cost: 0.03928445898376566\n",
      "Iteration 378, Training Cost: 0.02225709424195855, Testing Cost: 0.039309915429833024\n",
      "Iteration 379, Training Cost: 0.02225048400929857, Testing Cost: 0.0393326280170605\n",
      "Iteration 380, Training Cost: 0.022243866519715775, Testing Cost: 0.03935496838652076\n",
      "Iteration 381, Training Cost: 0.02223725872942332, Testing Cost: 0.03937505649552956\n",
      "Iteration 382, Training Cost: 0.022230676274902397, Testing Cost: 0.03939783173122973\n",
      "Iteration 383, Training Cost: 0.02222408940686817, Testing Cost: 0.039421486185381216\n",
      "Iteration 384, Training Cost: 0.022217494229302404, Testing Cost: 0.03944257196900535\n",
      "Iteration 385, Training Cost: 0.022210900237383705, Testing Cost: 0.039466037628707006\n",
      "Iteration 386, Training Cost: 0.02220431660678781, Testing Cost: 0.03948894406091389\n",
      "Iteration 387, Training Cost: 0.022197742235665195, Testing Cost: 0.03951374506605538\n",
      "Iteration 388, Training Cost: 0.022191140546622416, Testing Cost: 0.03953734687772946\n",
      "Iteration 389, Training Cost: 0.022184591274469572, Testing Cost: 0.039558990743417295\n",
      "Iteration 390, Training Cost: 0.022178022611932073, Testing Cost: 0.0395845128265551\n",
      "Iteration 391, Training Cost: 0.022171464870138803, Testing Cost: 0.03961323604852007\n",
      "Iteration 392, Training Cost: 0.022164934013830694, Testing Cost: 0.03963729403356521\n",
      "Iteration 393, Training Cost: 0.02215838058116011, Testing Cost: 0.03966141060216418\n",
      "Iteration 394, Training Cost: 0.022151863630277185, Testing Cost: 0.03968596334262181\n",
      "Iteration 395, Training Cost: 0.022145297342992563, Testing Cost: 0.0397091979277823\n",
      "Iteration 396, Training Cost: 0.02213877996281391, Testing Cost: 0.03973214814209447\n",
      "Iteration 397, Training Cost: 0.02213228319856576, Testing Cost: 0.039756041884507695\n",
      "Iteration 398, Training Cost: 0.02212577807118114, Testing Cost: 0.0397782244253045\n",
      "Iteration 399, Training Cost: 0.022119268814574943, Testing Cost: 0.03980264632915415\n",
      "Iteration 400, Training Cost: 0.02211278884878988, Testing Cost: 0.03982568471738034\n",
      "Iteration 401, Training Cost: 0.022106288596295293, Testing Cost: 0.0398503208813078\n",
      "Iteration 402, Training Cost: 0.022099806248099794, Testing Cost: 0.03987396762811191\n",
      "Iteration 403, Training Cost: 0.022093342048278893, Testing Cost: 0.039897646078848666\n",
      "Iteration 404, Training Cost: 0.022086900935015093, Testing Cost: 0.03992168611451285\n",
      "Iteration 405, Training Cost: 0.02208046857558846, Testing Cost: 0.039946456852102054\n",
      "Iteration 406, Training Cost: 0.022074036954469932, Testing Cost: 0.03996788013852382\n",
      "Iteration 407, Training Cost: 0.02206756679106564, Testing Cost: 0.039987477618570084\n",
      "Iteration 408, Training Cost: 0.02206114554144192, Testing Cost: 0.04001032371961829\n",
      "Iteration 409, Training Cost: 0.022054729632988115, Testing Cost: 0.040031234238759844\n",
      "Iteration 410, Training Cost: 0.02204832319472907, Testing Cost: 0.04005258184580295\n",
      "Iteration 411, Training Cost: 0.022041925740488823, Testing Cost: 0.04007257431845749\n",
      "Iteration 412, Training Cost: 0.022035542953945244, Testing Cost: 0.04009619206634792\n",
      "Iteration 413, Training Cost: 0.022029168545387678, Testing Cost: 0.040118464719707064\n",
      "Iteration 414, Training Cost: 0.02202282804032067, Testing Cost: 0.040141248662779114\n",
      "Iteration 415, Training Cost: 0.02201648898183344, Testing Cost: 0.040163779503527494\n",
      "Iteration 416, Training Cost: 0.022010163642690577, Testing Cost: 0.04018632213002623\n",
      "Iteration 417, Training Cost: 0.022003849884277202, Testing Cost: 0.04020993818963267\n",
      "Iteration 418, Training Cost: 0.021997534733046276, Testing Cost: 0.04023087751130153\n",
      "Iteration 419, Training Cost: 0.021991263286511263, Testing Cost: 0.040254770820255956\n",
      "Iteration 420, Training Cost: 0.021984977463967815, Testing Cost: 0.04027342796949803\n",
      "Iteration 421, Training Cost: 0.021978739278728582, Testing Cost: 0.04029291190216768\n",
      "Iteration 422, Training Cost: 0.021972537904319023, Testing Cost: 0.04031549380760942\n",
      "Iteration 423, Training Cost: 0.02196630198451936, Testing Cost: 0.04033869310440059\n",
      "Iteration 424, Training Cost: 0.021960115478722314, Testing Cost: 0.04035860167752159\n",
      "Iteration 425, Training Cost: 0.02195390774271409, Testing Cost: 0.04038272541767821\n",
      "Iteration 426, Training Cost: 0.02194772482567592, Testing Cost: 0.04040623334009082\n",
      "Iteration 427, Training Cost: 0.021941562281380726, Testing Cost: 0.04042581504827738\n",
      "Iteration 428, Training Cost: 0.021935415131595786, Testing Cost: 0.040446689166326696\n",
      "Iteration 429, Training Cost: 0.021929311568401923, Testing Cost: 0.04046831466793938\n",
      "Iteration 430, Training Cost: 0.021923208514515218, Testing Cost: 0.04048852851667045\n",
      "Iteration 431, Training Cost: 0.021917109708276823, Testing Cost: 0.040511615773918985\n",
      "Iteration 432, Training Cost: 0.021911040204441883, Testing Cost: 0.04053086993125984\n",
      "Iteration 433, Training Cost: 0.021904939512427227, Testing Cost: 0.04055126841131233\n",
      "Iteration 434, Training Cost: 0.021898876413025497, Testing Cost: 0.04057199523608046\n",
      "Iteration 435, Training Cost: 0.021892870815705415, Testing Cost: 0.04059319860598187\n",
      "Iteration 436, Training Cost: 0.021886849388782933, Testing Cost: 0.040613938801773455\n",
      "Iteration 437, Training Cost: 0.021880891307392507, Testing Cost: 0.04063518017416796\n",
      "Iteration 438, Training Cost: 0.02187493219967254, Testing Cost: 0.04065450941063154\n",
      "Iteration 439, Training Cost: 0.02186896633876882, Testing Cost: 0.040677074020220895\n",
      "Iteration 440, Training Cost: 0.02186300088621333, Testing Cost: 0.040697841164907086\n",
      "Iteration 441, Training Cost: 0.021857063971229357, Testing Cost: 0.04071787348853508\n",
      "Iteration 442, Training Cost: 0.021851139646650197, Testing Cost: 0.04073743067137314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 443, Training Cost: 0.0218452224428906, Testing Cost: 0.04075694323235038\n",
      "Iteration 444, Training Cost: 0.0218393363065593, Testing Cost: 0.04077788734914958\n",
      "Iteration 445, Training Cost: 0.02183348682352908, Testing Cost: 0.0407966216159941\n",
      "Iteration 446, Training Cost: 0.021827638845108115, Testing Cost: 0.040814898390698975\n",
      "Iteration 447, Training Cost: 0.021821816224717766, Testing Cost: 0.040835238457712066\n",
      "Iteration 448, Training Cost: 0.021816007076154692, Testing Cost: 0.04085538858955362\n",
      "Iteration 449, Training Cost: 0.021810253199730213, Testing Cost: 0.04087682174339121\n",
      "Iteration 450, Training Cost: 0.021804508120822984, Testing Cost: 0.04089831286093782\n",
      "Iteration 451, Training Cost: 0.02179878460463657, Testing Cost: 0.040920419232824884\n",
      "Iteration 452, Training Cost: 0.021793062384353398, Testing Cost: 0.04094091410761861\n",
      "Iteration 453, Training Cost: 0.021787377698816824, Testing Cost: 0.040961854176238495\n",
      "Iteration 454, Training Cost: 0.02178167663616268, Testing Cost: 0.04098192856159262\n",
      "Iteration 455, Training Cost: 0.021776031214601123, Testing Cost: 0.04100125613839132\n",
      "Iteration 456, Training Cost: 0.021770414582058344, Testing Cost: 0.04102012976591184\n",
      "Iteration 457, Training Cost: 0.021764772815647565, Testing Cost: 0.04104285197320274\n",
      "Iteration 458, Training Cost: 0.021759140815910568, Testing Cost: 0.041061524224670645\n",
      "Iteration 459, Training Cost: 0.021753590462805134, Testing Cost: 0.04107985893645478\n",
      "Iteration 460, Training Cost: 0.021748046947611753, Testing Cost: 0.04110170393155541\n",
      "Iteration 461, Training Cost: 0.021742504205960994, Testing Cost: 0.0411203055493465\n",
      "Iteration 462, Training Cost: 0.021736985391658227, Testing Cost: 0.04114035632852371\n",
      "Iteration 463, Training Cost: 0.021731475399237726, Testing Cost: 0.04115920767582341\n",
      "Iteration 464, Training Cost: 0.021725960882821232, Testing Cost: 0.041176083579817016\n",
      "Iteration 465, Training Cost: 0.021720461201283202, Testing Cost: 0.041195179141162996\n",
      "Iteration 466, Training Cost: 0.021715029171598545, Testing Cost: 0.041214079899870144\n",
      "Iteration 467, Training Cost: 0.02170962375071293, Testing Cost: 0.041232517843409235\n",
      "Iteration 468, Training Cost: 0.021704200952588095, Testing Cost: 0.0412483712811125\n",
      "Iteration 469, Training Cost: 0.02169881286259384, Testing Cost: 0.041268985672115434\n",
      "Iteration 470, Training Cost: 0.021693436893029144, Testing Cost: 0.041287371695915884\n",
      "Iteration 471, Training Cost: 0.021688067691009318, Testing Cost: 0.04130828195812942\n",
      "Iteration 472, Training Cost: 0.02168275510838376, Testing Cost: 0.04132862886206679\n",
      "Iteration 473, Training Cost: 0.02167742854259577, Testing Cost: 0.04134509767184809\n",
      "Iteration 474, Training Cost: 0.02167214069146572, Testing Cost: 0.041360446254732515\n",
      "Iteration 475, Training Cost: 0.021666876106822094, Testing Cost: 0.04138101168733881\n",
      "Iteration 476, Training Cost: 0.02166164007835907, Testing Cost: 0.04139814615780255\n",
      "Iteration 477, Training Cost: 0.021656421270249512, Testing Cost: 0.04141686454577678\n",
      "Iteration 478, Training Cost: 0.021651213218044866, Testing Cost: 0.04143218876494359\n",
      "Iteration 479, Training Cost: 0.021646015981616272, Testing Cost: 0.04145091345329957\n",
      "Iteration 480, Training Cost: 0.02164083158709425, Testing Cost: 0.041466224915923576\n",
      "Iteration 481, Training Cost: 0.021635699314263325, Testing Cost: 0.04148226321177188\n",
      "Iteration 482, Training Cost: 0.021630570952365737, Testing Cost: 0.04150202347667696\n",
      "Iteration 483, Training Cost: 0.021625448105047136, Testing Cost: 0.0415191368357329\n",
      "Iteration 484, Training Cost: 0.02162036420288327, Testing Cost: 0.041538116970970464\n",
      "Iteration 485, Training Cost: 0.021615265564294193, Testing Cost: 0.04155463398295338\n",
      "Iteration 486, Training Cost: 0.021610204413100395, Testing Cost: 0.0415729611550106\n",
      "Iteration 487, Training Cost: 0.02160518890828891, Testing Cost: 0.04158718136345844\n",
      "Iteration 488, Training Cost: 0.021600143429870974, Testing Cost: 0.0416036343838485\n",
      "Iteration 489, Training Cost: 0.021595117986566322, Testing Cost: 0.04162057720173581\n",
      "Iteration 490, Training Cost: 0.02159014724215746, Testing Cost: 0.04163949307277655\n",
      "Iteration 491, Training Cost: 0.02158519125694442, Testing Cost: 0.04165510496511305\n",
      "Iteration 492, Training Cost: 0.021580241860059336, Testing Cost: 0.04167008793579617\n",
      "Iteration 493, Training Cost: 0.021575321253119006, Testing Cost: 0.04168623916562156\n",
      "Iteration 494, Training Cost: 0.021570418147316917, Testing Cost: 0.04169974076013156\n",
      "Iteration 495, Training Cost: 0.021565519923187604, Testing Cost: 0.04171673588161301\n",
      "Iteration 496, Training Cost: 0.021560614534857296, Testing Cost: 0.041735381956227634\n",
      "Iteration 497, Training Cost: 0.02155579426289438, Testing Cost: 0.04174816895917177\n",
      "Iteration 498, Training Cost: 0.021550941943405785, Testing Cost: 0.0417650532036424\n",
      "Iteration 499, Training Cost: 0.021546106178529346, Testing Cost: 0.04178109358308215\n",
      "Iteration 500, Training Cost: 0.02154126836417437, Testing Cost: 0.04179973500382749\n",
      "Iteration 501, Training Cost: 0.021536484922529364, Testing Cost: 0.04181646570012376\n",
      "Iteration 502, Training Cost: 0.02153173943285927, Testing Cost: 0.04183109884685404\n",
      "Iteration 503, Training Cost: 0.02152698027121576, Testing Cost: 0.04185002282690516\n",
      "Iteration 504, Training Cost: 0.02152223740226689, Testing Cost: 0.0418649458171874\n",
      "Iteration 505, Training Cost: 0.02151752864854294, Testing Cost: 0.04188206847242188\n",
      "Iteration 506, Training Cost: 0.021512815970592982, Testing Cost: 0.041897191133480685\n",
      "Iteration 507, Training Cost: 0.021508131958807578, Testing Cost: 0.041911198110996195\n",
      "Iteration 508, Training Cost: 0.02150342462005297, Testing Cost: 0.041924515500933054\n",
      "Iteration 509, Training Cost: 0.021498773768740858, Testing Cost: 0.04193936097575424\n",
      "Iteration 510, Training Cost: 0.02149411785621045, Testing Cost: 0.04195526222636856\n",
      "Iteration 511, Training Cost: 0.02148950333493802, Testing Cost: 0.041969300179300194\n",
      "Iteration 512, Training Cost: 0.021484900619638377, Testing Cost: 0.04198317329355202\n",
      "Iteration 513, Training Cost: 0.021480325502954718, Testing Cost: 0.04199741664634139\n",
      "Iteration 514, Training Cost: 0.021475770704716683, Testing Cost: 0.04201211459455269\n",
      "Iteration 515, Training Cost: 0.021471241224298145, Testing Cost: 0.042027328486392734\n",
      "Iteration 516, Training Cost: 0.0214667153396182, Testing Cost: 0.04203860301599793\n",
      "Iteration 517, Training Cost: 0.021462187870051018, Testing Cost: 0.04205323411217875\n",
      "Iteration 518, Training Cost: 0.02145766291673394, Testing Cost: 0.04206852467946009\n",
      "Iteration 519, Training Cost: 0.02145317984021374, Testing Cost: 0.04208456068163595\n",
      "Iteration 520, Training Cost: 0.021448716058737743, Testing Cost: 0.04209799933782681\n",
      "Iteration 521, Training Cost: 0.021444257175595983, Testing Cost: 0.042114314048470734\n",
      "Iteration 522, Training Cost: 0.02143982601417096, Testing Cost: 0.04212732996609966\n",
      "Iteration 523, Training Cost: 0.021435394652798572, Testing Cost: 0.04214181817071652\n",
      "Iteration 524, Training Cost: 0.021430971162736637, Testing Cost: 0.04215693086838948\n",
      "Iteration 525, Training Cost: 0.02142657208234336, Testing Cost: 0.042169105272931606\n",
      "Iteration 526, Training Cost: 0.021422179643588057, Testing Cost: 0.04218283752930217\n",
      "Iteration 527, Training Cost: 0.021417827204252417, Testing Cost: 0.042198176739471724\n",
      "Iteration 528, Training Cost: 0.021413472919900766, Testing Cost: 0.04221023540883685\n",
      "Iteration 529, Training Cost: 0.02140913284551781, Testing Cost: 0.042225577680588613\n",
      "Iteration 530, Training Cost: 0.021404806948830426, Testing Cost: 0.04224118851164833\n",
      "Iteration 531, Training Cost: 0.021400488008618084, Testing Cost: 0.042253574651644386\n",
      "Iteration 532, Training Cost: 0.0213962030754702, Testing Cost: 0.04226418332533409\n",
      "Iteration 533, Training Cost: 0.02139195856617322, Testing Cost: 0.04227529820596478\n",
      "Iteration 534, Training Cost: 0.021387703013505236, Testing Cost: 0.042286678468493565\n",
      "Iteration 535, Training Cost: 0.021383433725890532, Testing Cost: 0.04230040511255082\n",
      "Iteration 536, Training Cost: 0.021379191744800404, Testing Cost: 0.042316543123695434\n",
      "Iteration 537, Training Cost: 0.02137495317462449, Testing Cost: 0.042329870335389166\n",
      "Iteration 538, Training Cost: 0.021370732551839744, Testing Cost: 0.04234136029128399\n",
      "Iteration 539, Training Cost: 0.02136651282833088, Testing Cost: 0.042356715469131646\n",
      "Iteration 540, Training Cost: 0.02136229757018689, Testing Cost: 0.04237250052517965\n",
      "Iteration 541, Training Cost: 0.021358133633353413, Testing Cost: 0.04238586740369568\n",
      "Iteration 542, Training Cost: 0.021353966625601177, Testing Cost: 0.04239733271649445\n",
      "Iteration 543, Training Cost: 0.021349808768399082, Testing Cost: 0.042411597833153\n",
      "Iteration 544, Training Cost: 0.021345685103187573, Testing Cost: 0.0424207888335526\n",
      "Iteration 545, Training Cost: 0.02134157554121073, Testing Cost: 0.04243339925940645\n",
      "Iteration 546, Training Cost: 0.02133748752550314, Testing Cost: 0.04244279549912817\n",
      "Iteration 547, Training Cost: 0.021333388408810484, Testing Cost: 0.042455627607073214\n",
      "Iteration 548, Training Cost: 0.0213292956723525, Testing Cost: 0.04246928447577439\n",
      "Iteration 549, Training Cost: 0.021325225938708118, Testing Cost: 0.04248115321333715\n",
      "Iteration 550, Training Cost: 0.02132114731566162, Testing Cost: 0.04249580559122116\n",
      "Iteration 551, Training Cost: 0.02131707856896988, Testing Cost: 0.042507085213182645\n",
      "Iteration 552, Training Cost: 0.021313040708497877, Testing Cost: 0.0425198015047362\n",
      "Iteration 553, Training Cost: 0.021309020419128227, Testing Cost: 0.042533235422389494\n",
      "Iteration 554, Training Cost: 0.021305019440717065, Testing Cost: 0.042544700168164064\n",
      "Iteration 555, Training Cost: 0.02130103857627361, Testing Cost: 0.04255445932554547\n",
      "Iteration 556, Training Cost: 0.021297034359144904, Testing Cost: 0.042567683296976244\n",
      "Iteration 557, Training Cost: 0.021293057784040285, Testing Cost: 0.042580082059107446\n",
      "Iteration 558, Training Cost: 0.02128910280101417, Testing Cost: 0.04259187191197317\n",
      "Iteration 559, Training Cost: 0.021285160194424505, Testing Cost: 0.04260193885903463\n",
      "Iteration 560, Training Cost: 0.021281186451886945, Testing Cost: 0.042615238565577584\n",
      "Iteration 561, Training Cost: 0.021277255066649285, Testing Cost: 0.0426245553100101\n",
      "Iteration 562, Training Cost: 0.021273344926120895, Testing Cost: 0.04263621088473249\n",
      "Iteration 563, Training Cost: 0.02126942610867984, Testing Cost: 0.04264585990861487\n",
      "Iteration 564, Training Cost: 0.02126552163098169, Testing Cost: 0.04265700738073158\n",
      "Iteration 565, Training Cost: 0.02126162031233088, Testing Cost: 0.04266824847951336\n",
      "Iteration 566, Training Cost: 0.021257750535179133, Testing Cost: 0.04268106924402935\n",
      "Iteration 567, Training Cost: 0.021253890261140333, Testing Cost: 0.04269378717902493\n",
      "Iteration 568, Training Cost: 0.021250034014258498, Testing Cost: 0.04270586658108897\n",
      "Iteration 569, Training Cost: 0.021246209120847934, Testing Cost: 0.04271666606044288\n",
      "Iteration 570, Training Cost: 0.021242378583116946, Testing Cost: 0.04272767664915749\n",
      "Iteration 571, Training Cost: 0.021238560467973993, Testing Cost: 0.04273803446357031\n",
      "Iteration 572, Training Cost: 0.02123475579645066, Testing Cost: 0.04274707621148424\n",
      "Iteration 573, Training Cost: 0.02123094826245322, Testing Cost: 0.042756878651607266\n",
      "Iteration 574, Training Cost: 0.021227169056127572, Testing Cost: 0.04277004577932406\n",
      "Iteration 575, Training Cost: 0.021223386692606566, Testing Cost: 0.04278094612331043\n",
      "Iteration 576, Training Cost: 0.021219627739275666, Testing Cost: 0.042792075594413594\n",
      "Iteration 577, Training Cost: 0.021215880301269602, Testing Cost: 0.04280209904841291\n",
      "Iteration 578, Training Cost: 0.021212129124563578, Testing Cost: 0.042811249515392835\n",
      "Iteration 579, Training Cost: 0.021208389722731924, Testing Cost: 0.04282179409670534\n",
      "Iteration 580, Training Cost: 0.0212046521742473, Testing Cost: 0.04282903728420408\n",
      "Iteration 581, Training Cost: 0.02120093113015571, Testing Cost: 0.04283923651876819\n",
      "Iteration 582, Training Cost: 0.021197240885950774, Testing Cost: 0.042850816984183755\n",
      "Iteration 583, Training Cost: 0.02119354600705291, Testing Cost: 0.042860638101039135\n",
      "Iteration 584, Training Cost: 0.02118987114092116, Testing Cost: 0.04286883146427167\n",
      "Iteration 585, Training Cost: 0.0211861864843676, Testing Cost: 0.0428797361034459\n",
      "Iteration 586, Training Cost: 0.02118252687364993, Testing Cost: 0.0428901563919398\n",
      "Iteration 587, Training Cost: 0.021178883323985143, Testing Cost: 0.04290071639558757\n",
      "Iteration 588, Training Cost: 0.021175207272138754, Testing Cost: 0.04290898299841422\n",
      "Iteration 589, Training Cost: 0.021171572208623382, Testing Cost: 0.04292023208156311\n",
      "Iteration 590, Training Cost: 0.02116795270119863, Testing Cost: 0.042931299158562225\n",
      "Iteration 591, Training Cost: 0.02116433401746973, Testing Cost: 0.04294131713968832\n",
      "Iteration 592, Training Cost: 0.021160727769475147, Testing Cost: 0.04295176058944527\n",
      "Iteration 593, Training Cost: 0.021157118005984205, Testing Cost: 0.04296054380652011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 594, Training Cost: 0.021153508990000354, Testing Cost: 0.042967715539125054\n",
      "Iteration 595, Training Cost: 0.021149941671621538, Testing Cost: 0.042978242303937035\n",
      "Iteration 596, Training Cost: 0.021146367497459066, Testing Cost: 0.04298629567545874\n",
      "Iteration 597, Training Cost: 0.021142806080102616, Testing Cost: 0.04299578227208052\n",
      "Iteration 598, Training Cost: 0.021139252247290723, Testing Cost: 0.04300482001919796\n",
      "Iteration 599, Training Cost: 0.021135701046056395, Testing Cost: 0.04301187442116718\n",
      "Iteration 600, Training Cost: 0.02113217460448031, Testing Cost: 0.043021342703579796\n",
      "Iteration 601, Training Cost: 0.02112864476637149, Testing Cost: 0.043031009472133716\n",
      "Iteration 602, Training Cost: 0.02112512792119691, Testing Cost: 0.04303863992844975\n",
      "Iteration 603, Training Cost: 0.021121628080515736, Testing Cost: 0.04304599598539644\n",
      "Iteration 604, Training Cost: 0.021118131699256852, Testing Cost: 0.04305397963268526\n",
      "Iteration 605, Training Cost: 0.021114657421105954, Testing Cost: 0.043061247251433454\n",
      "Iteration 606, Training Cost: 0.021111192668324154, Testing Cost: 0.04306806491431785\n",
      "Iteration 607, Training Cost: 0.021107723256498265, Testing Cost: 0.04307894080886378\n",
      "Iteration 608, Training Cost: 0.02110426911095014, Testing Cost: 0.04308659275664691\n",
      "Iteration 609, Training Cost: 0.02110081043334207, Testing Cost: 0.04309663882124216\n",
      "Iteration 610, Training Cost: 0.021097374395618235, Testing Cost: 0.043105459941123074\n",
      "Iteration 611, Training Cost: 0.021093933799525177, Testing Cost: 0.04311519719274207\n",
      "Iteration 612, Training Cost: 0.021090511827548435, Testing Cost: 0.04312289938218494\n",
      "Iteration 613, Training Cost: 0.021087102646605346, Testing Cost: 0.04312848306138506\n",
      "Iteration 614, Training Cost: 0.021083686232498046, Testing Cost: 0.043137638741436495\n",
      "Iteration 615, Training Cost: 0.02108029362910167, Testing Cost: 0.043145667696056704\n",
      "Iteration 616, Training Cost: 0.021076881596751956, Testing Cost: 0.04315495140237491\n",
      "Iteration 617, Training Cost: 0.02107351004417395, Testing Cost: 0.04316389086881669\n",
      "Iteration 618, Training Cost: 0.02107011608930134, Testing Cost: 0.04317284701094176\n",
      "Iteration 619, Training Cost: 0.02106675300875393, Testing Cost: 0.04318166070553753\n",
      "Iteration 620, Training Cost: 0.021063387861643704, Testing Cost: 0.04318880482448435\n",
      "Iteration 621, Training Cost: 0.021060032671452386, Testing Cost: 0.04319749916652593\n",
      "Iteration 622, Training Cost: 0.021056704100685643, Testing Cost: 0.043205802943917716\n",
      "Iteration 623, Training Cost: 0.021053364460060956, Testing Cost: 0.043212936148572634\n",
      "Iteration 624, Training Cost: 0.0210500425123893, Testing Cost: 0.043221064583583636\n",
      "Iteration 625, Training Cost: 0.021046744331998112, Testing Cost: 0.0432271525902837\n",
      "Iteration 626, Training Cost: 0.02104342960577822, Testing Cost: 0.043235114486781606\n",
      "Iteration 627, Training Cost: 0.021040120236837382, Testing Cost: 0.0432435705493593\n",
      "Iteration 628, Training Cost: 0.021036821663323838, Testing Cost: 0.04325261448020642\n",
      "Iteration 629, Training Cost: 0.021033532139559427, Testing Cost: 0.04325902745380498\n",
      "Iteration 630, Training Cost: 0.021030253420699675, Testing Cost: 0.04326732117499918\n",
      "Iteration 631, Training Cost: 0.021026976364611573, Testing Cost: 0.04327616430585559\n",
      "Iteration 632, Training Cost: 0.021023695776568412, Testing Cost: 0.043284816841664873\n",
      "Iteration 633, Training Cost: 0.02102043997202625, Testing Cost: 0.04329352321202735\n",
      "Iteration 634, Training Cost: 0.02101720641262048, Testing Cost: 0.04330109399576907\n",
      "Iteration 635, Training Cost: 0.021013959303515992, Testing Cost: 0.04330877583672624\n",
      "Iteration 636, Training Cost: 0.02101072813620324, Testing Cost: 0.04331539198621078\n",
      "Iteration 637, Training Cost: 0.02100749394829662, Testing Cost: 0.043323968155822935\n",
      "Iteration 638, Training Cost: 0.021004277231313803, Testing Cost: 0.0433325098549973\n",
      "Iteration 639, Training Cost: 0.021001065599889124, Testing Cost: 0.043339978945397054\n",
      "Iteration 640, Training Cost: 0.020997864265319986, Testing Cost: 0.043348102037366984\n",
      "Iteration 641, Training Cost: 0.02099466494163626, Testing Cost: 0.04335670171224111\n",
      "Iteration 642, Training Cost: 0.020991480143168758, Testing Cost: 0.04336499444734657\n",
      "Iteration 643, Training Cost: 0.020988310457834464, Testing Cost: 0.043373537138459275\n",
      "Iteration 644, Training Cost: 0.020985134057492014, Testing Cost: 0.043381823563818506\n",
      "Iteration 645, Training Cost: 0.020981954749318736, Testing Cost: 0.04338795279442828\n",
      "Iteration 646, Training Cost: 0.02097879019508698, Testing Cost: 0.04339434622843667\n",
      "Iteration 647, Training Cost: 0.020975625228706683, Testing Cost: 0.04340091447673805\n",
      "Iteration 648, Training Cost: 0.020972482070021614, Testing Cost: 0.04340775445898348\n",
      "Iteration 649, Training Cost: 0.02096934546539608, Testing Cost: 0.04341421194858645\n",
      "Iteration 650, Training Cost: 0.020966214324439684, Testing Cost: 0.04342106411507804\n",
      "Iteration 651, Training Cost: 0.020963091406491142, Testing Cost: 0.04342794212674184\n",
      "Iteration 652, Training Cost: 0.020959976957111423, Testing Cost: 0.04343419407252215\n",
      "Iteration 653, Training Cost: 0.02095686731778501, Testing Cost: 0.04344305556820436\n",
      "Iteration 654, Training Cost: 0.020953759694233146, Testing Cost: 0.04345175600566963\n",
      "Iteration 655, Training Cost: 0.020950663445595374, Testing Cost: 0.04345817985360423\n",
      "Iteration 656, Training Cost: 0.020947580116759217, Testing Cost: 0.043464413922507834\n",
      "Iteration 657, Training Cost: 0.02094450120515062, Testing Cost: 0.043471886384008535\n",
      "Iteration 658, Training Cost: 0.020941428681194463, Testing Cost: 0.04347678096353344\n",
      "Iteration 659, Training Cost: 0.02093836204123012, Testing Cost: 0.04348382202098257\n",
      "Iteration 660, Training Cost: 0.020935299070090934, Testing Cost: 0.043491174424524826\n",
      "Iteration 661, Training Cost: 0.020932240950952672, Testing Cost: 0.0434979192833766\n",
      "Iteration 662, Training Cost: 0.020929190241964157, Testing Cost: 0.0435056023050986\n",
      "Iteration 663, Training Cost: 0.020926151002994046, Testing Cost: 0.04351197504662304\n",
      "Iteration 664, Training Cost: 0.02092311926696895, Testing Cost: 0.04351947945340914\n",
      "Iteration 665, Training Cost: 0.020920095068248466, Testing Cost: 0.043527006154618304\n",
      "Iteration 666, Training Cost: 0.0209170755992852, Testing Cost: 0.04353442315322346\n",
      "Iteration 667, Training Cost: 0.020914051237728326, Testing Cost: 0.0435415368519638\n",
      "Iteration 668, Training Cost: 0.020911039631960966, Testing Cost: 0.04354860417679912\n",
      "Iteration 669, Training Cost: 0.020908038859164663, Testing Cost: 0.04355422528373531\n",
      "Iteration 670, Training Cost: 0.020905041565878957, Testing Cost: 0.04355924855000343\n",
      "Iteration 671, Training Cost: 0.02090204970726653, Testing Cost: 0.04356788248394598\n",
      "Iteration 672, Training Cost: 0.020899062113922437, Testing Cost: 0.04357331290158862\n",
      "Iteration 673, Training Cost: 0.020896094566967525, Testing Cost: 0.043579576209117475\n",
      "Iteration 674, Training Cost: 0.02089313374987999, Testing Cost: 0.043583874687880185\n",
      "Iteration 675, Training Cost: 0.02089017442575911, Testing Cost: 0.04359135285165533\n",
      "Iteration 676, Training Cost: 0.02088721838663587, Testing Cost: 0.04359943595912712\n",
      "Iteration 677, Training Cost: 0.020884265288249295, Testing Cost: 0.04360439559851458\n",
      "Iteration 678, Training Cost: 0.02088132495691777, Testing Cost: 0.04361114234092383\n",
      "Iteration 679, Training Cost: 0.020878381807301748, Testing Cost: 0.043617831458333596\n",
      "Iteration 680, Training Cost: 0.020875456851702483, Testing Cost: 0.04362508710039051\n",
      "Iteration 681, Training Cost: 0.0208725278856809, Testing Cost: 0.04363071872798126\n",
      "Iteration 682, Training Cost: 0.020869606959466387, Testing Cost: 0.043637374447699134\n",
      "Iteration 683, Training Cost: 0.020866700231890373, Testing Cost: 0.04364358195967825\n",
      "Iteration 684, Training Cost: 0.020863797432444573, Testing Cost: 0.04365051480092086\n",
      "Iteration 685, Training Cost: 0.020860901572490154, Testing Cost: 0.04365669627549761\n",
      "Iteration 686, Training Cost: 0.02085801192679655, Testing Cost: 0.04366154520114694\n",
      "Iteration 687, Training Cost: 0.020855114092507787, Testing Cost: 0.043667138370994125\n",
      "Iteration 688, Training Cost: 0.020852229985190436, Testing Cost: 0.04367465084878893\n",
      "Iteration 689, Training Cost: 0.020849356505769738, Testing Cost: 0.04368169216135972\n",
      "Iteration 690, Training Cost: 0.020846484279454996, Testing Cost: 0.04368632886853992\n",
      "Iteration 691, Training Cost: 0.020843607941668438, Testing Cost: 0.0436916307921972\n",
      "Iteration 692, Training Cost: 0.02084075102709495, Testing Cost: 0.04369635746333441\n",
      "Iteration 693, Training Cost: 0.02083789187010739, Testing Cost: 0.043700995137983985\n",
      "Iteration 694, Training Cost: 0.020835044382830903, Testing Cost: 0.0437063515003482\n",
      "Iteration 695, Training Cost: 0.020832206089064498, Testing Cost: 0.04371213346907788\n",
      "Iteration 696, Training Cost: 0.02082936515839487, Testing Cost: 0.043717856432933515\n",
      "Iteration 697, Training Cost: 0.02082653216226555, Testing Cost: 0.043725095890125\n",
      "Iteration 698, Training Cost: 0.020823713331131175, Testing Cost: 0.04372988827188262\n",
      "Iteration 699, Training Cost: 0.020820894428474245, Testing Cost: 0.043735439741926625\n",
      "Iteration 700, Training Cost: 0.020818084467269373, Testing Cost: 0.043741572036347424\n",
      "Iteration 701, Training Cost: 0.020815279193324484, Testing Cost: 0.04374683535319643\n",
      "Iteration 702, Training Cost: 0.02081247525242402, Testing Cost: 0.043752706745371545\n",
      "Iteration 703, Training Cost: 0.020809680530955887, Testing Cost: 0.043760234482369445\n",
      "Iteration 704, Training Cost: 0.020806896252230297, Testing Cost: 0.0437669692092619\n",
      "Iteration 705, Training Cost: 0.020804113850610626, Testing Cost: 0.043772429197902564\n",
      "Iteration 706, Training Cost: 0.020801337247919747, Testing Cost: 0.043778195078781224\n",
      "Iteration 707, Training Cost: 0.020798572556584825, Testing Cost: 0.04378535534852486\n",
      "Iteration 708, Training Cost: 0.020795798109803313, Testing Cost: 0.04378986501616819\n",
      "Iteration 709, Training Cost: 0.020793038053192645, Testing Cost: 0.04379483567681957\n",
      "Iteration 710, Training Cost: 0.020790278482350398, Testing Cost: 0.04379879276237515\n",
      "Iteration 711, Training Cost: 0.02078751653192427, Testing Cost: 0.04380277667193511\n",
      "Iteration 712, Training Cost: 0.020784776227009666, Testing Cost: 0.04380654826199373\n",
      "Iteration 713, Training Cost: 0.020782039678574952, Testing Cost: 0.04381272241952766\n",
      "Iteration 714, Training Cost: 0.020779299429434822, Testing Cost: 0.043819557233571255\n",
      "Iteration 715, Training Cost: 0.020776560784403153, Testing Cost: 0.04382616261751273\n",
      "Iteration 716, Training Cost: 0.02077383305145715, Testing Cost: 0.04383069012875572\n",
      "Iteration 717, Training Cost: 0.020771107613405997, Testing Cost: 0.04383646118194307\n",
      "Iteration 718, Training Cost: 0.020768394228446528, Testing Cost: 0.043841150912679074\n",
      "Iteration 719, Training Cost: 0.020765678931369054, Testing Cost: 0.043847386282691185\n",
      "Iteration 720, Training Cost: 0.02076299252493684, Testing Cost: 0.043852059654758434\n",
      "Iteration 721, Training Cost: 0.020760292229579883, Testing Cost: 0.043858588448894\n",
      "Iteration 722, Training Cost: 0.020757592540428884, Testing Cost: 0.0438644077116022\n",
      "Iteration 723, Training Cost: 0.020754908331404864, Testing Cost: 0.043871066436101974\n",
      "Iteration 724, Training Cost: 0.02075223424057501, Testing Cost: 0.04387548284438441\n",
      "Iteration 725, Training Cost: 0.020749564621281708, Testing Cost: 0.04388225373632298\n",
      "Iteration 726, Training Cost: 0.020746892884266833, Testing Cost: 0.043888100786492384\n",
      "Iteration 727, Training Cost: 0.02074422184207309, Testing Cost: 0.04389217546733284\n",
      "Iteration 728, Training Cost: 0.020741565923092033, Testing Cost: 0.04389829159519331\n",
      "Iteration 729, Training Cost: 0.020738911979943327, Testing Cost: 0.043905124238778896\n",
      "Iteration 730, Training Cost: 0.020736254014448695, Testing Cost: 0.04390884240991713\n",
      "Iteration 731, Training Cost: 0.020733600483781343, Testing Cost: 0.04391362594730111\n",
      "Iteration 732, Training Cost: 0.0207309620659795, Testing Cost: 0.043920502368185566\n",
      "Iteration 733, Training Cost: 0.020728328179780306, Testing Cost: 0.04392690535616955\n",
      "Iteration 734, Training Cost: 0.020725686916462403, Testing Cost: 0.04393155952938821\n",
      "Iteration 735, Training Cost: 0.020723063160467705, Testing Cost: 0.04393820537790874\n",
      "Iteration 736, Training Cost: 0.020720434524706233, Testing Cost: 0.043943258893598215\n",
      "Iteration 737, Training Cost: 0.020717799992125467, Testing Cost: 0.04394669375869022\n",
      "Iteration 738, Training Cost: 0.020715189699874056, Testing Cost: 0.04395138921293532\n",
      "Iteration 739, Training Cost: 0.0207125779490267, Testing Cost: 0.04395708552028654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 740, Training Cost: 0.020709985552130074, Testing Cost: 0.04396352376463123\n",
      "Iteration 741, Training Cost: 0.02070737510129659, Testing Cost: 0.04396741041195921\n",
      "Iteration 742, Training Cost: 0.020704785167623207, Testing Cost: 0.043973060451173225\n",
      "Iteration 743, Training Cost: 0.020702192336588203, Testing Cost: 0.0439765332936962\n",
      "Iteration 744, Training Cost: 0.02069960418120966, Testing Cost: 0.04398166661433204\n",
      "Iteration 745, Training Cost: 0.020697020319433195, Testing Cost: 0.04398534035631107\n",
      "Iteration 746, Training Cost: 0.02069443330010549, Testing Cost: 0.04398955328266629\n",
      "Iteration 747, Training Cost: 0.02069185842977657, Testing Cost: 0.043993379386949194\n",
      "Iteration 748, Training Cost: 0.0206892933121496, Testing Cost: 0.04399772260510863\n",
      "Iteration 749, Training Cost: 0.02068673447798039, Testing Cost: 0.04400318542922194\n",
      "Iteration 750, Training Cost: 0.020684174862103666, Testing Cost: 0.04400730098934732\n",
      "Iteration 751, Training Cost: 0.0206816168382047, Testing Cost: 0.044012540437872956\n",
      "Iteration 752, Training Cost: 0.020679064044282554, Testing Cost: 0.04401779389659283\n",
      "Iteration 753, Training Cost: 0.02067651628283414, Testing Cost: 0.044022295771599586\n",
      "Iteration 754, Training Cost: 0.0206739716593965, Testing Cost: 0.04402573345458276\n",
      "Iteration 755, Training Cost: 0.020671431642280795, Testing Cost: 0.04402985152560393\n",
      "Iteration 756, Training Cost: 0.020668904574919128, Testing Cost: 0.04403485085289341\n",
      "Iteration 757, Training Cost: 0.02066637509241299, Testing Cost: 0.044037672620830436\n",
      "Iteration 758, Training Cost: 0.02066385136028284, Testing Cost: 0.04404241931671227\n",
      "Iteration 759, Training Cost: 0.020661330307131576, Testing Cost: 0.04404660679415255\n",
      "Iteration 760, Training Cost: 0.02065882012898628, Testing Cost: 0.04405145369408839\n",
      "Iteration 761, Training Cost: 0.020656308081032713, Testing Cost: 0.04405642473455668\n",
      "Iteration 762, Training Cost: 0.020653802010605977, Testing Cost: 0.044060455749093515\n",
      "Iteration 763, Training Cost: 0.020651305973067302, Testing Cost: 0.04406493160924705\n",
      "Iteration 764, Training Cost: 0.020648808923658397, Testing Cost: 0.04406906798964629\n",
      "Iteration 765, Training Cost: 0.020646316145941118, Testing Cost: 0.04407399971925507\n",
      "Iteration 766, Training Cost: 0.020643817636474347, Testing Cost: 0.044078259272353496\n",
      "Iteration 767, Training Cost: 0.020641328002361473, Testing Cost: 0.04408229984448542\n",
      "Iteration 768, Training Cost: 0.02063884539668457, Testing Cost: 0.04408771465822292\n",
      "Iteration 769, Training Cost: 0.02063636400207279, Testing Cost: 0.04409180820935806\n",
      "Iteration 770, Training Cost: 0.02063388731212743, Testing Cost: 0.04409716148059973\n",
      "Iteration 771, Training Cost: 0.020631419033875612, Testing Cost: 0.04410157362024192\n",
      "Iteration 772, Training Cost: 0.02062895140169242, Testing Cost: 0.04410494346320337\n",
      "Iteration 773, Training Cost: 0.020626492012629123, Testing Cost: 0.044107530940004275\n",
      "Iteration 774, Training Cost: 0.02062403348390231, Testing Cost: 0.044111392894106705\n",
      "Iteration 775, Training Cost: 0.020621574167509798, Testing Cost: 0.04411618120813695\n",
      "Iteration 776, Training Cost: 0.02061913399269, Testing Cost: 0.04411892784009159\n",
      "Iteration 777, Training Cost: 0.020616680525327075, Testing Cost: 0.044124055560012136\n",
      "Iteration 778, Training Cost: 0.02061423787093823, Testing Cost: 0.04412831939649762\n",
      "Iteration 779, Training Cost: 0.020611796313957726, Testing Cost: 0.04413357259360953\n",
      "Iteration 780, Training Cost: 0.020609363442902522, Testing Cost: 0.04413844868632158\n",
      "Iteration 781, Training Cost: 0.020606931548726445, Testing Cost: 0.04414216085858251\n",
      "Iteration 782, Training Cost: 0.020604511442421362, Testing Cost: 0.04414689964242913\n",
      "Iteration 783, Training Cost: 0.020602092180102907, Testing Cost: 0.044152958977059675\n",
      "Iteration 784, Training Cost: 0.020599672180170444, Testing Cost: 0.04415683146138857\n",
      "Iteration 785, Training Cost: 0.020597255100840293, Testing Cost: 0.044160517829807525\n",
      "Iteration 786, Training Cost: 0.02059484795350859, Testing Cost: 0.04416561353285135\n",
      "Iteration 787, Training Cost: 0.020592448626288895, Testing Cost: 0.04417113373629634\n",
      "Iteration 788, Training Cost: 0.02059004466483114, Testing Cost: 0.04417531421481103\n",
      "Iteration 789, Training Cost: 0.02058763461246796, Testing Cost: 0.04417916024524925\n",
      "Iteration 790, Training Cost: 0.020585241074873476, Testing Cost: 0.04418208258367931\n",
      "Iteration 791, Training Cost: 0.02058284750002243, Testing Cost: 0.044185847988361816\n",
      "Iteration 792, Training Cost: 0.020580456355449885, Testing Cost: 0.04419018305609321\n",
      "Iteration 793, Training Cost: 0.020578071436469586, Testing Cost: 0.04419293646172155\n",
      "Iteration 794, Training Cost: 0.020575690200027166, Testing Cost: 0.04419810438295512\n",
      "Iteration 795, Training Cost: 0.020573307482650266, Testing Cost: 0.04420290083449154\n",
      "Iteration 796, Training Cost: 0.020570929317929858, Testing Cost: 0.044205317350992694\n",
      "Iteration 797, Training Cost: 0.020568559268222635, Testing Cost: 0.044209384875041516\n",
      "Iteration 798, Training Cost: 0.020566198024115186, Testing Cost: 0.04421451389801333\n",
      "Iteration 799, Training Cost: 0.020563836374163404, Testing Cost: 0.04421764610229744\n",
      "Iteration 800, Training Cost: 0.020561477641886417, Testing Cost: 0.04422343854463343\n",
      "Iteration 801, Training Cost: 0.020559123216874954, Testing Cost: 0.04422834229099198\n",
      "Iteration 802, Training Cost: 0.02055676892870137, Testing Cost: 0.0442322061605962\n",
      "Iteration 803, Training Cost: 0.020554422199953536, Testing Cost: 0.044237074234137856\n",
      "Iteration 804, Training Cost: 0.020552085551653127, Testing Cost: 0.04424119413079741\n",
      "Iteration 805, Training Cost: 0.020549739198201612, Testing Cost: 0.0442437144753904\n",
      "Iteration 806, Training Cost: 0.020547410968308442, Testing Cost: 0.04424840735377075\n",
      "Iteration 807, Training Cost: 0.020545092497892145, Testing Cost: 0.044253780865525706\n",
      "Iteration 808, Training Cost: 0.020542763921981698, Testing Cost: 0.044257975654718264\n",
      "Iteration 809, Training Cost: 0.02054044086325477, Testing Cost: 0.04426274527194296\n",
      "Iteration 810, Training Cost: 0.02053811760179906, Testing Cost: 0.04426609576099009\n",
      "Iteration 811, Training Cost: 0.020535805344308454, Testing Cost: 0.04426990517076036\n",
      "Iteration 812, Training Cost: 0.020533485169019643, Testing Cost: 0.044273193169289245\n",
      "Iteration 813, Training Cost: 0.02053117719680037, Testing Cost: 0.04427752061191125\n",
      "Iteration 814, Training Cost: 0.02052886526105941, Testing Cost: 0.044281072685711645\n",
      "Iteration 815, Training Cost: 0.020526554478182458, Testing Cost: 0.04428436199911883\n",
      "Iteration 816, Training Cost: 0.020524261486809137, Testing Cost: 0.04429003992087728\n",
      "Iteration 817, Training Cost: 0.02052196453822739, Testing Cost: 0.044293703235518296\n",
      "Iteration 818, Training Cost: 0.020519662233679462, Testing Cost: 0.0442973836304517\n",
      "Iteration 819, Training Cost: 0.02051736056405311, Testing Cost: 0.044299494055119994\n",
      "Iteration 820, Training Cost: 0.020515070087541103, Testing Cost: 0.04430291767373522\n",
      "Iteration 821, Training Cost: 0.020512777647323566, Testing Cost: 0.04430563157107224\n",
      "Iteration 822, Training Cost: 0.020510498312430382, Testing Cost: 0.04430811822785357\n",
      "Iteration 823, Training Cost: 0.020508220238521363, Testing Cost: 0.04431035710445455\n",
      "Iteration 824, Training Cost: 0.020505943041623387, Testing Cost: 0.04431314578238389\n",
      "Iteration 825, Training Cost: 0.020503665727846957, Testing Cost: 0.04431656092477935\n",
      "Iteration 826, Training Cost: 0.020501390191980537, Testing Cost: 0.04432096589479447\n",
      "Iteration 827, Training Cost: 0.020499121367937835, Testing Cost: 0.04432489463222998\n",
      "Iteration 828, Training Cost: 0.020496858381240567, Testing Cost: 0.044328062950246015\n",
      "Iteration 829, Training Cost: 0.020494595010113567, Testing Cost: 0.04433170672103623\n",
      "Iteration 830, Training Cost: 0.02049233058537121, Testing Cost: 0.044335471128065324\n",
      "Iteration 831, Training Cost: 0.020490072944046523, Testing Cost: 0.044337895323514315\n",
      "Iteration 832, Training Cost: 0.020487822693526878, Testing Cost: 0.0443411610799912\n",
      "Iteration 833, Training Cost: 0.02048556240044287, Testing Cost: 0.044347008035552776\n",
      "Iteration 834, Training Cost: 0.020483320465588886, Testing Cost: 0.04435168297157898\n",
      "Iteration 835, Training Cost: 0.02048106865239559, Testing Cost: 0.04435428096580624\n",
      "Iteration 836, Training Cost: 0.020478829148487258, Testing Cost: 0.04435718079565062\n",
      "Iteration 837, Training Cost: 0.02047659554247157, Testing Cost: 0.04435986604701288\n",
      "Iteration 838, Training Cost: 0.020474359410478128, Testing Cost: 0.04436430574419359\n",
      "Iteration 839, Training Cost: 0.02047213330107113, Testing Cost: 0.04436897373710298\n",
      "Iteration 840, Training Cost: 0.020469906817455592, Testing Cost: 0.044374305972081196\n",
      "Iteration 841, Training Cost: 0.020467684018520945, Testing Cost: 0.04437631492447558\n",
      "Iteration 842, Training Cost: 0.02046546634798642, Testing Cost: 0.04438024678900587\n",
      "Iteration 843, Training Cost: 0.020463247035383356, Testing Cost: 0.04438452871633347\n",
      "Iteration 844, Training Cost: 0.020461029414767018, Testing Cost: 0.04438823306846241\n",
      "Iteration 845, Training Cost: 0.020458813384535292, Testing Cost: 0.044390963421600295\n",
      "Iteration 846, Training Cost: 0.020456609558205923, Testing Cost: 0.04439283651207367\n",
      "Iteration 847, Training Cost: 0.02045440064615228, Testing Cost: 0.0443957162641707\n",
      "Iteration 848, Training Cost: 0.0204521934558671, Testing Cost: 0.04439918377485902\n",
      "Iteration 849, Training Cost: 0.020449989838187705, Testing Cost: 0.044401807688559065\n",
      "Iteration 850, Training Cost: 0.020447792610418195, Testing Cost: 0.04440567880959553\n",
      "Iteration 851, Training Cost: 0.020445600625411967, Testing Cost: 0.04440921207688321\n",
      "Iteration 852, Training Cost: 0.020443401202423227, Testing Cost: 0.044414651112595044\n",
      "Iteration 853, Training Cost: 0.020441209915271916, Testing Cost: 0.0444183763580232\n",
      "Iteration 854, Training Cost: 0.0204390173689468, Testing Cost: 0.044422784481684005\n",
      "Iteration 855, Training Cost: 0.020436828540226756, Testing Cost: 0.044425164221935404\n",
      "Iteration 856, Training Cost: 0.02043464548902656, Testing Cost: 0.04442784110796276\n",
      "Iteration 857, Training Cost: 0.02043246953041329, Testing Cost: 0.0444321923321768\n",
      "Iteration 858, Training Cost: 0.02043028427824831, Testing Cost: 0.04443440949905323\n",
      "Iteration 859, Training Cost: 0.020428101571919325, Testing Cost: 0.04443785831595124\n",
      "Iteration 860, Training Cost: 0.020425928994086563, Testing Cost: 0.0444413716395575\n",
      "Iteration 861, Training Cost: 0.020423750606714135, Testing Cost: 0.044443327488508856\n",
      "Iteration 862, Training Cost: 0.020421576447052687, Testing Cost: 0.04444597615237963\n",
      "Iteration 863, Training Cost: 0.020419415377923533, Testing Cost: 0.04445072323638148\n",
      "Iteration 864, Training Cost: 0.02041724065609318, Testing Cost: 0.044452529296301724\n",
      "Iteration 865, Training Cost: 0.020415079198414315, Testing Cost: 0.04445530978932091\n",
      "Iteration 866, Training Cost: 0.020412923051411663, Testing Cost: 0.04445904494991656\n",
      "Iteration 867, Training Cost: 0.02041076414997426, Testing Cost: 0.04446162015270866\n",
      "Iteration 868, Training Cost: 0.020408611818501426, Testing Cost: 0.04446400450318564\n",
      "Iteration 869, Training Cost: 0.02040646401293614, Testing Cost: 0.0444661069268347\n",
      "Iteration 870, Training Cost: 0.02040432113007527, Testing Cost: 0.04446875172874141\n",
      "Iteration 871, Training Cost: 0.02040217983843497, Testing Cost: 0.044471893676140496\n",
      "Iteration 872, Training Cost: 0.020400039726327254, Testing Cost: 0.04447467131361418\n",
      "Iteration 873, Training Cost: 0.020397906223069535, Testing Cost: 0.0444775360412449\n",
      "Iteration 874, Training Cost: 0.020395764873566105, Testing Cost: 0.04448148186860448\n",
      "Iteration 875, Training Cost: 0.020393625014617604, Testing Cost: 0.04448508565839635\n",
      "Iteration 876, Training Cost: 0.020391495412089244, Testing Cost: 0.04448757269965537\n",
      "Iteration 877, Training Cost: 0.020389365292422516, Testing Cost: 0.044490144645373694\n",
      "Iteration 878, Training Cost: 0.02038723235273915, Testing Cost: 0.04449301390143967\n",
      "Iteration 879, Training Cost: 0.02038509611322493, Testing Cost: 0.04449778113263494\n",
      "Iteration 880, Training Cost: 0.020382965382787686, Testing Cost: 0.04450167642443521\n",
      "Iteration 881, Training Cost: 0.020380844423947452, Testing Cost: 0.0445050174922007\n",
      "Iteration 882, Training Cost: 0.02037872294339384, Testing Cost: 0.04450798634115357\n",
      "Iteration 883, Training Cost: 0.02037659436561477, Testing Cost: 0.044512283868957184\n",
      "Iteration 884, Training Cost: 0.020374485690315452, Testing Cost: 0.044515616868059237\n",
      "Iteration 885, Training Cost: 0.02037238149700518, Testing Cost: 0.04451793930654869\n",
      "Iteration 886, Training Cost: 0.020370255523528814, Testing Cost: 0.04452252182994223\n",
      "Iteration 887, Training Cost: 0.020368151509782394, Testing Cost: 0.04452633986883089\n",
      "Iteration 888, Training Cost: 0.02036605048866995, Testing Cost: 0.04452896723855335\n",
      "Iteration 889, Training Cost: 0.020363942008125975, Testing Cost: 0.04453230709941355\n",
      "Iteration 890, Training Cost: 0.020361845494730916, Testing Cost: 0.044535126094900304\n",
      "Iteration 891, Training Cost: 0.020359747239949478, Testing Cost: 0.04453926700013232\n",
      "Iteration 892, Training Cost: 0.020357644357688744, Testing Cost: 0.04454145712921812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 893, Training Cost: 0.02035555829108953, Testing Cost: 0.04454289461528431\n",
      "Iteration 894, Training Cost: 0.020353470196306168, Testing Cost: 0.04454517402394304\n",
      "Iteration 895, Training Cost: 0.02035138271186503, Testing Cost: 0.044548193796868224\n",
      "Iteration 896, Training Cost: 0.02034930586978429, Testing Cost: 0.0445502097483741\n",
      "Iteration 897, Training Cost: 0.020347199720730774, Testing Cost: 0.04455546883528268\n",
      "Iteration 898, Training Cost: 0.0203451165847934, Testing Cost: 0.04455781638577293\n",
      "Iteration 899, Training Cost: 0.02034303167195172, Testing Cost: 0.04456092179173648\n",
      "Iteration 900, Training Cost: 0.020340958380403843, Testing Cost: 0.04456332347281259\n",
      "Iteration 901, Training Cost: 0.020338883281119033, Testing Cost: 0.0445658055127395\n",
      "Iteration 902, Training Cost: 0.02033681281983375, Testing Cost: 0.04456854914464297\n",
      "Iteration 903, Training Cost: 0.02033473385604263, Testing Cost: 0.04457142132528831\n",
      "Iteration 904, Training Cost: 0.020332665503889836, Testing Cost: 0.04457454030224902\n",
      "Iteration 905, Training Cost: 0.020330600279698113, Testing Cost: 0.044577197872709896\n",
      "Iteration 906, Training Cost: 0.020328530684023696, Testing Cost: 0.04458068626352364\n",
      "Iteration 907, Training Cost: 0.020326469684943223, Testing Cost: 0.04458273552293186\n",
      "Iteration 908, Training Cost: 0.020324380678380827, Testing Cost: 0.044586615865328194\n",
      "Iteration 909, Training Cost: 0.020322329410621454, Testing Cost: 0.044589373679905016\n",
      "Iteration 910, Training Cost: 0.020320267418214796, Testing Cost: 0.04459281833597431\n",
      "Iteration 911, Training Cost: 0.020318225042267702, Testing Cost: 0.04459576467907258\n",
      "Iteration 912, Training Cost: 0.020316158818395304, Testing Cost: 0.044600006905874096\n",
      "Iteration 913, Training Cost: 0.020314105804109528, Testing Cost: 0.044603586990219365\n",
      "Iteration 914, Training Cost: 0.020312060030157987, Testing Cost: 0.044604786931102224\n",
      "Iteration 915, Training Cost: 0.020310000731396462, Testing Cost: 0.04460880973272858\n",
      "Iteration 916, Training Cost: 0.020307949378212328, Testing Cost: 0.04461174701764014\n",
      "Iteration 917, Training Cost: 0.02030589751604329, Testing Cost: 0.04461462026168258\n",
      "Iteration 918, Training Cost: 0.0203038549281593, Testing Cost: 0.0446170703910092\n",
      "Iteration 919, Training Cost: 0.020301809063256678, Testing Cost: 0.04462052248671932\n",
      "Iteration 920, Training Cost: 0.020299767746536506, Testing Cost: 0.044623193806429116\n",
      "Iteration 921, Training Cost: 0.020297723808958654, Testing Cost: 0.04462566794676698\n",
      "Iteration 922, Training Cost: 0.020295689664592557, Testing Cost: 0.044627667835645736\n",
      "Iteration 923, Training Cost: 0.02029366039759387, Testing Cost: 0.04462966687815969\n",
      "Iteration 924, Training Cost: 0.020291636417072646, Testing Cost: 0.0446317444896384\n",
      "Iteration 925, Training Cost: 0.020289617061538284, Testing Cost: 0.04463356397364042\n",
      "Iteration 926, Training Cost: 0.0202875868630756, Testing Cost: 0.044637375474583796\n",
      "Iteration 927, Training Cost: 0.02028555814513991, Testing Cost: 0.044640406897590114\n",
      "Iteration 928, Training Cost: 0.020283542235120494, Testing Cost: 0.0446420994698729\n",
      "Iteration 929, Training Cost: 0.020281520594529534, Testing Cost: 0.04464437265375874\n",
      "Iteration 930, Training Cost: 0.020279505706196216, Testing Cost: 0.04464642992925377\n",
      "Iteration 931, Training Cost: 0.020277479340028555, Testing Cost: 0.04464913367636591\n",
      "Iteration 932, Training Cost: 0.020275463773501194, Testing Cost: 0.044651203252036566\n",
      "Iteration 933, Training Cost: 0.020273450226151887, Testing Cost: 0.04465379735831065\n",
      "Iteration 934, Training Cost: 0.020271440596332777, Testing Cost: 0.04465720934593181\n",
      "Iteration 935, Training Cost: 0.020269424539824863, Testing Cost: 0.04466081321177138\n",
      "Iteration 936, Training Cost: 0.020267406949989714, Testing Cost: 0.04466293287047032\n",
      "Iteration 937, Training Cost: 0.020265378873293438, Testing Cost: 0.04466661682354534\n",
      "Iteration 938, Training Cost: 0.02026337213296068, Testing Cost: 0.04466820533686025\n",
      "Iteration 939, Training Cost: 0.02026137579692292, Testing Cost: 0.0446697155057042\n",
      "Iteration 940, Training Cost: 0.020259375860536347, Testing Cost: 0.044670919133175134\n",
      "Iteration 941, Training Cost: 0.020257356009096786, Testing Cost: 0.044674913793628715\n",
      "Iteration 942, Training Cost: 0.02025535032147106, Testing Cost: 0.044678530641586944\n",
      "Iteration 943, Training Cost: 0.020253346967622068, Testing Cost: 0.0446822618732151\n",
      "Iteration 944, Training Cost: 0.020251354464433415, Testing Cost: 0.04468377574992393\n",
      "Iteration 945, Training Cost: 0.02024935322847319, Testing Cost: 0.044687761844755126\n",
      "Iteration 946, Training Cost: 0.020247351027727654, Testing Cost: 0.04469117119691836\n",
      "Iteration 947, Training Cost: 0.02024536343492754, Testing Cost: 0.04469320048227051\n",
      "Iteration 948, Training Cost: 0.02024336515681437, Testing Cost: 0.04469629373243016\n",
      "Iteration 949, Training Cost: 0.02024137751795035, Testing Cost: 0.04469879825260095\n",
      "Iteration 950, Training Cost: 0.020239387609451202, Testing Cost: 0.04470153598435912\n",
      "Iteration 951, Training Cost: 0.02023740665835176, Testing Cost: 0.04470219761684059\n",
      "Iteration 952, Training Cost: 0.020235419096918658, Testing Cost: 0.04470467893740194\n",
      "Iteration 953, Training Cost: 0.020233436774931557, Testing Cost: 0.04470728358588266\n",
      "Iteration 954, Training Cost: 0.020231450001486143, Testing Cost: 0.04470915188048319\n",
      "Iteration 955, Training Cost: 0.02022946455017318, Testing Cost: 0.044712056049874854\n",
      "Iteration 956, Training Cost: 0.02022749204791405, Testing Cost: 0.04471402152314233\n",
      "Iteration 957, Training Cost: 0.020225521446742164, Testing Cost: 0.044716238823289976\n",
      "Iteration 958, Training Cost: 0.020223533963278865, Testing Cost: 0.04472011281203192\n",
      "Iteration 959, Training Cost: 0.020221561476717387, Testing Cost: 0.044722382047201754\n",
      "Iteration 960, Training Cost: 0.020219579798616952, Testing Cost: 0.04472577678728055\n",
      "Iteration 961, Training Cost: 0.020217612732027736, Testing Cost: 0.04472861675487548\n",
      "Iteration 962, Training Cost: 0.020215640427086772, Testing Cost: 0.04473165942140266\n",
      "Iteration 963, Training Cost: 0.020213668851740835, Testing Cost: 0.044734554216259804\n",
      "Iteration 964, Training Cost: 0.020211692974347713, Testing Cost: 0.04473671045156882\n",
      "Iteration 965, Training Cost: 0.02020972702425723, Testing Cost: 0.04473772768664048\n",
      "Iteration 966, Training Cost: 0.02020776541257034, Testing Cost: 0.04474023983612762\n",
      "Iteration 967, Training Cost: 0.020205796726506714, Testing Cost: 0.044743432861520806\n",
      "Iteration 968, Training Cost: 0.020203838364129603, Testing Cost: 0.044745134355299435\n",
      "Iteration 969, Training Cost: 0.020201887528327158, Testing Cost: 0.04474674702663933\n",
      "Iteration 970, Training Cost: 0.020199928487800996, Testing Cost: 0.044748591116391956\n",
      "Iteration 971, Training Cost: 0.020197957719019315, Testing Cost: 0.04475190915067839\n",
      "Iteration 972, Training Cost: 0.020195999532559405, Testing Cost: 0.04475543689317217\n",
      "Iteration 973, Training Cost: 0.020194039197779824, Testing Cost: 0.044757755916417875\n",
      "Iteration 974, Training Cost: 0.020192087217220458, Testing Cost: 0.0447594509946746\n",
      "Iteration 975, Training Cost: 0.020190121243462155, Testing Cost: 0.04476190877713423\n",
      "Iteration 976, Training Cost: 0.02018817484529388, Testing Cost: 0.04476445783239545\n",
      "Iteration 977, Training Cost: 0.02018621904419219, Testing Cost: 0.04476651250965955\n",
      "Iteration 978, Training Cost: 0.02018426963559982, Testing Cost: 0.044768806143803526\n",
      "Iteration 979, Training Cost: 0.0201823130663625, Testing Cost: 0.044771070920265804\n",
      "Iteration 980, Training Cost: 0.020180367292415836, Testing Cost: 0.04477455497490713\n",
      "Iteration 981, Training Cost: 0.020178423270703454, Testing Cost: 0.04477633223401756\n",
      "Iteration 982, Training Cost: 0.020176473086824475, Testing Cost: 0.04477927754639096\n",
      "Iteration 983, Training Cost: 0.020174526451255954, Testing Cost: 0.044781493276794926\n",
      "Iteration 984, Training Cost: 0.020172583034919207, Testing Cost: 0.04478360589421064\n",
      "Iteration 985, Training Cost: 0.020170649440940703, Testing Cost: 0.04478570163818406\n",
      "Iteration 986, Training Cost: 0.020168712775551188, Testing Cost: 0.0447882997044249\n",
      "Iteration 987, Training Cost: 0.020166771029676574, Testing Cost: 0.044791862330831\n",
      "Iteration 988, Training Cost: 0.020164825335743024, Testing Cost: 0.04479531005042531\n",
      "Iteration 989, Training Cost: 0.020162886092747787, Testing Cost: 0.04479688667932269\n",
      "Iteration 990, Training Cost: 0.02016095625891646, Testing Cost: 0.04479915946638679\n",
      "Iteration 991, Training Cost: 0.020159019634472128, Testing Cost: 0.044802205678836735\n",
      "Iteration 992, Training Cost: 0.020157079526493118, Testing Cost: 0.044805561228458725\n",
      "Iteration 993, Training Cost: 0.0201551469965417, Testing Cost: 0.04480730500100518\n",
      "Iteration 994, Training Cost: 0.02015320831021972, Testing Cost: 0.04480885379573635\n",
      "Iteration 995, Training Cost: 0.02015127930909304, Testing Cost: 0.044810865280181925\n",
      "Iteration 996, Training Cost: 0.020149353760457556, Testing Cost: 0.04481393750094307\n",
      "Iteration 997, Training Cost: 0.020147421604167066, Testing Cost: 0.044816236342909974\n",
      "Iteration 998, Training Cost: 0.02014548947911912, Testing Cost: 0.04481837847266905\n",
      "Iteration 999, Training Cost: 0.020143564423143957, Testing Cost: 0.04482093279031585\n",
      "Iteration 1000, Training Cost: 0.020141638966962982, Testing Cost: 0.04482379777215459\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define your neural network model and parameters\n",
    "model = NeuralNetwork([X_cancer.shape[1], 5, y_encoded.shape[1]])  # Your desired architecture\n",
    "learning_rate = 0.01\n",
    "batch_size = 32\n",
    "max_iterations = 1000\n",
    "epsilon = 0.005\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_cancer, y_encoded, test_size=0.2)\n",
    "\n",
    "# Standardize features\n",
    "mean = np.mean(X_train, axis=0)\n",
    "std = np.std(X_train, axis=0)\n",
    "X_train_normalized = (X_train - mean) / std\n",
    "X_test_normalized = (X_test - mean) / std\n",
    "\n",
    "# Train the model using mini-batch gradient descent\n",
    "training_errors, testing_errors = train_mini_batch(X_train_normalized.to_numpy(), y_train, X_test_normalized.to_numpy(), y_test, model, learning_rate, batch_size, max_iterations, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9318df15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the learning curve\n",
    "step_size = 50\n",
    "plot_learning_curve(training_errors, testing_errors, step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d031edec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a735eeba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1408c281",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6393484a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1487da54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08576023",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Function to generate mini-batches\n",
    "def generate_mini_batches(X, y, batch_size):\n",
    "    num_samples = X.shape[0]\n",
    "    num_batches = num_samples // batch_size\n",
    "    mini_batches = []\n",
    "    shuffled_indices = np.random.permutation(num_samples)\n",
    "    X_shuffled = X[shuffled_indices]\n",
    "    y_shuffled = y[shuffled_indices]\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        mini_batches.append((X_shuffled[start_idx:end_idx], y_shuffled[start_idx:end_idx]))\n",
    "    if num_samples % batch_size != 0:\n",
    "        mini_batches.append((X_shuffled[num_batches*batch_size:], y_shuffled[num_batches*batch_size:]))\n",
    "    return mini_batches\n",
    "\n",
    "\n",
    "def train_mini_batch(X_train, y_train, X_test, y_test, model, learning_rate, batch_size, max_iterations, epsilon):\n",
    "    training_errors = []\n",
    "    testing_errors = []\n",
    "    for iteration in range(max_iterations):\n",
    "        mini_batches = generate_mini_batches(X_train, y_train, batch_size)\n",
    "        for mini_batch in mini_batches:\n",
    "            X_mini_batch, y_mini_batch = mini_batch\n",
    "            J = model.train(X_mini_batch, y_mini_batch, learning_rate=learning_rate, lam=lam, max_iterations=1, epsilon=epsilon)\n",
    "        training_cost = np.mean(np.square(model.forward_pass(X_train)[-1] - y_train))  # Compute training cost\n",
    "        testing_cost = np.mean(np.square(model.forward_pass(X_test)[-1] - y_test))  # Compute testing cost\n",
    "        training_errors.append(training_cost)\n",
    "        testing_errors.append(testing_cost)\n",
    "        print(f\"Iteration {iteration+1}, Training Cost: {training_cost}, Testing Cost: {testing_cost}\")\n",
    "        # Check for convergence\n",
    "        if training_cost < epsilon:\n",
    "            print(f\"Converged at training cost :{training_cost} while Epsilon:{epsilon} \")\n",
    "            break\n",
    "    return training_errors, testing_errors\n",
    "\n",
    "# Plot learning curve\n",
    "def plot_learning_curve(training_errors, testing_errors, step_size):\n",
    "    iterations = range(1, len(training_errors) + 1)\n",
    "    plt.plot(iterations, training_errors, label='Training Error')\n",
    "    plt.plot(iterations, testing_errors, label='Testing Error')\n",
    "    plt.title('Learning Curve')\n",
    "    plt.xlabel('Number of Training Examples')\n",
    "    plt.ylabel('Error (J)')\n",
    "    plt.xticks(np.arange(1, len(training_errors) + 1, step=step_size))\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bddf927b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Int64Index([127, 164,  45, 311, 300, 279, 196, 108, 198,  92,\\n            ...\\n            195, 236, 314,  22, 223,  52, 270, 325,  37, 104],\\n           dtype='int64', length=348)] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/bm/l2zpyjxn2gqg_1d20xgx5z0w0000gn/T/ipykernel_15709/2769990677.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Train the model using mini-batch gradient descent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mtraining_errors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesting_errors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_mini_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_normalized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_normalized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/bm/l2zpyjxn2gqg_1d20xgx5z0w0000gn/T/ipykernel_15709/3214990907.py\u001b[0m in \u001b[0;36mtrain_mini_batch\u001b[0;34m(X_train, y_train, X_test, y_test, model, learning_rate, batch_size, max_iterations, epsilon)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mtesting_errors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mmini_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_mini_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmini_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmini_batches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mX_mini_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_mini_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmini_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/bm/l2zpyjxn2gqg_1d20xgx5z0w0000gn/T/ipykernel_15709/3214990907.py\u001b[0m in \u001b[0;36mgenerate_mini_batches\u001b[0;34m(X, y, batch_size)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mmini_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mshuffled_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mX_shuffled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mshuffled_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0my_shuffled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mshuffled_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3462\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3463\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3464\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3466\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1312\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1314\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_read_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m         if needs_i8_conversion(ax.dtype) or isinstance(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis)\u001b[0m\n\u001b[1;32m   1372\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0muse_interval_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m                     \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1374\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"None of [{key}] are in the [{axis_name}]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Int64Index([127, 164,  45, 311, 300, 279, 196, 108, 198,  92,\\n            ...\\n            195, 236, 314,  22, 223,  52, 270, 325,  37, 104],\\n           dtype='int64', length=348)] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define your neural network model and parameters\n",
    "model = NeuralNetwork([X_house_votes.shape[1], 10, 8, y_encoded.shape[1]])  # Your desired architecture\n",
    "learning_rate = 0.01\n",
    "batch_size = 32\n",
    "max_iterations = 1000\n",
    "epsilon = 0.005\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_house_votes, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "mean = np.mean(X_train, axis=0)\n",
    "std = np.std(X_train, axis=0)\n",
    "X_train_normalized = (X_train - mean) / std\n",
    "X_test_normalized = (X_test - mean) / std\n",
    "\n",
    "# Train the model using mini-batch gradient descent\n",
    "training_errors, testing_errors = train_mini_batch(X_train_normalized, y_train, X_test_normalized, y_test, model, learning_rate, batch_size, max_iterations, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0025b465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curve\n",
    "step_size = 50  # Adjust as needed\n",
    "plot_learning_curve(training_errors, testing_errors, step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4cc6e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
