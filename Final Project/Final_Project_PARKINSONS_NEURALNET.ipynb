{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4487b3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy Results:\n",
      "       Architecture, Lambda  Mean Accuracy\n",
      "0      ([22, 4, 2], 0.0001)       0.847368\n",
      "1       ([22, 4, 2], 0.001)       0.863158\n",
      "2        ([22, 4, 2], 0.01)       0.847368\n",
      "3         ([22, 4, 2], 0.1)       0.836842\n",
      "4        ([22, 4, 2], 0.25)       0.863158\n",
      "5        ([22, 4, 2], 0.75)       0.847368\n",
      "6           ([22, 4, 2], 0)       0.842105\n",
      "7      ([22, 8, 2], 0.0001)       0.873684\n",
      "8       ([22, 8, 2], 0.001)       0.905263\n",
      "9        ([22, 8, 2], 0.01)       0.863158\n",
      "10        ([22, 8, 2], 0.1)       0.863158\n",
      "11       ([22, 8, 2], 0.25)       0.863158\n",
      "12       ([22, 8, 2], 0.75)       0.889474\n",
      "13          ([22, 8, 2], 0)       0.863158\n",
      "14  ([22, 4, 4, 2], 0.0001)       0.863158\n",
      "15   ([22, 4, 4, 2], 0.001)       0.852632\n",
      "16    ([22, 4, 4, 2], 0.01)       0.884211\n",
      "17     ([22, 4, 4, 2], 0.1)       0.878947\n",
      "18    ([22, 4, 4, 2], 0.25)       0.847368\n",
      "19    ([22, 4, 4, 2], 0.75)       0.852632\n",
      "20       ([22, 4, 4, 2], 0)       0.868421\n",
      "21  ([22, 4, 2, 2], 0.0001)       0.857895\n",
      "22   ([22, 4, 2, 2], 0.001)       0.873684\n",
      "23    ([22, 4, 2, 2], 0.01)       0.857895\n",
      "24     ([22, 4, 2, 2], 0.1)       0.884211\n",
      "25    ([22, 4, 2, 2], 0.25)       0.873684\n",
      "26    ([22, 4, 2, 2], 0.75)       0.852632\n",
      "27       ([22, 4, 2, 2], 0)       0.857895\n",
      "\n",
      "Mean F1 Score Results:\n",
      "       Architecture, Lambda  Mean F1 Score\n",
      "0      ([22, 4, 2], 0.0001)       0.851920\n",
      "1       ([22, 4, 2], 0.001)       0.865434\n",
      "2        ([22, 4, 2], 0.01)       0.850202\n",
      "3         ([22, 4, 2], 0.1)       0.838976\n",
      "4        ([22, 4, 2], 0.25)       0.866397\n",
      "5        ([22, 4, 2], 0.75)       0.847368\n",
      "6           ([22, 4, 2], 0)       0.842105\n",
      "7      ([22, 8, 2], 0.0001)       0.878363\n",
      "8       ([22, 8, 2], 0.001)       0.907539\n",
      "9        ([22, 8, 2], 0.01)       0.868947\n",
      "10        ([22, 8, 2], 0.1)       0.874726\n",
      "11       ([22, 8, 2], 0.25)       0.867994\n",
      "12       ([22, 8, 2], 0.75)       0.895412\n",
      "13          ([22, 8, 2], 0)       0.870544\n",
      "14  ([22, 4, 4, 2], 0.0001)       0.863158\n",
      "15   ([22, 4, 4, 2], 0.001)       0.855192\n",
      "16    ([22, 4, 4, 2], 0.01)       0.884211\n",
      "17     ([22, 4, 4, 2], 0.1)       0.881508\n",
      "18    ([22, 4, 4, 2], 0.25)       0.849502\n",
      "19    ([22, 4, 4, 2], 0.75)       0.852632\n",
      "20       ([22, 4, 4, 2], 0)       0.874474\n",
      "21  ([22, 4, 2, 2], 0.0001)       0.857895\n",
      "22   ([22, 4, 2, 2], 0.001)       0.873684\n",
      "23    ([22, 4, 2, 2], 0.01)       0.857895\n",
      "24     ([22, 4, 2, 2], 0.1)       0.886771\n",
      "25    ([22, 4, 2, 2], 0.25)       0.876788\n",
      "26    ([22, 4, 2, 2], 0.75)       0.852632\n",
      "27       ([22, 4, 2, 2], 0)       0.857895\n",
      "\n",
      "Mean J cost Results:\n",
      "       Architecture, Lambda  Mean J Cost\n",
      "0      ([22, 4, 2], 0.0001)     0.044212\n",
      "1       ([22, 4, 2], 0.001)     0.046213\n",
      "2        ([22, 4, 2], 0.01)     0.046570\n",
      "3         ([22, 4, 2], 0.1)     0.043711\n",
      "4        ([22, 4, 2], 0.25)     0.042839\n",
      "5        ([22, 4, 2], 0.75)     0.049211\n",
      "6           ([22, 4, 2], 0)     0.047768\n",
      "7      ([22, 8, 2], 0.0001)     0.026212\n",
      "8       ([22, 8, 2], 0.001)     0.025225\n",
      "9        ([22, 8, 2], 0.01)     0.027305\n",
      "10        ([22, 8, 2], 0.1)     0.029477\n",
      "11       ([22, 8, 2], 0.25)     0.028087\n",
      "12       ([22, 8, 2], 0.75)     0.027527\n",
      "13          ([22, 8, 2], 0)     0.029526\n",
      "14  ([22, 4, 4, 2], 0.0001)     0.030108\n",
      "15   ([22, 4, 4, 2], 0.001)     0.035290\n",
      "16    ([22, 4, 4, 2], 0.01)     0.032542\n",
      "17     ([22, 4, 4, 2], 0.1)     0.033470\n",
      "18    ([22, 4, 4, 2], 0.25)     0.036854\n",
      "19    ([22, 4, 4, 2], 0.75)     0.031245\n",
      "20       ([22, 4, 4, 2], 0)     0.036780\n",
      "21  ([22, 4, 2, 2], 0.0001)     0.037761\n",
      "22   ([22, 4, 2, 2], 0.001)     0.036123\n",
      "23    ([22, 4, 2, 2], 0.01)     0.039469\n",
      "24     ([22, 4, 2, 2], 0.1)     0.036618\n",
      "25    ([22, 4, 2, 2], 0.25)     0.041304\n",
      "26    ([22, 4, 2, 2], 0.75)     0.034140\n",
      "27       ([22, 4, 2, 2], 0)     0.036890\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        self.weights = [np.random.randn(layers[i], layers[i+1]) for i in range(len(layers)-1)]\n",
    "        self.biases = [np.zeros((1, layers[i+1])) for i in range(len(layers)-1)]\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    def forward_pass(self, X):\n",
    "        activations = [X]\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(activations[-1], w) + b\n",
    "            activations.append(self.sigmoid(z))\n",
    "        return activations\n",
    "    \n",
    "    def backward_pass(self, X, Y, activations):\n",
    "        deltas = [(activations[-1] - Y) * self.sigmoid_derivative(activations[-1])]\n",
    "        for i in range(len(self.layers) - 2, 0, -1):\n",
    "            delta = np.dot(deltas[0], self.weights[i].T) * self.sigmoid_derivative(activations[i])\n",
    "            deltas.insert(0, delta)\n",
    "        return deltas\n",
    "    \n",
    "    \n",
    "    def compute_gradients(self, activations, deltas):\n",
    "        gradients_weights = [np.dot(activations[i].T, deltas[i]) for i in range(len(self.layers) - 1)]\n",
    "        gradients_biases = [np.sum(deltas[i], axis=0) for i in range(len(self.layers) - 1)]\n",
    "        return gradients_weights, gradients_biases\n",
    "    \n",
    "    def update_weights(self, gradients_weights, gradients_biases, learning_rate):\n",
    "        self.weights = [w - learning_rate * gw for w, gw in zip(self.weights, gradients_weights)]\n",
    "        self.biases = [b - learning_rate * gb for b, gb in zip(self.biases, gradients_biases)]\n",
    "    \n",
    "    def train(self, X, Y, learning_rate, lam, max_iterations, epsilon):\n",
    "        for iteration in range(max_iterations):\n",
    "            activations = self.forward_pass(X)\n",
    "            deltas = self.backward_pass(X, Y, activations)\n",
    "            gradients_weights, gradients_biases = self.compute_gradients(activations, deltas)\n",
    "            self.update_weights(gradients_weights, gradients_biases, learning_rate)\n",
    "            # Compute cost function\n",
    "            J = np.mean(np.square(activations[-1] - Y))\n",
    "            #print(f\"Iteration {iteration+1}, Cost: {J}\")\n",
    "            # Check for convergence\n",
    "            if J < epsilon:\n",
    "                #print(f\"Converged at cost :{J} while Epsilon:{epsilon} \")\n",
    "                return J\n",
    "        return J\n",
    "            \n",
    "\n",
    "    def accuracy(self, y_true, y_pred):\n",
    "        correct = np.sum(np.all(y_true == y_pred, axis=1))\n",
    "        return correct / len(y_true)\n",
    "\n",
    "\n",
    "    def f1_score(self, y_true, y_pred):\n",
    "        tp = np.sum(np.logical_and(y_true, y_pred))\n",
    "        fp = np.sum(np.logical_and(np.logical_not(y_true), y_pred))\n",
    "        fn = np.sum(np.logical_and(y_true, np.logical_not(y_pred)))\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        return f1\n",
    "\n",
    "\n",
    "    def evaluate(self, X_test, y_test, J):\n",
    "        activations = self.forward_pass(X_test)[-1]\n",
    "        y_pred = (activations > 0.5).astype(int)\n",
    "        acc = self.accuracy(y_test, y_pred)\n",
    "        f1 = self.f1_score(y_test, y_pred)\n",
    "        return J, acc, f1\n",
    "    \n",
    "    def k_fold_cross_validation(X, y, architectures, regularization_params, learning_rate, max_iterations, epsilon):\n",
    "        results_accuracy = {}\n",
    "        results_f1_score = {}\n",
    "        results_J_cost = {}\n",
    "        \n",
    "        num_splits = 10\n",
    "        fold_size = len(X) // num_splits\n",
    "\n",
    "        for arch in architectures:\n",
    "            for lam in regularization_params:\n",
    "                accuracy_list = []\n",
    "                f1_score_list = []\n",
    "                J_list = []\n",
    "                \n",
    "                for i in range(num_splits):\n",
    "                    start = i * fold_size\n",
    "                    end = (i + 1) * fold_size\n",
    "                    \n",
    "                    X_train = pd.concat([X[:start], X[end:]])\n",
    "                    y_train = np.concatenate([y[:start], y[end:]])\n",
    "                    X_test = X[start:end]\n",
    "                    y_test = y[start:end]\n",
    "\n",
    "                    mean = np.mean(X_train, axis=0)\n",
    "                    std = np.std(X_train, axis=0)\n",
    "                    X_train_normalized = (X_train - mean) / std\n",
    "                    X_test_normalized = (X_test - mean) / std\n",
    "\n",
    "                    model = NeuralNetwork(arch)\n",
    "                    J = model.train(X_train_normalized, y_train, learning_rate=learning_rate, lam=lam, max_iterations=max_iterations, epsilon=epsilon)\n",
    "                    J, accuracy, f1_score = model.evaluate(X_test_normalized, y_test, J)\n",
    "                    accuracy_list.append(accuracy)\n",
    "                    f1_score_list.append(f1_score)\n",
    "                    J_list.append(J)\n",
    "\n",
    "                mean_accuracy = np.mean(accuracy_list)\n",
    "                mean_f1_score = np.mean(f1_score_list)\n",
    "                mean_J_cost   = np.mean(J_list)\n",
    "\n",
    "                results_accuracy[(str(arch), lam)] = mean_accuracy\n",
    "                results_f1_score[(str(arch), lam)] = mean_f1_score\n",
    "                results_J_cost[(str(arch), lam)] = mean_J_cost\n",
    "\n",
    "        return results_accuracy, results_f1_score, results_J_cost\n",
    "\n",
    "\n",
    "df_parkinsons = pd.read_csv(\"/Users/noshitha/Downloads/final_project/parkinsons.csv\", delimiter=\",\")\n",
    "\n",
    "# Shuffle the dataset\n",
    "df_parkinsons_shuffle = shuffle(df_parkinsons)\n",
    "\n",
    "# Split the dataset into features and target variable\n",
    "X =  pd.get_dummies(df_parkinsons_shuffle.drop(columns=['Diagnosis']))  \n",
    "y = df_parkinsons_shuffle['Diagnosis'] \n",
    "\n",
    "# Normalize data\n",
    "y_resized = y.values.reshape(-1, 1)\n",
    "\n",
    "# Initialize the OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Fit and transform the target variable\n",
    "y_encoded = encoder.fit_transform(y_resized)\n",
    "\n",
    "# Define model architectures and regularization parameters\n",
    "architectures = [\n",
    "    [X.shape[1], 4, y_encoded.shape[1]], \n",
    "    [X.shape[1], 8, y_encoded.shape[1]],   \n",
    "    [X.shape[1], 4, 4, y_encoded.shape[1]],  \n",
    "    [X.shape[1], 4, 2, y_encoded.shape[1]]\n",
    "]\n",
    "\n",
    "regularization_params = [0.0001, 0.001, 0.01, 0.1, 0.25, 0.75, 0]  # Example regularization parameters\n",
    "\n",
    "# Initialize lists to store results\n",
    "results_accuracy = {}\n",
    "results_f1_score = {}\n",
    "results_J_cost = {}\n",
    "\n",
    "# Perform stratified k-fold cross-validation\n",
    "results_accuracy, results_f1_score, results_J_cost = NeuralNetwork.k_fold_cross_validation(X, y_encoded, architectures, regularization_params, learning_rate=0.01, max_iterations=1000, epsilon=0.005)\n",
    "\n",
    "# Convert the results into a DataFrame for tabular representation\n",
    "accuracy_df = pd.DataFrame(list(results_accuracy.items()), columns=['Architecture, Lambda', 'Mean Accuracy'])\n",
    "f1_score_df = pd.DataFrame(list(results_f1_score.items()), columns=['Architecture, Lambda', 'Mean F1 Score'])\n",
    "J_cost_df = pd.DataFrame(list(results_J_cost.items()), columns=['Architecture, Lambda', 'Mean J Cost'])\n",
    "\n",
    "print(\"Mean Accuracy Results:\")\n",
    "print(accuracy_df)\n",
    "print(\"\\nMean F1 Score Results:\")\n",
    "print(f1_score_df)\n",
    "print(\"\\nMean J cost Results:\")\n",
    "print(J_cost_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "109cc433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Architecture, Lambda</th>\n",
       "      <th>Mean Accuracy</th>\n",
       "      <th>Mean F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>([22, 4, 2], 0.0001)</td>\n",
       "      <td>0.847368</td>\n",
       "      <td>0.851920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>([22, 4, 2], 0.001)</td>\n",
       "      <td>0.863158</td>\n",
       "      <td>0.865434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>([22, 4, 2], 0.01)</td>\n",
       "      <td>0.847368</td>\n",
       "      <td>0.850202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>([22, 4, 2], 0.1)</td>\n",
       "      <td>0.836842</td>\n",
       "      <td>0.838976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>([22, 4, 2], 0.25)</td>\n",
       "      <td>0.863158</td>\n",
       "      <td>0.866397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>([22, 4, 2], 0.75)</td>\n",
       "      <td>0.847368</td>\n",
       "      <td>0.847368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>([22, 4, 2], 0)</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.842105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>([22, 8, 2], 0.0001)</td>\n",
       "      <td>0.873684</td>\n",
       "      <td>0.878363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>([22, 8, 2], 0.001)</td>\n",
       "      <td>0.905263</td>\n",
       "      <td>0.907539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>([22, 8, 2], 0.01)</td>\n",
       "      <td>0.863158</td>\n",
       "      <td>0.868947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>([22, 8, 2], 0.1)</td>\n",
       "      <td>0.863158</td>\n",
       "      <td>0.874726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>([22, 8, 2], 0.25)</td>\n",
       "      <td>0.863158</td>\n",
       "      <td>0.867994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>([22, 8, 2], 0.75)</td>\n",
       "      <td>0.889474</td>\n",
       "      <td>0.895412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>([22, 8, 2], 0)</td>\n",
       "      <td>0.863158</td>\n",
       "      <td>0.870544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>([22, 4, 4, 2], 0.0001)</td>\n",
       "      <td>0.863158</td>\n",
       "      <td>0.863158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>([22, 4, 4, 2], 0.001)</td>\n",
       "      <td>0.852632</td>\n",
       "      <td>0.855192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>([22, 4, 4, 2], 0.01)</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.884211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>([22, 4, 4, 2], 0.1)</td>\n",
       "      <td>0.878947</td>\n",
       "      <td>0.881508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>([22, 4, 4, 2], 0.25)</td>\n",
       "      <td>0.847368</td>\n",
       "      <td>0.849502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>([22, 4, 4, 2], 0.75)</td>\n",
       "      <td>0.852632</td>\n",
       "      <td>0.852632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>([22, 4, 4, 2], 0)</td>\n",
       "      <td>0.868421</td>\n",
       "      <td>0.874474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>([22, 4, 2, 2], 0.0001)</td>\n",
       "      <td>0.857895</td>\n",
       "      <td>0.857895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>([22, 4, 2, 2], 0.001)</td>\n",
       "      <td>0.873684</td>\n",
       "      <td>0.873684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>([22, 4, 2, 2], 0.01)</td>\n",
       "      <td>0.857895</td>\n",
       "      <td>0.857895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>([22, 4, 2, 2], 0.1)</td>\n",
       "      <td>0.884211</td>\n",
       "      <td>0.886771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>([22, 4, 2, 2], 0.25)</td>\n",
       "      <td>0.873684</td>\n",
       "      <td>0.876788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>([22, 4, 2, 2], 0.75)</td>\n",
       "      <td>0.852632</td>\n",
       "      <td>0.852632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>([22, 4, 2, 2], 0)</td>\n",
       "      <td>0.857895</td>\n",
       "      <td>0.857895</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Architecture, Lambda  Mean Accuracy  Mean F1 Score\n",
       "0      ([22, 4, 2], 0.0001)       0.847368       0.851920\n",
       "1       ([22, 4, 2], 0.001)       0.863158       0.865434\n",
       "2        ([22, 4, 2], 0.01)       0.847368       0.850202\n",
       "3         ([22, 4, 2], 0.1)       0.836842       0.838976\n",
       "4        ([22, 4, 2], 0.25)       0.863158       0.866397\n",
       "5        ([22, 4, 2], 0.75)       0.847368       0.847368\n",
       "6           ([22, 4, 2], 0)       0.842105       0.842105\n",
       "7      ([22, 8, 2], 0.0001)       0.873684       0.878363\n",
       "8       ([22, 8, 2], 0.001)       0.905263       0.907539\n",
       "9        ([22, 8, 2], 0.01)       0.863158       0.868947\n",
       "10        ([22, 8, 2], 0.1)       0.863158       0.874726\n",
       "11       ([22, 8, 2], 0.25)       0.863158       0.867994\n",
       "12       ([22, 8, 2], 0.75)       0.889474       0.895412\n",
       "13          ([22, 8, 2], 0)       0.863158       0.870544\n",
       "14  ([22, 4, 4, 2], 0.0001)       0.863158       0.863158\n",
       "15   ([22, 4, 4, 2], 0.001)       0.852632       0.855192\n",
       "16    ([22, 4, 4, 2], 0.01)       0.884211       0.884211\n",
       "17     ([22, 4, 4, 2], 0.1)       0.878947       0.881508\n",
       "18    ([22, 4, 4, 2], 0.25)       0.847368       0.849502\n",
       "19    ([22, 4, 4, 2], 0.75)       0.852632       0.852632\n",
       "20       ([22, 4, 4, 2], 0)       0.868421       0.874474\n",
       "21  ([22, 4, 2, 2], 0.0001)       0.857895       0.857895\n",
       "22   ([22, 4, 2, 2], 0.001)       0.873684       0.873684\n",
       "23    ([22, 4, 2, 2], 0.01)       0.857895       0.857895\n",
       "24     ([22, 4, 2, 2], 0.1)       0.884211       0.886771\n",
       "25    ([22, 4, 2, 2], 0.25)       0.873684       0.876788\n",
       "26    ([22, 4, 2, 2], 0.75)       0.852632       0.852632\n",
       "27       ([22, 4, 2, 2], 0)       0.857895       0.857895"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge accuracy and f1_score DataFrames on 'Architecture, Lambda'\n",
    "merged_df = pd.merge(accuracy_df, f1_score_df, on='Architecture, Lambda')\n",
    "\n",
    "# Merge the merged DataFrame with J_cost_df on 'Architecture, Lambda'\n",
    "final_df = pd.merge(merged_df, J_cost_df, on='Architecture, Lambda')\n",
    "\n",
    "# Rename columns for clarity\n",
    "merged_df.columns = ['Architecture, Lambda', 'Mean Accuracy', 'Mean F1 Score']\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f36b6d",
   "metadata": {},
   "source": [
    "## TO BE RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "edadf100",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Function to generate mini-batches\n",
    "def generate_mini_batches(X, y, batch_size):\n",
    "    num_samples = X.shape[0]\n",
    "    num_batches = num_samples // batch_size\n",
    "    mini_batches = []\n",
    "    shuffled_indices = np.random.permutation(num_samples)\n",
    "    X_shuffled = X[shuffled_indices]\n",
    "    y_shuffled = y[shuffled_indices]\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        mini_batches.append((X_shuffled[start_idx:end_idx], y_shuffled[start_idx:end_idx]))\n",
    "    if num_samples % batch_size != 0:\n",
    "        mini_batches.append((X_shuffled[num_batches*batch_size:], y_shuffled[num_batches*batch_size:]))\n",
    "    return mini_batches\n",
    "\n",
    "\n",
    "def train_mini_batch(X_train, y_train, model, learning_rate, batch_size, max_iterations, epsilon):\n",
    "    training_errors = []\n",
    "    for iteration in range(max_iterations):\n",
    "        mini_batches = generate_mini_batches(X_train, y_train, batch_size)\n",
    "        for mini_batch in mini_batches:\n",
    "            X_mini_batch, y_mini_batch = mini_batch\n",
    "            J = model.train(X_mini_batch, y_mini_batch, learning_rate=learning_rate, lam=0.001, max_iterations=100, epsilon=epsilon)\n",
    "        training_cost = np.mean(np.square(model.forward_pass(X_train)[-1] - y_train))  # Compute training cost\n",
    "        training_errors.append(training_cost)\n",
    "        #print(f\"Iteration {iteration+1}, Training Cost: {training_cost}\")\n",
    "#         # Check for convergence\n",
    "#         if training_cost < epsilon:\n",
    "#             print(f\"Converged at training cost :{training_cost} while Epsilon:{epsilon} \")\n",
    "#             break\n",
    "    return training_errors\n",
    "\n",
    "# Plot learning curve\n",
    "def plot_learning_curve(training_errors, step_size):\n",
    "    iterations = range(1, len(training_errors) + 1)\n",
    "    plt.plot(iterations, training_errors, label='Training Error')\n",
    "    plt.title('Learning Curve')\n",
    "    plt.xlabel('Number of Training Samples')\n",
    "    plt.ylabel('J values')\n",
    "    plt.xticks(np.arange(1, len(training_errors) + 1, step=step_size))\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "812823a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_normalized:       MDVP:Fo(Hz)  MDVP:Fhi(Hz)  MDVP:Flo(Hz)  MDVP:Jitter(%)  \\\n",
      "90      0.299787      0.097561     -0.882122        0.248060   \n",
      "116     0.096657      2.689690     -1.022275       -0.302017   \n",
      "56     -1.061231     -0.723650     -0.298427        0.334914   \n",
      "180    -0.091771     -0.368893      0.654494       -0.467454   \n",
      "20     -0.009220     -0.343722     -1.164317        0.450720   \n",
      "..           ...           ...           ...             ...   \n",
      "130    -0.822106     -0.755692     -0.371245       -0.268930   \n",
      "111     1.315052      0.254339      1.904999       -0.026979   \n",
      "115    -0.057084      3.241266     -1.088228        0.483807   \n",
      "108    -0.054250     -0.435759      0.380845       -0.926541   \n",
      "97     -0.707121     -0.582521     -0.003171        1.360621   \n",
      "\n",
      "     MDVP:Jitter(Abs)  MDVP:RAP  MDVP:PPQ  Jitter:DDP  MDVP:Shimmer  \\\n",
      "90          -0.113985  0.190392  0.393780    0.190311      1.950758   \n",
      "116         -0.401899 -0.394037 -0.500150   -0.394117     -0.804376   \n",
      "56           0.749759  0.072155  0.615446    0.072074      0.395612   \n",
      "180         -0.401899 -0.421063 -0.405669   -0.420016     -0.223257   \n",
      "20           0.173930  0.328898  0.382879    0.329943      0.446121   \n",
      "..                ...       ...       ...         ...           ...   \n",
      "130         -0.113985 -0.208236 -0.387500   -0.207190     -0.828833   \n",
      "111         -0.401899  0.126206 -0.020480    0.127251     -0.643279   \n",
      "115          0.461845  0.247821  0.146678    0.246614     -0.599682   \n",
      "108         -0.977729 -0.863607 -0.903509   -0.863686     -1.035123   \n",
      "97           1.613504  1.393032  1.011537    1.391822      0.486528   \n",
      "\n",
      "     MDVP:Shimmer(dB)  ...  MDVP:APQ  Shimmer:DDA       NHR       HNR  \\\n",
      "90           1.809621  ...  1.600790     2.057126  1.547896 -2.297474   \n",
      "116         -0.690675  ... -0.648485    -0.948379 -0.230861  1.012601   \n",
      "56           0.266229  ...  0.233004     0.460071 -0.375969 -0.105105   \n",
      "180         -0.232802  ... -0.153905    -0.242014 -0.464771  0.282490   \n",
      "20           0.235361  ...  0.974282     0.098989  0.343868 -0.985405   \n",
      "..                ...  ...       ...          ...       ...       ...   \n",
      "130         -0.809002  ... -0.718294    -0.852266 -0.216970  0.467340   \n",
      "111         -0.654662  ... -0.651443    -0.607046 -0.410448  0.118029   \n",
      "115         -0.243091  ... -0.672741    -0.760432  0.884614  0.409348   \n",
      "108         -0.973630  ... -0.837206    -1.097815 -0.557293  1.821770   \n",
      "97           0.307386  ...  0.402202     0.561780  1.405516 -1.483321   \n",
      "\n",
      "         RPDE       DFA   spread1   spread2        D2       PPE  \n",
      "90   1.493848  0.272965  1.080930  1.955986  2.450812  1.061325  \n",
      "116 -0.460454 -1.164553 -0.337349  0.562192  0.678263 -0.405456  \n",
      "56   0.417126  1.870953  1.145792  0.141693 -1.201851  1.429439  \n",
      "180 -1.409595  0.746841 -0.530665 -0.520393 -0.308002 -0.694716  \n",
      "20   1.558618 -0.253870  1.461230  0.433381  0.937320  1.767082  \n",
      "..        ...       ...       ...       ...       ...       ...  \n",
      "130 -0.302634  0.107729 -0.414655 -0.275032  0.413544 -0.407492  \n",
      "111 -1.547516 -0.102059 -0.723767 -0.506449  0.315623 -0.780546  \n",
      "115 -0.876148 -1.004299  0.929876  0.421180  0.839485  0.754888  \n",
      "108 -1.805322 -0.761554 -0.969970 -0.791238 -0.222497 -1.017062  \n",
      "97   1.263937 -0.065479  1.530570  0.466449  0.759525  1.491539  \n",
      "\n",
      "[195 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define your neural network model and parameters\n",
    "model = NeuralNetwork([X.shape[1], 8, y_encoded.shape[1]])  # Your desired architecture\n",
    "learning_rate = 0.01\n",
    "batch_size = 200\n",
    "max_iterations = 100\n",
    "epsilon = 0.005\n",
    "\n",
    "\n",
    "# Standardize features\n",
    "mean = np.mean(X, axis=0)\n",
    "std = np.std(X, axis=0)\n",
    "X_normalized = (X - mean) / std\n",
    "\n",
    "print(\"X_normalized: \",X_normalized)\n",
    "\n",
    "# Train the model using mini-batch gradient descent\n",
    "training_errors = train_mini_batch(X_normalized.to_numpy(), y_encoded, model, learning_rate, batch_size, max_iterations, epsilon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9a5c990e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the learning curve\n",
    "step_size = 25\n",
    "plot_learning_curve(training_errors, step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb7713b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
