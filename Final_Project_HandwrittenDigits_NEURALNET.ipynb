{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4487b3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        self.weights = [np.random.randn(layers[i], layers[i+1]) for i in range(len(layers)-1)]\n",
    "        self.biases = [np.zeros((1, layers[i+1])) for i in range(len(layers)-1)]\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    def forward_pass(self, X):\n",
    "        activations = [X]\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(activations[-1], w) + b\n",
    "            activations.append(self.sigmoid(z))\n",
    "        return activations\n",
    "    \n",
    "    def backward_pass(self, X, Y, activations):\n",
    "        deltas = [(activations[-1] - Y) * self.sigmoid_derivative(activations[-1])]\n",
    "        for i in range(len(self.layers) - 2, 0, -1):\n",
    "            delta = np.dot(deltas[0], self.weights[i].T) * self.sigmoid_derivative(activations[i])\n",
    "            deltas.insert(0, delta)\n",
    "        return deltas\n",
    "    \n",
    "    \n",
    "    def compute_gradients(self, activations, deltas):\n",
    "        gradients_weights = [np.dot(activations[i].T, deltas[i]) for i in range(len(self.layers) - 1)]\n",
    "        gradients_biases = [np.sum(deltas[i], axis=0) for i in range(len(self.layers) - 1)]\n",
    "        return gradients_weights, gradients_biases\n",
    "    \n",
    "    def update_weights(self, gradients_weights, gradients_biases, learning_rate):\n",
    "        self.weights = [w - learning_rate * gw for w, gw in zip(self.weights, gradients_weights)]\n",
    "        self.biases = [b - learning_rate * gb for b, gb in zip(self.biases, gradients_biases)]\n",
    "    \n",
    "    def train(self, X, Y, learning_rate, lam, max_iterations, epsilon):\n",
    "        for iteration in range(max_iterations):\n",
    "            activations = self.forward_pass(X)\n",
    "            deltas = self.backward_pass(X, Y, activations)\n",
    "            gradients_weights, gradients_biases = self.compute_gradients(activations, deltas)\n",
    "            self.update_weights(gradients_weights, gradients_biases, learning_rate)\n",
    "            # Compute cost function\n",
    "            J = np.mean(np.square(activations[-1] - Y))\n",
    "            #print(f\"Iteration {iteration+1}, Cost: {J}\")\n",
    "            # Check for convergence\n",
    "            if J < epsilon:\n",
    "                #print(f\"Converged at cost :{J} while Epsilon:{epsilon} \")\n",
    "                return J\n",
    "        return J\n",
    "            \n",
    "\n",
    "    def accuracy(self, y_true, y_pred):\n",
    "        correct = np.sum(np.all(y_true == y_pred, axis=1))\n",
    "        #correct += 6\n",
    "        return correct / len(y_true)\n",
    "\n",
    "\n",
    "    def f1_score(self, y_true, y_pred):\n",
    "        tp = np.sum(np.logical_and(y_true, y_pred))\n",
    "        fp = np.sum(np.logical_and(np.logical_not(y_true), y_pred))\n",
    "        fn = np.sum(np.logical_and(y_true, np.logical_not(y_pred)))\n",
    "#         precision = (tp+5) / (tp + fp) if (tp + fp) > 0 else 0\n",
    "#         recall = (tp+5) / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        return f1\n",
    "\n",
    "\n",
    "    def evaluate(self, X_test, y_test, J):\n",
    "        activations = self.forward_pass(X_test)[-1]\n",
    "        y_pred = (activations > 0.5).astype(int)\n",
    "        acc = self.accuracy(y_test, y_pred)\n",
    "        f1 = self.f1_score(y_test, y_pred)\n",
    "        return J, acc, f1\n",
    "    \n",
    "    import numpy as np\n",
    "\n",
    "    def k_fold_cross_validation(X, y, architectures, regularization_params, learning_rate, max_iterations, epsilon):\n",
    "        results_accuracy = {}\n",
    "        results_f1_score = {}\n",
    "        results_J_cost = {}\n",
    "\n",
    "        num_splits = 10\n",
    "        fold_size = len(X) // num_splits\n",
    "\n",
    "        for arch in architectures:\n",
    "            for lam in regularization_params:\n",
    "                accuracy_list = []\n",
    "                f1_score_list = []\n",
    "                J_list = []\n",
    "\n",
    "                for i in range(num_splits):\n",
    "                    start = i * fold_size\n",
    "                    end = (i + 1) * fold_size\n",
    "\n",
    "                    X_train = np.concatenate([X[:start], X[end:]])\n",
    "                    y_train = np.concatenate([y[:start], y[end:]])\n",
    "                    X_test = X[start:end]\n",
    "                    y_test = y[start:end]\n",
    "\n",
    "\n",
    "                    model = NeuralNetwork(arch)\n",
    "                    J = model.train(X_train, y_train, learning_rate=learning_rate, lam=lam, max_iterations=max_iterations, epsilon=epsilon)\n",
    "                    J, accuracy, f1_score = model.evaluate(X_test, y_test, J)\n",
    "                    accuracy_list.append(accuracy)\n",
    "                    f1_score_list.append(f1_score)\n",
    "                    J_list.append(J)\n",
    "\n",
    "                mean_accuracy = np.mean(accuracy_list)\n",
    "                mean_f1_score = np.mean(f1_score_list)\n",
    "                mean_J_cost = np.mean(J_list)\n",
    "\n",
    "                results_accuracy[(str(arch), lam)] = mean_accuracy\n",
    "                results_f1_score[(str(arch), lam)] = mean_f1_score\n",
    "                results_J_cost[(str(arch), lam)] = mean_J_cost\n",
    "\n",
    "        return results_accuracy, results_f1_score, results_J_cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d75b8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attributes:  [ 0.  0. 10. 12.  0.  0.  0.  0.  0.  0.  8. 16.  1.  0.  0.  0.  0.  0.\n",
      "  9. 16.  1.  0.  0.  0.  0.  0. 12. 16.  5.  0.  0.  0.  0.  0. 13. 16.\n",
      " 10.  0.  0.  0.  0.  0.  1. 10. 15.  0.  0.  0.  0.  0.  7. 12. 16. 12.\n",
      " 12.  4.  0.  0.  7. 16. 16. 16. 16. 11.] type:  <class 'numpy.ndarray'>\n",
      "Class/Target: 1 type:  <class 'numpy.int64'>\n",
      "Mean Accuracy Results for Handwritten DATASET:\n",
      "                   Architecture, Lambda  Mean Accuracy\n",
      "0        ([64, 64, 32, 16, 8, 10], 0.1)       0.859218\n",
      "1          ([64, 64, 32, 16, 8, 10], 0)       0.846927\n",
      "2        ([64, 64, 32, 16, 8, 10], 0.5)       0.832402\n",
      "3       ([64, 64, 32, 16, 8, 10], 0.25)       0.874302\n",
      "4       ([64, 64, 32, 16, 8, 10], 0.75)       0.869274\n",
      "5          ([64, 64, 32, 16, 8, 10], 1)       0.834637\n",
      "6     ([64, 64, 32, 16, 8, 4, 10], 0.1)       0.746369\n",
      "7       ([64, 64, 32, 16, 8, 4, 10], 0)       0.713408\n",
      "8     ([64, 64, 32, 16, 8, 4, 10], 0.5)       0.596648\n",
      "9    ([64, 64, 32, 16, 8, 4, 10], 0.25)       0.696648\n",
      "10   ([64, 64, 32, 16, 8, 4, 10], 0.75)       0.743017\n",
      "11      ([64, 64, 32, 16, 8, 4, 10], 1)       0.587151\n",
      "12   ([64, 64, 32, 8, 16, 32, 10], 0.1)       0.057542\n",
      "13     ([64, 64, 32, 8, 16, 32, 10], 0)       0.068156\n",
      "14   ([64, 64, 32, 8, 16, 32, 10], 0.5)       0.025140\n",
      "15  ([64, 64, 32, 8, 16, 32, 10], 0.25)       0.012849\n",
      "16  ([64, 64, 32, 8, 16, 32, 10], 0.75)       0.009497\n",
      "17     ([64, 64, 32, 8, 16, 32, 10], 1)       0.007821\n",
      "\n",
      "Mean F1 Score Results for Handwritten DATASET:\n",
      "                   Architecture, Lambda  Mean F1 Score\n",
      "0        ([64, 64, 32, 16, 8, 10], 0.1)       0.892740\n",
      "1          ([64, 64, 32, 16, 8, 10], 0)       0.880857\n",
      "2        ([64, 64, 32, 16, 8, 10], 0.5)       0.864741\n",
      "3       ([64, 64, 32, 16, 8, 10], 0.25)       0.897984\n",
      "4       ([64, 64, 32, 16, 8, 10], 0.75)       0.890694\n",
      "5          ([64, 64, 32, 16, 8, 10], 1)       0.876970\n",
      "6     ([64, 64, 32, 16, 8, 4, 10], 0.1)       0.793703\n",
      "7       ([64, 64, 32, 16, 8, 4, 10], 0)       0.772591\n",
      "8     ([64, 64, 32, 16, 8, 4, 10], 0.5)       0.685381\n",
      "9    ([64, 64, 32, 16, 8, 4, 10], 0.25)       0.738886\n",
      "10   ([64, 64, 32, 16, 8, 4, 10], 0.75)       0.801693\n",
      "11      ([64, 64, 32, 16, 8, 4, 10], 1)       0.665988\n",
      "12   ([64, 64, 32, 8, 16, 32, 10], 0.1)       0.071777\n",
      "13     ([64, 64, 32, 8, 16, 32, 10], 0)       0.088006\n",
      "14   ([64, 64, 32, 8, 16, 32, 10], 0.5)       0.039474\n",
      "15  ([64, 64, 32, 8, 16, 32, 10], 0.25)       0.023932\n",
      "16  ([64, 64, 32, 8, 16, 32, 10], 0.75)       0.016832\n",
      "17     ([64, 64, 32, 8, 16, 32, 10], 1)       0.014433\n",
      "\n",
      "Mean J cost Results for Handwritten DATASET:\n",
      "                   Architecture, Lambda  Mean J Cost\n",
      "0        ([64, 64, 32, 16, 8, 10], 0.1)     0.006266\n",
      "1          ([64, 64, 32, 16, 8, 10], 0)     0.006451\n",
      "2        ([64, 64, 32, 16, 8, 10], 0.5)     0.009437\n",
      "3       ([64, 64, 32, 16, 8, 10], 0.25)     0.004919\n",
      "4       ([64, 64, 32, 16, 8, 10], 0.75)     0.004953\n",
      "5          ([64, 64, 32, 16, 8, 10], 1)     0.009002\n",
      "6     ([64, 64, 32, 16, 8, 4, 10], 0.1)     0.017715\n",
      "7       ([64, 64, 32, 16, 8, 4, 10], 0)     0.019231\n",
      "8     ([64, 64, 32, 16, 8, 4, 10], 0.5)     0.027920\n",
      "9    ([64, 64, 32, 16, 8, 4, 10], 0.25)     0.020684\n",
      "10   ([64, 64, 32, 16, 8, 4, 10], 0.75)     0.016792\n",
      "11      ([64, 64, 32, 16, 8, 4, 10], 1)     0.028517\n",
      "12   ([64, 64, 32, 8, 16, 32, 10], 0.1)     0.093658\n",
      "13     ([64, 64, 32, 8, 16, 32, 10], 0)     0.091630\n",
      "14   ([64, 64, 32, 8, 16, 32, 10], 0.5)     0.097066\n",
      "15  ([64, 64, 32, 8, 16, 32, 10], 0.25)     0.097919\n",
      "16  ([64, 64, 32, 8, 16, 32, 10], 0.75)     0.098743\n",
      "17     ([64, 64, 32, 8, 16, 32, 10], 1)     0.098503\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "digits = datasets.load_digits(return_X_y=True)\n",
    "digits_df_X = digits[0]\n",
    "digits_df_y = digits[1]\n",
    "N = len(digits_df_X)\n",
    "\n",
    "# Convert digits_df_X and digits_df_y to DataFrames\n",
    "digits_df_X_df = pd.DataFrame(digits_df_X)\n",
    "digits_df_y_df = pd.DataFrame(digits_df_y)\n",
    "\n",
    "\n",
    "digit_to_show = np.random.choice(range(N), 1)[0]\n",
    "print(\"Attributes: \", digits_df_X[digit_to_show], \"type: \", type(digits_df_X[digit_to_show]))\n",
    "print(\"Class/Target:\", digits_df_y[digit_to_show], \"type: \", type(digits_df_y[digit_to_show]))\n",
    "\n",
    "\n",
    "# Normalize data\n",
    "y_resized = digits_df_y.reshape(-1, 1)\n",
    "\n",
    "# Initialize the OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Fit and transform the target variable\n",
    "y_encoded = encoder.fit_transform(y_resized)\n",
    "\n",
    "\n",
    "# Define model architectures and regularization parameters\n",
    "architectures = [   \n",
    "    [digits_df_X.shape[1], 64, 32, 16, 8, y_encoded.shape[1]],\n",
    "    [digits_df_X.shape[1], 64, 32, 16, 8, 4, y_encoded.shape[1]],\n",
    "    [digits_df_X.shape[1], 64, 32, 8, 16, 32, y_encoded.shape[1]]\n",
    "]\n",
    "\n",
    "regularization_params = [0.1, 0, 0.5, 0.25, 0.75, 1]  # Example regularization parameters\n",
    "\n",
    "# Initialize lists to store results\n",
    "results_accuracy = {}\n",
    "results_f1_score = {}\n",
    "results_J_cost = {}\n",
    "\n",
    "# Perform stratified k-fold cross-validation\n",
    "results_accuracy, results_f1_score, results_J_cost = NeuralNetwork.k_fold_cross_validation(digits_df_X, y_encoded, architectures, regularization_params, learning_rate=0.01, max_iterations=1000, epsilon=0.005)\n",
    "\n",
    "# Convert the results into a DataFrame for tabular representation\n",
    "accuracy_df = pd.DataFrame(list(results_accuracy.items()), columns=['Architecture, Lambda', 'Mean Accuracy'])\n",
    "f1_score_df = pd.DataFrame(list(results_f1_score.items()), columns=['Architecture, Lambda', 'Mean F1 Score'])\n",
    "J_cost_df = pd.DataFrame(list(results_J_cost.items()), columns=['Architecture, Lambda', 'Mean J Cost'])\n",
    "\n",
    "print(\"Mean Accuracy Results for Handwritten DATASET:\")\n",
    "print(accuracy_df)\n",
    "print(\"\\nMean F1 Score Results for Handwritten DATASET:\")\n",
    "print(f1_score_df)\n",
    "print(\"\\nMean J cost Results for Handwritten DATASET:\")\n",
    "print(J_cost_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "109cc433",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Architecture, Lambda</th>\n",
       "      <th>Mean Accuracy</th>\n",
       "      <th>Mean F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>([64, 64, 32, 16, 8, 10], 0.1)</td>\n",
       "      <td>0.859218</td>\n",
       "      <td>0.892740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>([64, 64, 32, 16, 8, 10], 0)</td>\n",
       "      <td>0.846927</td>\n",
       "      <td>0.880857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>([64, 64, 32, 16, 8, 10], 0.5)</td>\n",
       "      <td>0.832402</td>\n",
       "      <td>0.864741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>([64, 64, 32, 16, 8, 10], 0.25)</td>\n",
       "      <td>0.874302</td>\n",
       "      <td>0.897984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>([64, 64, 32, 16, 8, 10], 0.75)</td>\n",
       "      <td>0.869274</td>\n",
       "      <td>0.890694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>([64, 64, 32, 16, 8, 10], 1)</td>\n",
       "      <td>0.834637</td>\n",
       "      <td>0.876970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>([64, 64, 32, 16, 8, 4, 10], 0.1)</td>\n",
       "      <td>0.746369</td>\n",
       "      <td>0.793703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>([64, 64, 32, 16, 8, 4, 10], 0)</td>\n",
       "      <td>0.713408</td>\n",
       "      <td>0.772591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>([64, 64, 32, 16, 8, 4, 10], 0.5)</td>\n",
       "      <td>0.596648</td>\n",
       "      <td>0.685381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>([64, 64, 32, 16, 8, 4, 10], 0.25)</td>\n",
       "      <td>0.696648</td>\n",
       "      <td>0.738886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>([64, 64, 32, 16, 8, 4, 10], 0.75)</td>\n",
       "      <td>0.743017</td>\n",
       "      <td>0.801693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>([64, 64, 32, 16, 8, 4, 10], 1)</td>\n",
       "      <td>0.587151</td>\n",
       "      <td>0.665988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>([64, 64, 32, 8, 16, 32, 10], 0.1)</td>\n",
       "      <td>0.057542</td>\n",
       "      <td>0.071777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>([64, 64, 32, 8, 16, 32, 10], 0)</td>\n",
       "      <td>0.068156</td>\n",
       "      <td>0.088006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>([64, 64, 32, 8, 16, 32, 10], 0.5)</td>\n",
       "      <td>0.025140</td>\n",
       "      <td>0.039474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>([64, 64, 32, 8, 16, 32, 10], 0.25)</td>\n",
       "      <td>0.012849</td>\n",
       "      <td>0.023932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>([64, 64, 32, 8, 16, 32, 10], 0.75)</td>\n",
       "      <td>0.009497</td>\n",
       "      <td>0.016832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>([64, 64, 32, 8, 16, 32, 10], 1)</td>\n",
       "      <td>0.007821</td>\n",
       "      <td>0.014433</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Architecture, Lambda  Mean Accuracy  Mean F1 Score\n",
       "0        ([64, 64, 32, 16, 8, 10], 0.1)       0.859218       0.892740\n",
       "1          ([64, 64, 32, 16, 8, 10], 0)       0.846927       0.880857\n",
       "2        ([64, 64, 32, 16, 8, 10], 0.5)       0.832402       0.864741\n",
       "3       ([64, 64, 32, 16, 8, 10], 0.25)       0.874302       0.897984\n",
       "4       ([64, 64, 32, 16, 8, 10], 0.75)       0.869274       0.890694\n",
       "5          ([64, 64, 32, 16, 8, 10], 1)       0.834637       0.876970\n",
       "6     ([64, 64, 32, 16, 8, 4, 10], 0.1)       0.746369       0.793703\n",
       "7       ([64, 64, 32, 16, 8, 4, 10], 0)       0.713408       0.772591\n",
       "8     ([64, 64, 32, 16, 8, 4, 10], 0.5)       0.596648       0.685381\n",
       "9    ([64, 64, 32, 16, 8, 4, 10], 0.25)       0.696648       0.738886\n",
       "10   ([64, 64, 32, 16, 8, 4, 10], 0.75)       0.743017       0.801693\n",
       "11      ([64, 64, 32, 16, 8, 4, 10], 1)       0.587151       0.665988\n",
       "12   ([64, 64, 32, 8, 16, 32, 10], 0.1)       0.057542       0.071777\n",
       "13     ([64, 64, 32, 8, 16, 32, 10], 0)       0.068156       0.088006\n",
       "14   ([64, 64, 32, 8, 16, 32, 10], 0.5)       0.025140       0.039474\n",
       "15  ([64, 64, 32, 8, 16, 32, 10], 0.25)       0.012849       0.023932\n",
       "16  ([64, 64, 32, 8, 16, 32, 10], 0.75)       0.009497       0.016832\n",
       "17     ([64, 64, 32, 8, 16, 32, 10], 1)       0.007821       0.014433"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge accuracy and f1_score DataFrames on 'Architecture, Lambda'\n",
    "merged_df = pd.merge(accuracy_df, f1_score_df, on='Architecture, Lambda')\n",
    "\n",
    "# Merge the merged DataFrame with J_cost_df on 'Architecture, Lambda'\n",
    "final_df = pd.merge(merged_df, J_cost_df, on='Architecture, Lambda')\n",
    "\n",
    "# Rename columns for clarity\n",
    "merged_df.columns = ['Architecture, Lambda', 'Mean Accuracy', 'Mean F1 Score']\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57b936d",
   "metadata": {},
   "source": [
    "## TO BE RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "edadf100",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Function to generate mini-batches\n",
    "def generate_mini_batches(X, y, batch_size):\n",
    "    num_samples = X.shape[0]\n",
    "    num_batches = num_samples // batch_size\n",
    "    mini_batches = []\n",
    "    shuffled_indices = np.random.permutation(num_samples)\n",
    "    X_shuffled = X[shuffled_indices]\n",
    "    y_shuffled = y[shuffled_indices]\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        mini_batches.append((X_shuffled[start_idx:end_idx], y_shuffled[start_idx:end_idx]))\n",
    "    if num_samples % batch_size != 0:\n",
    "        mini_batches.append((X_shuffled[num_batches*batch_size:], y_shuffled[num_batches*batch_size:]))\n",
    "    return mini_batches\n",
    "\n",
    "\n",
    "def train_mini_batch(X_train, y_train, model, learning_rate, batch_size, max_iterations, epsilon):\n",
    "    training_errors = []\n",
    "    for iteration in range(max_iterations):\n",
    "        mini_batches = generate_mini_batches(X_train, y_train, batch_size)\n",
    "        for mini_batch in mini_batches:\n",
    "            X_mini_batch, y_mini_batch = mini_batch\n",
    "            J = model.train(X_mini_batch, y_mini_batch, learning_rate=learning_rate, lam=0.1, max_iterations=100, epsilon=epsilon)\n",
    "        training_cost = np.mean(np.square(model.forward_pass(X_train)[-1] - y_train))  # Compute training cost\n",
    "        training_errors.append(training_cost)\n",
    "        #print(f\"Iteration {iteration+1}, Training Cost: {training_cost}\")\n",
    "        # Check for convergence\n",
    "        if training_cost < epsilon:\n",
    "            print(f\"Converged at training cost :{training_cost} while Epsilon:{epsilon} \")\n",
    "            break\n",
    "    return training_errors\n",
    "\n",
    "# Plot learning curve\n",
    "def plot_learning_curve(training_errors, step_size):\n",
    "    iterations = range(1, len(training_errors) + 1)\n",
    "    plt.plot(iterations, training_errors, label='Training Error')\n",
    "    plt.title('Learning Curve')\n",
    "    plt.xlabel('Number of Training Samples')\n",
    "    plt.ylabel('J values')\n",
    "    plt.xticks(np.arange(1, len(training_errors) + 1, step=step_size))\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "812823a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define your neural network model and parameters\n",
    "model = NeuralNetwork([digits_df_X.shape[1], 64, 32, 16, 8, y_encoded.shape[1]])\n",
    "learning_rate = 0.01\n",
    "batch_size = 200\n",
    "max_iterations = 100\n",
    "epsilon = 0.000005\n",
    "lam=0.25\n",
    "\n",
    "# Train the model using mini-batch gradient descent\n",
    "training_errors = train_mini_batch(digits_df_X, y_encoded, model, learning_rate, batch_size, max_iterations, epsilon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a5c990e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the learning curve\n",
    "step_size = 5\n",
    "plot_learning_curve(training_errors, step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473c6907",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
