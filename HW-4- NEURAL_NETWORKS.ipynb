{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abf87141",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine, fetch_openml\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Wine Dataset\n",
    "wine = pd.read_csv('/Users/noshitha/Downloads/hw4/datasets/hw3_wine.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ae820d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wine = pd.read_csv(\"/Users/noshitha/Downloads/hw3/datasets/hw3_wine.csv\", delimiter=\"\\t\")\n",
    "\n",
    "X_wine = df_wine.iloc[:, 1:]\n",
    "y_wine = df_wine.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d463802",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wine = pd.read_csv(\"/Users/noshitha/Downloads/hw3/datasets/hw3_wine.csv\", delimiter=\"\\t\")\n",
    "\n",
    "X_wine = df_wine.iloc[:, 1:]\n",
    "y_wine = df_wine.iloc[:, 0]\n",
    "# Convert categorical attributes to numerical inputs using one-hot encoding\n",
    "X_wine = pd.get_dummies(df_wine.iloc[:, 1:])  # Exclude the target column\n",
    "y_wine = df_wine.iloc[:, 0]\n",
    "X_wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5de8a690",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       1     2     3     4    5     6     7     8     9    10    11    12  \\\n",
       "0  14.23  1.71  2.43  15.6  127  2.80  3.06  0.28  2.29  5.64  1.04  3.92   \n",
       "1  13.20  1.78  2.14  11.2  100  2.65  2.76  0.26  1.28  4.38  1.05  3.40   \n",
       "2  13.16  2.36  2.67  18.6  101  2.80  3.24  0.30  2.81  5.68  1.03  3.17   \n",
       "3  14.37  1.95  2.50  16.8  113  3.85  3.49  0.24  2.18  7.80  0.86  3.45   \n",
       "4  13.24  2.59  2.87  21.0  118  2.80  2.69  0.39  1.82  4.32  1.04  2.93   \n",
       "\n",
       "     13  \n",
       "0  1065  \n",
       "1  1050  \n",
       "2  1185  \n",
       "3  1480  \n",
       "4   735  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f8ac5fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    1\n",
       "3    1\n",
       "4    1\n",
       "Name: # class, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f1a46561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X):\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        X = X.values\n",
    "    mean = np.mean(X, axis=0)\n",
    "    std = np.std(X, axis=0)\n",
    "    X_normalized = (X - mean) / std\n",
    "    return X_normalized, mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b16aea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.56685714]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        self.weights = [np.random.rand(layers[i], layers[i+1]) for i in range(len(layers)-1)]\n",
    "        self.biases = [np.zeros((1, layers[i+1])) for i in range(len(layers)-1)]\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Forward pass\n",
    "        activations = X\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(activations, w) + b\n",
    "            activations = self.sigmoid(z)\n",
    "        return activations\n",
    "    \n",
    "    def train(self, X, y, learning_rate, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            activations = X\n",
    "            zs = []\n",
    "            for w, b in zip(self.weights, self.biases):\n",
    "                z = np.dot(activations, w) + b\n",
    "                zs.append(z)\n",
    "                activations = self.sigmoid(z)\n",
    "\n",
    "            # Backward pass\n",
    "            error = y - activations\n",
    "            deltas = [error * self.sigmoid_derivative(activations)]\n",
    "            for i in range(len(self.weights) - 1, 0, -1):\n",
    "                delta = np.dot(deltas[-1], self.weights[i].T) * self.sigmoid_derivative(zs[i-1])\n",
    "                deltas.append(delta)\n",
    "            deltas.reverse()\n",
    "\n",
    "            # Weight update\n",
    "            for i in range(len(self.weights)):\n",
    "                activations = np.atleast_2d(activations)\n",
    "                self.weights[i] += learning_rate * np.dot(activations.T, deltas[i])\n",
    "                self.biases[i] += learning_rate * deltas[i].sum(axis=0, keepdims=True)\n",
    "\n",
    "# Example usage\n",
    "nn = NeuralNetwork([2, 3, 1])  # Network with 2 input, 3 hidden, and 1 output neuron\n",
    "X = np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])  # Sample input data\n",
    "y = np.array([[0.7], [0.9], [0.1]])  # Desired output\n",
    "nn.train(X, y, learning_rate=0.1, epochs=1000)  # Train the network\n",
    "\n",
    "# Make prediction on new data\n",
    "prediction = nn.predict(np.array([[0.8, 0.9]]))\n",
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76aa56ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([0.13, 0.42])\n",
    "y_train = np.array([0.9, 0.23])\n",
    "nn.train(X_train, y_train, learning_rate=0.1, epochs=1000)  # Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526cce80",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_x = nn.predict(np.array([[0.8, 0.9]]))\n",
    "print(prediction_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d3e269",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7566410a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e9295f4",
   "metadata": {},
   "source": [
    "## VALIDATION -  1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d36d3bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid activation function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Forward propagation\n",
    "def forward_propagation(x, theta1, theta2):\n",
    "    a1 = np.concatenate((np.ones((x.shape[0], 1)), x), axis=1)  # Add bias to input layer\n",
    "    z2 = np.dot(a1, theta1.T)\n",
    "    a2 = np.concatenate((np.ones((z2.shape[0], 1)), sigmoid(z2)), axis=1)  # Add bias to hidden layer\n",
    "    z3 = np.dot(a2, theta2.T)\n",
    "    a3 = sigmoid(z3)\n",
    "    return a1, z2, a2, z3, a3\n",
    "\n",
    "# Cost function\n",
    "def compute_cost(y, a3):\n",
    "    m = y.shape[0]\n",
    "    J = -1/m * np.sum(y * np.log(a3) + (1 - y) * np.log(1 - a3))\n",
    "    return J\n",
    "\n",
    "# Backpropagation\n",
    "def backpropagation(x, y, theta1, theta2, a1, z2, a2, z3, a3, lam):\n",
    "    m = y.shape[0]\n",
    "    \n",
    "    # Compute deltas\n",
    "    delta3 = a3 - y\n",
    "    delta2 = np.dot(delta3, theta2) * (a2 * (1 - a2))\n",
    "    delta2 = delta2[:, 1:]  # Remove delta for bias neuron\n",
    "    \n",
    "    # Compute gradients\n",
    "    grad1 = np.dot(delta2.T, a1)\n",
    "    grad2 = np.dot(delta3.T, a2)\n",
    "    \n",
    "    # Regularization (excluding bias terms)\n",
    "    grad1[:, 1:] += (lam / m) * theta1[:, 1:]\n",
    "    grad2[:, 1:] += (lam / m) * theta2[:, 1:]\n",
    "    \n",
    "    # Average gradients\n",
    "    grad1 /= m\n",
    "    grad2 /= m\n",
    "    \n",
    "    return grad1, grad2\n",
    "\n",
    "# Update weights\n",
    "def update_weights(theta1, theta2, grad1, grad2, alpha):\n",
    "    theta1 -= alpha * grad1\n",
    "    theta2 -= alpha * grad2\n",
    "    return theta1, theta2\n",
    "\n",
    "# Function to print required outputs\n",
    "def print_outputs(theta1, theta2, grad1, grad2, a3, y):\n",
    "    print(\"Theta1:\")\n",
    "    print(theta1)\n",
    "    print(\"\\nTheta2:\")\n",
    "    print(theta2)\n",
    "    print(\"\\nRegularized Gradients of Theta1:\")\n",
    "    print(grad1)\n",
    "    print(\"\\nRegularized Gradients of Theta2:\")\n",
    "    print(grad2)\n",
    "    print(\"\\nFinal Prediction of Our Network:\")\n",
    "    print(a3)\n",
    "    print(\"\\nExpected Prediction:\")\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "767bfa90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_1():\n",
    "    # Initialize network architecture\n",
    "    layers = [1, 2, 1]\n",
    "    theta1 = np.array([[0.4, 0.1], [0.3, 0.2]])\n",
    "    theta2 = np.array([[0.7, 0.5, 0.6]])\n",
    "\n",
    "    # Training instances\n",
    "    x = np.array([[0.13], [0.42]])\n",
    "    y = np.array([[0.9], [0.23]])\n",
    "\n",
    "    # Regularization parameter\n",
    "    lam = 0.000\n",
    "\n",
    "    # Hyperparameters\n",
    "    alpha = 0.01  # Learning rate\n",
    "\n",
    "    # Forward propagation\n",
    "    a1, z2, a2, z3, a3 = forward_propagation(x, theta1, theta2)\n",
    "    \n",
    "    print(\"a1:\",a1)\n",
    "    print(\"z2:\",z2)\n",
    "    print(\"a2:\",a2)\n",
    "    print(\"z3:\",z3)\n",
    "    print(\"a3:\",a3)\n",
    "    \n",
    "    # Calculate cost\n",
    "    J = compute_cost(y, a3)\n",
    "    \n",
    "    print(\"Regularization cost: \",J)\n",
    "    \n",
    "    # Backpropagation\n",
    "    grad1, grad2 = backpropagation(x, y, theta1, theta2, a1, z2, a2, z3, a3, lam)\n",
    "\n",
    "    # Update weights\n",
    "    theta1, theta2 = update_weights(theta1, theta2, grad1, grad2, alpha)\n",
    "\n",
    "    # Print outputs\n",
    "    print_outputs(theta1, theta2, grad1, grad2, a3, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "015ec96d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def example_2():\n",
    "    # Initialize network architecture\n",
    "    layers = [2, 4, 3, 2]\n",
    "    theta1 = np.array([[0.42, 0.15, 0.4],\n",
    "                       [0.72, 0.1, 0.54],\n",
    "                       [0.01, 0.19, 0.42],\n",
    "                       [0.3, 0.35, 0.68]])\n",
    "    theta2 = np.array([[0.21, 0.67, 0.14, 0.96, 0.87],\n",
    "                       [0.87, 0.42, 0.2, 0.32, 0.89],\n",
    "                       [0.03, 0.56, 0.8, 0.69, 0.09]])\n",
    "    theta3 = np.array([[0.04, 0.87, 0.42, 0.53],\n",
    "                       [0.17, 0.1, 0.95, 0.69]])\n",
    "\n",
    "    # Training instances\n",
    "    x = np.array([[0.32, 0.68], [0.83, 0.02]])\n",
    "    y = np.array([[0.75, 0.98], [0.75, 0.28]])\n",
    "\n",
    "    # Regularization parameter\n",
    "    lam = 0.25\n",
    "\n",
    "    # Hyperparameters\n",
    "    alpha = 0.01  # Learning rate\n",
    "\n",
    "    # Forward propagation\n",
    "    a1, z2, a2, z3, a3, z4, a4 = forward_propagation(x, theta1, theta2, theta3)\n",
    "    \n",
    "    # Calculate cost\n",
    "    J = compute_cost(y, a4)\n",
    "    \n",
    "    # Backpropagation\n",
    "    grad1, grad2, grad3 = backpropagation(x, y, theta1, theta2, theta3, a1, z2, a2, z3, a3, z4, a4, lam)\n",
    "\n",
    "    # Update weights\n",
    "    theta1, theta2, theta3 = update_weights(theta1, theta2, theta3, grad1, grad2, grad3, alpha)\n",
    "\n",
    "    # Print outputs\n",
    "    print_outputs(theta1, theta2, theta3, grad1, grad2, grad3, a4, y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c0378f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXAMPLE WITH GIVEN WEIGHTS :\n",
      "a1: [[1.   0.13]\n",
      " [1.   0.42]]\n",
      "z2: [[0.413 0.326]\n",
      " [0.442 0.384]]\n",
      "a2: [[1.         0.601807   0.5807858 ]\n",
      " [1.         0.60873549 0.59483749]]\n",
      "z3: [[1.34937498]\n",
      " [1.36127024]]\n",
      "a3: [[0.79402743]\n",
      " [0.79596607]]\n",
      "Regularization cost:  0.8209757904998143\n",
      "Theta1:\n",
      "[[0.39972649 0.09986671]\n",
      " [0.2996682  0.1998382 ]]\n",
      "\n",
      "Theta2:\n",
      "[[0.69770003 0.49859626 0.59862445]]\n",
      "\n",
      "Regularized Gradients of Theta1:\n",
      "[[0.02735127 0.01332866]\n",
      " [0.03317988 0.01618028]]\n",
      "\n",
      "Regularized Gradients of Theta2:\n",
      "[[0.22999675 0.1403743  0.13755523]]\n",
      "\n",
      "Final Prediction of Our Network:\n",
      "[[0.79402743]\n",
      " [0.79596607]]\n",
      "\n",
      "Expected Prediction:\n",
      "[[0.9 ]\n",
      " [0.23]]\n"
     ]
    }
   ],
   "source": [
    "print(\"EXAMPLE WITH GIVEN WEIGHTS :\")\n",
    "example_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "930bf805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXAMPLE WITH RANDOM WEIGHTS :\n",
      "a1: [[1.   0.13]\n",
      " [1.   0.42]]\n",
      "z2: [[0.86333966 0.1051517 ]\n",
      " [1.04952884 0.15202402]]\n",
      "a2: [[1.         0.70335793 0.52626373]\n",
      " [1.         0.74068441 0.53793298]]\n",
      "z3: [[1.32993095]\n",
      " [1.35267414]]\n",
      "a3: [[0.79082921]\n",
      " [0.79456647]]\n",
      "Regularization cost:  0.819592029062117\n",
      "Theta1:\n",
      "[[0.77961582 0.64190253]\n",
      " [0.08413476 0.16162617]]\n",
      "\n",
      "Theta2:\n",
      "[[0.89627721 0.60472216 0.00796582]]\n",
      "\n",
      "Regularized Gradients of Theta1:\n",
      "[[0.02597301 0.01291158]\n",
      " [0.00052015 0.00025476]]\n",
      "\n",
      "Regularized Gradients of Theta2:\n",
      "[[0.22769784 0.17068972 0.12312315]]\n",
      "\n",
      "Final Prediction of Our Network:\n",
      "[[0.79082921]\n",
      " [0.79456647]]\n",
      "\n",
      "Expected Prediction:\n",
      "[[0.9 ]\n",
      " [0.23]]\n"
     ]
    }
   ],
   "source": [
    "def example_1_random():\n",
    "    # Initialize network architecture\n",
    "    layers = [1, 2, 1]\n",
    "    # Randomly initialize weights\n",
    "    theta1 = np.random.rand(layers[1], layers[0] + 1)  # +1 for bias\n",
    "    theta2 = np.random.rand(layers[2], layers[1] + 1)  # +1 for bias\n",
    "\n",
    "    # Training instances\n",
    "    x = np.array([[0.13], [0.42]])\n",
    "    y = np.array([[0.9], [0.23]])\n",
    "\n",
    "    # Regularization parameter\n",
    "    lam = 0.000\n",
    "\n",
    "    # Hyperparameters\n",
    "    alpha = 0.01  # Learning rate\n",
    "\n",
    "    # Forward propagation\n",
    "    a1, z2, a2, z3, a3 = forward_propagation(x, theta1, theta2)\n",
    "    \n",
    "    print(\"a1:\", a1)\n",
    "    print(\"z2:\", z2)\n",
    "    print(\"a2:\", a2)\n",
    "    print(\"z3:\", z3)\n",
    "    print(\"a3:\", a3)\n",
    "    \n",
    "    # Calculate cost\n",
    "    J = compute_cost(y, a3)\n",
    "    \n",
    "    print(\"Regularization cost: \", J)\n",
    "    \n",
    "    # Backpropagation\n",
    "    grad1, grad2 = backpropagation(x, y, theta1, theta2, a1, z2, a2, z3, a3, lam)\n",
    "\n",
    "    # Update weights\n",
    "    theta1, theta2 = update_weights(theta1, theta2, grad1, grad2, alpha)\n",
    "\n",
    "    # Print outputs\n",
    "    print_outputs(theta1, theta2, grad1, grad2, a3, y)\n",
    "\n",
    "print(\"EXAMPLE WITH RANDOM WEIGHTS :\")\n",
    "example_1_random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18834ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"EXAMPLE WIHT GIVEN WEIGHTS : \")\n",
    "example_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e762b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def example_2_RANDOM():\n",
    "    # Initialize network architecture\n",
    "    layers = [2, 4, 3, 2]\n",
    "    # Randomly initialize weights\n",
    "    theta1 = np.random.rand(layers[1], layers[0] + 1)  # +1 for bias\n",
    "    theta2 = np.random.rand(layers[2], layers[1] + 1)  # +1 for bias\n",
    "    theta3 = np.random.rand(layers[3], layers[2] + 1)  # +1 for bias\n",
    "    \n",
    "#     theta1 = np.array([[0.42, 0.15, 0.4],\n",
    "#                        [0.72, 0.1, 0.54],\n",
    "#                        [0.01, 0.19, 0.42],\n",
    "#                        [0.3, 0.35, 0.68]])\n",
    "#     theta2 = np.array([[0.21, 0.67, 0.14, 0.96, 0.87],\n",
    "#                        [0.87, 0.42, 0.2, 0.32, 0.89],\n",
    "#                        [0.03, 0.56, 0.8, 0.69, 0.09]])\n",
    "#     theta3 = np.array([[0.04, 0.87, 0.42, 0.53],\n",
    "#                        [0.17, 0.1, 0.95, 0.69]])\n",
    "\n",
    "    # Training instances\n",
    "    x = np.array([[0.32, 0.68], [0.83, 0.02]])\n",
    "    y = np.array([[0.75, 0.98], [0.75, 0.28]])\n",
    "\n",
    "    # Regularization parameter\n",
    "    lam = 0.25\n",
    "\n",
    "    # Hyperparameters\n",
    "    alpha = 0.01  # Learning rate\n",
    "\n",
    "    # Forward propagation\n",
    "    a1, z2, a2, z3, a3, z4, a4 = forward_propagation(x, theta1, theta2, theta3)\n",
    "    \n",
    "    # Calculate cost\n",
    "    J = compute_cost(y, a4)\n",
    "    \n",
    "    # Backpropagation\n",
    "    grad1, grad2, grad3 = backpropagation(x, y, theta1, theta2, theta3, a1, z2, a2, z3, a3, z4, a4, lam)\n",
    "\n",
    "    # Update weights\n",
    "    theta1, theta2, theta3 = update_weights(theta1, theta2, theta3, grad1, grad2, grad3, alpha)\n",
    "\n",
    "    # Print outputs\n",
    "    print_outputs(theta1, theta2, theta3, grad1, grad2, grad3, a4, y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed4cd41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9b49a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7f509c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1:\n",
      "Training instance 1\n",
      "    x: [0.13]\n",
      "    y: [0.9]\n",
      "\n",
      "Forward propagation:\n",
      "    a1: [[1.   0.13]]\n",
      "    z2: [[0.413 0.326]]\n",
      "    a2: [[1.        0.601807  0.5807858]]\n",
      "    z3: [[1.34937498]]\n",
      "    a3: [[0.79402743]]\n",
      "\n",
      "Predicted output: [[0.79402743]]\n",
      "Expected output: [0.9]\n",
      "Cost, J: 0.36557477431084995\n",
      "\n",
      "Training instance 2\n",
      "    x: [0.42]\n",
      "    y: [0.23]\n",
      "\n",
      "Forward propagation:\n",
      "    a1: [[1.   0.42]]\n",
      "    z2: [[0.442 0.384]]\n",
      "    a2: [[1.         0.60873549 0.59483749]]\n",
      "    z3: [[1.36127024]]\n",
      "    a3: [[0.79596607]]\n",
      "\n",
      "Predicted output: [[0.79596607]]\n",
      "Expected output: [0.23]\n",
      "Cost, J: 1.2763768066887786\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, theta1=None, theta2=None):\n",
    "        self.layers = layers\n",
    "        if theta1 is None:\n",
    "            self.theta1 = np.array([[0.4, 0.1], [0.3, 0.2]])  # Initial Theta1\n",
    "        else:\n",
    "            self.theta1 = theta1\n",
    "        if theta2 is None:\n",
    "            self.theta2 = np.array([[0.7, 0.5, 0.6]])  # Initial Theta2\n",
    "        else:\n",
    "            self.theta2 = theta2\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def forward_propagation(self, x):\n",
    "        a1 = np.concatenate((np.ones((x.shape[0], 1)), x), axis=1)  # Add bias to input layer\n",
    "        z2 = np.dot(a1, self.theta1.T)\n",
    "        a2 = np.concatenate((np.ones((z2.shape[0], 1)), self.sigmoid(z2)), axis=1)  # Add bias to hidden layer\n",
    "        z3 = np.dot(a2, self.theta2.T)\n",
    "        a3 = self.sigmoid(z3)\n",
    "        return a1, z2, a2, z3, a3\n",
    "\n",
    "    def compute_cost(self, y, a3):\n",
    "        m = y.shape[0]\n",
    "        J = -1/m * np.sum(y * np.log(a3) + (1 - y) * np.log(1 - a3))\n",
    "        return J\n",
    "\n",
    "    def train(self, X, Y):\n",
    "        for i, (x, y) in enumerate(zip(X, Y), 1):\n",
    "            print(f\"Training instance {i}\")\n",
    "            print(f\"    x: {x}\")\n",
    "            print(f\"    y: {y}\")\n",
    "\n",
    "            # Forward propagation\n",
    "            a1, z2, a2, z3, a3 = self.forward_propagation(x.reshape(1, -1))\n",
    "\n",
    "            # Print forward propagation results\n",
    "            print(\"\\nForward propagation:\")\n",
    "            print(f\"    a1: {a1}\")\n",
    "            print(f\"    z2: {z2}\")\n",
    "            print(f\"    a2: {a2}\")\n",
    "            print(f\"    z3: {z3}\")\n",
    "            print(f\"    a3: {a3}\")\n",
    "\n",
    "            # Compute cost\n",
    "            J = self.compute_cost(y.reshape(1, -1), a3)\n",
    "            print(f\"\\nPredicted output: {a3}\")\n",
    "            print(f\"Expected output: {y}\")\n",
    "            print(f\"Cost, J: {J}\\n\")\n",
    "\n",
    "# Example usage\n",
    "nn_example1 = NeuralNetwork([1, 2, 1])  # Example 1 network with 1 input, 2 hidden, and 1 output neuron\n",
    "nn_example2 = NeuralNetwork([2, 4, 3, 2])  # Example 2 network with 2 input, 4 hidden, 3 hidden, and 2 output neurons\n",
    "\n",
    "X_example1 = np.array([[0.13], [0.42]])  # Example 1 input data\n",
    "Y_example1 = np.array([[0.9], [0.23]])  # Example 1 desired output\n",
    "\n",
    "print(\"Example 1:\")\n",
    "nn_example1.train(X_example1, Y_example1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "64756909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1:\n",
      "Training instance 1\n",
      "    x: [0.13]\n",
      "    y: [0.9]\n",
      "\n",
      "Forward propagation:\n",
      "    a1: [0.13]\n",
      "    z2: [[0.00344647 0.07615083]]\n",
      "    a2: [0.68343712]\n",
      "\n",
      "Predicted output: [[0.68343712]]\n",
      "Expected output: [0.9]\n",
      "Cost, J, associated with instance 2: 0.45758190338572863\n",
      "Training instance 2\n",
      "    x: [0.42]\n",
      "    y: [0.23]\n",
      "\n",
      "Forward propagation:\n",
      "    a1: [0.42]\n",
      "    z2: [[0.01113475 0.24602574]]\n",
      "    a2: [0.68905122]\n",
      "\n",
      "Predicted output: [[0.68905122]]\n",
      "Expected output: [0.23]\n",
      "Cost, J, associated with instance 2: 0.985118967292997\n",
      "FINAL REG COST : 0.0\n",
      "\n",
      "Example 2:\n",
      "Training instance 1\n",
      "    x: [0.32 0.68]\n",
      "    y: [0.75 0.98]\n",
      "\n",
      "Forward propagation:\n",
      "    a1: [0.32 0.68]\n",
      "    z2: [[0.76441045 0.46840202 0.80025051 0.79023581]]\n",
      "    a2: [0.74723525 0.83655055]\n",
      "\n",
      "Predicted output: [[0.74723525 0.83655055]]\n",
      "Expected output: [0.75 0.98]\n",
      "Cost, J, associated with instance 2: 0.7734794243084668\n",
      "Training instance 2\n",
      "    x: [0.83 0.02]\n",
      "    y: [0.75 0.28]\n",
      "\n",
      "Forward propagation:\n",
      "    a1: [0.83 0.02]\n",
      "    z2: [[0.34101033 0.54165127 0.39957371 0.47096895]]\n",
      "    a2: [0.74238115 0.83164585]\n",
      "\n",
      "Predicted output: [[0.74238115 0.83164585]]\n",
      "Expected output: [0.75 0.28]\n",
      "Cost, J, associated with instance 2: 1.896919043143436\n",
      "FINAL REG COST : 0.8454828202262639\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        self.weights = [np.random.rand(layers[i], layers[i+1]) for i in range(len(layers)-1)]\n",
    "        self.biases = [np.zeros((1, layers[i+1])) for i in range(len(layers)-1)]\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Forward pass\n",
    "        activations = X\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(activations, w) + b\n",
    "            activations = self.sigmoid(z)\n",
    "        return activations\n",
    "    \n",
    "    def train(self, X, Y, lam):\n",
    "        total_cost = 0.0\n",
    "        total_reg_cost = 0.0  # Variable to store the total regularization cost\n",
    "        m = len(X)\n",
    "        for i, (x, y) in enumerate(zip(X, Y), 1):\n",
    "            print(f\"Training instance {i}\")\n",
    "            print(f\"    x: {x}\")\n",
    "            print(f\"    y: {y}\")\n",
    "\n",
    "            # Forward propagation\n",
    "            activations = x\n",
    "            zs = []\n",
    "            for w, b in zip(self.weights, self.biases):\n",
    "                z = np.dot(activations, w) + b\n",
    "                zs.append(z)\n",
    "                activations = self.sigmoid(z)\n",
    "\n",
    "            # Compute cost\n",
    "            prediction = activations\n",
    "            cost = -y * np.log(prediction) - (1 - y) * np.log(1 - prediction)\n",
    "            instance_cost = np.sum(cost)\n",
    "\n",
    "            # Regularization term\n",
    "            reg_term = sum(np.sum(w[:, 1:] ** 2) for w in self.weights)\n",
    "            total_reg_cost += reg_term  # Accumulate regularization cost for this instance\n",
    "\n",
    "            # Update total cost with regularization\n",
    "            total_cost += instance_cost\n",
    "\n",
    "            # Print forward propagation results\n",
    "            print(\"\\nForward propagation:\")\n",
    "            print(f\"    a1: {x}\")\n",
    "            for i, (z, a) in enumerate(zip(zs, activations), 2):\n",
    "                print(f\"    z{i}: {z}\")\n",
    "                print(f\"    a{i}: {a}\")\n",
    "\n",
    "            # Predicted output\n",
    "            prediction = activations\n",
    "            print(f\"\\nPredicted output: {prediction}\")\n",
    "            print(f\"Expected output: {y}\")\n",
    "\n",
    "            # Print cost associated with instance\n",
    "            print(f\"Cost, J, associated with instance {i}: {instance_cost}\")\n",
    "\n",
    "        # Average cost over all training instances\n",
    "        avg_cost = total_cost / m\n",
    "\n",
    "        # Calculate final regularization cost\n",
    "        final_reg_cost = (lam / (2 * m)) * total_reg_cost\n",
    "\n",
    "        print(\"FINAL REG COST :\", final_reg_cost)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "nn_example1 = NeuralNetwork([1, 2, 1])  # Example 1 network with 1 input, 2 hidden, and 1 output neuron\n",
    "nn_example2 = NeuralNetwork([2, 4, 3, 2])  # Example 2 network with 2 input, 4 hidden, 3 hidden, and 2 output neurons\n",
    "\n",
    "X_example1 = np.array([[0.13], [0.42]])  # Example 1 input data\n",
    "Y_example1 = np.array([[0.9], [0.23]])  # Example 1 desired output\n",
    "\n",
    "X_example2 = np.array([[0.32, 0.68], [0.83, 0.02]])  # Example 2 input data\n",
    "Y_example2 = np.array([[0.75, 0.98], [0.75, 0.28]])  # Example 2 desired output\n",
    "\n",
    "print(\"Example 1:\")\n",
    "nn_example1.train(X_example1, Y_example1,lam=0.00)\n",
    "\n",
    "print(\"\\nExample 2:\")\n",
    "nn_example2.train(X_example2, Y_example2,lam=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fda06d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f20695b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1:\n",
      "Training instance 1\n",
      "    x: [0.13]\n",
      "    y: [0.9]\n",
      "\n",
      "Forward propagation:\n",
      "    a1: [0.13]\n",
      "    z2: [[0.0478408  0.02195366]]\n",
      "    a2: [0.59203681]\n",
      "\n",
      "Predicted output: [[0.59203681]]\n",
      "Expected output: [0.9]\n",
      "Cost associated with instance 2: 0.561425654399335\n",
      "Training instance 2\n",
      "    x: [0.42]\n",
      "    y: [0.23]\n",
      "\n",
      "Forward propagation:\n",
      "    a1: [0.42]\n",
      "    z2: [[0.15456259 0.07092722]]\n",
      "    a2: [0.59635407]\n",
      "\n",
      "Predicted output: [[0.59635407]]\n",
      "Expected output: [0.23]\n",
      "Cost associated with instance 2: 0.8174490057991711\n",
      "\n",
      "Example 2:\n",
      "Training instance 1\n",
      "    x: [0.32 0.68]\n",
      "    y: [0.75 0.98]\n",
      "\n",
      "Forward propagation:\n",
      "    a1: [0.32 0.68]\n",
      "    z2: [[0.92849484 0.62270586 0.41044249 0.62668405]]\n",
      "    a2: [0.82233176 0.87781949]\n",
      "\n",
      "Predicted output: [[0.82233176 0.87781949]]\n",
      "Expected output: [0.75 0.98]\n",
      "Cost associated with instance 2: 0.7484209743143136\n",
      "Training instance 2\n",
      "    x: [0.83 0.02]\n",
      "    y: [0.75 0.28]\n",
      "\n",
      "Forward propagation:\n",
      "    a1: [0.83 0.02]\n",
      "    z2: [[0.69463309 0.42657113 0.42826308 0.45767375]]\n",
      "    a2: [0.81901571 0.87461462]\n",
      "\n",
      "Predicted output: [[0.81901571 0.87461462]]\n",
      "Expected output: [0.75 0.28]\n",
      "Cost associated with instance 2: 2.1095689447739625\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        self.weights = [np.random.rand(layers[i], layers[i+1]) for i in range(len(layers)-1)]\n",
    "        self.biases = [np.zeros((1, layers[i+1])) for i in range(len(layers)-1)]\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Forward pass\n",
    "        activations = X\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(activations, w) + b\n",
    "            activations = self.sigmoid(z)\n",
    "        return activations\n",
    "    \n",
    "    def train(self, X, Y, lam):\n",
    "        total_cost = 0.0\n",
    "        total_reg_cost = 0.0  # Variable to store the total regularization cost\n",
    "        m = len(X)\n",
    "        for i, (x, y) in enumerate(zip(X, Y), 1):\n",
    "            print(f\"Training instance {i}\")\n",
    "            print(f\"    x: {x}\")\n",
    "            print(f\"    y: {y}\")\n",
    "\n",
    "            # Forward propagation\n",
    "            activations = x\n",
    "            zs = []\n",
    "            for w, b in zip(self.weights, self.biases):\n",
    "                z = np.dot(activations, w) + b\n",
    "                zs.append(z)\n",
    "                activations = self.sigmoid(z)\n",
    "\n",
    "            # Compute cost\n",
    "            prediction = activations\n",
    "            cost = -y * np.log(prediction) - (1 - y) * np.log(1 - prediction)\n",
    "            instance_cost = np.sum(cost)\n",
    "\n",
    "            # Regularization term\n",
    "            reg_term = sum(np.sum(w[:, 1:] ** 2) for w in self.weights)\n",
    "            total_reg_cost += reg_term  # Accumulate regularization cost for this instance\n",
    "\n",
    "            # Update total cost with regularization\n",
    "            total_cost += instance_cost\n",
    "\n",
    "            # Print forward propagation results\n",
    "            print(\"\\nForward propagation:\")\n",
    "            print(f\"    a1: {x}\")\n",
    "            for i, (z, a) in enumerate(zip(zs, activations), 2):\n",
    "                print(f\"    z{i}: {z}\")\n",
    "                print(f\"    a{i}: {a}\")\n",
    "\n",
    "            # Predicted output\n",
    "            prediction = activations\n",
    "            print(f\"\\nPredicted output: {prediction}\")\n",
    "            print(f\"Expected output: {y}\")\n",
    "\n",
    "            # Print cost associated with instance\n",
    "            print(f\"Cost associated with instance {i}: {instance_cost}\")\n",
    "\n",
    "        # Average cost over all training instances\n",
    "        avg_cost = total_cost / m\n",
    "\n",
    "        # Calculate final regularization cost\n",
    "        final_reg_cost = (lam / (2 * m)) * total_reg_cost\n",
    "\n",
    "        # Add regularization term to the total cost\n",
    "        total_cost += final_reg_cost\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "nn_example1 = NeuralNetwork([1, 2, 1]) \n",
    "nn_example2 = NeuralNetwork([2, 4, 3, 2])  \n",
    "X_example1 = np.array([[0.13], [0.42]])  # Example 1 input data\n",
    "Y_example1 = np.array([[0.9], [0.23]])  # Example 1 desired output\n",
    "\n",
    "X_example2 = np.array([[0.32, 0.68], [0.83, 0.02]])  # Example 2 input data\n",
    "Y_example2 = np.array([[0.75, 0.98], [0.75, 0.28]])  # Example 2 desired output\n",
    "\n",
    "print(\"Example 1:\")\n",
    "nn_example1.train(X_example1, Y_example1, lam=0.25)  # Using regularization parameter lambda=0.25\n",
    "\n",
    "print(\"\\nExample 2:\")\n",
    "nn_example2.train(X_example2, Y_example2, lam=0.25)  # Using regularization parameter lambda=0.25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d31edf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8989c1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1:\n",
      "Iteration 1\n",
      "Total cost: 0.7199456326315257\n",
      "Iteration 2\n",
      "Total cost: 0.7199456326315257\n",
      "Stopping criteria reached: Improvement in cost function less than epsilon.\n",
      "\n",
      "Example 2:\n",
      "Iteration 1\n",
      "Total cost: 2.0859261907417728\n",
      "Iteration 2\n",
      "Total cost: 2.0859261907417728\n",
      "Stopping criteria reached: Improvement in cost function less than epsilon.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        self.weights = [np.random.rand(layers[i], layers[i+1]) for i in range(len(layers)-1)]\n",
    "        self.biases = [np.zeros((1, layers[i+1])) for i in range(len(layers)-1)]\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Forward pass\n",
    "        activations = X\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(activations, w) + b\n",
    "            activations = self.sigmoid(z)\n",
    "        return activations\n",
    "    \n",
    "    def train(self, X, Y, lam, epsilon, max_iterations):\n",
    "        total_cost = 0.0\n",
    "        total_reg_cost = 0.0  # Variable to store the total regularization cost\n",
    "        m = len(X)\n",
    "        prev_cost = float('inf')\n",
    "        iteration = 0\n",
    "        \n",
    "        while True:\n",
    "            iteration += 1\n",
    "            print(f\"Iteration {iteration}\")\n",
    "            for i, (x, y) in enumerate(zip(X, Y), 1):\n",
    "                # Forward propagation\n",
    "                activations = x\n",
    "                zs = []\n",
    "                for w, b in zip(self.weights, self.biases):\n",
    "                    z = np.dot(activations, w) + b\n",
    "                    zs.append(z)\n",
    "                    activations = self.sigmoid(z)\n",
    "\n",
    "                # Compute cost\n",
    "                prediction = activations\n",
    "                cost = -y * np.log(prediction) - (1 - y) * np.log(1 - prediction)\n",
    "                instance_cost = np.sum(cost)\n",
    "\n",
    "                # Regularization term\n",
    "                reg_term = sum(np.sum(w[:, 1:] ** 2) for w in self.weights)\n",
    "                total_reg_cost += reg_term  # Accumulate regularization cost for this instance\n",
    "\n",
    "                # Update total cost with regularization\n",
    "                total_cost += instance_cost\n",
    "            \n",
    "            # Average cost over all training instances\n",
    "            avg_cost = total_cost / m\n",
    "\n",
    "            # Calculate final regularization cost\n",
    "            final_reg_cost = (lam / (2 * m)) * total_reg_cost\n",
    "\n",
    "            # Print total cost\n",
    "            print(f\"Total cost: {avg_cost + final_reg_cost}\")\n",
    "\n",
    "            # Check for improvement in cost function\n",
    "            if abs(prev_cost - (avg_cost + final_reg_cost)) < epsilon:\n",
    "                print(\"Stopping criteria reached: Improvement in cost function less than epsilon.\")\n",
    "                break\n",
    "            \n",
    "            # Check for maximum number of iterations\n",
    "            if iteration >= max_iterations:\n",
    "                print(\"Stopping criteria reached: Maximum number of iterations reached.\")\n",
    "                break\n",
    "            \n",
    "            prev_cost = avg_cost + final_reg_cost\n",
    "            \n",
    "            # Reset total cost and regularization cost for next iteration\n",
    "            total_cost = 0.0\n",
    "            total_reg_cost = 0.0\n",
    "            \n",
    "            # Shuffle training data for next iteration\n",
    "            combined = list(zip(X, Y))\n",
    "            np.random.shuffle(combined)\n",
    "            X, Y = zip(*combined)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "nn_example1 = NeuralNetwork([1, 2, 1])  # Example 1 network with 1 input, 2 hidden, and 1 output neuron\n",
    "nn_example2 = NeuralNetwork([2, 4, 3, 2])  # Example 2 network with 2 input, 4 hidden, 3 hidden, and 2 output neurons\n",
    "\n",
    "X_example1 = np.array([[0.13], [0.42]])  # Example 1 input data\n",
    "Y_example1 = np.array([[0.9], [0.23]])  # Example 1 desired output\n",
    "\n",
    "X_example2 = np.array([[0.32, 0.68], [0.83, 0.02]])  # Example 2 input data\n",
    "Y_example2 = np.array([[0.75, 0.98], [0.75, 0.28]])  # Example 2 desired output\n",
    "\n",
    "print(\"Example 1:\")\n",
    "nn_example1.train(X_example1, Y_example1, lam=0.00, epsilon=0.0001, max_iterations=100)\n",
    "\n",
    "print(\"\\nExample 2:\")\n",
    "nn_example2.train(X_example2, Y_example2, lam=0.25, epsilon=0.0001, max_iterations=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c528c563",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec91b2ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "97fdac39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_encoded:       1  2  3\n",
      "0    1  0  0\n",
      "1    1  0  0\n",
      "2    1  0  0\n",
      "3    1  0  0\n",
      "4    1  0  0\n",
      "..  .. .. ..\n",
      "173  0  0  1\n",
      "174  0  0  1\n",
      "175  0  0  1\n",
      "176  0  0  1\n",
      "177  0  0  1\n",
      "\n",
      "[178 rows x 3 columns]\n",
      "Iteration 1\n",
      "Total cost: -0.04882722492668656\n",
      "Iteration 2\n",
      "Total cost: -0.04882722492668656\n",
      "Stopping criteria reached: Improvement in cost function less than epsilon.\n",
      "Accuracy: 0.08333333333333333\n",
      "F1 Score: 1.0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd956d92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdbcad0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ed0629",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b4faee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "800d5264",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (1,5) (1,3) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/bm/l2zpyjxn2gqg_1d20xgx5z0w0000gn/T/ipykernel_72260/2621096704.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;31m# Evaluate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/bm/l2zpyjxn2gqg_1d20xgx5z0w0000gn/T/ipykernel_72260/2621096704.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, Y, learning_rate, lam)\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m                 \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid_derivative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlam\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbiases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (1,5) (1,3) "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define function for normalization\n",
    "def normalize(X):\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        X = X.values\n",
    "    mean = np.mean(X, axis=0)\n",
    "    std = np.std(X, axis=0)\n",
    "    X_normalized = (X - mean) / std\n",
    "    return X_normalized, mean, std\n",
    "\n",
    "# Define function to calculate accuracy\n",
    "def accuracy_score(y_true, y_pred):\n",
    "    correct = sum(1 for yt, yp in zip(y_true, y_pred) if yt == yp)\n",
    "    return correct / len(y_true)\n",
    "\n",
    "# Define function to calculate F1 score\n",
    "def f1_score(y_true, y_pred):\n",
    "    tp = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 1 and yp == 1)\n",
    "    fp = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 0 and yp == 1)\n",
    "    fn = sum(1 for yt, yp in zip(y_true, y_pred) if yt == 1 and yp == 0)\n",
    "    precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "    recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "    return 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "\n",
    "# Define function to evaluate model performance\n",
    "def evaluate_model_performance(model, X_train, y_train, X_test, y_test):\n",
    "    model.train(X_train, y_train, learning_rate=0.01, lam=0.01)  # Example: Training the model with lambda=0.01\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    return accuracy, f1\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        self.weights = [np.random.rand(layers[i], layers[i+1]) for i in range(len(layers)-1)]\n",
    "        self.biases = [np.zeros((1, layers[i+1])) for i in range(len(layers)-1)]\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Forward pass\n",
    "        activations = X\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(activations, w) + b\n",
    "            activations = self.sigmoid(z)\n",
    "        return activations\n",
    "    \n",
    "    def train(self, X, Y, learning_rate, lam):\n",
    "        total_cost = 0.0\n",
    "        total_reg_cost = 0.0\n",
    "        m = len(X)\n",
    "        for i, (x, y) in enumerate(zip(X, Y), 1):\n",
    "            activations = x\n",
    "            zs = []\n",
    "            activation_layers = [activations]  # Store activations for all layers\n",
    "            # Forward pass\n",
    "            for w, b in zip(self.weights, self.biases):\n",
    "                z = np.dot(activations, w) + b\n",
    "                zs.append(z)\n",
    "                activations = self.sigmoid(z)\n",
    "                activation_layers.append(activations)\n",
    "\n",
    "            prediction = activations\n",
    "            cost = -y * np.log(prediction) - (1 - y) * np.log(1 - prediction)\n",
    "            instance_cost = np.sum(cost)\n",
    "\n",
    "            reg_term = sum(np.sum(w[:, 1:] ** 2) for w in self.weights)\n",
    "            total_reg_cost += reg_term\n",
    "\n",
    "            total_cost += instance_cost\n",
    "\n",
    "            delta = prediction - y\n",
    "            for i in range(len(self.weights)-1, -1, -1):\n",
    "                delta = np.dot(delta, self.weights[i].T) * self.sigmoid_derivative(activation_layers[i+1])\n",
    "                self.weights[i] -= (learning_rate * np.dot(activation_layers[i].T, delta) + (lam / m) * self.weights[i])\n",
    "                self.biases[i] -= learning_rate * np.sum(delta, axis=0)\n",
    "\n",
    "        avg_cost = total_cost / m\n",
    "        final_reg_cost = (lam / (2 * m)) * total_reg_cost\n",
    "        print(\"FINAL REG COST :\", final_reg_cost)\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "df_wine = pd.read_csv(\"/Users/noshitha/Downloads/hw3/datasets/hw3_wine.csv\", delimiter=\"\\t\")\n",
    "\n",
    "# Preprocess data\n",
    "X_wine = pd.get_dummies(df_wine.iloc[:, 1:])  # Exclude the target column\n",
    "y_wine = df_wine.iloc[:, 0]\n",
    "\n",
    "# Normalize data\n",
    "X_wine_normalized, mean, std = normalize(X_wine)\n",
    "\n",
    "# Reshape y_wine to a 2D array\n",
    "y_wine = y_wine.values.reshape(-1, 1)\n",
    "\n",
    "# Initialize the OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Fit and transform the target variable\n",
    "y_encoded = encoder.fit_transform(y_wine)\n",
    "\n",
    "# Convert y_encoded to a DataFrame\n",
    "y_encoded_df = pd.DataFrame(y_encoded, columns=['y1', 'y2', 'y3'])\n",
    "\n",
    "# Define model architecture\n",
    "architectures = [\n",
    "    [X_wine_normalized.shape[1], 5, 3, 3],  # Example architecture\n",
    "]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_wine_normalized, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Instantiate the NeuralNetwork class with the desired architecture\n",
    "model = NeuralNetwork(architectures[0])\n",
    "\n",
    "# Train the model\n",
    "model.train(X_train, y_train, learning_rate=0.01, lam=0.01)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6278ff58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df_wine = pd.read_csv(\"/Users/noshitha/Downloads/hw3/datasets/hw3_wine.csv\", delimiter=\"\\t\")\n",
    "\n",
    "# Preprocess data\n",
    "X_wine = pd.get_dummies(df_wine.iloc[:, 1:])  # Exclude the target column\n",
    "y_wine = df_wine.iloc[:, 0]\n",
    "\n",
    "# Normalize data\n",
    "X_wine_normalized, mean, std = normalize(X_wine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "abb9c59a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_wine.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "99424be8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># class</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   # class      1     2     3     4    5     6     7     8     9    10    11  \\\n",
       "0        1  14.23  1.71  2.43  15.6  127  2.80  3.06  0.28  2.29  5.64  1.04   \n",
       "1        1  13.20  1.78  2.14  11.2  100  2.65  2.76  0.26  1.28  4.38  1.05   \n",
       "2        1  13.16  2.36  2.67  18.6  101  2.80  3.24  0.30  2.81  5.68  1.03   \n",
       "3        1  14.37  1.95  2.50  16.8  113  3.85  3.49  0.24  2.18  7.80  0.86   \n",
       "4        1  13.24  2.59  2.87  21.0  118  2.80  2.69  0.39  1.82  4.32  1.04   \n",
       "\n",
       "     12    13  \n",
       "0  3.92  1065  \n",
       "1  3.40  1050  \n",
       "2  3.17  1185  \n",
       "3  3.45  1480  \n",
       "4  2.93   735  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b689cb38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    1\n",
       "3    1\n",
       "4    1\n",
       "Name: # class, dtype: int64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a29055e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1:\n",
      "Regularization parameter lambda=0.000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (1,2) and (1,2) not aligned: 2 (dim 1) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/bm/l2zpyjxn2gqg_1d20xgx5z0w0000gn/T/ipykernel_84064/4172211924.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;31m# Train the neural network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m \u001b[0mnn_example_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_example_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train_example_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.005\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/bm/l2zpyjxn2gqg_1d20xgx5z0w0000gn/T/ipykernel_84064/4172211924.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, Y, learning_rate, lam, max_iterations, epsilon)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mactivations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0mdeltas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mgradients_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradients_biases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeltas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/bm/l2zpyjxn2gqg_1d20xgx5z0w0000gn/T/ipykernel_84064/4172211924.py\u001b[0m in \u001b[0;36mforward_pass\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mactivations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbiases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (1,2) and (1,2) not aligned: 2 (dim 1) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        self.weights = [np.random.randn(layers[i], layers[i+1]) for i in range(len(layers)-1)]\n",
    "        self.biases = [np.zeros((1, layers[i+1])) for i in range(len(layers)-1)]\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    def forward_pass(self, X):\n",
    "        activations = [X]\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(activations[-1], w) + b\n",
    "            activations.append(self.sigmoid(z))\n",
    "        return activations\n",
    "    \n",
    "    def backward_pass(self, X, Y, activations):\n",
    "        deltas = [(activations[-1] - Y) * self.sigmoid_derivative(activations[-1])]\n",
    "        for i in range(len(self.layers) - 2, 0, -1):\n",
    "            delta = np.dot(deltas[0], self.weights[i].T) * self.sigmoid_derivative(activations[i])\n",
    "            deltas.insert(0, delta)\n",
    "        return deltas\n",
    "    \n",
    "    def compute_gradients(self, activations, deltas):\n",
    "        gradients_weights = [np.dot(activations[i].T, deltas[i]) for i in range(len(self.layers) - 1)]\n",
    "        gradients_biases = [np.sum(deltas[i], axis=0) for i in range(len(self.layers) - 1)]\n",
    "        return gradients_weights, gradients_biases\n",
    "    \n",
    "    def update_weights(self, gradients_weights, gradients_biases, learning_rate):\n",
    "        self.weights = [w - learning_rate * gw for w, gw in zip(self.weights, gradients_weights)]\n",
    "        self.biases = [b - learning_rate * gb for b, gb in zip(self.biases, gradients_biases)]\n",
    "    \n",
    "    def train(self, X, Y, learning_rate, lam, max_iterations, epsilon):\n",
    "        for iteration in range(max_iterations):\n",
    "            activations = self.forward_pass(X)\n",
    "            deltas = self.backward_pass(X, Y, activations)\n",
    "            gradients_weights, gradients_biases = self.compute_gradients(activations, deltas)\n",
    "            self.update_weights(gradients_weights, gradients_biases, learning_rate)\n",
    "            # Compute cost function\n",
    "            J = np.mean(np.square(activations[-1] - Y))\n",
    "            print(f\"Iteration {iteration+1}, Cost: {J}\")\n",
    "            # Check for convergence\n",
    "            if J < epsilon:\n",
    "                print(\"Converged!\")\n",
    "                break\n",
    "\n",
    "# Example 1\n",
    "print(\"Example 1:\")\n",
    "print(\"Regularization parameter lambda=0.000\")\n",
    "\n",
    "# Define the structure of the neural network\n",
    "nn_example_1 = NeuralNetwork([1, 2, 1])\n",
    "\n",
    "# Initialize Theta1 and Theta2 with provided values\n",
    "nn_example_1.weights[0] = np.array([[0.4, 0.1], [0.3, 0.2]])\n",
    "nn_example_1.weights[1] = np.array([[0.7, 0.5]])\n",
    "\n",
    "# Define the training data\n",
    "X_train_example_1 = np.array([[0.13, 0.9]])\n",
    "Y_train_example_1 = np.array([[0.5]])  # Placeholder target output\n",
    "\n",
    "# Train the neural network\n",
    "nn_example_1.train(X_train_example_1, Y_train_example_1, learning_rate=0.1, lam=0.0, max_iterations=1000, epsilon=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5ad9c9ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tuple' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/bm/l2zpyjxn2gqg_1d20xgx5z0w0000gn/T/ipykernel_72260/3793863026.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_encoded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'tuple' object is not callable"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a9e3b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient_weights:  [array([[ 0.00476952,  0.00076406, -0.00162829],\n",
      "       [ 0.01648216,  0.00303619, -0.00192312]]), array([[-0.04202719],\n",
      "       [-0.05293396],\n",
      "       [-0.04772457]])]\n",
      "gradients_biases:  [array([ 0.01874646,  0.00397823, -0.0041006 ]), array([-0.09949241])]\n",
      "Iteration 1, Cost: 0.2620492738553714\n",
      "gradient_weights:  [array([[ 0.00440974,  0.00064801, -0.00159181],\n",
      "       [ 0.01615018,  0.00288054, -0.00190987]]), array([[-0.04064788],\n",
      "       [-0.05075277],\n",
      "       [-0.04602825]])]\n",
      "gradients_biases:  [array([ 0.01794559,  0.00369349, -0.0040653 ]), array([-0.09587024])]\n",
      "Iteration 2, Cost: 0.26119324302246755\n",
      "gradient_weights:  [array([[ 0.00406447,  0.00053957, -0.00155146],\n",
      "       [ 0.01582726,  0.00273338, -0.00189252]]), array([[-0.03930246],\n",
      "       [-0.04862567],\n",
      "       [-0.04436967]])]\n",
      "gradients_biases:  [array([ 0.01717104,  0.00342531, -0.00402111]), array([-0.09233135])]\n",
      "Iteration 3, Cost: 0.2603989674328\n",
      "gradient_weights:  [array([[ 0.00373349,  0.00043834, -0.00150777],\n",
      "       [ 0.01551364,  0.00259439, -0.00187152]]), array([[-0.03799214],\n",
      "       [-0.04655462],\n",
      "       [-0.04275076]])]\n",
      "gradients_biases:  [array([ 0.01642291,  0.00317296, -0.00396904]), array([-0.0888797])]\n",
      "Iteration 4, Cost: 0.2596627063009946\n",
      "gradient_weights:  [array([[ 0.00341652,  0.00034394, -0.0014612 ],\n",
      "       [ 0.01520946,  0.00246321, -0.00184727]]), array([[-0.03671784],\n",
      "       [-0.04454112],\n",
      "       [-0.04117308]])]\n",
      "gradients_biases:  [array([ 0.01570116,  0.00293571, -0.00391003]), array([-0.08551842])]\n",
      "Iteration 5, Cost: 0.2589808311644643\n",
      "gradient_weights:  [array([[ 0.00311326,  0.00025597, -0.00141223],\n",
      "       [ 0.01491481,  0.0023395 , -0.00182019]]), array([[-0.0354802 ],\n",
      "       [-0.04258623],\n",
      "       [-0.03963781]])]\n",
      "gradients_biases:  [array([ 0.01500561,  0.00271285, -0.00384496]), array([-0.08224985])]\n",
      "Iteration 6, Cost: 0.25834984161625324\n",
      "gradient_weights:  [array([[ 0.00282336,  0.00017406, -0.00136128],\n",
      "       [ 0.01462973,  0.00222291, -0.00179065]]), array([[-0.03427964],\n",
      "       [-0.04069062],\n",
      "       [-0.03814582]])]\n",
      "gradients_biases:  [array([ 0.01433599,  0.00250362, -0.00377467]), array([-0.0790757])]\n",
      "Iteration 7, Cost: 0.25776637720649587\n",
      "gradient_weights:  [array([[ 2.54645146e-03,  9.78238681e-05, -1.30873406e-03],\n",
      "       [ 1.43541943e-02,  2.11308637e-03, -1.75900180e-03]]), array([[-0.03311634],\n",
      "       [-0.03885461],\n",
      "       [-0.03669771]])]\n",
      "gradients_biases:  [array([ 0.01369192,  0.0023073 , -0.00369993]), array([-0.07599703])]\n",
      "Iteration 8, Cost: 0.25722722594520003\n",
      "gradient_weights:  [array([[ 2.28214372e-03,  2.69079963e-05, -1.25495219e-03],\n",
      "       [ 1.40881481e-02,  2.00967967e-03, -1.72556376e-03]]), array([[-0.03199031],\n",
      "       [-0.03707824],\n",
      "       [-0.03529379]])]\n",
      "gradients_biases:  [array([ 0.01307293,  0.00212318, -0.00362147]), array([-0.07301433])]\n",
      "Iteration 9, Cost: 0.2567293298338997\n",
      "gradient_weights:  [array([[ 2.03002799e-03, -3.90373956e-05, -1.20026144e-03],\n",
      "       [ 1.38314902e-02,  1.91235398e-03, -1.69063921e-03]]), array([[-0.03090138],\n",
      "       [-0.03536124],\n",
      "       [-0.03393415]])]\n",
      "gradients_biases:  [array([ 0.01247852,  0.00195056, -0.00353995]), array([-0.07012764])]\n",
      "Iteration 10, Cost: 0.25626978783752274\n",
      "gradient_weights:  [array([[ 0.00178969, -0.00010035, -0.00114496],\n",
      "       [ 0.01358409,  0.00182078, -0.0016545 ]]), array([[-0.02984924],\n",
      "       [-0.03370312],\n",
      "       [-0.03261868]])]\n",
      "gradients_biases:  [array([ 0.01190809,  0.00178877, -0.00345598]), array([-0.06733656])]\n",
      "Iteration 11, Cost: 0.255845856683689\n",
      "gradient_weights:  [array([[ 0.0015607 , -0.00015734, -0.00108931],\n",
      "       [ 0.01334579,  0.00173464, -0.00161741]]), array([[-0.02883345],\n",
      "       [-0.03210321],\n",
      "       [-0.03134709]])]\n",
      "gradients_biases:  [array([ 0.01136102,  0.00163715, -0.00337012]), array([-0.06464029])]\n",
      "Iteration 12, Cost: 0.25545494984726225\n",
      "gradient_weights:  [array([[ 0.00134264, -0.00021031, -0.00103356],\n",
      "       [ 0.01311641,  0.00165362, -0.00157959]]), array([[-0.02785346],\n",
      "       [-0.03056063],\n",
      "       [-0.03011894]])]\n",
      "gradients_biases:  [array([ 0.01083668,  0.00149508, -0.00328286]), array([-0.06203772])]\n",
      "Iteration 13, Cost: 0.25509463504553065\n",
      "gradient_weights:  [array([[ 0.00113508, -0.00025955, -0.00097791],\n",
      "       [ 0.01289574,  0.00157742, -0.00154124]]), array([[-0.02690863],\n",
      "       [-0.02907437],\n",
      "       [-0.02893364]])]\n",
      "gradients_biases:  [array([ 0.01033436,  0.00136196, -0.00319467]), array([-0.05952749])]\n",
      "Iteration 14, Cost: 0.2547626305356359\n",
      "gradient_weights:  [array([[ 0.00093761, -0.00030533, -0.00092256],\n",
      "       [ 0.01268358,  0.00150577, -0.00150257]]), array([[-0.02599825],\n",
      "       [-0.0276433 ],\n",
      "       [-0.02779049]])]\n",
      "gradients_biases:  [array([ 0.00985339,  0.00123722, -0.00310596]), array([-0.05710797])]\n",
      "Iteration 15, Cost: 0.25445680047217917\n",
      "gradient_weights:  [array([[ 0.0007498 , -0.0003479 , -0.00086768],\n",
      "       [ 0.01247971,  0.0014384 , -0.00146372]]), array([[-0.02512155],\n",
      "       [-0.02626619],\n",
      "       [-0.02668872]])]\n",
      "gradients_biases:  [array([ 0.00939305,  0.00112032, -0.0030171 ]), array([-0.05477735])]\n",
      "Iteration 16, Cost: 0.2541751495503064\n",
      "gradient_weights:  [array([[ 0.00057126, -0.00038751, -0.00081342],\n",
      "       [ 0.01228388,  0.00137504, -0.00142487]]), array([[-0.0242777 ],\n",
      "       [-0.02494171],\n",
      "       [-0.02562745]])]\n",
      "gradients_biases:  [array([ 0.00895262,  0.00101076, -0.00292841]), array([-0.05253367])]\n",
      "Iteration 17, Cost: 0.25391581712872313\n",
      "gradient_weights:  [array([[ 0.00040158, -0.00042436, -0.00075989],\n",
      "       [ 0.01209585,  0.00131546, -0.00138613]]), array([[-0.02346583],\n",
      "       [-0.0236685 ],\n",
      "       [-0.02460575]])]\n",
      "gradients_biases:  [array([ 0.00853141,  0.00090804, -0.00284018]), array([-0.05037482])]\n",
      "Iteration 18, Cost: 0.2536770709984748\n",
      "gradient_weights:  [array([[ 0.00024039, -0.00045868, -0.00070722],\n",
      "       [ 0.01191539,  0.00125942, -0.00134764]]), array([[-0.02268506],\n",
      "       [-0.02244512],\n",
      "       [-0.02362264]])]\n",
      "gradients_biases:  [array([ 0.0081287 ,  0.00081172, -0.00275268]), array([-0.04829862])]\n",
      "Iteration 19, Cost: 0.25345730093722574\n",
      "gradient_weights:  [array([[ 8.72990165e-05, -4.90653091e-04, -6.55485967e-04],\n",
      "       [ 1.17422365e-02,  1.20669807e-03, -1.30948333e-03]]), array([[-0.02193447],\n",
      "       [-0.02127013],\n",
      "       [-0.0226771 ]])]\n",
      "gradients_biases:  [array([ 0.0077438 ,  0.00072136, -0.00266613]), array([-0.04630277])]\n",
      "Iteration 20, Cost: 0.253255012165278\n",
      "gradient_weights:  [array([[-5.80509454e-05, -5.20460483e-04, -6.04779225e-04],\n",
      "       [ 1.15761562e-02,  1.15710025e-03, -1.27176275e-03]]), array([[-0.02121313],\n",
      "       [-0.02014203],\n",
      "       [-0.02176807]])]\n",
      "gradients_biases:  [array([ 0.00737601,  0.00063657, -0.00258072]), array([-0.04438498])]\n",
      "Iteration 21, Cost: 0.2530688187987092\n",
      "gradient_weights:  [array([[-0.00019601, -0.00054827, -0.00055516],\n",
      "       [ 0.0114169 ,  0.00111043, -0.00123455]]), array([[-0.02052012],\n",
      "       [-0.01905935],\n",
      "       [-0.02089449]])]\n",
      "gradients_biases:  [array([ 0.00702465,  0.00055696, -0.00249662]), array([-0.04254286])]\n",
      "Iteration 22, Cost: 0.25289743737667286\n",
      "gradient_weights:  [array([[-0.00032692, -0.00057424, -0.00050669],\n",
      "       [ 0.01126422,  0.0010665 , -0.00119793]]), array([[-0.01985451],\n",
      "       [-0.01802059],\n",
      "       [-0.02005527]])]\n",
      "gradients_biases:  [array([ 0.00668908,  0.00048219, -0.00241397]), array([-0.04077404])]\n",
      "Iteration 23, Cost: 0.25273968052397533\n",
      "gradient_weights:  [array([[-0.0004511 , -0.00059851, -0.0004594 ],\n",
      "       [ 0.01111788,  0.00102514, -0.00116194]]), array([[-0.01921535],\n",
      "       [-0.01702426],\n",
      "       [-0.01924933]])]\n",
      "gradients_biases:  [array([ 0.00636865,  0.00041192, -0.0023329 ]), array([-0.03907614])]\n",
      "Iteration 24, Cost: 0.2525944507963249\n",
      "gradient_weights:  [array([[-0.00056887, -0.00062121, -0.00041334],\n",
      "       [ 0.01097765,  0.0009862 , -0.00112663]]), array([[-0.01860175],\n",
      "       [-0.0160689 ],\n",
      "       [-0.01847558]])]\n",
      "gradients_biases:  [array([ 0.00606272,  0.00034586, -0.00225351]), array([-0.03744679])]\n",
      "Iteration 25, Cost: 0.25246073474396574\n",
      "gradient_weights:  [array([[-0.00068053, -0.00064248, -0.00036851],\n",
      "       [ 0.01084328,  0.00094952, -0.00109205]]), array([[-0.01801277],\n",
      "       [-0.01515304],\n",
      "       [-0.01773294]])]\n",
      "gradients_biases:  [array([ 0.0057707 ,  0.0002837 , -0.00217587]), array([-0.03588364])]\n",
      "Iteration 26, Cost: 0.25233759721953786\n",
      "gradient_weights:  [array([[-0.00078637, -0.00066241, -0.00032495],\n",
      "       [ 0.01071457,  0.00091496, -0.00105824]]), array([[-0.01744754],\n",
      "       [-0.01427526],\n",
      "       [-0.01702033]])]\n",
      "gradients_biases:  [array([ 0.00549199,  0.0002252 , -0.00210006]), array([-0.03438435])]\n",
      "Iteration 27, Cost: 0.2522241759477603\n",
      "gradient_weights:  [array([[-0.00088665, -0.00068112, -0.00028267],\n",
      "       [ 0.01059127,  0.00088239, -0.0010252 ]]), array([[-0.01690518],\n",
      "       [-0.01343416],\n",
      "       [-0.01633671]])]\n",
      "gradients_biases:  [array([ 0.00522602,  0.00017008, -0.00202613]), array([-0.03294663])]\n",
      "Iteration 28, Cost: 0.25211967636771204\n",
      "gradient_weights:  [array([[-0.00098165, -0.00069871, -0.00024165],\n",
      "       [ 0.01047319,  0.00085168, -0.00099298]]), array([[-0.01638482],\n",
      "       [-0.01262835],\n",
      "       [-0.01568103]])]\n",
      "gradients_biases:  [array([ 0.00497225,  0.00011814, -0.00195412]), array([-0.03156825])]\n",
      "Iteration 29, Cost: 0.25202336675290526\n",
      "gradient_weights:  [array([[-0.00107161, -0.00071526, -0.00020192],\n",
      "       [ 0.0103601 ,  0.00082272, -0.00096157]]), array([[-0.01588563],\n",
      "       [-0.0118565 ],\n",
      "       [-0.01505228]])]\n",
      "gradients_biases:  [array([ 4.73015043e-03,  6.91397752e-05, -1.88404740e-03]), array([-0.03024698])]\n",
      "Iteration 30, Cost: 0.2519345736098385\n",
      "gradient_weights:  [array([[-0.00115678, -0.00073085, -0.00016345],\n",
      "       [ 0.01025182,  0.00079541, -0.000931  ]]), array([[-0.0154068 ],\n",
      "       [-0.01111729],\n",
      "       [-0.01444946]])]\n",
      "gradients_biases:  [array([ 4.49920639e-03,  2.28926609e-05, -1.81593742e-03]), array([-0.02898069])]\n",
      "Iteration 31, Cost: 0.25185267735213074\n",
      "gradient_weights:  [array([[-0.00123739, -0.00074556, -0.00012623],\n",
      "       [ 0.01014813,  0.00076964, -0.00090126]]), array([[-0.01494753],\n",
      "       [-0.01040945],\n",
      "       [-0.01387159]])]\n",
      "gradients_biases:  [array([ 4.27893027e-03, -2.07906241e-05, -1.74979473e-03]), array([-0.02776727])]\n",
      "Iteration 32, Cost: 0.25177710824453264\n",
      "gradient_weights:  [array([[-1.31364119e-03, -7.59463147e-04, -9.02682811e-05],\n",
      "       [ 1.00488645e-02,  7.45309627e-04, -8.72357503e-04]]), array([[-0.01450704],\n",
      "       [-0.00973175],\n",
      "       [-0.01331773]])]\n",
      "gradients_biases:  [array([ 4.06885147e-03, -6.20828325e-05, -1.68561835e-03]), array([-0.02660468])]\n",
      "Iteration 33, Cost: 0.25170734260896255\n",
      "gradient_weights:  [array([[-1.38575829e-03, -7.72619214e-04, -5.55300747e-05],\n",
      "       [ 9.95382625e-03,  7.22342022e-04, -8.44293410e-04]]), array([[-0.0140846 ],\n",
      "       [-0.00908299],\n",
      "       [-0.01278695]])]\n",
      "gradients_biases:  [array([ 0.00386852, -0.00010114, -0.0016234 ]), array([-0.02549094])]\n",
      "Iteration 34, Cost: 0.2516428992831258\n",
      "gradient_weights:  [array([[-1.45393476e-03, -7.85087620e-04, -2.20007939e-05],\n",
      "       [ 9.86284262e-03,  7.00650788e-04, -8.17061868e-04]]), array([[-0.01367948],\n",
      "       [-0.00846201],\n",
      "       [-0.01227835]])]\n",
      "gradients_biases:  [array([ 0.0036775 , -0.00013812, -0.00156313]), array([-0.02442413])]\n",
      "Iteration 35, Cost: 0.2515833363211345\n",
      "gradient_weights:  [array([[-1.51835949e-03, -7.96922326e-04,  1.03409674e-05],\n",
      "       [ 9.77574376e-03,  6.80159000e-04, -7.90656095e-04]]), array([[-0.01329097],\n",
      "       [-0.00786769],\n",
      "       [-0.01179107]])]\n",
      "gradients_biases:  [array([ 0.00349537, -0.00017316, -0.00150478]), array([-0.02340237])]\n",
      "Iteration 36, Cost: 0.251528247924792\n",
      "gradient_weights:  [array([[-1.57921233e-03, -8.08172903e-04,  4.15184425e-05],\n",
      "       [ 9.69236621e-03,  6.60794789e-04, -7.65067113e-04]]), array([[-0.01291841],\n",
      "       [-0.00729893],\n",
      "       [-0.01132426]])]\n",
      "gradients_biases:  [array([ 0.00332175, -0.00020638, -0.00144832]), array([-0.02242386])]\n",
      "Iteration 37, Cost: 0.251477261593749\n",
      "gradient_weights:  [array([[-1.63666445e-03, -8.18884876e-04,  7.15563697e-05],\n",
      "       [ 9.61255269e-03,  6.42490999e-04, -7.40284058e-04]]), array([[-0.01256114],\n",
      "       [-0.0067547 ],\n",
      "       [-0.0108771 ]])]\n",
      "gradients_biases:  [array([ 0.00315624, -0.00023791, -0.00139374]), array([-0.02148686])]\n",
      "Iteration 38, Cost: 0.25143003548254045\n",
      "gradient_weights:  [array([[-0.00169088, -0.0008291 ,  0.00010048],\n",
      "       [ 0.00953615,  0.00062518, -0.00071629]]), array([[-0.01221852],\n",
      "       [-0.00623398],\n",
      "       [-0.01044881]])]\n",
      "gradients_biases:  [array([ 0.00299849, -0.00026785, -0.001341  ]), array([-0.02058968])]\n",
      "Iteration 39, Cost: 0.2513862559525036\n",
      "gradient_weights:  [array([[-0.00174201, -0.00083886,  0.00012832],\n",
      "       [ 0.00946302,  0.00060882, -0.00069308]]), array([[-0.01188997],\n",
      "       [-0.00573581],\n",
      "       [-0.01003862]])]\n",
      "gradients_biases:  [array([ 0.00284814, -0.00029631, -0.00129007]), array([-0.01973069])]\n",
      "Iteration 40, Cost: 0.25134563530672743\n",
      "gradient_weights:  [array([[-0.00179021, -0.00084819,  0.0001551 ],\n",
      "       [ 0.00939301,  0.00059333, -0.00067064]]), array([[-0.01157488],\n",
      "       [-0.00525923],\n",
      "       [-0.0096458 ]])]\n",
      "gradients_biases:  [array([ 0.00270486, -0.00032338, -0.0012409 ]), array([-0.01890834])]\n",
      "Iteration 41, Cost: 0.2513079096964533\n",
      "gradient_weights:  [array([[-0.00183561, -0.00085713,  0.00018084],\n",
      "       [ 0.009326  ,  0.00057868, -0.00064894]]), array([[-0.01127271],\n",
      "       [-0.00480336],\n",
      "       [-0.00926964]])]\n",
      "gradients_biases:  [array([ 0.00256833, -0.00034916, -0.00119346]), array([-0.0181211])]\n",
      "Iteration 42, Cost: 0.25127283718770616\n",
      "gradient_weights:  [array([[-0.00187835, -0.00086571,  0.00020559],\n",
      "       [ 0.00926186,  0.00056482, -0.00062798]]), array([[-0.0109829 ],\n",
      "       [-0.00436733],\n",
      "       [-0.00890945]])]\n",
      "gradients_biases:  [array([ 0.00243826, -0.00037372, -0.00114771]), array([-0.01736751])]\n",
      "Iteration 43, Cost: 0.2512401959773608\n",
      "gradient_weights:  [array([[-0.00191855, -0.00087396,  0.00022936],\n",
      "       [ 0.00920046,  0.00055169, -0.00060773]]), array([[-0.01070494],\n",
      "       [-0.00395029],\n",
      "       [-0.00856458]])]\n",
      "gradients_biases:  [array([ 0.00231433, -0.00039714, -0.0011036 ]), array([-0.01664619])]\n",
      "Iteration 44, Cost: 0.2512097827483171\n",
      "gradient_weights:  [array([[-0.00195634, -0.0008819 ,  0.00025219],\n",
      "       [ 0.00914168,  0.00053927, -0.00058817]]), array([[-0.01043833],\n",
      "       [-0.00355145],\n",
      "       [-0.00823439]])]\n",
      "gradients_biases:  [array([ 0.00219629, -0.00041949, -0.00106109]), array([-0.01595579])]\n",
      "Iteration 45, Cost: 0.25118141115396186\n",
      "gradient_weights:  [array([[-0.00199184, -0.00088955,  0.0002741 ],\n",
      "       [ 0.00908541,  0.0005275 , -0.0005693 ]]), array([[-0.01018259],\n",
      "       [-0.00317004],\n",
      "       [-0.00791827]])]\n",
      "gradients_biases:  [array([ 0.00208386, -0.00044084, -0.00102015]), array([-0.015295])]\n",
      "Iteration 46, Cost: 0.2511549104226075\n",
      "gradient_weights:  [array([[-0.00202514, -0.00089694,  0.00029512],\n",
      "       [ 0.00903155,  0.00051635, -0.00055108]]), array([[-0.00993727],\n",
      "       [-0.00280532],\n",
      "       [-0.00761564]])]\n",
      "gradients_biases:  [array([ 0.00197678, -0.00046125, -0.00098072]), array([-0.01466259])]\n",
      "Iteration 47, Cost: 0.2511301240731261\n",
      "gradient_weights:  [array([[-0.00205635, -0.00090408,  0.00031529],\n",
      "       [ 0.00897999,  0.0005058 , -0.00053351]]), array([[-0.00970191],\n",
      "       [-0.00245658],\n",
      "       [-0.00732593]])]\n",
      "gradients_biases:  [array([ 0.00187482, -0.00048078, -0.00094277]), array([-0.01405736])]\n",
      "Iteration 48, Cost: 0.25110690873351293\n",
      "gradient_weights:  [array([[-0.00208558, -0.000911  ,  0.00033462],\n",
      "       [ 0.00893063,  0.00049579, -0.00051656]]), array([[-0.0094761 ],\n",
      "       [-0.00212314],\n",
      "       [-0.0070486 ]])]\n",
      "gradients_biases:  [array([ 0.00177775, -0.00049947, -0.00090624]), array([-0.01347816])]\n",
      "Iteration 49, Cost: 0.2510851330546301\n",
      "gradient_weights:  [array([[-0.00211292, -0.0009177 ,  0.00035315],\n",
      "       [ 0.00888337,  0.00048631, -0.00050022]]), array([[-0.00925942],\n",
      "       [-0.00180434],\n",
      "       [-0.00678313]])]\n",
      "gradients_biases:  [array([ 0.00168534, -0.00051739, -0.0008711 ]), array([-0.01292389])]\n",
      "Iteration 50, Cost: 0.2510646767118781\n",
      "gradient_weights:  [array([[-0.00213845, -0.00092421,  0.00037091],\n",
      "       [ 0.00883813,  0.00047733, -0.00048446]]), array([[-0.0090515 ],\n",
      "       [-0.00149957],\n",
      "       [-0.00652902]])]\n",
      "gradients_biases:  [array([ 0.00159739, -0.00053457, -0.0008373 ]), array([-0.01239349])]\n",
      "Iteration 51, Cost: 0.2510454294880235\n",
      "gradient_weights:  [array([[-0.00216227, -0.00093053,  0.00038791],\n",
      "       [ 0.00879482,  0.00046882, -0.00046927]]), array([[-0.00885195],\n",
      "       [-0.00120821],\n",
      "       [-0.00628579]])]\n",
      "gradients_biases:  [array([ 0.00151369, -0.00055107, -0.00080479]), array([-0.01188595])]\n",
      "Iteration 52, Cost: 0.25102729043087824\n",
      "gradient_weights:  [array([[-0.00218445, -0.00093669,  0.00040419],\n",
      "       [ 0.00875335,  0.00046075, -0.00045463]]), array([[-0.00866041],\n",
      "       [-0.0009297 ],\n",
      "       [-0.00605298]])]\n",
      "gradients_biases:  [array([ 0.00143404, -0.00056691, -0.00077355]), array([-0.0114003])]\n",
      "Iteration 53, Cost: 0.2510101670799619\n",
      "gradient_weights:  [array([[-0.00220507, -0.00094268,  0.00041978],\n",
      "       [ 0.00871364,  0.00045311, -0.00044052]]), array([[-0.00847655],\n",
      "       [-0.00066348],\n",
      "       [-0.00583015]])]\n",
      "gradients_biases:  [array([ 0.00135827, -0.00058214, -0.00074352]), array([-0.0109356])]\n",
      "Iteration 54, Cost: 0.2509939747566998\n",
      "gradient_weights:  [array([[-0.0022242 , -0.00094853,  0.00043469],\n",
      "       [ 0.00867562,  0.00044586, -0.00042692]]), array([[-0.00830004],\n",
      "       [-0.00040903],\n",
      "       [-0.00561688]])]\n",
      "gradients_biases:  [array([ 0.00128621, -0.00059679, -0.00071466]), array([-0.01049096])]\n",
      "Iteration 55, Cost: 0.25097863591310776\n",
      "gradient_weights:  [array([[-0.00224191, -0.00095424,  0.00044895],\n",
      "       [ 0.00863922,  0.00043899, -0.00041383]]), array([[-0.00813056],\n",
      "       [-0.00016583],\n",
      "       [-0.00541275]])]\n",
      "gradients_biases:  [array([ 0.00121768, -0.00061089, -0.00068694]), array([-0.01006553])]\n",
      "Iteration 56, Cost: 0.2509640795342826\n",
      "gradient_weights:  [array([[-0.00225827, -0.00095982,  0.00046259],\n",
      "       [ 0.00860436,  0.00043248, -0.00040121]]), array([[-7.96780038e-03],\n",
      "       [ 6.65867806e-05],\n",
      "       [-5.21738954e-03]])]\n",
      "gradients_biases:  [array([ 0.00115252, -0.00062448, -0.00066031]), array([-0.00965847])]\n",
      "Iteration 57, Cost: 0.25095024059037335\n",
      "gradient_weights:  [array([[-0.00227335, -0.00096528,  0.00047562],\n",
      "       [ 0.00857098,  0.00042631, -0.00038906]]), array([[-0.00781148],\n",
      "       [ 0.0002887 ],\n",
      "       [-0.00503041]])]\n",
      "gradients_biases:  [array([ 0.00109059, -0.00063758, -0.00063473]), array([-0.009269])]\n",
      "Iteration 58, Cost: 0.2509370595340322\n",
      "gradient_weights:  [array([[-0.0022872 , -0.00097063,  0.00048808],\n",
      "       [ 0.00853901,  0.00042046, -0.00037735]]), array([[-0.00766133],\n",
      "       [ 0.00050095],\n",
      "       [-0.00485146]])]\n",
      "gradients_biases:  [array([ 0.00103174, -0.00065022, -0.00061018]), array([-0.00889638])]\n",
      "Iteration 59, Cost: 0.25092448183965443\n",
      "gradient_weights:  [array([[-0.00229988, -0.00097587,  0.00049998],\n",
      "       [ 0.0085084 ,  0.00041492, -0.00036608]]), array([[-0.00751707],\n",
      "       [ 0.00070376],\n",
      "       [-0.0046802 ]])]\n",
      "gradients_biases:  [array([ 0.00097582, -0.00066243, -0.00058661]), array([-0.00853987])]\n",
      "Iteration 60, Cost: 0.25091245758099967\n",
      "gradient_weights:  [array([[-0.00231145, -0.00098102,  0.00051134],\n",
      "       [ 0.00847909,  0.00040967, -0.00035522]]), array([[-0.00737846],\n",
      "       [ 0.00089754],\n",
      "       [-0.00451629]])]\n",
      "gradients_biases:  [array([ 0.00092271, -0.00067422, -0.00056398]), array([-0.00819879])]\n",
      "Iteration 61, Cost: 0.25090094104405475\n",
      "gradient_weights:  [array([[-0.00232195, -0.00098607,  0.00052219],\n",
      "       [ 0.00845101,  0.0004047 , -0.00034476]]), array([[-0.00724525],\n",
      "       [ 0.00108268],\n",
      "       [-0.00435942]])]\n",
      "gradients_biases:  [array([ 0.00087228, -0.00068563, -0.00054226]), array([-0.00787246])]\n",
      "Iteration 62, Cost: 0.2508898903722459\n",
      "gradient_weights:  [array([[-0.00233144, -0.00099103,  0.00053254],\n",
      "       [ 0.00842412,  0.00039999, -0.00033469]]), array([[-0.0071172 ],\n",
      "       [ 0.00125955],\n",
      "       [-0.0042093 ]])]\n",
      "gradients_biases:  [array([ 0.00082441, -0.00069666, -0.00052142]), array([-0.00756027])]\n",
      "Iteration 63, Cost: 0.25087926724133636\n",
      "gradient_weights:  [array([[-0.00233996, -0.00099591,  0.00054241],\n",
      "       [ 0.00839837,  0.00039553, -0.00032499]]), array([[-0.00699411],\n",
      "       [ 0.00142852],\n",
      "       [-0.00406562]])]\n",
      "gradients_biases:  [array([ 0.00077899, -0.00070735, -0.00050143]), array([-0.00726159])]\n",
      "Iteration 64, Cost: 0.2508690365615588\n",
      "gradient_weights:  [array([[-0.00234757, -0.00100071,  0.00055183],\n",
      "       [ 0.0083737 ,  0.00039131, -0.00031565]]), array([[-0.00687574],\n",
      "       [ 0.00158992],\n",
      "       [-0.00392812]])]\n",
      "gradients_biases:  [array([ 0.0007359 , -0.0007177 , -0.00048224]), array([-0.00697585])]\n",
      "Iteration 65, Cost: 0.25085916620472926\n",
      "gradient_weights:  [array([[-0.00235429, -0.00100544,  0.00056081],\n",
      "       [ 0.00835007,  0.00038731, -0.00030665]]), array([[-0.0067619 ],\n",
      "       [ 0.00174408],\n",
      "       [-0.00379653]])]\n",
      "gradients_biases:  [array([ 0.00069505, -0.00072774, -0.00046385]), array([-0.00670248])]\n",
      "Iteration 66, Cost: 0.25084962675426986\n",
      "gradient_weights:  [array([[-0.00236018, -0.0010101 ,  0.00056936],\n",
      "       [ 0.00832744,  0.00038354, -0.00029799]]), array([[-0.0066524 ],\n",
      "       [ 0.00189131],\n",
      "       [-0.0036706 ]])]\n",
      "gradients_biases:  [array([ 0.00065632, -0.00073748, -0.0004462 ]), array([-0.00644097])]\n",
      "Iteration 67, Cost: 0.2508403912762368\n",
      "gradient_weights:  [array([[-0.00236527, -0.0010147 ,  0.00057751],\n",
      "       [ 0.00830576,  0.00037996, -0.00028964]]), array([[-0.00654704],\n",
      "       [ 0.00203192],\n",
      "       [-0.00355008]])]\n",
      "gradients_biases:  [array([ 0.00061963, -0.00074693, -0.00042927]), array([-0.00619079])]\n",
      "Iteration 68, Cost: 0.2508314351096046\n",
      "gradient_weights:  [array([[-0.00236959, -0.00101923,  0.00058526],\n",
      "       [ 0.00828499,  0.00037658, -0.00028161]]), array([[-0.00644566],\n",
      "       [ 0.0021662 ],\n",
      "       [-0.00343474]])]\n",
      "gradients_biases:  [array([ 0.00058488, -0.00075612, -0.00041305]), array([-0.00595145])]\n",
      "Iteration 69, Cost: 0.2508227356741998\n",
      "gradient_weights:  [array([[-0.0023732 , -0.00102371,  0.00059265],\n",
      "       [ 0.00826509,  0.00037339, -0.00027386]]), array([[-0.00634806],\n",
      "       [ 0.00229441],\n",
      "       [-0.00332435]])]\n",
      "gradients_biases:  [array([ 0.00055198, -0.00076505, -0.00039749]), array([-0.00572249])]\n",
      "Iteration 70, Cost: 0.2508142722948124\n",
      "gradient_weights:  [array([[-0.00237611, -0.00102813,  0.00059967],\n",
      "       [ 0.00824603,  0.00037037, -0.00026641]]), array([[-0.00625411],\n",
      "       [ 0.00241682],\n",
      "       [-0.00321871]])]\n",
      "gradients_biases:  [array([ 0.00052086, -0.00077373, -0.00038257]), array([-0.00550347])]\n",
      "Iteration 71, Cost: 0.25080602604012747\n",
      "gradient_weights:  [array([[-0.00237836, -0.00103249,  0.00060635],\n",
      "       [ 0.00822778,  0.00036753, -0.00025922]]), array([[-0.00616363],\n",
      "       [ 0.00253369],\n",
      "       [-0.00311761]])]\n",
      "gradients_biases:  [array([ 0.00049144, -0.00078218, -0.00036827]), array([-0.00529394])]\n",
      "Iteration 72, Cost: 0.25079797957523997\n",
      "gradient_weights:  [array([[-0.00237998, -0.00103681,  0.0006127 ],\n",
      "       [ 0.00821028,  0.00036484, -0.0002523 ]]), array([[-0.00607647],\n",
      "       [ 0.00264524],\n",
      "       [-0.00302086]])]\n",
      "gradients_biases:  [array([ 0.00046363, -0.00079041, -0.00035456]), array([-0.00509351])]\n",
      "Iteration 73, Cost: 0.2507901170266126\n",
      "gradient_weights:  [array([[-0.002381  , -0.00104107,  0.00061873],\n",
      "       [ 0.00819353,  0.00036231, -0.00024563]]), array([[-0.0059925 ],\n",
      "       [ 0.00275173],\n",
      "       [-0.00292826]])]\n",
      "gradients_biases:  [array([ 0.00043737, -0.00079843, -0.00034142]), array([-0.00490178])]\n",
      "Iteration 74, Cost: 0.2507824238584303\n",
      "gradient_weights:  [array([[-0.00238145, -0.0010453 ,  0.00062446],\n",
      "       [ 0.00817747,  0.00035993, -0.0002392 ]]), array([[-0.00591158],\n",
      "       [ 0.00285335],\n",
      "       [-0.00283964]])]\n",
      "gradients_biases:  [array([ 0.00041258, -0.00080625, -0.00032883]), array([-0.00471837])]\n",
      "Iteration 75, Cost: 0.2507748867593971\n",
      "gradient_weights:  [array([[-0.00238135, -0.00104947,  0.00062989],\n",
      "       [ 0.00816209,  0.00035768, -0.00023301]]), array([[-0.00583357],\n",
      "       [ 0.00295033],\n",
      "       [-0.00275482]])]\n",
      "gradients_biases:  [array([ 0.0003892 , -0.00081387, -0.00031677]), array([-0.00454292])]\n",
      "Iteration 76, Cost: 0.2507674935390948\n",
      "gradient_weights:  [array([[-0.00238074, -0.00105361,  0.00063504],\n",
      "       [ 0.00814735,  0.00035557, -0.00022703]]), array([[-0.00575836],\n",
      "       [ 0.00304287],\n",
      "       [-0.00267365]])]\n",
      "gradients_biases:  [array([ 0.00036718, -0.00082132, -0.0003052 ]), array([-0.00437509])]\n",
      "Iteration 77, Cost: 0.2507602330331008\n",
      "gradient_weights:  [array([[-0.00237962, -0.00105771,  0.00063993],\n",
      "       [ 0.00813323,  0.00035359, -0.00022128]]), array([[-0.00568581],\n",
      "       [ 0.00313116],\n",
      "       [-0.00259596]])]\n",
      "gradients_biases:  [array([ 0.00034643, -0.00082859, -0.00029413]), array([-0.00421455])]\n",
      "Iteration 78, Cost: 0.25075309501612575\n",
      "gradient_weights:  [array([[-0.00237804, -0.00106177,  0.00064456],\n",
      "       [ 0.0081197 ,  0.00035173, -0.00021573]]), array([[-0.00561582],\n",
      "       [ 0.00321538],\n",
      "       [-0.00252161]])]\n",
      "gradients_biases:  [array([ 0.00032692, -0.00083569, -0.00028351]), array([-0.00406099])]\n",
      "Iteration 79, Cost: 0.2507460701224967\n",
      "gradient_weights:  [array([[-0.002376  , -0.00106579,  0.00064894],\n",
      "       [ 0.00810673,  0.00034998, -0.00021037]]), array([[-0.00554827],\n",
      "       [ 0.00329571],\n",
      "       [-0.00245044]])]\n",
      "gradients_biases:  [array([ 0.00030859, -0.00084263, -0.00027334]), array([-0.0039141])]\n",
      "Iteration 80, Cost: 0.25073914977336526\n",
      "gradient_weights:  [array([[-0.00237354, -0.00106978,  0.00065309],\n",
      "       [ 0.00809431,  0.00034835, -0.00020521]]), array([[-0.00548306],\n",
      "       [ 0.00337233],\n",
      "       [-0.00238234]])]\n",
      "gradients_biases:  [array([ 0.00029137, -0.00084943, -0.0002636 ]), array([-0.00377359])]\n",
      "Iteration 81, Cost: 0.25073232611007445\n",
      "gradient_weights:  [array([[-0.00237066, -0.00107373,  0.00065701],\n",
      "       [ 0.00808241,  0.00034682, -0.00020023]]), array([[-0.00542009],\n",
      "       [ 0.00344539],\n",
      "       [-0.00231715]])]\n",
      "gradients_biases:  [array([ 0.00027523, -0.00085607, -0.00025427]), array([-0.00363919])]\n",
      "Iteration 82, Cost: 0.25072559193316385\n",
      "gradient_weights:  [array([[-0.00236739, -0.00107765,  0.00066071],\n",
      "       [ 0.00807101,  0.0003454 , -0.00019542]]), array([[-0.00535926],\n",
      "       [ 0.00351504],\n",
      "       [-0.00225475]])]\n",
      "gradients_biases:  [array([ 0.00026011, -0.00086258, -0.00024533]), array([-0.00351064])]\n",
      "Iteration 83, Cost: 0.2507189406465375\n",
      "gradient_weights:  [array([[-0.00236376, -0.00108154,  0.00066421],\n",
      "       [ 0.00806008,  0.00034407, -0.00019078]]), array([[-0.00530049],\n",
      "       [ 0.00358144],\n",
      "       [-0.00219504]])]\n",
      "gradients_biases:  [array([ 0.00024597, -0.00086896, -0.00023677]), array([-0.00338768])]\n",
      "Iteration 84, Cost: 0.2507123662063575\n",
      "gradient_weights:  [array([[-0.00235976, -0.0010854 ,  0.0006675 ],\n",
      "       [ 0.00804962,  0.00034284, -0.0001863 ]]), array([[-0.00524367],\n",
      "       [ 0.00364473],\n",
      "       [-0.00213788]])]\n",
      "gradients_biases:  [array([ 0.00023276, -0.00087521, -0.00022857]), array([-0.00327007])]\n",
      "Iteration 85, Cost: 0.25070586307426534\n",
      "gradient_weights:  [array([[-0.00235543, -0.00108923,  0.00067061],\n",
      "       [ 0.00803959,  0.00034169, -0.00018198]]), array([[-0.00518874],\n",
      "       [ 0.00370504],\n",
      "       [-0.00208317]])]\n",
      "gradients_biases:  [array([ 0.00022045, -0.00088134, -0.00022072]), array([-0.00315758])]\n",
      "Iteration 86, Cost: 0.25069942617456403\n",
      "gradient_weights:  [array([[-0.00235078, -0.00109304,  0.00067353],\n",
      "       [ 0.00802998,  0.00034063, -0.0001778 ]]), array([[-0.0051356],\n",
      "       [ 0.0037625],\n",
      "       [-0.0020308]])]\n",
      "gradients_biases:  [array([ 0.00020899, -0.00088735, -0.0002132 ]), array([-0.00304998])]\n",
      "Iteration 87, Cost: 0.2506930508550267\n",
      "gradient_weights:  [array([[-0.00234581, -0.00109682,  0.00067627],\n",
      "       [ 0.00802078,  0.00033965, -0.00017377]]), array([[-0.00508418],\n",
      "       [ 0.00381724],\n",
      "       [-0.00198068]])]\n",
      "gradients_biases:  [array([ 0.00019834, -0.00089325, -0.000206  ]), array([-0.00294707])]\n",
      "Iteration 88, Cost: 0.2506867328510246\n",
      "gradient_weights:  [array([[-0.00234056, -0.00110057,  0.00067885],\n",
      "       [ 0.00801196,  0.00033874, -0.00016987]]), array([[-0.00503441],\n",
      "       [ 0.00386937],\n",
      "       [-0.00193269]])]\n",
      "gradients_biases:  [array([ 0.00018847, -0.00089905, -0.0001991 ]), array([-0.00284864])]\n",
      "Iteration 89, Cost: 0.2506804682526946\n",
      "gradient_weights:  [array([[-0.00233503, -0.00110429,  0.00068127],\n",
      "       [ 0.00800351,  0.00033792, -0.00016611]]), array([[-0.00498621],\n",
      "       [ 0.00391901],\n",
      "       [-0.00188677]])]\n",
      "gradients_biases:  [array([ 0.00017935, -0.00090474, -0.0001925 ]), array([-0.0027545])]\n",
      "Iteration 90, Cost: 0.25067425347488687\n",
      "gradient_weights:  [array([[-0.00232923, -0.001108  ,  0.00068354],\n",
      "       [ 0.00799542,  0.00033716, -0.00016247]]), array([[-0.00493952],\n",
      "       [ 0.00396627],\n",
      "       [-0.0018428 ]])]\n",
      "gradients_biases:  [array([ 0.00017094, -0.00091034, -0.00018618]), array([-0.00266447])]\n",
      "Iteration 91, Cost: 0.250668085229661\n",
      "gradient_weights:  [array([[-0.00232317, -0.00111168,  0.00068565],\n",
      "       [ 0.00798767,  0.00033647, -0.00015894]]), array([[-0.00489428],\n",
      "       [ 0.00401125],\n",
      "       [-0.00180071]])]\n",
      "gradients_biases:  [array([ 0.0001632 , -0.00091584, -0.00018012]), array([-0.00257836])]\n",
      "Iteration 92, Cost: 0.2506619605011102\n",
      "gradient_weights:  [array([[-0.00231688, -0.00111533,  0.00068763],\n",
      "       [ 0.00798024,  0.00033585, -0.00015554]]), array([[-0.00485041],\n",
      "       [ 0.00405404],\n",
      "       [-0.00176042]])]\n",
      "gradients_biases:  [array([ 0.00015612, -0.00092125, -0.00017433]), array([-0.002496])]\n",
      "Iteration 93, Cost: 0.2506558765223215\n",
      "gradient_weights:  [array([[-0.00231036, -0.00111897,  0.00068947],\n",
      "       [ 0.00797313,  0.00033529, -0.00015224]]), array([[-0.00480787],\n",
      "       [ 0.00409476],\n",
      "       [-0.00172186]])]\n",
      "gradients_biases:  [array([ 0.00014965, -0.00092658, -0.00016878]), array([-0.00241724])]\n",
      "Iteration 94, Cost: 0.25064983075428643\n",
      "gradient_weights:  [array([[-0.00230362, -0.00112258,  0.00069118],\n",
      "       [ 0.00796632,  0.00033478, -0.00014905]]), array([[-0.0047666 ],\n",
      "       [ 0.00413348],\n",
      "       [-0.00168493]])]\n",
      "gradients_biases:  [array([ 0.00014378, -0.00093182, -0.00016347]), array([-0.00234191])]\n",
      "Iteration 95, Cost: 0.25064382086660036\n",
      "gradient_weights:  [array([[-0.00229667, -0.00112618,  0.00069277],\n",
      "       [ 0.00795981,  0.00033434, -0.00014597]]), array([[-0.00472654],\n",
      "       [ 0.00417029],\n",
      "       [-0.00164958]])]\n",
      "gradients_biases:  [array([ 0.00013847, -0.00093699, -0.00015839]), array([-0.00226988])]\n",
      "Iteration 96, Cost: 0.2506378447197976\n",
      "gradient_weights:  [array([[-0.00228952, -0.00112975,  0.00069424],\n",
      "       [ 0.00795356,  0.00033395, -0.00014297]]), array([[-0.00468763],\n",
      "       [ 0.00420528],\n",
      "       [-0.00161574]])]\n",
      "gradients_biases:  [array([ 0.0001337 , -0.00094208, -0.00015352]), array([-0.00220099])]\n",
      "Iteration 97, Cost: 0.2506319003491836\n",
      "gradient_weights:  [array([[-0.00228218, -0.00113331,  0.0006956 ],\n",
      "       [ 0.00794759,  0.00033361, -0.00014008]]), array([[-0.00464984],\n",
      "       [ 0.00423853],\n",
      "       [-0.00158334]])]\n",
      "gradients_biases:  [array([ 0.00012944, -0.00094709, -0.00014887]), array([-0.00213511])]\n",
      "Iteration 98, Cost: 0.2506259859500373\n",
      "gradient_weights:  [array([[-0.00227466, -0.00113684,  0.00069685],\n",
      "       [ 0.00794187,  0.00033333, -0.00013727]]), array([[-0.00461311],\n",
      "       [ 0.00427011],\n",
      "       [-0.00155232]])]\n",
      "gradients_biases:  [array([ 0.00012568, -0.00095204, -0.00014441]), array([-0.00207211])]\n",
      "Iteration 99, Cost: 0.2506200998640693\n",
      "gradient_weights:  [array([[-0.00226697, -0.00114036,  0.00069799],\n",
      "       [ 0.00793639,  0.00033309, -0.00013455]]), array([[-0.0045774 ],\n",
      "       [ 0.00430009],\n",
      "       [-0.00152261]])]\n",
      "gradients_biases:  [array([ 0.00012239, -0.00095692, -0.00014014]), array([-0.00201187])]\n",
      "Iteration 100, Cost: 0.25061424056702625\n",
      "gradient_weights:  [array([[-0.00225912, -0.00114386,  0.00069904],\n",
      "       [ 0.00793115,  0.0003329 , -0.00013191]]), array([[-0.00454266],\n",
      "       [ 0.00432855],\n",
      "       [-0.00149417]])]\n",
      "gradients_biases:  [array([ 0.00011955, -0.00096174, -0.00013606]), array([-0.00195426])]\n",
      "Iteration 101, Cost: 0.2506084066573477\n",
      "gradient_weights:  [array([[-0.00225111, -0.00114734,  0.00069999],\n",
      "       [ 0.00792614,  0.00033275, -0.00012935]]), array([[-0.00450886],\n",
      "       [ 0.00435555],\n",
      "       [-0.00146694]])]\n",
      "gradients_biases:  [array([ 0.00011713, -0.00096649, -0.00013215]), array([-0.00189918])]\n",
      "Iteration 102, Cost: 0.2506025968457831\n",
      "gradient_weights:  [array([[-0.00224296, -0.00115081,  0.00070085],\n",
      "       [ 0.00792135,  0.00033265, -0.00012686]]), array([[-0.00447594],\n",
      "       [ 0.00438116],\n",
      "       [-0.00144086]])]\n",
      "gradients_biases:  [array([ 0.00011513, -0.00097119, -0.00012841]), array([-0.00184651])]\n",
      "Iteration 103, Cost: 0.2505968099458909\n",
      "gradient_weights:  [array([[-0.00223467, -0.00115426,  0.00070162],\n",
      "       [ 0.00791676,  0.00033258, -0.00012445]]), array([[-0.00444389],\n",
      "       [ 0.00440543],\n",
      "       [-0.00141589]])]\n",
      "gradients_biases:  [array([ 0.00011352, -0.00097582, -0.00012484]), array([-0.00179614])]\n",
      "Iteration 104, Cost: 0.25059104486534134\n",
      "gradient_weights:  [array([[-0.00222624, -0.00115769,  0.00070231],\n",
      "       [ 0.00791237,  0.00033256, -0.0001221 ]]), array([[-0.00441265],\n",
      "       [ 0.00442843],\n",
      "       [-0.00139198]])]\n",
      "gradients_biases:  [array([ 0.00011228, -0.00098041, -0.00012141]), array([-0.00174799])]\n",
      "Iteration 105, Cost: 0.2505853005979582\n",
      "gradient_weights:  [array([[-0.00221769, -0.00116111,  0.00070292],\n",
      "       [ 0.00790818,  0.00033257, -0.00011982]]), array([[-0.00438219],\n",
      "       [ 0.0044502 ],\n",
      "       [-0.00136908]])]\n",
      "gradients_biases:  [array([ 0.0001114 , -0.00098494, -0.00011814]), array([-0.00170195])]\n",
      "Iteration 106, Cost: 0.2505795762164339\n",
      "gradient_weights:  [array([[-0.00220901, -0.00116452,  0.00070346],\n",
      "       [ 0.00790418,  0.00033262, -0.00011761]]), array([[-0.00435249],\n",
      "       [ 0.00447081],\n",
      "       [-0.00134714]])]\n",
      "gradients_biases:  [array([ 0.00011086, -0.00098942, -0.000115  ]), array([-0.00165793])]\n",
      "Iteration 107, Cost: 0.25057387086566413\n",
      "gradient_weights:  [array([[-0.00220023, -0.00116791,  0.00070392],\n",
      "       [ 0.00790035,  0.0003327 , -0.00011546]]), array([[-0.0043235 ],\n",
      "       [ 0.00449031],\n",
      "       [-0.00132614]])]\n",
      "gradients_biases:  [array([ 0.00011065, -0.00099385, -0.000112  ]), array([-0.00161585])]\n",
      "Iteration 108, Cost: 0.25056818375664525\n",
      "gradient_weights:  [array([[-0.00219133, -0.00117128,  0.00070431],\n",
      "       [ 0.0078967 ,  0.00033282, -0.00011336]]), array([[-0.0042952 ],\n",
      "       [ 0.00450874],\n",
      "       [-0.00130602]])]\n",
      "gradients_biases:  [array([ 0.00011074, -0.00099823, -0.00010913]), array([-0.00157562])]\n",
      "Iteration 109, Cost: 0.2505625141608915\n",
      "gradient_weights:  [array([[-0.00218233, -0.00117464,  0.00070464],\n",
      "       [ 0.00789321,  0.00033296, -0.00011132]]), array([[-0.00426756],\n",
      "       [ 0.00452615],\n",
      "       [-0.00128675]])]\n",
      "gradients_biases:  [array([ 0.00011114, -0.00100257, -0.00010639]), array([-0.00153716])]\n",
      "Iteration 110, Cost: 0.2505568614053235\n",
      "gradient_weights:  [array([[-0.00217323, -0.00117799,  0.0007049 ],\n",
      "       [ 0.00788988,  0.00033314, -0.00010934]]), array([[-0.00424055],\n",
      "       [ 0.00454259],\n",
      "       [-0.00126829]])]\n",
      "gradients_biases:  [array([ 0.00011181, -0.00100687, -0.00010376]), array([-0.00150039])]\n",
      "Iteration 111, Cost: 0.2505512248675922\n",
      "gradient_weights:  [array([[-0.00216404, -0.00118133,  0.0007051 ],\n",
      "       [ 0.0078867 ,  0.00033335, -0.0001074 ]]), array([[-0.00421415],\n",
      "       [ 0.00455809],\n",
      "       [-0.00125061]])]\n",
      "gradients_biases:  [array([ 0.00011276, -0.00101112, -0.00010125]), array([-0.00146524])]\n",
      "Iteration 112, Cost: 0.2505456039717974\n",
      "gradient_weights:  [array([[-0.00215477, -0.00118465,  0.00070525],\n",
      "       [ 0.00788367,  0.00033358, -0.00010552]]), array([[-0.00418832],\n",
      "       [ 0.00457271],\n",
      "       [-0.00123367]])]\n",
      "gradients_biases:  [array([ 1.13967239e-04, -1.01533720e-03, -9.88504873e-05]), array([-0.00143165])]\n",
      "Iteration 113, Cost: 0.2505399981845706\n",
      "gradient_weights:  [array([[-0.0021454 , -0.00118796,  0.00070533],\n",
      "       [ 0.00788078,  0.00033384, -0.00010368]]), array([[-0.00416305],\n",
      "       [ 0.00458647],\n",
      "       [-0.00121744]])]\n",
      "gradients_biases:  [array([ 1.15419606e-04, -1.01951347e-03, -9.65529422e-05]), array([-0.00139954])]\n",
      "Iteration 114, Cost: 0.2505344070114895\n",
      "gradient_weights:  [array([[-0.00213596, -0.00119125,  0.00070537],\n",
      "       [ 0.00787803,  0.00033413, -0.00010189]]), array([[-0.00413831],\n",
      "       [ 0.00459942],\n",
      "       [-0.0012019 ]])]\n",
      "gradients_biases:  [array([ 1.17107318e-04, -1.02365270e-03, -9.43555572e-05]), array([-0.00136885])]\n",
      "Iteration 115, Cost: 0.25052882999379644\n",
      "gradient_weights:  [array([[-0.00212645, -0.00119454,  0.00070535],\n",
      "       [ 0.00787541,  0.00033444, -0.00010014]]), array([[-0.00411408],\n",
      "       [ 0.00461159],\n",
      "       [-0.001187  ]])]\n",
      "gradients_biases:  [array([ 1.19019658e-04, -1.02775628e-03, -9.22540839e-05]), array([-0.00133951])]\n",
      "Iteration 116, Cost: 0.25052326670539515\n",
      "gradient_weights:  [array([[-2.11686116e-03, -1.19780892e-03,  7.05287318e-04],\n",
      "       [ 7.87291798e-03,  3.34780076e-04, -9.84329820e-05]]), array([[-0.00409034],\n",
      "       [ 0.00462302],\n",
      "       [-0.00117273]])]\n",
      "gradients_biases:  [array([ 1.21146385e-04, -1.03182556e-03, -9.02444528e-05]), array([-0.00131148])]\n",
      "Iteration 117, Cost: 0.2505177167501025\n",
      "gradient_weights:  [array([[-2.10720745e-03, -1.20107004e-03,  7.05176326e-04],\n",
      "       [ 7.87054830e-03,  3.35138694e-04, -9.67652869e-05]]), array([[-0.00406706],\n",
      "       [ 0.00463373],\n",
      "       [-0.00115905]])]\n",
      "gradients_biases:  [array([ 1.23477708e-04, -1.03586180e-03, -8.83227657e-05]), array([-0.00128469])]\n",
      "Iteration 118, Cost: 0.2505121797591322\n",
      "gradient_weights:  [array([[-2.09749016e-03, -1.20431990e-03,  7.05021030e-04],\n",
      "       [ 7.86829672e-03,  3.35519168e-04, -9.51356789e-05]]), array([[-0.00404423],\n",
      "       [ 0.00464377],\n",
      "       [-0.00114594]])]\n",
      "gradients_biases:  [array([ 1.26004268e-04, -1.03986623e-03, -8.64852888e-05]), array([-0.00125909])]\n",
      "Iteration 119, Cost: 0.2505066553887937\n",
      "gradient_weights:  [array([[-2.08771274e-03, -1.20755874e-03,  7.04823401e-04],\n",
      "       [ 7.86615908e-03,  3.35920626e-04, -9.35425613e-05]]), array([[-0.00402183],\n",
      "       [ 0.00465315],\n",
      "       [-0.00113338]])]\n",
      "gradients_biases:  [array([ 1.28717120e-04, -1.04384003e-03, -8.47284457e-05]), array([-0.00123463])]\n",
      "Iteration 120, Cost: 0.2505011433183836\n",
      "gradient_weights:  [array([[-2.07787849e-03, -1.21078680e-03,  7.04585328e-04],\n",
      "       [ 7.86413136e-03,  3.36342233e-04, -9.19844039e-05]]), array([[-0.00399983],\n",
      "       [ 0.00466192],\n",
      "       [-0.00112134]])]\n",
      "gradients_biases:  [array([ 1.31607712e-04, -1.04778432e-03, -8.30488112e-05]), array([-0.00121125])]\n",
      "Iteration 121, Cost: 0.2504956432482583\n",
      "gradient_weights:  [array([[-2.06799056e-03, -1.21400432e-03,  7.04308617e-04],\n",
      "       [ 7.86220975e-03,  3.36783191e-04, -9.04597409e-05]]), array([[-0.00397824],\n",
      "       [ 0.00467009],\n",
      "       [-0.0011098 ]])]\n",
      "gradients_biases:  [array([ 1.34667869e-04, -1.05170018e-03, -8.14431050e-05]), array([-0.00118892])]\n",
      "Iteration 122, Cost: 0.2504901548980687\n",
      "gradient_weights:  [array([[-2.05805196e-03, -1.21721150e-03,  7.03994998e-04],\n",
      "       [ 7.86039057e-03,  3.37242734e-04, -8.89671676e-05]]), array([[-0.00395701],\n",
      "       [ 0.00467769],\n",
      "       [-0.00109874]])]\n",
      "gradients_biases:  [array([ 1.37889778e-04, -1.05558865e-03, -7.99081852e-05]), array([-0.00116759])]\n",
      "Iteration 123, Cost: 0.25048467800514435\n",
      "gradient_weights:  [array([[-2.04806555e-03, -1.22040857e-03,  7.03646127e-04],\n",
      "       [ 7.85867033e-03,  3.37720129e-04, -8.75053387e-05]]), array([[-0.00393615],\n",
      "       [ 0.00468475],\n",
      "       [-0.00108814]])]\n",
      "gradients_biases:  [array([ 1.41265969e-04, -1.05945072e-03, -7.84410434e-05]), array([-0.00114721])]\n",
      "Iteration 124, Cost: 0.25047921232301545\n",
      "gradient_weights:  [array([[-2.03803408e-03, -1.22359573e-03,  7.03263588e-04],\n",
      "       [ 7.85704567e-03,  3.38214674e-04, -8.60729649e-05]]), array([[-0.00391564],\n",
      "       [ 0.0046913 ],\n",
      "       [-0.00107798]])]\n",
      "gradients_biases:  [array([ 1.44789303e-04, -1.06328735e-03, -7.70387984e-05]), array([-0.00112774])]\n",
      "Iteration 125, Cost: 0.2504737576200597\n",
      "gradient_weights:  [array([[-2.02796017e-03, -1.22677319e-03,  7.02848899e-04],\n",
      "       [ 7.85551338e-03,  3.38725695e-04, -8.46688115e-05]]), array([[-0.00389546],\n",
      "       [ 0.00469734],\n",
      "       [-0.00106823]])]\n",
      "gradients_biases:  [array([ 1.48452953e-04, -1.06709946e-03, -7.56986912e-05]), array([-0.00110915])]\n",
      "Iteration 126, Cost: 0.25046831367826317\n",
      "gradient_weights:  [array([[-2.01784630e-03, -1.22994113e-03,  7.02403511e-04],\n",
      "       [ 7.85407039e-03,  3.39252550e-04, -8.32916955e-05]]), array([[-0.00387559],\n",
      "       [ 0.00470292],\n",
      "       [-0.00105889]])]\n",
      "gradients_biases:  [array([ 1.52250395e-04, -1.07088792e-03, -7.44180798e-05]), array([-0.00109139])]\n",
      "Iteration 127, Cost: 0.2504628802920875\n",
      "gradient_weights:  [array([[-2.00769487e-03, -1.23309975e-03,  7.01928814e-04],\n",
      "       [ 7.85271376e-03,  3.39794621e-04, -8.19404835e-05]]), array([[-0.00385603],\n",
      "       [ 0.00470804],\n",
      "       [-0.00104992]])]\n",
      "gradients_biases:  [array([ 1.56175393e-04, -1.07465358e-03, -7.31944346e-05]), array([-0.00107443])]\n",
      "Iteration 128, Cost: 0.250457457267432\n",
      "gradient_weights:  [array([[-1.99750816e-03, -1.23624923e-03,  7.01426138e-04],\n",
      "       [ 7.85144068e-03,  3.40351317e-04, -8.06140903e-05]]), array([[-0.00383676],\n",
      "       [ 0.00471273],\n",
      "       [-0.00104133]])]\n",
      "gradients_biases:  [array([ 1.60221984e-04, -1.07839724e-03, -7.20253334e-05]), array([-0.00105824])]\n",
      "Iteration 129, Cost: 0.2504520444206839\n",
      "gradient_weights:  [array([[-1.98728832e-03, -1.23938974e-03,  7.00896757e-04],\n",
      "       [ 7.85024846e-03,  3.40922072e-04, -7.93114759e-05]]), array([[-0.00381778],\n",
      "       [ 0.00471701],\n",
      "       [-0.00103309]])]\n",
      "gradients_biases:  [array([ 1.64384470e-04, -1.08211970e-03, -7.09084568e-05]), array([-0.00104278])]\n",
      "Iteration 130, Cost: 0.25044664157784957\n",
      "gradient_weights:  [array([[-1.97703743e-03, -1.24252146e-03,  7.00341888e-04],\n",
      "       [ 7.84913451e-03,  3.41506344e-04, -7.80316444e-05]]), array([[-0.00379906],\n",
      "       [ 0.0047209 ],\n",
      "       [-0.00102518]])]\n",
      "gradients_biases:  [array([ 1.68657402e-04, -1.08582168e-03, -6.98415843e-05]), array([-0.00102802])]\n",
      "Iteration 131, Cost: 0.25044124857375877\n",
      "gradient_weights:  [array([[-1.96675748e-03, -1.24564454e-03,  6.99762698e-04],\n",
      "       [ 7.84809638e-03,  3.42103613e-04, -7.67736418e-05]]), array([[-0.0037806],\n",
      "       [ 0.0047244],\n",
      "       [-0.0010176]])]\n",
      "gradients_biases:  [array([ 1.73035574e-04, -1.08950391e-03, -6.88225897e-05]), array([-0.00101393])]\n",
      "Iteration 132, Cost: 0.25043586525133615\n",
      "gradient_weights:  [array([[-1.95645035e-03, -1.24875916e-03,  6.99160303e-04],\n",
      "       [ 7.84713171e-03,  3.42713382e-04, -7.55365544e-05]]), array([[-0.00376238],\n",
      "       [ 0.00472755],\n",
      "       [-0.00101032]])]\n",
      "gradients_biases:  [array([ 1.77514007e-04, -1.09316708e-03, -6.78494375e-05]), array([-0.00100048])]\n",
      "Iteration 133, Cost: 0.25043049146093554\n",
      "gradient_weights:  [array([[-1.94611783e-03, -1.25186547e-03,  6.98535770e-04],\n",
      "       [ 7.84623823e-03,  3.43335175e-04, -7.43195069e-05]]), array([[-0.0037444 ],\n",
      "       [ 0.00473035],\n",
      "       [-0.00100334]])]\n",
      "gradients_biases:  [array([ 1.82087942e-04, -1.09681185e-03, -6.69201788e-05]), array([-0.00098764])]\n",
      "Iteration 134, Cost: 0.25042512705972886\n",
      "gradient_weights:  [array([[-1.93576165e-03, -1.25496361e-03,  6.97890121e-04],\n",
      "       [ 7.84541379e-03,  3.43968535e-04, -7.31216611e-05]]), array([[-0.00372665],\n",
      "       [ 0.00473283],\n",
      "       [-0.00099664]])]\n",
      "gradients_biases:  [array([ 1.86752831e-04, -1.10043885e-03, -6.60329479e-05]), array([-0.0009754])]\n",
      "Iteration 135, Cost: 0.2504197719111484\n",
      "gradient_weights:  [array([[-1.92538344e-03, -1.25805373e-03,  6.97224334e-04],\n",
      "       [ 7.84465631e-03,  3.44613025e-04, -7.19422141e-05]]), array([[-0.00370912],\n",
      "       [ 0.00473499],\n",
      "       [-0.00099021]])]\n",
      "gradients_biases:  [array([ 1.91504327e-04, -1.10404869e-03, -6.51859585e-05]), array([-0.00096371])]\n",
      "Iteration 136, Cost: 0.2504144258843751\n",
      "gradient_weights:  [array([[-1.91498477e-03, -1.26113599e-03,  6.96539346e-04],\n",
      "       [ 7.84396382e-03,  3.45268226e-04, -7.07803970e-05]]), array([[-0.00369179],\n",
      "       [ 0.00473685],\n",
      "       [-0.00098404]])]\n",
      "gradients_biases:  [array([ 1.96338272e-04, -1.10764196e-03, -6.43775008e-05]), array([-0.00095256])]\n",
      "Iteration 137, Cost: 0.25040908885387136\n",
      "gradient_weights:  [array([[-1.90456713e-03, -1.26421050e-03,  6.95836051e-04],\n",
      "       [ 7.84333441e-03,  3.45933737e-04, -6.96354732e-05]]), array([[-0.00367467],\n",
      "       [ 0.00473843],\n",
      "       [-0.00097811]])]\n",
      "gradients_biases:  [array([ 2.01250694e-04, -1.11121921e-03, -6.36059375e-05]), array([-0.00094193])]\n",
      "Iteration 138, Cost: 0.2504037606989517\n",
      "gradient_weights:  [array([[-1.89413195e-03, -1.26727742e-03,  6.95115307e-04],\n",
      "       [ 7.84276628e-03,  3.46609174e-04, -6.85067376e-05]]), array([[-0.00365774],\n",
      "       [ 0.00473974],\n",
      "       [-0.00097243]])]\n",
      "gradients_biases:  [array([ 2.06237795e-04, -1.11478100e-03, -6.28697018e-05]), array([-0.00093179])]\n",
      "Iteration 139, Cost: 0.2503984413033911\n",
      "gradient_weights:  [array([[-1.88368057e-03, -1.27033686e-03,  6.94377933e-04],\n",
      "       [ 7.84225768e-03,  3.47294169e-04, -6.73935144e-05]]), array([[-0.00364099],\n",
      "       [ 0.00474078],\n",
      "       [-0.00096696]])]\n",
      "gradients_biases:  [array([ 2.11295945e-04, -1.11832783e-03, -6.21672933e-05]), array([-0.00092212])]\n",
      "Iteration 140, Cost: 0.2503931305550664\n",
      "gradient_weights:  [array([[-1.87321429e-03, -1.27338895e-03,  6.93624714e-04],\n",
      "       [ 7.84180695e-03,  3.47988369e-04, -6.62951568e-05]]), array([[-0.00362442],\n",
      "       [ 0.00474157],\n",
      "       [-0.00096172]])]\n",
      "gradients_biases:  [array([ 2.16421675e-04, -1.12186021e-03, -6.14972760e-05]), array([-0.0009129])]\n",
      "Iteration 141, Cost: 0.2503878283456267\n",
      "gradient_weights:  [array([[-1.86273434e-03, -1.27643382e-03,  6.92856401e-04],\n",
      "       [ 7.84141250e-03,  3.48691438e-04, -6.52110452e-05]]), array([[-0.00360802],\n",
      "       [ 0.00474213],\n",
      "       [-0.00095668]])]\n",
      "gradients_biases:  [array([ 2.21611668e-04, -1.12537861e-03, -6.08582753e-05]), array([-0.00090412])]\n",
      "Iteration 142, Cost: 0.2503825345701931\n",
      "gradient_weights:  [array([[-1.85224190e-03, -1.27947158e-03,  6.92073712e-04],\n",
      "       [ 7.84107280e-03,  3.49403052e-04, -6.41405861e-05]]), array([[-0.00359178],\n",
      "       [ 0.00474246],\n",
      "       [-0.00095185]])]\n",
      "gradients_biases:  [array([ 2.26862753e-04, -1.12888350e-03, -6.02489753e-05]), array([-0.00089574])]\n",
      "Iteration 143, Cost: 0.25037724912708337\n",
      "gradient_weights:  [array([[-1.84173810e-03, -1.28250236e-03,  6.91277335e-04],\n",
      "       [ 7.84078639e-03,  3.50122902e-04, -6.30832112e-05]]), array([[-0.0035757 ],\n",
      "       [ 0.00474257],\n",
      "       [-0.0009472 ]])]\n",
      "gradients_biases:  [array([ 2.32171900e-04, -1.13237531e-03, -5.96681162e-05]), array([-0.00088776])]\n",
      "Iteration 144, Cost: 0.250371971917559\n",
      "gradient_weights:  [array([[-1.83122398e-03, -1.28552625e-03,  6.90467927e-04],\n",
      "       [ 7.84055187e-03,  3.50850690e-04, -6.20383764e-05]]), array([[-0.00355977],\n",
      "       [ 0.00474248],\n",
      "       [-0.00094273]])]\n",
      "gradients_biases:  [array([ 2.37536213e-04, -1.13585446e-03, -5.91144926e-05]), array([-0.00088016])]\n",
      "Iteration 145, Cost: 0.2503667028455949\n",
      "gradient_weights:  [array([[-1.82070059e-03, -1.28854338e-03,  6.89646116e-04],\n",
      "       [ 7.84036791e-03,  3.51586135e-04, -6.10055603e-05]]), array([[-0.00354399],\n",
      "       [ 0.00474219],\n",
      "       [-0.00093844]])]\n",
      "gradients_biases:  [array([ 2.42952923e-04, -1.13932138e-03, -5.85869502e-05]), array([-0.00087293])]\n",
      "Iteration 146, Cost: 0.2503614418176676\n",
      "gradient_weights:  [array([[-1.81016888e-03, -1.29155384e-03,  6.88812506e-04],\n",
      "       [ 7.84023322e-03,  3.52328963e-04, -5.99842638e-05]]), array([[-0.00352834],\n",
      "       [ 0.00474172],\n",
      "       [-0.00093432]])]\n",
      "gradients_biases:  [array([ 2.48419385e-04, -1.14277644e-03, -5.80843844e-05]), array([-0.00086603])]\n",
      "Iteration 147, Cost: 0.25035618874256155\n",
      "gradient_weights:  [array([[-1.79962977e-03, -1.29455774e-03,  6.87967673e-04],\n",
      "       [ 7.84014657e-03,  3.53078915e-04, -5.89740088e-05]]), array([[-0.00351283],\n",
      "       [ 0.00474106],\n",
      "       [-0.00093035]])]\n",
      "gradients_biases:  [array([ 2.53933069e-04, -1.14622003e-03, -5.76057378e-05]), array([-0.00085948])]\n",
      "Iteration 148, Cost: 0.25035094353119164\n",
      "gradient_weights:  [array([[-1.78908416e-03, -1.29755518e-03,  6.87112166e-04],\n",
      "       [ 7.84010679e-03,  3.53835741e-04, -5.79743376e-05]]), array([[-0.00349744],\n",
      "       [ 0.00474023],\n",
      "       [-0.00092654]])]\n",
      "gradients_biases:  [array([ 2.59491557e-04, -1.14965250e-03, -5.71499983e-05]), array([-0.00085323])]\n",
      "Iteration 149, Cost: 0.2503457060964415\n",
      "gradient_weights:  [array([[-1.77853286e-03, -1.30054625e-03,  6.86246513e-04],\n",
      "       [ 7.84011274e-03,  3.54599204e-04, -5.69848117e-05]]), array([[-0.00348218],\n",
      "       [ 0.00473924],\n",
      "       [-0.00092288]])]\n",
      "gradients_biases:  [array([ 2.65092540e-04, -1.15307420e-03, -5.67161973e-05]), array([-0.00084729])]\n",
      "Iteration 150, Cost: 0.2503404763530137\n",
      "gradient_weights:  [array([[-1.76797670e-03, -1.30353106e-03,  6.85371220e-04],\n",
      "       [ 7.84016336e-03,  3.55369075e-04, -5.60050112e-05]]), array([[-0.00346703],\n",
      "       [ 0.0047381 ],\n",
      "       [-0.00091935]])]\n",
      "gradients_biases:  [array([ 2.70733810e-04, -1.15648547e-03, -5.63034075e-05]), array([-0.00084165])]\n",
      "Iteration 151, Cost: 0.25033525421729413\n",
      "gradient_weights:  [array([[-1.75741641e-03, -1.30650968e-03,  6.84486767e-04],\n",
      "       [ 7.84025760e-03,  3.56145135e-04, -5.50345340e-05]]), array([[-0.003452  ],\n",
      "       [ 0.0047368 ],\n",
      "       [-0.00091596]])]\n",
      "gradients_biases:  [array([ 2.76413255e-04, -1.15988663e-03, -5.59107416e-05]), array([-0.00083628])]\n",
      "Iteration 152, Cost: 0.2503300396072269\n",
      "gradient_weights:  [array([[-1.74685273e-03, -1.30948222e-03,  6.83593617e-04],\n",
      "       [ 7.84039448e-03,  3.56927176e-04, -5.40729950e-05]]), array([[-0.00343708],\n",
      "       [ 0.00473537],\n",
      "       [-0.0009127 ]])]\n",
      "gradients_biases:  [array([ 2.82128858e-04, -1.16327798e-03, -5.55373501e-05]), array([-0.00083118])]\n",
      "Iteration 153, Cost: 0.2503248324422\n",
      "gradient_weights:  [array([[-1.73628635e-03, -1.31244876e-03,  6.82692211e-04],\n",
      "       [ 7.84057304e-03,  3.57714997e-04, -5.31200254e-05]]), array([[-0.00342226],\n",
      "       [ 0.0047338 ],\n",
      "       [-0.00090956]])]\n",
      "gradients_biases:  [array([ 2.87878693e-04, -1.16665982e-03, -5.51824201e-05]), array([-0.00082633])]\n",
      "Iteration 154, Cost: 0.2503196326429407\n",
      "gradient_weights:  [array([[-1.72571791e-03, -1.31540938e-03,  6.81782973e-04],\n",
      "       [ 7.84079238e-03,  3.58508407e-04, -5.21752721e-05]]), array([[-0.00340754],\n",
      "       [ 0.0047321 ],\n",
      "       [-0.00090654]])]\n",
      "gradients_biases:  [array([ 2.93660917e-04, -1.17003243e-03, -5.48451736e-05]), array([-0.00082172])]\n",
      "Iteration 155, Cost: 0.25031444013141924\n",
      "gradient_weights:  [array([[-1.71514804e-03, -1.31836417e-03,  6.80866305e-04],\n",
      "       [ 7.84105162e-03,  3.59307223e-04, -5.12383967e-05]]), array([[-0.00339292],\n",
      "       [ 0.00473028],\n",
      "       [-0.00090363]])]\n",
      "gradients_biases:  [array([ 2.99473768e-04, -1.17339610e-03, -5.45248658e-05]), array([-0.00081735])]\n",
      "Iteration 156, Cost: 0.250309254830761\n",
      "gradient_weights:  [array([[-1.70457733e-03, -1.32131320e-03,  6.79942594e-04],\n",
      "       [ 7.84134994e-03,  3.60111269e-04, -5.03090753e-05]]), array([[-0.00337839],\n",
      "       [ 0.00472834],\n",
      "       [-0.00090083]])]\n",
      "gradients_biases:  [array([ 3.05315566e-04, -1.17675107e-03, -5.42207841e-05]), array([-0.00081321])]\n",
      "Iteration 157, Cost: 0.25030407666516596\n",
      "gradient_weights:  [array([[-1.69400635e-03, -1.32425655e-03,  6.79012211e-04],\n",
      "       [ 7.84168652e-03,  3.60920378e-04, -4.93869977e-05]]), array([[-0.00336396],\n",
      "       [ 0.00472629],\n",
      "       [-0.00089814]])]\n",
      "gradients_biases:  [array([ 3.11184700e-04, -1.18009761e-03, -5.39322463e-05]), array([-0.00080928])]\n",
      "Iteration 158, Cost: 0.25029890555983453\n",
      "gradient_weights:  [array([[-1.68343563e-03, -1.32719430e-03,  6.78075509e-04],\n",
      "       [ 7.84206061e-03,  3.61734389e-04, -4.84718668e-05]]), array([[-0.0033496 ],\n",
      "       [ 0.00472414],\n",
      "       [-0.00089554]])]\n",
      "gradients_biases:  [array([ 3.17079636e-04, -1.18343595e-03, -5.36585995e-05]), array([-0.00080555])]\n",
      "Iteration 159, Cost: 0.25029374144089966\n",
      "gradient_weights:  [array([[-1.67286568e-03, -1.33012653e-03,  6.77132827e-04],\n",
      "       [ 7.84247147e-03,  3.62553148e-04, -4.75633980e-05]]), array([[-0.00333534],\n",
      "       [ 0.00472189],\n",
      "       [-0.00089304]])]\n",
      "gradients_biases:  [array([ 3.22998905e-04, -1.18676633e-03, -5.33992191e-05]), array([-0.00080202])]\n",
      "Iteration 160, Cost: 0.25028858423536515\n",
      "gradient_weights:  [array([[-1.66229697e-03, -1.33305330e-03,  6.76184488e-04],\n",
      "       [ 7.84291840e-03,  3.63376510e-04, -4.66613187e-05]]), array([[-0.00332115],\n",
      "       [ 0.00471954],\n",
      "       [-0.00089062]])]\n",
      "gradients_biases:  [array([ 3.28941103e-04, -1.19008898e-03, -5.31535069e-05]), array([-0.00079868])]\n",
      "Iteration 161, Cost: 0.2502834338710483\n",
      "gradient_weights:  [array([[-1.65172998e-03, -1.33597469e-03,  6.75230801e-04],\n",
      "       [ 7.84340071e-03,  3.64204333e-04, -4.57653680e-05]]), array([[-0.00330704],\n",
      "       [ 0.0047171 ],\n",
      "       [-0.0008883 ]])]\n",
      "gradients_biases:  [array([ 3.34904890e-04, -1.19340410e-03, -5.29208908e-05]), array([-0.00079551])]\n",
      "Iteration 162, Cost: 0.2502782902765276\n",
      "gradient_weights:  [array([[-1.64116513e-03, -1.33889076e-03,  6.74272064e-04],\n",
      "       [ 7.84391776e-03,  3.65036483e-04, -4.48752958e-05]]), array([[-0.003293  ],\n",
      "       [ 0.00471457],\n",
      "       [-0.00088606]])]\n",
      "gradients_biases:  [array([ 3.40888986e-04, -1.19671191e-03, -5.27008230e-05]), array([-0.00079252])]\n",
      "Iteration 163, Cost: 0.250273153381095\n",
      "gradient_weights:  [array([[-1.63060284e-03, -1.34180158e-03,  6.73308559e-04],\n",
      "       [ 7.84446894e-03,  3.65872833e-04, -4.39908628e-05]]), array([[-0.00327903],\n",
      "       [ 0.00471196],\n",
      "       [-0.0008839 ]])]\n",
      "gradients_biases:  [array([ 3.46892164e-04, -1.20001261e-03, -5.24927793e-05]), array([-0.0007897])]\n",
      "Iteration 164, Cost: 0.25026802311471175\n",
      "gradient_weights:  [array([[-1.62004349e-03, -1.34470721e-03,  6.72340557e-04],\n",
      "       [ 7.84505364e-03,  3.66713259e-04, -4.31118396e-05]]), array([[-0.00326513],\n",
      "       [ 0.00470927],\n",
      "       [-0.00088181]])]\n",
      "gradients_biases:  [array([ 3.52913258e-04, -1.20330638e-03, -5.22962583e-05]), array([-0.00078703])]\n",
      "Iteration 165, Cost: 0.25026289940796775\n",
      "gradient_weights:  [array([[-1.60948747e-03, -1.34760773e-03,  6.71368317e-04],\n",
      "       [ 7.84567130e-03,  3.67557644e-04, -4.22380065e-05]]), array([[-0.0032513 ],\n",
      "       [ 0.00470651],\n",
      "       [-0.0008798 ]])]\n",
      "gradients_biases:  [array([ 3.58951148e-04, -1.20659341e-03, -5.21107797e-05]), array([-0.00078451])]\n",
      "Iteration 166, Cost: 0.250257782192045\n",
      "gradient_weights:  [array([[-1.59893512e-03, -1.35050318e-03,  6.70392088e-04],\n",
      "       [ 7.84632136e-03,  3.68405876e-04, -4.13691532e-05]]), array([[-0.00323753],\n",
      "       [ 0.00470368],\n",
      "       [-0.00087786]])]\n",
      "gradients_biases:  [array([ 3.65004767e-04, -1.20987388e-03, -5.19358841e-05]), array([-0.00078214])]\n",
      "Iteration 167, Cost: 0.2502526713986829\n",
      "gradient_weights:  [array([[-1.58838677e-03, -1.35339364e-03,  6.69412106e-04],\n",
      "       [ 7.84700331e-03,  3.69257849e-04, -4.05050781e-05]]), array([[-0.00322382],\n",
      "       [ 0.00470078],\n",
      "       [-0.00087599]])]\n",
      "gradients_biases:  [array([ 3.71073095e-04, -1.21314795e-03, -5.17711320e-05]), array([-0.00077991])]\n",
      "Iteration 168, Cost: 0.25024756696014727\n",
      "gradient_weights:  [array([[-1.57784274e-03, -1.35627915e-03,  6.68428598e-04],\n",
      "       [ 7.84771664e-03,  3.70113459e-04, -3.96455883e-05]]), array([[-0.00321017],\n",
      "       [ 0.00469782],\n",
      "       [-0.00087418]])]\n",
      "gradients_biases:  [array([ 3.77155158e-04, -1.21641580e-03, -5.16161024e-05]), array([-0.00077781])]\n",
      "Iteration 169, Cost: 0.25024246880920187\n",
      "gradient_weights:  [array([[-1.56730333e-03, -1.35915978e-03,  6.67441782e-04],\n",
      "       [ 7.84846086e-03,  3.70972610e-04, -3.87904989e-05]]), array([[-0.00319658],\n",
      "       [ 0.00469479],\n",
      "       [-0.00087243]])]\n",
      "gradients_biases:  [array([ 3.83250025e-04, -1.21967757e-03, -5.14703928e-05]), array([-0.00077584])]\n",
      "Iteration 170, Cost: 0.25023737687908126\n",
      "gradient_weights:  [array([[-1.55676882e-03, -1.36203559e-03,  6.66451863e-04],\n",
      "       [ 7.84923551e-03,  3.71835208e-04, -3.79396327e-05]]), array([[-0.00318305],\n",
      "       [ 0.00469171],\n",
      "       [-0.00087075]])]\n",
      "gradients_biases:  [array([ 3.89356807e-04, -1.22293342e-03, -5.13336178e-05]), array([-0.00077399])]\n",
      "Iteration 171, Cost: 0.25023229110346695\n",
      "gradient_weights:  [array([[-1.54623948e-03, -1.36490662e-03,  6.65459042e-04],\n",
      "       [ 7.85004016e-03,  3.72701163e-04, -3.70928201e-05]]), array([[-0.00316956],\n",
      "       [ 0.00468858],\n",
      "       [-0.00086911]])]\n",
      "gradients_biases:  [array([ 3.95474653e-04, -1.22618349e-03, -5.12054086e-05]), array([-0.00077226])]\n",
      "Iteration 172, Cost: 0.25022721141646487\n",
      "gradient_weights:  [array([[-1.53571556e-03, -1.36777293e-03,  6.64463506e-04],\n",
      "       [ 7.85087436e-03,  3.73570391e-04, -3.62498986e-05]]), array([[-0.00315613],\n",
      "       [ 0.00468539],\n",
      "       [-0.00086754]])]\n",
      "gradients_biases:  [array([ 4.01602754e-04, -1.22942792e-03, -5.10854123e-05]), array([-0.00077063])]\n",
      "Iteration 173, Cost: 0.25022213775258506\n",
      "gradient_weights:  [array([[-1.52519730e-03, -1.37063458e-03,  6.63465439e-04],\n",
      "       [ 7.85173771e-03,  3.74442811e-04, -3.54107125e-05]]), array([[-0.00314275],\n",
      "       [ 0.00468215],\n",
      "       [-0.00086602]])]\n",
      "gradients_biases:  [array([ 4.07740333e-04, -1.23266685e-03, -5.09732913e-05]), array([-0.00076912])]\n",
      "Iteration 174, Cost: 0.25021707004672195\n",
      "gradient_weights:  [array([[-1.51468493e-03, -1.37349161e-03,  6.62465013e-04],\n",
      "       [ 7.85262982e-03,  3.75318346e-04, -3.45751127e-05]]), array([[-0.00312941],\n",
      "       [ 0.00467887],\n",
      "       [-0.00086454]])]\n",
      "gradients_biases:  [array([ 4.13886651e-04, -1.23590041e-03, -5.08687222e-05]), array([-0.00076771])]\n",
      "Iteration 175, Cost: 0.2502120082341379\n",
      "gradient_weights:  [array([[-1.50417864e-03, -1.37634407e-03,  6.61462395e-04],\n",
      "       [ 7.85355031e-03,  3.76196920e-04, -3.37429563e-05]]), array([[-0.00311612],\n",
      "       [ 0.00467555],\n",
      "       [-0.00086312]])]\n",
      "gradients_biases:  [array([ 4.20041000e-04, -1.23912872e-03, -5.07713960e-05]), array([-0.00076639])]\n",
      "Iteration 176, Cost: 0.2502069522504463\n",
      "gradient_weights:  [array([[-1.49367865e-03, -1.37919202e-03,  6.60457742e-04],\n",
      "       [ 7.85449881e-03,  3.77078465e-04, -3.29141064e-05]]), array([[-0.00310287],\n",
      "       [ 0.00467218],\n",
      "       [-0.00086174]])]\n",
      "gradients_biases:  [array([ 4.26202706e-04, -1.24235190e-03, -5.06810165e-05]), array([-0.00076517])]\n",
      "Iteration 177, Cost: 0.25020190203159687\n",
      "gradient_weights:  [array([[-1.48318514e-03, -1.38203550e-03,  6.59451207e-04],\n",
      "       [ 7.85547498e-03,  3.77962913e-04, -3.20884318e-05]]), array([[-0.00308967],\n",
      "       [ 0.00466877],\n",
      "       [-0.0008604 ]])]\n",
      "gradients_biases:  [array([ 4.32371124e-04, -1.24557006e-03, -5.05973005e-05]), array([-0.00076405])]\n",
      "Iteration 178, Cost: 0.2501968575138621\n",
      "gradient_weights:  [array([[-1.47269829e-03, -1.38487455e-03,  6.58442935e-04],\n",
      "       [ 7.85647849e-03,  3.78850199e-04, -3.12658069e-05]]), array([[-0.00307651],\n",
      "       [ 0.00466533],\n",
      "       [-0.00085911]])]\n",
      "gradients_biases:  [array([ 4.38545637e-04, -1.24878331e-03, -5.05199771e-05]), array([-0.000763])]\n",
      "Iteration 179, Cost: 0.25019181863382456\n",
      "gradient_weights:  [array([[-1.46221827e-03, -1.38770923e-03,  6.57433064e-04],\n",
      "       [ 7.85750901e-03,  3.79740263e-04, -3.04461113e-05]]), array([[-0.00306339],\n",
      "       [ 0.00466185],\n",
      "       [-0.00085786]])]\n",
      "gradients_biases:  [array([ 4.44725659e-04, -1.25199177e-03, -5.04487870e-05]), array([-0.00076204])]\n",
      "Iteration 180, Cost: 0.2501867853283651\n",
      "gradient_weights:  [array([[-1.45174522e-03, -1.39053957e-03,  6.56421727e-04],\n",
      "       [ 7.85856623e-03,  3.80633046e-04, -2.96292296e-05]]), array([[-0.0030503 ],\n",
      "       [ 0.00465834],\n",
      "       [-0.00085665]])]\n",
      "gradients_biases:  [array([ 4.50910627e-04, -1.25519553e-03, -5.03834818e-05]), array([-0.00076116])]\n",
      "Iteration 181, Cost: 0.2501817575346519\n",
      "gradient_weights:  [array([[-1.44127929e-03, -1.39336562e-03,  6.55409051e-04],\n",
      "       [ 7.85964985e-03,  3.81528494e-04, -2.88150512e-05]]), array([[-0.00303726],\n",
      "       [ 0.0046548 ],\n",
      "       [-0.00085547]])]\n",
      "gradients_biases:  [array([ 4.57100005e-04, -1.25839469e-03, -5.03238241e-05]), array([-0.00076036])]\n",
      "Iteration 182, Cost: 0.25017673519013073\n",
      "gradient_weights:  [array([[-1.43082063e-03, -1.39618743e-03,  6.54395157e-04],\n",
      "       [ 7.86075960e-03,  3.82426553e-04, -2.80034703e-05]]), array([[-0.00302425],\n",
      "       [ 0.00465123],\n",
      "       [-0.00085433]])]\n",
      "gradients_biases:  [array([ 4.63293283e-04, -1.26158935e-03, -5.02695867e-05]), array([-0.00075963])]\n",
      "Iteration 183, Cost: 0.2501717182325154\n",
      "gradient_weights:  [array([[-1.42036935e-03, -1.39900504e-03,  6.53380161e-04],\n",
      "       [ 7.86189520e-03,  3.83327173e-04, -2.71943852e-05]]), array([[-0.00301127],\n",
      "       [ 0.00464763],\n",
      "       [-0.00085323]])]\n",
      "gradients_biases:  [array([ 4.69489971e-04, -1.26477959e-03, -5.02205521e-05]), array([-0.00075897])]\n",
      "Iteration 184, Cost: 0.25016670659977913\n",
      "gradient_weights:  [array([[-1.40992558e-03, -1.40181847e-03,  6.52364175e-04],\n",
      "       [ 7.86305638e-03,  3.84230306e-04, -2.63876988e-05]]), array([[-0.00299833],\n",
      "       [ 0.00464401],\n",
      "       [-0.00085215]])]\n",
      "gradients_biases:  [array([ 4.75689603e-04, -1.26796551e-03, -5.01765121e-05]), array([-0.00075837])]\n",
      "Iteration 185, Cost: 0.2501617002301468\n",
      "gradient_weights:  [array([[-1.39948942e-03, -1.40462779e-03,  6.51347303e-04],\n",
      "       [ 7.86424289e-03,  3.85135908e-04, -2.55833176e-05]]), array([[-0.00298542],\n",
      "       [ 0.00464036],\n",
      "       [-0.00085111]])]\n",
      "gradients_biases:  [array([ 4.81891735e-04, -1.27114719e-03, -5.01372677e-05]), array([-0.00075785])]\n",
      "Iteration 186, Cost: 0.2501566990620871\n",
      "gradient_weights:  [array([[-1.38906098e-03, -1.40743302e-03,  6.50329647e-04],\n",
      "       [ 7.86545449e-03,  3.86043934e-04, -2.47811524e-05]]), array([[-0.00297255],\n",
      "       [ 0.00463669],\n",
      "       [-0.00085011]])]\n",
      "gradients_biases:  [array([ 4.88095940e-04, -1.27432470e-03, -5.01026283e-05]), array([-0.00075738])]\n",
      "Iteration 187, Cost: 0.25015170303430606\n",
      "gradient_weights:  [array([[-1.37864036e-03, -1.41023421e-03,  6.49311304e-04],\n",
      "       [ 7.86669094e-03,  3.86954344e-04, -2.39811175e-05]]), array([[-0.0029597 ],\n",
      "       [ 0.004633  ],\n",
      "       [-0.00084913]])]\n",
      "gradients_biases:  [array([ 4.94301814e-04, -1.27749813e-03, -5.00724115e-05]), array([-0.00075697])]\n",
      "Iteration 188, Cost: 0.2501467120857401\n",
      "gradient_weights:  [array([[-1.36822764e-03, -1.41303139e-03,  6.48292367e-04],\n",
      "       [ 7.86795202e-03,  3.87867099e-04, -2.31831308e-05]]), array([[-0.00294688],\n",
      "       [ 0.00462929],\n",
      "       [-0.00084817]])]\n",
      "gradients_biases:  [array([ 5.00508971e-04, -1.28066756e-03, -5.00464429e-05]), array([-0.00075662])]\n",
      "Iteration 189, Cost: 0.25014172615555047\n",
      "gradient_weights:  [array([[-1.35782290e-03, -1.41582460e-03,  6.47272924e-04],\n",
      "       [ 7.86923750e-03,  3.88782161e-04, -2.23871134e-05]]), array([[-0.0029341 ],\n",
      "       [ 0.00462556],\n",
      "       [-0.00084725]])]\n",
      "gradients_biases:  [array([ 5.06717040e-04, -1.28383305e-03, -5.00245555e-05]), array([-0.00075632])]\n",
      "Iteration 190, Cost: 0.25013674518311746\n",
      "gradient_weights:  [array([[-1.34742622e-03, -1.41861388e-03,  6.46253060e-04],\n",
      "       [ 7.87054719e-03,  3.89699496e-04, -2.15929901e-05]]), array([[-0.00292134],\n",
      "       [ 0.00462182],\n",
      "       [-0.00084635]])]\n",
      "gradients_biases:  [array([ 5.12925669e-04, -1.28699467e-03, -5.00065895e-05]), array([-0.00075608])]\n",
      "Iteration 191, Cost: 0.25013176910803486\n",
      "gradient_weights:  [array([[-1.33703767e-03, -1.42139925e-03,  6.45232855e-04],\n",
      "       [ 7.87188087e-03,  3.90619069e-04, -2.08006882e-05]]), array([[-0.0029086 ],\n",
      "       [ 0.00461805],\n",
      "       [-0.00084548]])]\n",
      "gradients_biases:  [array([ 5.19134523e-04, -1.29015249e-03, -4.99923921e-05]), array([-0.00075588])]\n",
      "Iteration 192, Cost: 0.2501267978701056\n",
      "gradient_weights:  [array([[-1.32665731e-03, -1.42418077e-03,  6.44212387e-04],\n",
      "       [ 7.87323837e-03,  3.91540851e-04, -2.00101386e-05]]), array([[-0.00289589],\n",
      "       [ 0.00461428],\n",
      "       [-0.00084463]])]\n",
      "gradients_biases:  [array([ 5.25343281e-04, -1.29330658e-03, -4.99818170e-05]), array([-0.00075574])]\n",
      "Iteration 193, Cost: 0.2501218314093366\n",
      "gradient_weights:  [array([[-1.31628519e-03, -1.42695845e-03,  6.43191730e-04],\n",
      "       [ 7.87461948e-03,  3.92464810e-04, -1.92212747e-05]]), array([[-0.00288321],\n",
      "       [ 0.00461048],\n",
      "       [-0.0008438 ]])]\n",
      "gradients_biases:  [array([ 5.31551637e-04, -1.29645700e-03, -4.99747242e-05]), array([-0.00075564])]\n",
      "Iteration 194, Cost: 0.2501168696659346\n",
      "gradient_weights:  [array([[-1.30592137e-03, -1.42973234e-03,  6.42170954e-04],\n",
      "       [ 7.87602403e-03,  3.93390918e-04, -1.84340326e-05]]), array([[-0.00287056],\n",
      "       [ 0.00460668],\n",
      "       [-0.000843  ]])]\n",
      "gradients_biases:  [array([ 5.37759299e-04, -1.29960380e-03, -4.99709798e-05]), array([-0.00075558])]\n",
      "Iteration 195, Cost: 0.25011191258030246\n",
      "gradient_weights:  [array([[-1.29556590e-03, -1.43250247e-03,  6.41150127e-04],\n",
      "       [ 7.87745185e-03,  3.94319148e-04, -1.76483511e-05]]), array([[-0.00285792],\n",
      "       [ 0.00460287],\n",
      "       [-0.00084221]])]\n",
      "gradients_biases:  [array([ 5.43965989e-04, -1.30274704e-03, -4.99704556e-05]), array([-0.00075557])]\n",
      "Iteration 196, Cost: 0.2501069600930346\n",
      "gradient_weights:  [array([[-1.28521881e-03, -1.43526888e-03,  6.40129314e-04],\n",
      "       [ 7.87890276e-03,  3.95249475e-04, -1.68641716e-05]]), array([[-0.00284531],\n",
      "       [ 0.00459904],\n",
      "       [-0.00084145]])]\n",
      "gradients_biases:  [array([ 5.50171443e-04, -1.30588679e-03, -4.99730291e-05]), array([-0.0007556])]\n",
      "Iteration 197, Cost: 0.25010201214491407\n",
      "gradient_weights:  [array([[-1.27488014e-03, -1.43803158e-03,  6.39108575e-04],\n",
      "       [ 7.88037662e-03,  3.96181875e-04, -1.60814378e-05]]), array([[-0.00283272],\n",
      "       [ 0.0045952 ],\n",
      "       [-0.00084071]])]\n",
      "gradients_biases:  [array([ 5.56375406e-04, -1.30902309e-03, -4.99785828e-05]), array([-0.00075566])]\n",
      "Iteration 198, Cost: 0.2500970686769086\n",
      "gradient_weights:  [array([[-1.26454993e-03, -1.44079062e-03,  6.38087970e-04],\n",
      "       [ 7.88187325e-03,  3.97116326e-04, -1.53000957e-05]]), array([[-0.00282016],\n",
      "       [ 0.00459136],\n",
      "       [-0.00083999]])]\n",
      "gradients_biases:  [array([ 5.62577639e-04, -1.31215599e-03, -4.99870044e-05]), array([-0.00075577])]\n",
      "Iteration 199, Cost: 0.25009212963016775\n",
      "gradient_weights:  [array([[-1.25422820e-03, -1.44354603e-03,  6.37067554e-04],\n",
      "       [ 7.88339253e-03,  3.98052805e-04, -1.45200933e-05]]), array([[-0.00280761],\n",
      "       [ 0.00458751],\n",
      "       [-0.00083928]])]\n",
      "gradients_biases:  [array([ 5.68777911e-04, -1.31528554e-03, -4.99981867e-05]), array([-0.00075591])]\n",
      "Iteration 200, Cost: 0.25008719494601933\n",
      "gradient_weights:  [array([[-1.24391498e-03, -1.44629783e-03,  6.36047383e-04],\n",
      "       [ 7.88493429e-03,  3.98991293e-04, -1.37413811e-05]]), array([[-0.00279509],\n",
      "       [ 0.00458365],\n",
      "       [-0.0008386 ]])]\n",
      "gradients_biases:  [array([ 5.74976003e-04, -1.31841180e-03, -5.00120267e-05]), array([-0.00075608])]\n",
      "Iteration 201, Cost: 0.25008226456596705\n",
      "gradient_weights:  [array([[-1.23361029e-03, -1.44904606e-03,  6.35027506e-04],\n",
      "       [ 7.88649840e-03,  3.99931770e-04, -1.29639114e-05]]), array([[-0.00278258],\n",
      "       [ 0.00457978],\n",
      "       [-0.00083793]])]\n",
      "gradients_biases:  [array([ 5.81171707e-04, -1.32153481e-03, -5.00284262e-05]), array([-0.00075629])]\n",
      "Iteration 202, Cost: 0.2500773384316875\n",
      "gradient_weights:  [array([[-1.22331414e-03, -1.45179075e-03,  6.34007974e-04],\n",
      "       [ 7.88808473e-03,  4.00874220e-04, -1.21876382e-05]]), array([[-0.0027701 ],\n",
      "       [ 0.00457591],\n",
      "       [-0.00083727]])]\n",
      "gradients_biases:  [array([ 5.87364823e-04, -1.32465460e-03, -5.00472910e-05]), array([-0.00075653])]\n",
      "Iteration 203, Cost: 0.2500724164850272\n",
      "gradient_weights:  [array([[-1.21302654e-03, -1.45453193e-03,  6.32988833e-04],\n",
      "       [ 7.88969314e-03,  4.01818624e-04, -1.14125178e-05]]), array([[-0.00275763],\n",
      "       [ 0.00457203],\n",
      "       [-0.00083663]])]\n",
      "gradients_biases:  [array([ 5.93555163e-04, -1.32777123e-03, -5.00685310e-05]), array([-0.00075681])]\n",
      "Iteration 204, Cost: 0.2500674986680006\n",
      "gradient_weights:  [array([[-1.20274752e-03, -1.45726961e-03,  6.31970127e-04],\n",
      "       [ 7.89132351e-03,  4.02764968e-04, -1.06385078e-05]]), array([[-0.00274518],\n",
      "       [ 0.00456815],\n",
      "       [-0.00083601]])]\n",
      "gradients_biases:  [array([ 5.99742546e-04, -1.33088474e-03, -5.00920602e-05]), array([-0.00075711])]\n",
      "Iteration 205, Cost: 0.2500625849227871\n",
      "gradient_weights:  [array([[-1.19247706e-03, -1.46000385e-03,  6.30951900e-04],\n",
      "       [ 7.89297572e-03,  4.03713236e-04, -9.86556791e-06]]), array([[-0.00273275],\n",
      "       [ 0.00456427],\n",
      "       [-0.0008354 ]])]\n",
      "gradients_biases:  [array([ 6.05926800e-04, -1.33399515e-03, -5.01177961e-05]), array([-0.00075744])]\n",
      "Iteration 206, Cost: 0.2500576751917291\n",
      "gradient_weights:  [array([[-1.18221518e-03, -1.46273465e-03,  6.29934193e-04],\n",
      "       [ 7.89464966e-03,  4.04663415e-04, -9.09365911e-06]]), array([[-0.00272034],\n",
      "       [ 0.00456038],\n",
      "       [-0.00083481]])]\n",
      "gradients_biases:  [array([ 6.12107761e-04, -1.33710253e-03, -5.01456598e-05]), array([-0.0007578])]\n",
      "Iteration 207, Cost: 0.25005276941732973\n",
      "gradient_weights:  [array([[-1.17196188e-03, -1.46546205e-03,  6.28917044e-04],\n",
      "       [ 7.89634520e-03,  4.05615492e-04, -8.32274410e-06]]), array([[-0.00270794],\n",
      "       [ 0.00455649],\n",
      "       [-0.00083423]])]\n",
      "gradients_biases:  [array([ 6.18285273e-04, -1.34020689e-03, -5.01755759e-05]), array([-0.00075818])]\n",
      "Iteration 208, Cost: 0.2500478675422505\n",
      "gradient_weights:  [array([[-1.16171715e-03, -1.46818608e-03,  6.27900490e-04],\n",
      "       [ 7.89806224e-03,  4.06569455e-04, -7.55278702e-06]]), array([[-0.00269556],\n",
      "       [ 0.0045526 ],\n",
      "       [-0.00083366]])]\n",
      "gradients_biases:  [array([ 6.24459190e-04, -1.34330828e-03, -5.02074721e-05]), array([-0.00075859])]\n",
      "Iteration 209, Cost: 0.2500429695093094\n",
      "gradient_weights:  [array([[-1.15148099e-03, -1.47090676e-03,  6.26884567e-04],\n",
      "       [ 7.89980068e-03,  4.07525292e-04, -6.78375342e-06]]), array([[-0.0026832 ],\n",
      "       [ 0.0045487 ],\n",
      "       [-0.00083311]])]\n",
      "gradients_biases:  [array([ 6.30629368e-04, -1.34640673e-03, -5.02412795e-05]), array([-0.00075903])]\n",
      "Iteration 210, Cost: 0.2500380752614787\n",
      "gradient_weights:  [array([[-1.14125338e-03, -1.47362412e-03,  6.25869310e-04],\n",
      "       [ 7.90156041e-03,  4.08482994e-04, -6.01561022e-06]]), array([[-0.00267085],\n",
      "       [ 0.00454481],\n",
      "       [-0.00083257]])]\n",
      "gradients_biases:  [array([ 6.36795675e-04, -1.34950227e-03, -5.02769318e-05]), array([-0.00075948])]\n",
      "Iteration 211, Cost: 0.2500331847418833\n",
      "gradient_weights:  [array([[-1.13103433e-03, -1.47633819e-03,  6.24854750e-04],\n",
      "       [ 7.90334133e-03,  4.09442549e-04, -5.24832562e-06]]), array([[-0.00265851],\n",
      "       [ 0.00454091],\n",
      "       [-0.00083204]])]\n",
      "gradients_biases:  [array([ 6.42957981e-04, -1.35259493e-03, -5.03143659e-05]), array([-0.00075997])]\n",
      "Iteration 212, Cost: 0.25002829789379855\n",
      "gradient_weights:  [array([[-1.12082381e-03, -1.47904898e-03,  6.23840919e-04],\n",
      "       [ 7.90514334e-03,  4.10403951e-04, -4.48186905e-06]]), array([[-0.0026462 ],\n",
      "       [ 0.00453702],\n",
      "       [-0.00083152]])]\n",
      "gradients_biases:  [array([ 6.49116167e-04, -1.35568476e-03, -5.03535213e-05]), array([-0.00076047])]\n",
      "Iteration 213, Cost: 0.2500234146606488\n",
      "gradient_weights:  [array([[-1.11062182e-03, -1.48175653e-03,  6.22827845e-04],\n",
      "       [ 7.90696636e-03,  4.11367189e-04, -3.71621117e-06]]), array([[-0.00263389],\n",
      "       [ 0.00453312],\n",
      "       [-0.00083101]])]\n",
      "gradients_biases:  [array([ 6.55270116e-04, -1.35877176e-03, -5.03943401e-05]), array([-0.00076099])]\n",
      "Iteration 214, Cost: 0.25001853498600524\n",
      "gradient_weights:  [array([[-1.10042833e-03, -1.48446087e-03,  6.21815558e-04],\n",
      "       [ 7.90881030e-03,  4.12332257e-04, -2.95132373e-06]]), array([[-0.0026216 ],\n",
      "       [ 0.00452923],\n",
      "       [-0.00083051]])]\n",
      "gradients_biases:  [array([ 6.61419719e-04, -1.36185598e-03, -5.04367668e-05]), array([-0.00076154])]\n",
      "Iteration 215, Cost: 0.25001365881358417\n",
      "gradient_weights:  [array([[-1.09024333e-03, -1.48716201e-03,  6.20804084e-04],\n",
      "       [ 7.91067505e-03,  4.13299148e-04, -2.18717958e-06]]), array([[-0.00260932],\n",
      "       [ 0.00452534],\n",
      "       [-0.00083003]])]\n",
      "gradients_biases:  [array([ 6.67564871e-04, -1.36493745e-03, -5.04807486e-05]), array([-0.0007621])]\n",
      "Iteration 216, Cost: 0.25000878608724586\n",
      "gradient_weights:  [array([[-1.08006679e-03, -1.48985998e-03,  6.19793449e-04],\n",
      "       [ 7.91256055e-03,  4.14267856e-04, -1.42375263e-06]]), array([[-0.00259706],\n",
      "       [ 0.00452145],\n",
      "       [-0.00082955]])]\n",
      "gradients_biases:  [array([ 6.73705474e-04, -1.36801618e-03, -5.05262347e-05]), array([-0.00076268])]\n",
      "Iteration 217, Cost: 0.25000391675099226\n",
      "gradient_weights:  [array([[-1.06989870e-03, -1.49255481e-03,  6.18783677e-04],\n",
      "       [ 7.91446671e-03,  4.15238374e-04, -6.61017768e-07]]), array([[-0.00258481],\n",
      "       [ 0.00451756],\n",
      "       [-0.00082909]])]\n",
      "gradients_biases:  [array([ 6.79841433e-04, -1.37109221e-03, -5.05731767e-05]), array([-0.00076328])]\n",
      "Iteration 218, Cost: 0.2499990507489659\n",
      "gradient_weights:  [array([[-1.05973903e-03, -1.49524651e-03,  6.17774792e-04],\n",
      "       [ 7.91639345e-03,  4.16210697e-04,  1.01049164e-07]]), array([[-0.00257257],\n",
      "       [ 0.00451368],\n",
      "       [-0.00082863]])]\n",
      "gradients_biases:  [array([ 6.85972658e-04, -1.37416555e-03, -5.06215281e-05]), array([-0.0007639])]\n",
      "Iteration 219, Cost: 0.24999418802544798\n",
      "gradient_weights:  [array([[-1.04958775e-03, -1.49793512e-03,  6.16766816e-04],\n",
      "       [ 7.91834069e-03,  4.17184821e-04,  8.62471408e-07]]), array([[-0.00256035],\n",
      "       [ 0.00450979],\n",
      "       [-0.00082818]])]\n",
      "gradients_biases:  [array([ 6.92099066e-04, -1.37723625e-03, -5.06712446e-05]), array([-0.00076454])]\n",
      "Iteration 220, Cost: 0.24998932852485706\n",
      "gradient_weights:  [array([[-1.03944483e-03, -1.50062065e-03,  6.15759770e-04],\n",
      "       [ 7.92030836e-03,  4.18160741e-04,  1.62327133e-06]]), array([[-0.00254813],\n",
      "       [ 0.00450591],\n",
      "       [-0.00082774]])]\n",
      "gradients_biases:  [array([ 6.98220576e-04, -1.38030430e-03, -5.07222837e-05]), array([-0.00076519])]\n",
      "Iteration 221, Cost: 0.24998447219174724\n",
      "gradient_weights:  [array([[-1.02931025e-03, -1.50330314e-03,  6.14753674e-04],\n",
      "       [ 7.92229638e-03,  4.19138455e-04,  2.38347047e-06]]), array([[-0.00253593],\n",
      "       [ 0.00450204],\n",
      "       [-0.00082731]])]\n",
      "gradients_biases:  [array([ 7.04337110e-04, -1.38336975e-03, -5.07746048e-05]), array([-0.00076585])]\n",
      "Iteration 222, Cost: 0.2499796189708074\n",
      "gradient_weights:  [array([[-1.01918398e-03, -1.50598259e-03,  6.13748549e-04],\n",
      "       [ 7.92430469e-03,  4.20117958e-04,  3.14308957e-06]]), array([[-0.00252374],\n",
      "       [ 0.00449816],\n",
      "       [-0.00082689]])]\n",
      "gradients_biases:  [array([ 7.10448598e-04, -1.38643261e-03, -5.08281690e-05]), array([-0.00076654])]\n",
      "Iteration 223, Cost: 0.24997476880685884\n",
      "gradient_weights:  [array([[-1.00906598e-03, -1.50865904e-03,  6.12744412e-04],\n",
      "       [ 7.92633321e-03,  4.21099247e-04,  3.90214858e-06]]), array([[-0.00251156],\n",
      "       [ 0.0044943 ],\n",
      "       [-0.00082648]])]\n",
      "gradients_biases:  [array([ 7.16554970e-04, -1.38949291e-03, -5.08829392e-05]), array([-0.00076723])]\n",
      "Iteration 224, Cost: 0.2499699216448545\n",
      "gradient_weights:  [array([[-9.98956218e-04, -1.51133251e-03,  6.11741280e-04],\n",
      "       [ 7.92838188e-03,  4.22082321e-04,  4.66066676e-06]]), array([[-0.0024994 ],\n",
      "       [ 0.00449043],\n",
      "       [-0.00082607]])]\n",
      "gradients_biases:  [array([ 7.22656162e-04, -1.39255066e-03, -5.09388797e-05]), array([-0.00076795])]\n",
      "Iteration 225, Cost: 0.2499650774298773\n",
      "gradient_weights:  [array([[-9.88854666e-04, -1.51400302e-03,  6.10739171e-04],\n",
      "       [ 7.93045064e-03,  4.23067178e-04,  5.41866264e-06]]), array([[-0.00248724],\n",
      "       [ 0.00448657],\n",
      "       [-0.00082567]])]\n",
      "gradients_biases:  [array([ 7.28752111e-04, -1.39560588e-03, -5.09959567e-05]), array([-0.00076867])]\n",
      "Iteration 226, Cost: 0.24996023610713883\n",
      "gradient_weights:  [array([[-9.78761284e-04, -1.51667059e-03,  6.09738101e-04],\n",
      "       [ 7.93253941e-03,  4.24053816e-04,  6.17615408e-06]]), array([[-0.00247509],\n",
      "       [ 0.00448272],\n",
      "       [-0.00082528]])]\n",
      "gradients_biases:  [array([ 7.34842760e-04, -1.39865859e-03, -5.10541375e-05]), array([-0.00076941])]\n",
      "Iteration 227, Cost: 0.249955397621978\n",
      "gradient_weights:  [array([[-9.68676037e-04, -1.51933524e-03,  6.08738085e-04],\n",
      "       [ 7.93464815e-03,  4.25042233e-04,  6.93315831e-06]]), array([[-0.00246295],\n",
      "       [ 0.00447887],\n",
      "       [-0.0008249 ]])]\n",
      "gradients_biases:  [array([ 7.40928054e-04, -1.40170882e-03, -5.11133910e-05]), array([-0.00077016])]\n",
      "Iteration 228, Cost: 0.24995056191985965\n",
      "gradient_weights:  [array([[-9.58598886e-04, -1.52199699e-03,  6.07739137e-04],\n",
      "       [ 7.93677678e-03,  4.26032429e-04,  7.68969195e-06]]), array([[-0.00245083],\n",
      "       [ 0.00447502],\n",
      "       [-0.00082452]])]\n",
      "gradients_biases:  [array([ 7.47007940e-04, -1.40475658e-03, -5.11736874e-05]), array([-0.00077093])]\n",
      "Iteration 229, Cost: 0.2499457289463733\n",
      "gradient_weights:  [array([[-9.48529793e-04, -1.52465588e-03,  6.06741272e-04],\n",
      "       [ 7.93892526e-03,  4.27024403e-04,  8.44577101e-06]]), array([[-0.00243871],\n",
      "       [ 0.00447118],\n",
      "       [-0.00082415]])]\n",
      "gradients_biases:  [array([ 7.53082370e-04, -1.40780188e-03, -5.12349983e-05]), array([-0.0007717])]\n",
      "Iteration 230, Cost: 0.24994089864723182\n",
      "gradient_weights:  [array([[-9.38468719e-04, -1.52731191e-03,  6.05744501e-04],\n",
      "       [ 7.94109351e-03,  4.28018155e-04,  9.20141097e-06]]), array([[-0.0024266 ],\n",
      "       [ 0.00446735],\n",
      "       [-0.00082379]])]\n",
      "gradients_biases:  [array([ 7.59151296e-04, -1.41084475e-03, -5.12972965e-05]), array([-0.00077249])]\n",
      "Iteration 231, Cost: 0.24993607096826997\n",
      "gradient_weights:  [array([[-9.28415622e-04, -1.52996511e-03,  6.04748839e-04],\n",
      "       [ 7.94328150e-03,  4.29013686e-04,  9.95662674e-06]]), array([[-0.0024145 ],\n",
      "       [ 0.00446352],\n",
      "       [-0.00082343]])]\n",
      "gradients_biases:  [array([ 7.65214676e-04, -1.41388520e-03, -5.13605558e-05]), array([-0.00077329])]\n",
      "Iteration 232, Cost: 0.24993124585544352\n",
      "gradient_weights:  [array([[-9.18370462e-04, -1.53261550e-03,  6.03754296e-04],\n",
      "       [ 7.94548916e-03,  4.30010995e-04,  1.07114327e-05]]), array([[-0.00240241],\n",
      "       [ 0.0044597 ],\n",
      "       [-0.00082308]])]\n",
      "gradients_biases:  [array([ 7.71272466e-04, -1.41692324e-03, -5.14247513e-05]), array([-0.0007741])]\n",
      "Iteration 233, Cost: 0.2499264232548275\n",
      "gradient_weights:  [array([[-9.08333196e-04, -1.53526309e-03,  6.02760884e-04],\n",
      "       [ 7.94771644e-03,  4.31010083e-04,  1.14658429e-05]]), array([[-0.00239033],\n",
      "       [ 0.00445588],\n",
      "       [-0.00082274]])]\n",
      "gradients_biases:  [array([ 7.77324630e-04, -1.41995890e-03, -5.14898592e-05]), array([-0.00077492])]\n",
      "Iteration 234, Cost: 0.2499216031126154\n",
      "gradient_weights:  [array([[-8.98303781e-04, -1.53790792e-03,  6.01768614e-04],\n",
      "       [ 7.94996329e-03,  4.32010951e-04,  1.22198706e-05]]), array([[-0.00237826],\n",
      "       [ 0.00445207],\n",
      "       [-0.0008224 ]])]\n",
      "gradients_biases:  [array([ 7.83371129e-04, -1.42299218e-03, -5.15558568e-05]), array([-0.00077575])]\n",
      "Iteration 235, Cost: 0.2499167853751177\n",
      "gradient_weights:  [array([[-8.88282173e-04, -1.54055000e-03,  6.00777496e-04],\n",
      "       [ 7.95222966e-03,  4.33013601e-04,  1.29735289e-05]]), array([[-0.0023662 ],\n",
      "       [ 0.00444827],\n",
      "       [-0.00082206]])]\n",
      "gradients_biases:  [array([ 7.89411930e-04, -1.42602311e-03, -5.16227221e-05]), array([-0.0007766])]\n",
      "Iteration 236, Cost: 0.24991196998876036\n",
      "gradient_weights:  [array([[-8.78268329e-04, -1.54318935e-03,  5.99787540e-04],\n",
      "       [ 7.95451551e-03,  4.34018033e-04,  1.37268304e-05]]), array([[-0.00235414],\n",
      "       [ 0.00444447],\n",
      "       [-0.00082174]])]\n",
      "gradients_biases:  [array([ 7.95447000e-04, -1.42905169e-03, -5.16904344e-05]), array([-0.00077745])]\n",
      "Iteration 237, Cost: 0.2499071569000845\n",
      "gradient_weights:  [array([[-8.68262202e-04, -1.54582599e-03,  5.98798755e-04],\n",
      "       [ 7.95682077e-03,  4.35024250e-04,  1.44797871e-05]]), array([[-0.00234209],\n",
      "       [ 0.00444068],\n",
      "       [-0.00082142]])]\n",
      "gradients_biases:  [array([ 8.01476307e-04, -1.43207794e-03, -5.17589736e-05]), array([-0.00077831])]\n",
      "Iteration 238, Cost: 0.24990234605574413\n",
      "gradient_weights:  [array([[-8.58263748e-04, -1.54845995e-03,  5.97811150e-04],\n",
      "       [ 7.95914541e-03,  4.36032252e-04,  1.52324110e-05]]), array([[-0.00233005],\n",
      "       [ 0.0044369 ],\n",
      "       [-0.0008211 ]])]\n",
      "gradients_biases:  [array([ 8.07499825e-04, -1.43510187e-03, -5.18283208e-05]), array([-0.00077918])]\n",
      "Iteration 239, Cost: 0.24989753740250548\n",
      "gradient_weights:  [array([[-8.48272920e-04, -1.55109123e-03,  5.96824734e-04],\n",
      "       [ 7.96148939e-03,  4.37042043e-04,  1.59847132e-05]]), array([[-0.00231802],\n",
      "       [ 0.00443312],\n",
      "       [-0.00082079]])]\n",
      "gradients_biases:  [array([ 8.13517525e-04, -1.43812350e-03, -5.18984577e-05]), array([-0.00078006])]\n",
      "Iteration 240, Cost: 0.24989273088724598\n",
      "gradient_weights:  [array([[-8.38289671e-04, -1.55371986e-03,  5.95839513e-04],\n",
      "       [ 7.96385265e-03,  4.38053625e-04,  1.67367049e-05]]), array([[-0.002306  ],\n",
      "       [ 0.00442935],\n",
      "       [-0.00082049]])]\n",
      "gradients_biases:  [array([ 8.19529382e-04, -1.44114284e-03, -5.19693669e-05]), array([-0.00078094])]\n",
      "Iteration 241, Cost: 0.24988792645695287\n",
      "gradient_weights:  [array([[-8.28313954e-04, -1.55634586e-03,  5.94855497e-04],\n",
      "       [ 7.96623515e-03,  4.39066999e-04,  1.74883968e-05]]), array([[-0.00229398],\n",
      "       [ 0.00442559],\n",
      "       [-0.00082019]])]\n",
      "gradients_biases:  [array([ 8.25535374e-04, -1.44415990e-03, -5.20410316e-05]), array([-0.00078184])]\n",
      "Iteration 242, Cost: 0.24988312405872182\n",
      "gradient_weights:  [array([[-8.18345721e-04, -1.55896925e-03,  5.93872692e-04],\n",
      "       [ 7.96863685e-03,  4.40082169e-04,  1.82397991e-05]]), array([[-0.00228197],\n",
      "       [ 0.00442184],\n",
      "       [-0.00081989]])]\n",
      "gradients_biases:  [array([ 8.31535478e-04, -1.44717470e-03, -5.21134359e-05]), array([-0.00078275])]\n",
      "Iteration 243, Cost: 0.2498783236397561\n",
      "gradient_weights:  [array([[-8.08384923e-04, -1.56159005e-03,  5.92891104e-04],\n",
      "       [ 7.97105771e-03,  4.41099137e-04,  1.89909219e-05]]), array([[-0.00226997],\n",
      "       [ 0.00441809],\n",
      "       [-0.0008196 ]])]\n",
      "gradients_biases:  [array([ 8.37529673e-04, -1.45018724e-03, -5.21865646e-05]), array([-0.00078366])]\n",
      "Iteration 244, Cost: 0.24987352514736538\n",
      "gradient_weights:  [array([[-7.98431513e-04, -1.56420828e-03,  5.91910741e-04],\n",
      "       [ 7.97349769e-03,  4.42117906e-04,  1.97417749e-05]]), array([[-0.00225797],\n",
      "       [ 0.00441435],\n",
      "       [-0.00081931]])]\n",
      "gradients_biases:  [array([ 8.43517941e-04, -1.45319753e-03, -5.22604031e-05]), array([-0.00078458])]\n",
      "Iteration 245, Cost: 0.24986872852896452\n",
      "gradient_weights:  [array([[-7.88485440e-04, -1.56682395e-03,  5.90931608e-04],\n",
      "       [ 7.97595674e-03,  4.43138479e-04,  2.04923675e-05]]), array([[-0.00224598],\n",
      "       [ 0.00441062],\n",
      "       [-0.00081903]])]\n",
      "gradients_biases:  [array([ 8.49500264e-04, -1.45620559e-03, -5.23349374e-05]), array([-0.00078551])]\n",
      "Iteration 246, Cost: 0.24986393373207236\n",
      "gradient_weights:  [array([[-7.78546655e-04, -1.56943708e-03,  5.89953712e-04],\n",
      "       [ 7.97843483e-03,  4.44160860e-04,  2.12427090e-05]]), array([[-0.002234  ],\n",
      "       [ 0.00440689],\n",
      "       [-0.00081876]])]\n",
      "gradients_biases:  [array([ 8.55476624e-04, -1.45921143e-03, -5.24101542e-05]), array([-0.00078644])]\n",
      "Iteration 247, Cost: 0.24985914070431067\n",
      "gradient_weights:  [array([[-7.68615109e-04, -1.57204770e-03,  5.88977057e-04],\n",
      "       [ 7.98093192e-03,  4.45185052e-04,  2.19928081e-05]]), array([[-0.00222202],\n",
      "       [ 0.00440318],\n",
      "       [-0.00081849]])]\n",
      "gradients_biases:  [array([ 8.61447008e-04, -1.46221506e-03, -5.24860409e-05]), array([-0.00078739])]\n",
      "Iteration 248, Cost: 0.24985434939340312\n",
      "gradient_weights:  [array([[-7.58690750e-04, -1.57465582e-03,  5.88001649e-04],\n",
      "       [ 7.98344796e-03,  4.46211058e-04,  2.27426736e-05]]), array([[-0.00221005],\n",
      "       [ 0.00439947],\n",
      "       [-0.00081822]])]\n",
      "gradients_biases:  [array([ 8.67411401e-04, -1.46521649e-03, -5.25625851e-05]), array([-0.00078834])]\n",
      "Iteration 249, Cost: 0.24984955974717393\n",
      "gradient_weights:  [array([[-7.48773529e-04, -1.57726146e-03,  5.87027492e-04],\n",
      "       [ 7.98598294e-03,  4.47238882e-04,  2.34923137e-05]]), array([[-0.00219809],\n",
      "       [ 0.00439577],\n",
      "       [-0.00081796]])]\n",
      "gradients_biases:  [array([ 8.73369790e-04, -1.46821573e-03, -5.26397753e-05]), array([-0.00078929])]\n",
      "Iteration 250, Cost: 0.24984477171354708\n",
      "gradient_weights:  [array([[-7.38863394e-04, -1.57986464e-03,  5.86054593e-04],\n",
      "       [ 7.98853680e-03,  4.48268528e-04,  2.42417368e-05]]), array([[-0.00218613],\n",
      "       [ 0.00439208],\n",
      "       [-0.0008177 ]])]\n",
      "gradients_biases:  [array([ 8.79322163e-04, -1.47121278e-03, -5.27176002e-05]), array([-0.00079026])]\n",
      "Iteration 251, Cost: 0.24983998524054496\n",
      "gradient_weights:  [array([[-7.28960294e-04, -1.58246537e-03,  5.85082954e-04],\n",
      "       [ 7.99110951e-03,  4.49299999e-04,  2.49909507e-05]]), array([[-0.00217418],\n",
      "       [ 0.00438839],\n",
      "       [-0.00081744]])]\n",
      "gradients_biases:  [array([ 8.85268509e-04, -1.47420767e-03, -5.27960494e-05]), array([-0.00079123])]\n",
      "Iteration 252, Cost: 0.2498352002762873\n",
      "gradient_weights:  [array([[-7.19064178e-04, -1.58506368e-03,  5.84112580e-04],\n",
      "       [ 7.99370103e-03,  4.50333300e-04,  2.57399632e-05]]), array([[-0.00216223],\n",
      "       [ 0.00438472],\n",
      "       [-0.00081719]])]\n",
      "gradients_biases:  [array([ 8.91208818e-04, -1.47720039e-03, -5.28751126e-05]), array([-0.00079221])]\n",
      "Iteration 253, Cost: 0.2498304167689901\n",
      "gradient_weights:  [array([[-7.09174994e-04, -1.58765958e-03,  5.83143475e-04],\n",
      "       [ 7.99631134e-03,  4.51368434e-04,  2.64887817e-05]]), array([[-0.00215029],\n",
      "       [ 0.00438105],\n",
      "       [-0.00081695]])]\n",
      "gradients_biases:  [array([ 8.97143082e-04, -1.48019096e-03, -5.29547801e-05]), array([-0.00079319])]\n",
      "Iteration 254, Cost: 0.2498256346669646\n",
      "gradient_weights:  [array([[-6.99292689e-04, -1.59025309e-03,  5.82175642e-04],\n",
      "       [ 7.99894040e-03,  4.52405406e-04,  2.72374136e-05]]), array([[-0.00213835],\n",
      "       [ 0.00437739],\n",
      "       [-0.00081671]])]\n",
      "gradients_biases:  [array([ 9.03071291e-04, -1.48317939e-03, -5.30350426e-05]), array([-0.00079418])]\n",
      "Iteration 255, Cost: 0.24982085391861636\n",
      "gradient_weights:  [array([[-6.89417213e-04, -1.59284423e-03,  5.81209086e-04],\n",
      "       [ 8.00158817e-03,  4.53444220e-04,  2.79858661e-05]]), array([[-0.00212642],\n",
      "       [ 0.00437374],\n",
      "       [-0.00081647]])]\n",
      "gradients_biases:  [array([ 9.08993439e-04, -1.48616568e-03, -5.31158912e-05]), array([-0.00079518])]\n",
      "Iteration 256, Cost: 0.24981607447244364\n",
      "gradient_weights:  [array([[-6.79548511e-04, -1.59543302e-03,  5.80243809e-04],\n",
      "       [ 8.00425462e-03,  4.54484880e-04,  2.87341461e-05]]), array([[-0.0021145 ],\n",
      "       [ 0.0043701 ],\n",
      "       [-0.00081624]])]\n",
      "gradients_biases:  [array([ 9.14909518e-04, -1.48914984e-03, -5.31973175e-05]), array([-0.00079618])]\n",
      "Iteration 257, Cost: 0.2498112962770371\n",
      "gradient_weights:  [array([[-6.69686532e-04, -1.59801947e-03,  5.79279814e-04],\n",
      "       [ 8.00693972e-03,  4.55527390e-04,  2.94822605e-05]]), array([[-0.00210258],\n",
      "       [ 0.00436646],\n",
      "       [-0.00081601]])]\n",
      "gradients_biases:  [array([ 9.20819524e-04, -1.49213189e-03, -5.32793134e-05]), array([-0.00079719])]\n",
      "Iteration 258, Cost: 0.24980651928107794\n",
      "gradient_weights:  [array([[-6.59831224e-04, -1.60060360e-03,  5.78317105e-04],\n",
      "       [ 8.00964344e-03,  4.56571755e-04,  3.02302158e-05]]), array([[-0.00209066],\n",
      "       [ 0.00436284],\n",
      "       [-0.00081578]])]\n",
      "gradients_biases:  [array([ 9.26723450e-04, -1.49511183e-03, -5.33618710e-05]), array([-0.0007982])]\n",
      "Iteration 259, Cost: 0.2498017434333375\n",
      "gradient_weights:  [array([[-6.49982532e-04, -1.60318542e-03,  5.77355684e-04],\n",
      "       [ 8.01236575e-03,  4.57617979e-04,  3.09780186e-05]]), array([[-0.00207875],\n",
      "       [ 0.00435922],\n",
      "       [-0.00081556]])]\n",
      "gradients_biases:  [array([ 9.32621293e-04, -1.49808966e-03, -5.34449831e-05]), array([-0.00079922])]\n",
      "Iteration 260, Cost: 0.24979696868267576\n",
      "gradient_weights:  [array([[-6.40140404e-04, -1.60576497e-03,  5.76395553e-04],\n",
      "       [ 8.01510661e-03,  4.58666067e-04,  3.17256751e-05]]), array([[-0.00206685],\n",
      "       [ 0.00435561],\n",
      "       [-0.00081534]])]\n",
      "gradients_biases:  [array([ 9.38513048e-04, -1.50106541e-03, -5.35286425e-05]), array([-0.00080025])]\n",
      "Iteration 261, Cost: 0.24979219497804067\n",
      "gradient_weights:  [array([[-6.30304787e-04, -1.60834224e-03,  5.75436716e-04],\n",
      "       [ 8.01786600e-03,  4.59716024e-04,  3.24731916e-05]]), array([[-0.00205495],\n",
      "       [ 0.00435201],\n",
      "       [-0.00081512]])]\n",
      "gradients_biases:  [array([ 9.44398713e-04, -1.50403906e-03, -5.36128425e-05]), array([-0.00080128])]\n",
      "Iteration 262, Cost: 0.24978742226846684\n",
      "gradient_weights:  [array([[-6.20475627e-04, -1.61091727e-03,  5.74479174e-04],\n",
      "       [ 8.02064389e-03,  4.60767854e-04,  3.32205742e-05]]), array([[-0.00204305],\n",
      "       [ 0.00434842],\n",
      "       [-0.00081491]])]\n",
      "gradients_biases:  [array([ 9.50278284e-04, -1.50701064e-03, -5.36975767e-05]), array([-0.00080231])]\n",
      "Iteration 263, Cost: 0.24978265050307438\n",
      "gradient_weights:  [array([[-6.10652870e-04, -1.61349007e-03,  5.73522928e-04],\n",
      "       [ 8.02344024e-03,  4.61821561e-04,  3.39678287e-05]]), array([[-0.00203116],\n",
      "       [ 0.00434484],\n",
      "       [-0.0008147 ]])]\n",
      "gradients_biases:  [array([ 9.56151759e-04, -1.50998015e-03, -5.37828389e-05]), array([-0.00080336])]\n",
      "Iteration 264, Cost: 0.24977787963106826\n",
      "gradient_weights:  [array([[-6.00836465e-04, -1.61606064e-03,  5.72567983e-04],\n",
      "       [ 8.02625503e-03,  4.62877152e-04,  3.47149609e-05]]), array([[-0.00201927],\n",
      "       [ 0.00434126],\n",
      "       [-0.0008145 ]])]\n",
      "gradients_biases:  [array([ 9.62019138e-04, -1.51294759e-03, -5.38686233e-05]), array([-0.0008044])]\n",
      "Iteration 265, Cost: 0.2497731096017372\n",
      "gradient_weights:  [array([[-5.91026355e-04, -1.61862902e-03,  5.71614338e-04],\n",
      "       [ 8.02908823e-03,  4.63934629e-04,  3.54619767e-05]]), array([[-0.00200738],\n",
      "       [ 0.0043377 ],\n",
      "       [-0.0008143 ]])]\n",
      "gradients_biases:  [array([ 9.67880417e-04, -1.51591298e-03, -5.39549241e-05]), array([-0.00080545])]\n",
      "Iteration 266, Cost: 0.24976834036445236\n",
      "gradient_weights:  [array([[-5.81222489e-04, -1.62119522e-03,  5.70661996e-04],\n",
      "       [ 8.03193981e-03,  4.64994000e-04,  3.62088814e-05]]), array([[-0.0019955 ],\n",
      "       [ 0.00433414],\n",
      "       [-0.0008141 ]])]\n",
      "gradients_biases:  [array([ 9.73735597e-04, -1.51887631e-03, -5.40417361e-05]), array([-0.00080651])]\n",
      "Iteration 267, Cost: 0.2497635718686664\n",
      "gradient_weights:  [array([[-5.71424811e-04, -1.62375925e-03,  5.69710959e-04],\n",
      "       [ 8.03480975e-03,  4.66055267e-04,  3.69556806e-05]]), array([[-0.00198363],\n",
      "       [ 0.00433059],\n",
      "       [-0.00081391]])]\n",
      "gradients_biases:  [array([ 9.79584678e-04, -1.52183761e-03, -5.41290542e-05]), array([-0.00080757])]\n",
      "Iteration 268, Cost: 0.24975880406391285\n",
      "gradient_weights:  [array([[-5.61633269e-04, -1.62632113e-03,  5.68761227e-04],\n",
      "       [ 8.03769801e-03,  4.67118436e-04,  3.77023797e-05]]), array([[-0.00197176],\n",
      "       [ 0.00432705],\n",
      "       [-0.00081372]])]\n",
      "gradients_biases:  [array([ 9.85427660e-04, -1.52479687e-03, -5.42168735e-05]), array([-0.00080864])]\n",
      "Iteration 269, Cost: 0.24975403689980435\n",
      "gradient_weights:  [array([[-5.51847807e-04, -1.62888088e-03,  5.67812802e-04],\n",
      "       [ 8.04060457e-03,  4.68183512e-04,  3.84489839e-05]]), array([[-0.00195989],\n",
      "       [ 0.00432352],\n",
      "       [-0.00081353]])]\n",
      "gradients_biases:  [array([ 9.91264542e-04, -1.52775410e-03, -5.43051892e-05]), array([-0.00080971])]\n",
      "Iteration 270, Cost: 0.24974927032603272\n",
      "gradient_weights:  [array([[-5.42068373e-04, -1.63143852e-03,  5.66865686e-04],\n",
      "       [ 8.04352941e-03,  4.69250501e-04,  3.91954984e-05]]), array([[-0.00194802],\n",
      "       [ 0.00432   ],\n",
      "       [-0.00081335]])]\n",
      "gradients_biases:  [array([ 9.97095327e-04, -1.53070931e-03, -5.43939971e-05]), array([-0.00081079])]\n",
      "Iteration 271, Cost: 0.24974450429236694\n",
      "gradient_weights:  [array([[-5.32294911e-04, -1.63399406e-03,  5.65919879e-04],\n",
      "       [ 8.04647249e-03,  4.70319406e-04,  3.99419283e-05]]), array([[-0.00193616],\n",
      "       [ 0.00431649],\n",
      "       [-0.00081317]])]\n",
      "gradients_biases:  [array([ 1.00292002e-03, -1.53366251e-03, -5.44832928e-05]), array([-0.00081187])]\n",
      "Iteration 272, Cost: 0.24973973874865263\n",
      "gradient_weights:  [array([[-5.22527369e-04, -1.63654751e-03,  5.64975382e-04],\n",
      "       [ 8.04943380e-03,  4.71390233e-04,  4.06882785e-05]]), array([[-0.0019243 ],\n",
      "       [ 0.00431298],\n",
      "       [-0.00081299]])]\n",
      "gradients_biases:  [array([ 1.00873861e-03, -1.53661369e-03, -5.45730723e-05]), array([-0.00081296])]\n",
      "Iteration 273, Cost: 0.24973497364481095\n",
      "gradient_weights:  [array([[-5.12765691e-04, -1.63909890e-03,  5.64032197e-04],\n",
      "       [ 8.05241329e-03,  4.72462988e-04,  4.14345540e-05]]), array([[-0.00191245],\n",
      "       [ 0.00430949],\n",
      "       [-0.00081282]])]\n",
      "gradients_biases:  [array([ 1.01455111e-03, -1.53956288e-03, -5.46633317e-05]), array([-0.00081405])]\n",
      "Iteration 274, Cost: 0.24973020893083786\n",
      "gradient_weights:  [array([[-5.03009824e-04, -1.64164823e-03,  5.63090324e-04],\n",
      "       [ 8.05541097e-03,  4.73537674e-04,  4.21807596e-05]]), array([[-0.00190059],\n",
      "       [ 0.004306  ],\n",
      "       [-0.00081265]])]\n",
      "gradients_biases:  [array([ 1.02035751e-03, -1.54251007e-03, -5.47540674e-05]), array([-0.00081514])]\n",
      "Iteration 275, Cost: 0.24972544455680265\n",
      "gradient_weights:  [array([[-4.93259713e-04, -1.64419554e-03,  5.62149764e-04],\n",
      "       [ 8.05842678e-03,  4.74614297e-04,  4.29269000e-05]]), array([[-0.00188875],\n",
      "       [ 0.00430253],\n",
      "       [-0.00081248]])]\n",
      "gradients_biases:  [array([ 1.02615783e-03, -1.54545526e-03, -5.48452758e-05]), array([-0.00081624])]\n",
      "Iteration 276, Cost: 0.2497206804728475\n",
      "gradient_weights:  [array([[-4.83515304e-04, -1.64674082e-03,  5.61210518e-04],\n",
      "       [ 8.06146072e-03,  4.75692863e-04,  4.36729800e-05]]), array([[-0.0018769 ],\n",
      "       [ 0.00429906],\n",
      "       [-0.00081231]])]\n",
      "gradients_biases:  [array([ 1.03195207e-03, -1.54839848e-03, -5.49369537e-05]), array([-0.00081735])]\n",
      "Iteration 277, Cost: 0.24971591662918613\n",
      "gradient_weights:  [array([[-4.73776544e-04, -1.64928411e-03,  5.60272586e-04],\n",
      "       [ 8.06451275e-03,  4.76773376e-04,  4.44190042e-05]]), array([[-0.00186506],\n",
      "       [ 0.0042956 ],\n",
      "       [-0.00081215]])]\n",
      "gradients_biases:  [array([ 1.03774022e-03, -1.55133972e-03, -5.50290979e-05]), array([-0.00081846])]\n",
      "Iteration 278, Cost: 0.24971115297610286\n",
      "gradient_weights:  [array([[-4.64043377e-04, -1.65182540e-03,  5.59335969e-04],\n",
      "       [ 8.06758286e-03,  4.77855842e-04,  4.51649771e-05]]), array([[-0.00185322],\n",
      "       [ 0.00429215],\n",
      "       [-0.000812  ]])]\n",
      "gradients_biases:  [array([ 1.04352229e-03, -1.55427898e-03, -5.51217052e-05]), array([-0.00081957])]\n",
      "Iteration 279, Cost: 0.2497063894639518\n",
      "gradient_weights:  [array([[-4.54315749e-04, -1.65436473e-03,  5.58400668e-04],\n",
      "       [ 8.07067101e-03,  4.78940264e-04,  4.59109032e-05]]), array([[-0.00184138],\n",
      "       [ 0.00428871],\n",
      "       [-0.00081184]])]\n",
      "gradients_biases:  [array([ 1.04929828e-03, -1.55721628e-03, -5.52147728e-05]), array([-0.00082069])]\n",
      "Iteration 280, Cost: 0.24970162604315593\n",
      "gradient_weights:  [array([[-4.44593607e-04, -1.65690211e-03,  5.57466682e-04],\n",
      "       [ 8.07377719e-03,  4.80026650e-04,  4.66567870e-05]]), array([[-0.00182954],\n",
      "       [ 0.00428528],\n",
      "       [-0.00081169]])]\n",
      "gradients_biases:  [array([ 1.05506820e-03, -1.56015162e-03, -5.53082980e-05]), array([-0.00082181])]\n",
      "Iteration 281, Cost: 0.24969686266420582\n",
      "gradient_weights:  [array([[-4.34876896e-04, -1.65943755e-03,  5.56534012e-04],\n",
      "       [ 8.07690138e-03,  4.81115003e-04,  4.74026328e-05]]), array([[-0.00181771],\n",
      "       [ 0.00428185],\n",
      "       [-0.00081154]])]\n",
      "gradients_biases:  [array([ 1.06083205e-03, -1.56308500e-03, -5.54022782e-05]), array([-0.00082293])]\n",
      "Iteration 282, Cost: 0.24969209927765906\n",
      "gradient_weights:  [array([[-4.25165563e-04, -1.66197106e-03,  5.55602658e-04],\n",
      "       [ 8.08004354e-03,  4.82205329e-04,  4.81484449e-05]]), array([[-0.00180588],\n",
      "       [ 0.00427844],\n",
      "       [-0.0008114 ]])]\n",
      "gradients_biases:  [array([ 1.06658983e-03, -1.56601644e-03, -5.54967108e-05]), array([-0.00082406])]\n",
      "Iteration 283, Cost: 0.249687335834139\n",
      "gradient_weights:  [array([[-4.15459551e-04, -1.66450267e-03,  5.54672621e-04],\n",
      "       [ 8.08320366e-03,  4.83297633e-04,  4.88942277e-05]]), array([[-0.00179405],\n",
      "       [ 0.00427503],\n",
      "       [-0.00081125]])]\n",
      "gradients_biases:  [array([ 1.07234155e-03, -1.56894593e-03, -5.55915935e-05]), array([-0.0008252])]\n",
      "Iteration 284, Cost: 0.2496825722843341\n",
      "gradient_weights:  [array([[-4.05758809e-04, -1.66703239e-03,  5.53743901e-04],\n",
      "       [ 8.08638171e-03,  4.84391921e-04,  4.96399853e-05]]), array([[-0.00178223],\n",
      "       [ 0.00427164],\n",
      "       [-0.00081111]])]\n",
      "gradients_biases:  [array([ 1.07808721e-03, -1.57187349e-03, -5.56869240e-05]), array([-0.00082634])]\n",
      "Iteration 285, Cost: 0.2496778085789965\n",
      "gradient_weights:  [array([[-3.96063281e-04, -1.66956023e-03,  5.52816498e-04],\n",
      "       [ 8.08957768e-03,  4.85488196e-04,  5.03857220e-05]]), array([[-0.0017704 ],\n",
      "       [ 0.00426825],\n",
      "       [-0.00081098]])]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradients_biases:  [array([ 1.08382682e-03, -1.57479911e-03, -5.57827003e-05]), array([-0.00082748])]\n",
      "Iteration 286, Cost: 0.2496730446689419\n",
      "gradient_weights:  [array([[-3.86372914e-04, -1.67208622e-03,  5.51890411e-04],\n",
      "       [ 8.09279154e-03,  4.86586465e-04,  5.11314418e-05]]), array([[-0.00175858],\n",
      "       [ 0.00426487],\n",
      "       [-0.00081084]])]\n",
      "gradients_biases:  [array([ 1.08956038e-03, -1.57772281e-03, -5.58789201e-05]), array([-0.00082863])]\n",
      "Iteration 287, Cost: 0.24966828050504766\n",
      "gradient_weights:  [array([[-3.76687654e-04, -1.67461036e-03,  5.50965642e-04],\n",
      "       [ 8.09602326e-03,  4.87686732e-04,  5.18771488e-05]]), array([[-0.00174676],\n",
      "       [ 0.0042615 ],\n",
      "       [-0.00081071]])]\n",
      "gradients_biases:  [array([ 1.09528790e-03, -1.58064459e-03, -5.59755817e-05]), array([-0.00082978])]\n",
      "Iteration 288, Cost: 0.2496635160382525\n",
      "gradient_weights:  [array([[-3.67007445e-04, -1.67713267e-03,  5.50042189e-04],\n",
      "       [ 8.09927283e-03,  4.88789002e-04,  5.26228472e-05]]), array([[-0.00173494],\n",
      "       [ 0.00425814],\n",
      "       [-0.00081059]])]\n",
      "gradients_biases:  [array([ 1.10100937e-03, -1.58356445e-03, -5.60726831e-05]), array([-0.00083093])]\n",
      "Iteration 289, Cost: 0.24965875121955552\n",
      "gradient_weights:  [array([[-3.57332236e-04, -1.67965317e-03,  5.49120053e-04],\n",
      "       [ 8.10254023e-03,  4.89893281e-04,  5.33685408e-05]]), array([[-0.00172313],\n",
      "       [ 0.00425479],\n",
      "       [-0.00081046]])]\n",
      "gradients_biases:  [array([ 1.10672481e-03, -1.58648240e-03, -5.61702227e-05]), array([-0.00083209])]\n",
      "Iteration 290, Cost: 0.24965398600001498\n",
      "gradient_weights:  [array([[-3.47661971e-04, -1.68217187e-03,  5.48199234e-04],\n",
      "       [ 8.10582543e-03,  4.90999574e-04,  5.41142338e-05]]), array([[-0.00171131],\n",
      "       [ 0.00425145],\n",
      "       [-0.00081034]])]\n",
      "gradients_biases:  [array([ 1.11243422e-03, -1.58939844e-03, -5.62681987e-05]), array([-0.00083325])]\n",
      "Iteration 291, Cost: 0.24964922033074766\n",
      "gradient_weights:  [array([[-3.37996597e-04, -1.68468879e-03,  5.47279731e-04],\n",
      "       [ 8.10912841e-03,  4.92107886e-04,  5.48599299e-05]]), array([[-0.0016995 ],\n",
      "       [ 0.00424812],\n",
      "       [-0.00081022]])]\n",
      "gradients_biases:  [array([ 1.11813761e-03, -1.59231259e-03, -5.63666097e-05]), array([-0.00083442])]\n",
      "Iteration 292, Cost: 0.2496444541629278\n",
      "gradient_weights:  [array([[-3.28336059e-04, -1.68720394e-03,  5.46361545e-04],\n",
      "       [ 8.11244916e-03,  4.93218221e-04,  5.56056332e-05]]), array([[-0.00168769],\n",
      "       [ 0.00424479],\n",
      "       [-0.00081011]])]\n",
      "gradients_biases:  [array([ 1.12383497e-03, -1.59522484e-03, -5.64654540e-05]), array([-0.00083559])]\n",
      "Iteration 293, Cost: 0.24963968744778636\n",
      "gradient_weights:  [array([[-3.18680305e-04, -1.68971734e-03,  5.45444675e-04],\n",
      "       [ 8.11578765e-03,  4.94330586e-04,  5.63513475e-05]]), array([[-0.00167588],\n",
      "       [ 0.00424148],\n",
      "       [-0.00080999]])]\n",
      "gradients_biases:  [array([ 1.12952631e-03, -1.59813520e-03, -5.65647303e-05]), array([-0.00083676])]\n",
      "Iteration 294, Cost: 0.2496349201366097\n",
      "gradient_weights:  [array([[-3.09029281e-04, -1.69222900e-03,  5.44529121e-04],\n",
      "       [ 8.11914386e-03,  4.95444984e-04,  5.70970766e-05]]), array([[-0.00166407],\n",
      "       [ 0.00423817],\n",
      "       [-0.00080988]])]\n",
      "gradients_biases:  [array([ 1.13521165e-03, -1.60104368e-03, -5.66644373e-05]), array([-0.00083794])]\n",
      "Iteration 295, Cost: 0.24963015218073922\n",
      "gradient_weights:  [array([[-2.99382932e-04, -1.69473894e-03,  5.43614883e-04],\n",
      "       [ 8.12251777e-03,  4.96561421e-04,  5.78428245e-05]]), array([[-0.00165226],\n",
      "       [ 0.00423487],\n",
      "       [-0.00080978]])]\n",
      "gradients_biases:  [array([ 1.14089098e-03, -1.60395028e-03, -5.67645736e-05]), array([-0.00083912])]\n",
      "Iteration 296, Cost: 0.2496253835315701\n",
      "gradient_weights:  [array([[-2.89741205e-04, -1.69724718e-03,  5.42701960e-04],\n",
      "       [ 8.12590937e-03,  4.97679902e-04,  5.85885947e-05]]), array([[-0.00164045],\n",
      "       [ 0.00423158],\n",
      "       [-0.00080967]])]\n",
      "gradients_biases:  [array([ 1.14656431e-03, -1.60685500e-03, -5.68651382e-05]), array([-0.00084031])]\n",
      "Iteration 297, Cost: 0.24962061414055037\n",
      "gradient_weights:  [array([[-2.80104047e-04, -1.69975372e-03,  5.41790352e-04],\n",
      "       [ 8.12931862e-03,  4.98800432e-04,  5.93343912e-05]]), array([[-0.00162865],\n",
      "       [ 0.00422831],\n",
      "       [-0.00080957]])]\n",
      "gradients_biases:  [array([ 1.15223164e-03, -1.60975785e-03, -5.69661298e-05]), array([-0.0008415])]\n",
      "Iteration 298, Cost: 0.24961584395918035\n",
      "gradient_weights:  [array([[-2.70471405e-04, -1.70225859e-03,  5.40880059e-04],\n",
      "       [ 8.13274552e-03,  4.99923016e-04,  6.00802177e-05]]), array([[-0.00161684],\n",
      "       [ 0.00422503],\n",
      "       [-0.00080947]])]\n",
      "gradients_biases:  [array([ 1.15789298e-03, -1.61265884e-03, -5.70675474e-05]), array([-0.00084269])]\n",
      "Iteration 299, Cost: 0.24961107293901136\n",
      "gradient_weights:  [array([[-2.60843224e-04, -1.70476179e-03,  5.39971080e-04],\n",
      "       [ 8.13619005e-03,  5.01047659e-04,  6.08260778e-05]]), array([[-0.00160504],\n",
      "       [ 0.00422177],\n",
      "       [-0.00080938]])]\n",
      "gradients_biases:  [array([ 1.16354833e-03, -1.61555797e-03, -5.71693900e-05]), array([-0.00084389])]\n",
      "Iteration 300, Cost: 0.249606301031645\n",
      "gradient_weights:  [array([[-2.51219451e-04, -1.70726335e-03,  5.39063415e-04],\n",
      "       [ 8.13965217e-03,  5.02174365e-04,  6.15719753e-05]]), array([[-0.00159324],\n",
      "       [ 0.00421852],\n",
      "       [-0.00080928]])]\n",
      "gradients_biases:  [array([ 1.16919771e-03, -1.61845525e-03, -5.72716567e-05]), array([-0.00084509])]\n",
      "Iteration 301, Cost: 0.2496015281887326\n",
      "gradient_weights:  [array([[-2.41600034e-04, -1.70976328e-03,  5.38157063e-04],\n",
      "       [ 8.14313189e-03,  5.03303141e-04,  6.23179139e-05]]), array([[-0.00158144],\n",
      "       [ 0.00421528],\n",
      "       [-0.00080919]])]\n",
      "gradients_biases:  [array([ 1.17484111e-03, -1.62135067e-03, -5.73743465e-05]), array([-0.00084629])]\n",
      "Iteration 302, Cost: 0.24959675436197348\n",
      "gradient_weights:  [array([[-2.31984919e-04, -1.71226159e-03,  5.37252024e-04],\n",
      "       [ 8.14662916e-03,  5.04433990e-04,  6.30638971e-05]]), array([[-0.00156963],\n",
      "       [ 0.00421204],\n",
      "       [-0.00080911]])]\n",
      "gradients_biases:  [array([ 1.18047854e-03, -1.62424425e-03, -5.74774586e-05]), array([-0.0008475])]\n",
      "Iteration 303, Cost: 0.24959197950311512\n",
      "gradient_weights:  [array([[-2.22374052e-04, -1.71475830e-03,  5.36348298e-04],\n",
      "       [ 8.15014399e-03,  5.05566917e-04,  6.38099287e-05]]), array([[-0.00155783],\n",
      "       [ 0.00420882],\n",
      "       [-0.00080902]])]\n",
      "gradients_biases:  [array([ 1.18611000e-03, -1.62713599e-03, -5.75809922e-05]), array([-0.00084872])]\n",
      "Iteration 304, Cost: 0.24958720356395137\n",
      "gradient_weights:  [array([[-2.12767381e-04, -1.71725342e-03,  5.35445884e-04],\n",
      "       [ 8.15367634e-03,  5.06701928e-04,  6.45560123e-05]]), array([[-0.00154603],\n",
      "       [ 0.0042056 ],\n",
      "       [-0.00080894]])]\n",
      "gradients_biases:  [array([ 1.19173551e-03, -1.63002589e-03, -5.76849466e-05]), array([-0.00084993])]\n",
      "Iteration 305, Cost: 0.2495824264963221\n",
      "gradient_weights:  [array([[-2.03164853e-04, -1.71974697e-03,  5.34544781e-04],\n",
      "       [ 8.15722620e-03,  5.07839028e-04,  6.53021514e-05]]), array([[-0.00153423],\n",
      "       [ 0.00420239],\n",
      "       [-0.00080886]])]\n",
      "gradients_biases:  [array([ 1.19735506e-03, -1.63291396e-03, -5.77893210e-05]), array([-0.00085115])]\n",
      "Iteration 306, Cost: 0.24957764825211237\n",
      "gradient_weights:  [array([[-1.93566415e-04, -1.72223896e-03,  5.33644990e-04],\n",
      "       [ 8.16079356e-03,  5.08978220e-04,  6.60483497e-05]]), array([[-0.00152243],\n",
      "       [ 0.00419919],\n",
      "       [-0.00080879]])]\n",
      "gradients_biases:  [array([ 1.20296865e-03, -1.63580021e-03, -5.78941149e-05]), array([-0.00085237])]\n",
      "Iteration 307, Cost: 0.24957286878325124\n",
      "gradient_weights:  [array([[-1.83972014e-04, -1.72472941e-03,  5.32746509e-04],\n",
      "       [ 8.16437839e-03,  5.10119510e-04,  6.67946107e-05]]), array([[-0.00151063],\n",
      "       [ 0.004196  ],\n",
      "       [-0.00080871]])]\n",
      "gradients_biases:  [array([ 1.20857631e-03, -1.63868464e-03, -5.79993276e-05]), array([-0.0008536])]\n",
      "Iteration 308, Cost: 0.24956808804171104\n",
      "gradient_weights:  [array([[-1.74381597e-04, -1.72721833e-03,  5.31849337e-04],\n",
      "       [ 8.16798067e-03,  5.11262903e-04,  6.75409380e-05]]), array([[-0.00149883],\n",
      "       [ 0.00419282],\n",
      "       [-0.00080864]])]\n",
      "gradients_biases:  [array([ 1.21417802e-03, -1.64156724e-03, -5.81049586e-05]), array([-0.00085483])]\n",
      "Iteration 309, Cost: 0.2495633059795066\n",
      "gradient_weights:  [array([[-1.64795111e-04, -1.72970574e-03,  5.30953476e-04],\n",
      "       [ 8.17160040e-03,  5.12408403e-04,  6.82873351e-05]]), array([[-0.00148704],\n",
      "       [ 0.00418965],\n",
      "       [-0.00080857]])]\n",
      "gradients_biases:  [array([ 1.21977380e-03, -1.64444804e-03, -5.82110073e-05]), array([-0.00085606])]\n",
      "Iteration 310, Cost: 0.24955852254869437\n",
      "gradient_weights:  [array([[-1.55212505e-04, -1.73219164e-03,  5.30058923e-04],\n",
      "       [ 8.17523754e-03,  5.13556016e-04,  6.90338056e-05]]), array([[-0.00147524],\n",
      "       [ 0.00418649],\n",
      "       [-0.00080851]])]\n",
      "gradients_biases:  [array([ 1.22536365e-03, -1.64732703e-03, -5.83174733e-05]), array([-0.0008573])]\n",
      "Iteration 311, Cost: 0.2495537377013713\n",
      "gradient_weights:  [array([[-1.45633724e-04, -1.73467606e-03,  5.29165678e-04],\n",
      "       [ 8.17889208e-03,  5.14705746e-04,  6.97803530e-05]]), array([[-0.00146344],\n",
      "       [ 0.00418333],\n",
      "       [-0.00080845]])]\n",
      "gradients_biases:  [array([ 1.23094758e-03, -1.65020421e-03, -5.84243561e-05]), array([-0.00085854])]\n",
      "Iteration 312, Cost: 0.2495489513896746\n",
      "gradient_weights:  [array([[-1.36058718e-04, -1.73715901e-03,  5.28273740e-04],\n",
      "       [ 8.18256401e-03,  5.15857597e-04,  7.05269808e-05]]), array([[-0.00145164],\n",
      "       [ 0.00418019],\n",
      "       [-0.00080839]])]\n",
      "gradients_biases:  [array([ 1.23652558e-03, -1.65307960e-03, -5.85316552e-05]), array([-0.00085979])]\n",
      "Iteration 313, Cost: 0.24954416356578016\n",
      "gradient_weights:  [array([[-1.26487433e-04, -1.73964050e-03,  5.27383110e-04],\n",
      "       [ 8.18625331e-03,  5.17011574e-04,  7.12736924e-05]]), array([[-0.00143984],\n",
      "       [ 0.00417705],\n",
      "       [-0.00080833]])]\n",
      "gradients_biases:  [array([ 1.24209767e-03, -1.65595319e-03, -5.86393703e-05]), array([-0.00086103])]\n",
      "Iteration 314, Cost: 0.24953937418190217\n",
      "gradient_weights:  [array([[-1.16919817e-04, -1.74212055e-03,  5.26493786e-04],\n",
      "       [ 8.18995996e-03,  5.18167682e-04,  7.20204915e-05]]), array([[-0.00142804],\n",
      "       [ 0.00417392],\n",
      "       [-0.00080828]])]\n",
      "gradients_biases:  [array([ 1.24766385e-03, -1.65882499e-03, -5.87475010e-05]), array([-0.00086229])]\n",
      "Iteration 315, Cost: 0.24953458319029226\n",
      "gradient_weights:  [array([[-1.07355819e-04, -1.74459917e-03,  5.25605768e-04],\n",
      "       [ 8.19368395e-03,  5.19325926e-04,  7.27673814e-05]]), array([[-0.00141624],\n",
      "       [ 0.0041708 ],\n",
      "       [-0.00080823]])]\n",
      "gradients_biases:  [array([ 1.25322413e-03, -1.66169501e-03, -5.88560471e-05]), array([-0.00086354])]\n",
      "Iteration 316, Cost: 0.24952979054323848\n",
      "gradient_weights:  [array([[-9.77953847e-05, -1.74707638e-03,  5.24719055e-04],\n",
      "       [ 8.19742525e-03,  5.20486310e-04,  7.35143656e-05]]), array([[-0.00140444],\n",
      "       [ 0.00416769],\n",
      "       [-0.00080818]])]\n",
      "gradients_biases:  [array([ 1.25877850e-03, -1.66456325e-03, -5.89650082e-05]), array([-0.0008648])]\n",
      "Iteration 317, Cost: 0.24952499619306456\n",
      "gradient_weights:  [array([[-8.82384635e-05, -1.74955219e-03,  5.23833646e-04],\n",
      "       [ 8.20118385e-03,  5.21648839e-04,  7.42614477e-05]]), array([[-0.00139263],\n",
      "       [ 0.00416459],\n",
      "       [-0.00080813]])]\n",
      "gradients_biases:  [array([ 1.26432697e-03, -1.66742971e-03, -5.90743840e-05]), array([-0.00086606])]\n",
      "Iteration 318, Cost: 0.249520200092129\n",
      "gradient_weights:  [array([[-7.86850031e-05, -1.75202662e-03,  5.22949542e-04],\n",
      "       [ 8.20495973e-03,  5.22813517e-04,  7.50086310e-05]]), array([[-0.00138083],\n",
      "       [ 0.0041615 ],\n",
      "       [-0.00080809]])]\n",
      "gradients_biases:  [array([ 1.26986956e-03, -1.67029440e-03, -5.91841744e-05]), array([-0.00086733])]\n",
      "Iteration 319, Cost: 0.24951540219282434\n",
      "gradient_weights:  [array([[-6.91349516e-05, -1.75449967e-03,  5.22066740e-04],\n",
      "       [ 8.20875288e-03,  5.23980348e-04,  7.57559191e-05]]), array([[-0.00136903],\n",
      "       [ 0.00415841],\n",
      "       [-0.00080805]])]\n",
      "gradients_biases:  [array([ 1.27540625e-03, -1.67315733e-03, -5.92943790e-05]), array([-0.0008686])]\n",
      "Iteration 320, Cost: 0.24951060244757645\n",
      "gradient_weights:  [array([[-5.95882572e-05, -1.75697137e-03,  5.21185241e-04],\n",
      "       [ 8.21256329e-03,  5.25149338e-04,  7.65033153e-05]]), array([[-0.00135723],\n",
      "       [ 0.00415534],\n",
      "       [-0.00080801]])]\n",
      "gradients_biases:  [array([ 1.28093706e-03, -1.67601849e-03, -5.94049978e-05]), array([-0.00086987])]\n",
      "Iteration 321, Cost: 0.24950580080884338\n",
      "gradient_weights:  [array([[-5.00448682e-05, -1.75944173e-03,  5.20305043e-04],\n",
      "       [ 8.21639092e-03,  5.26320491e-04,  7.72508231e-05]]), array([[-0.00134542],\n",
      "       [ 0.00415227],\n",
      "       [-0.00080798]])]\n",
      "gradients_biases:  [array([ 1.28646200e-03, -1.67887789e-03, -5.95160306e-05]), array([-0.00087115])]\n",
      "Iteration 322, Cost: 0.24950099722911487\n",
      "gradient_weights:  [array([[-4.05047329e-05, -1.76191075e-03,  5.19426147e-04],\n",
      "       [ 8.22023578e-03,  5.27493811e-04,  7.79984460e-05]]), array([[-0.00133361],\n",
      "       [ 0.00414921],\n",
      "       [-0.00080795]])]\n",
      "gradients_biases:  [array([ 1.29198106e-03, -1.68173554e-03, -5.96274771e-05]), array([-0.00087243])]\n",
      "Iteration 323, Cost: 0.24949619166091125\n",
      "gradient_weights:  [array([[-3.09677998e-05, -1.76437846e-03,  5.18548551e-04],\n",
      "       [ 8.22409783e-03,  5.28669302e-04,  7.87461874e-05]]), array([[-0.00132181],\n",
      "       [ 0.00414616],\n",
      "       [-0.00080792]])]\n",
      "gradients_biases:  [array([ 1.29749425e-03, -1.68459144e-03, -5.97393374e-05]), array([-0.00087371])]\n",
      "Iteration 324, Cost: 0.24949138405678273\n",
      "gradient_weights:  [array([[-2.14340174e-05, -1.76684487e-03,  5.17672255e-04],\n",
      "       [ 8.22797708e-03,  5.29846970e-04,  7.94940506e-05]]), array([[-0.00131   ],\n",
      "       [ 0.00414312],\n",
      "       [-0.00080789]])]\n",
      "gradients_biases:  [array([ 1.30300157e-03, -1.68744559e-03, -5.98516112e-05]), array([-0.00087499])]\n",
      "Iteration 325, Cost: 0.2494865743693088\n",
      "gradient_weights:  [array([[-1.19033343e-05, -1.76931000e-03,  5.16797258e-04],\n",
      "       [ 8.23187349e-03,  5.31026818e-04,  8.02420392e-05]]), array([[-0.00129819],\n",
      "       [ 0.00414009],\n",
      "       [-0.00080787]])]\n",
      "gradients_biases:  [array([ 1.30850303e-03, -1.69029800e-03, -5.99642986e-05]), array([-0.00087628])]\n",
      "Iteration 326, Cost: 0.24948176255109716\n",
      "gradient_weights:  [array([[-2.37569917e-06, -1.77177385e-03,  5.15923559e-04],\n",
      "       [ 8.23578706e-03,  5.32208851e-04,  8.09901566e-05]]), array([[-0.00128638],\n",
      "       [ 0.00413707],\n",
      "       [-0.00080785]])]\n",
      "gradients_biases:  [array([ 1.31399863e-03, -1.69314868e-03, -6.00773994e-05]), array([-0.00087758])]\n",
      "Iteration 327, Cost: 0.2494769485547829\n",
      "gradient_weights:  [array([[ 7.14893913e-06, -1.77423644e-03,  5.15051159e-04],\n",
      "       [ 8.23971777e-03,  5.33393072e-04,  8.17384061e-05]]), array([[-0.00127457],\n",
      "       [ 0.00413405],\n",
      "       [-0.00080783]])]\n",
      "gradients_biases:  [array([ 1.31948838e-03, -1.69599762e-03, -6.01909136e-05]), array([-0.00087888])]\n",
      "Iteration 328, Cost: 0.2494721323330279\n",
      "gradient_weights:  [array([[ 1.66706318e-05, -1.77669779e-03,  5.14180055e-04],\n",
      "       [ 8.24366560e-03,  5.34579487e-04,  8.24867912e-05]]), array([[-0.00126276],\n",
      "       [ 0.00413105],\n",
      "       [-0.00080782]])]\n",
      "gradients_biases:  [array([ 1.32497227e-03, -1.69884484e-03, -6.03048413e-05]), array([-0.00088018])]\n",
      "Iteration 329, Cost: 0.24946731383852\n",
      "gradient_weights:  [array([[ 2.61894298e-05, -1.77915791e-03,  5.13310247e-04],\n",
      "       [ 8.24763054e-03,  5.35768100e-04,  8.32353153e-05]]), array([[-0.00125094],\n",
      "       [ 0.00412805],\n",
      "       [-0.0008078 ]])]\n",
      "gradients_biases:  [array([ 1.33045032e-03, -1.70169033e-03, -6.04191823e-05]), array([-0.00088148])]\n",
      "Iteration 330, Cost: 0.24946249302397183\n",
      "gradient_weights:  [array([[ 3.57053841e-05, -1.78161681e-03,  5.12441735e-04],\n",
      "       [ 8.25161257e-03,  5.36958914e-04,  8.39839818e-05]]), array([[-0.00123913],\n",
      "       [ 0.00412506],\n",
      "       [-0.00080779]])]\n",
      "gradients_biases:  [array([ 1.33592253e-03, -1.70453411e-03, -6.05339367e-05]), array([-0.00088279])]\n",
      "Iteration 331, Cost: 0.24945766984212064\n",
      "gradient_weights:  [array([[ 4.52185456e-05, -1.78407450e-03,  5.11574518e-04],\n",
      "       [ 8.25561168e-03,  5.38151934e-04,  8.47327941e-05]]), array([[-0.00122731],\n",
      "       [ 0.00412209],\n",
      "       [-0.00080779]])]\n",
      "gradients_biases:  [array([ 1.34138890e-03, -1.70737616e-03, -6.06491046e-05]), array([-0.0008841])]\n",
      "Iteration 332, Cost: 0.24945284424572695\n",
      "gradient_weights:  [array([[ 5.47289652e-05, -1.78653101e-03,  5.10708596e-04],\n",
      "       [ 8.25962785e-03,  5.39347165e-04,  8.54817556e-05]]), array([[-0.00121549],\n",
      "       [ 0.00411911],\n",
      "       [-0.00080778]])]\n",
      "gradients_biases:  [array([ 1.34684942e-03, -1.71021651e-03, -6.07646859e-05]), array([-0.00088541])]\n",
      "Iteration 333, Cost: 0.24944801618757412\n",
      "gradient_weights:  [array([[ 6.42366934e-05, -1.78898634e-03,  5.09843966e-04],\n",
      "       [ 8.26366108e-03,  5.40544609e-04,  8.62308698e-05]]), array([[-0.00120367],\n",
      "       [ 0.00411615],\n",
      "       [-0.00080778]])]\n",
      "gradients_biases:  [array([ 1.35230412e-03, -1.71305515e-03, -6.08806807e-05]), array([-0.00088673])]\n",
      "Iteration 334, Cost: 0.24944318562046744\n",
      "gradient_weights:  [array([[ 7.37417809e-05, -1.79144051e-03,  5.08980630e-04],\n",
      "       [ 8.26771133e-03,  5.41744273e-04,  8.69801400e-05]]), array([[-0.00119185],\n",
      "       [ 0.0041132 ],\n",
      "       [-0.00080778]])]\n",
      "gradients_biases:  [array([ 1.35775299e-03, -1.71589209e-03, -6.09970892e-05]), array([-0.00088805])]\n",
      "Iteration 335, Cost: 0.24943835249723317\n",
      "gradient_weights:  [array([[ 8.32442782e-05, -1.79389353e-03,  5.08118586e-04],\n",
      "       [ 8.27177860e-03,  5.42946158e-04,  8.77295697e-05]]), array([[-0.00118002],\n",
      "       [ 0.00411025],\n",
      "       [-0.00080778]])]\n",
      "gradients_biases:  [array([ 1.36319603e-03, -1.71872733e-03, -6.11139113e-05]), array([-0.00088937])]\n",
      "Iteration 336, Cost: 0.24943351677071823\n",
      "gradient_weights:  [array([[ 9.27442358e-05, -1.79634542e-03,  5.07257833e-04],\n",
      "       [ 8.27586288e-03,  5.44150271e-04,  8.84791622e-05]]), array([[-0.00116819],\n",
      "       [ 0.00410732],\n",
      "       [-0.00080779]])]\n",
      "gradients_biases:  [array([ 1.36863324e-03, -1.72156088e-03, -6.12311472e-05]), array([-0.0008907])]\n",
      "Iteration 337, Cost: 0.2494286783937888\n",
      "gradient_weights:  [array([[ 1.02241704e-04, -1.79879618e-03,  5.06398371e-04],\n",
      "       [ 8.27996414e-03,  5.45356614e-04,  8.92289209e-05]]), array([[-0.00115637],\n",
      "       [ 0.00410439],\n",
      "       [-0.0008078 ]])]\n",
      "gradients_biases:  [array([ 1.37406464e-03, -1.72439274e-03, -6.13487970e-05]), array([-0.00089203])]\n",
      "Iteration 338, Cost: 0.2494238373193301\n",
      "gradient_weights:  [array([[ 1.11736733e-04, -1.80124583e-03,  5.05540198e-04],\n",
      "       [ 8.28408238e-03,  5.46565191e-04,  8.99788493e-05]]), array([[-0.00114453],\n",
      "       [ 0.00410147],\n",
      "       [-0.00080781]])]\n",
      "gradients_biases:  [array([ 1.37949021e-03, -1.72722291e-03, -6.14668608e-05]), array([-0.00089337])]\n",
      "Iteration 339, Cost: 0.24941899350024516\n",
      "gradient_weights:  [array([[ 1.21229373e-04, -1.80369439e-03,  5.04683316e-04],\n",
      "       [ 8.28821758e-03,  5.47776008e-04,  9.07289508e-05]]), array([[-0.0011327 ],\n",
      "       [ 0.00409856],\n",
      "       [-0.00080782]])]\n",
      "gradients_biases:  [array([ 1.38490997e-03, -1.73005140e-03, -6.15853388e-05]), array([-0.00089471])]\n",
      "Iteration 340, Cost: 0.24941414688945457\n",
      "gradient_weights:  [array([[ 1.30719675e-04, -1.80614187e-03,  5.03827722e-04],\n",
      "       [ 8.29236973e-03,  5.48989066e-04,  9.14792288e-05]]), array([[-0.00112087],\n",
      "       [ 0.00409566],\n",
      "       [-0.00080784]])]\n",
      "gradients_biases:  [array([ 1.39032392e-03, -1.73287821e-03, -6.17042310e-05]), array([-0.00089605])]\n",
      "Iteration 341, Cost: 0.24940929743989526\n",
      "gradient_weights:  [array([[ 1.40207687e-04, -1.80858829e-03,  5.02973415e-04],\n",
      "       [ 8.29653880e-03,  5.50204372e-04,  9.22296866e-05]]), array([[-0.00110903],\n",
      "       [ 0.00409276],\n",
      "       [-0.00080786]])]\n",
      "gradients_biases:  [array([ 1.39573206e-03, -1.73570335e-03, -6.18235376e-05]), array([-0.00089739])]\n",
      "Iteration 342, Cost: 0.2494044451045198\n",
      "gradient_weights:  [array([[ 1.49693461e-04, -1.81103365e-03,  5.02120397e-04],\n",
      "       [ 8.30072480e-03,  5.51421928e-04,  9.29803277e-05]]), array([[-0.00109719],\n",
      "       [ 0.00408988],\n",
      "       [-0.00080788]])]\n",
      "gradients_biases:  [array([ 1.40113439e-03, -1.73852682e-03, -6.19432589e-05]), array([-0.00089874])]\n",
      "Iteration 343, Cost: 0.2493995898362959\n",
      "gradient_weights:  [array([[ 1.59177045e-04, -1.81347796e-03,  5.01268664e-04],\n",
      "       [ 8.30492770e-03,  5.52641738e-04,  9.37311556e-05]]), array([[-0.00108534],\n",
      "       [ 0.004087  ],\n",
      "       [-0.00080791]])]\n",
      "gradients_biases:  [array([ 1.40653091e-03, -1.74134863e-03, -6.20633949e-05]), array([-0.00090009])]\n",
      "Iteration 344, Cost: 0.2493947315882053\n",
      "gradient_weights:  [array([[ 1.68658490e-04, -1.81592125e-03,  5.00418218e-04],\n",
      "       [ 8.30914749e-03,  5.53863806e-04,  9.44821735e-05]]), array([[-0.0010735 ],\n",
      "       [ 0.00408413],\n",
      "       [-0.00080794]])]\n",
      "gradients_biases:  [array([ 1.41192164e-03, -1.74416878e-03, -6.21839459e-05]), array([-0.00090145])]\n",
      "Iteration 345, Cost: 0.24938987031324344\n",
      "gradient_weights:  [array([[ 1.78137846e-04, -1.81836353e-03,  4.99569056e-04],\n",
      "       [ 8.31338415e-03,  5.55088136e-04,  9.52333850e-05]]), array([[-0.00106165],\n",
      "       [ 0.00408127],\n",
      "       [-0.00080797]])]\n",
      "gradients_biases:  [array([ 1.41730656e-03, -1.74698727e-03, -6.23049120e-05]), array([-0.00090281])]\n",
      "Iteration 346, Cost: 0.24938500596441815\n",
      "gradient_weights:  [array([[ 1.87615161e-04, -1.82080481e-03,  4.98721180e-04],\n",
      "       [ 8.31763768e-03,  5.56314731e-04,  9.59847935e-05]]), array([[-0.0010498 ],\n",
      "       [ 0.00407842],\n",
      "       [-0.000808  ]])]\n",
      "gradients_biases:  [array([ 1.42268568e-03, -1.74980411e-03, -6.24262935e-05]), array([-0.00090417])]\n",
      "Iteration 347, Cost: 0.24938013849474946\n",
      "gradient_weights:  [array([[ 1.97090486e-04, -1.82324510e-03,  4.97874587e-04],\n",
      "       [ 8.32190805e-03,  5.57543597e-04,  9.67364023e-05]]), array([[-0.00103795],\n",
      "       [ 0.00407558],\n",
      "       [-0.00080804]])]\n",
      "gradients_biases:  [array([ 1.42805900e-03, -1.75261930e-03, -6.25480906e-05]), array([-0.00090553])]\n",
      "Iteration 348, Cost: 0.24937526785726855\n",
      "gradient_weights:  [array([[ 2.06563870e-04, -1.82568441e-03,  4.97029277e-04],\n",
      "       [ 8.32619526e-03,  5.58774735e-04,  9.74882149e-05]]), array([[-0.00102609],\n",
      "       [ 0.00407275],\n",
      "       [-0.00080807]])]\n",
      "gradients_biases:  [array([ 1.43342653e-03, -1.75543284e-03, -6.26703034e-05]), array([-0.0009069])]\n",
      "Iteration 349, Cost: 0.24937039400501695\n",
      "gradient_weights:  [array([[ 2.16035362e-04, -1.82812277e-03,  4.96185250e-04],\n",
      "       [ 8.33049929e-03,  5.60008150e-04,  9.82402347e-05]]), array([[-0.00101423],\n",
      "       [ 0.00406992],\n",
      "       [-0.00080812]])]\n",
      "gradients_biases:  [array([ 1.43878826e-03, -1.75824475e-03, -6.27929322e-05]), array([-0.00090827])]\n",
      "Iteration 350, Cost: 0.2493655168910459\n",
      "gradient_weights:  [array([[ 2.25505011e-04, -1.83056017e-03,  4.95342504e-04],\n",
      "       [ 8.33482013e-03,  5.61243846e-04,  9.89924652e-05]]), array([[-0.00100237],\n",
      "       [ 0.0040671 ],\n",
      "       [-0.00080816]])]\n",
      "gradients_biases:  [array([ 1.44414421e-03, -1.76105502e-03, -6.29159773e-05]), array([-0.00090965])]\n",
      "Iteration 351, Cost: 0.24936063646841578\n",
      "gradient_weights:  [array([[ 2.34972867e-04, -1.83299665e-03,  4.94501040e-04],\n",
      "       [ 8.33915776e-03,  5.62481826e-04,  9.97449097e-05]]), array([[-0.0009905 ],\n",
      "       [ 0.0040643 ],\n",
      "       [-0.00080821]])]\n",
      "gradients_biases:  [array([ 1.44949436e-03, -1.76386366e-03, -6.30394389e-05]), array([-0.00091103])]\n",
      "Iteration 352, Cost: 0.24935575269019494\n",
      "gradient_weights:  [array([[ 0.00024444, -0.00183543,  0.00049366],\n",
      "       [ 0.00834351,  0.00056372,  0.0001005 ]]), array([[-0.00097863],\n",
      "       [ 0.0040615 ],\n",
      "       [-0.00080826]])]\n",
      "gradients_biases:  [array([ 1.45483872e-03, -1.76667067e-03, -6.31633171e-05]), array([-0.00091241])]\n",
      "Iteration 353, Cost: 0.24935086550945929\n",
      "gradient_weights:  [array([[ 0.0002539 , -0.00183787,  0.00049282],\n",
      "       [ 0.00834788,  0.00056496,  0.00010125]]), array([[-0.00096676],\n",
      "       [ 0.0040587 ],\n",
      "       [-0.00080831]])]\n",
      "gradients_biases:  [array([ 1.46017729e-03, -1.76947606e-03, -6.32876124e-05]), array([-0.00091379])]\n",
      "Iteration 354, Cost: 0.24934597487929178\n",
      "gradient_weights:  [array([[ 0.00026337, -0.0018403 ,  0.00049198],\n",
      "       [ 0.00835227,  0.00056621,  0.000102  ]]), array([[-0.00095489],\n",
      "       [ 0.00405592],\n",
      "       [-0.00080836]])]\n",
      "gradients_biases:  [array([ 1.46551008e-03, -1.77227983e-03, -6.34123249e-05]), array([-0.00091518])]\n",
      "Iteration 355, Cost: 0.24934108075278094\n",
      "gradient_weights:  [array([[ 0.00027283, -0.00184273,  0.00049115],\n",
      "       [ 0.00835668,  0.00056746,  0.00010276]]), array([[-0.00094301],\n",
      "       [ 0.00405314],\n",
      "       [-0.00080842]])]\n",
      "gradients_biases:  [array([ 1.47083708e-03, -1.77508198e-03, -6.35374549e-05]), array([-0.00091657])]\n",
      "Iteration 356, Cost: 0.2493361830830208\n",
      "gradient_weights:  [array([[ 0.00028229, -0.00184517,  0.00049031],\n",
      "       [ 0.0083611 ,  0.00056871,  0.00010351]]), array([[-0.00093113],\n",
      "       [ 0.00405038],\n",
      "       [-0.00080848]])]\n",
      "gradients_biases:  [array([ 1.47615829e-03, -1.77788252e-03, -6.36630026e-05]), array([-0.00091797])]\n",
      "Iteration 357, Cost: 0.24933128182310998\n",
      "gradient_weights:  [array([[ 0.00029175, -0.0018476 ,  0.00048948],\n",
      "       [ 0.00836554,  0.00056996,  0.00010426]]), array([[-0.00091924],\n",
      "       [ 0.00404762],\n",
      "       [-0.00080854]])]\n",
      "gradients_biases:  [array([ 1.48147372e-03, -1.78068146e-03, -6.37889685e-05]), array([-0.00091937])]\n",
      "Iteration 358, Cost: 0.24932637692615084\n",
      "gradient_weights:  [array([[ 0.0003012 , -0.00185003,  0.00048865],\n",
      "       [ 0.00836999,  0.00057121,  0.00010502]]), array([[-0.00090735],\n",
      "       [ 0.00404487],\n",
      "       [-0.00080861]])]\n",
      "gradients_biases:  [array([ 1.48678337e-03, -1.78347879e-03, -6.39153527e-05]), array([-0.00092077])]\n",
      "Iteration 359, Cost: 0.24932146834524901\n",
      "gradient_weights:  [array([[ 0.00031066, -0.00185246,  0.00048782],\n",
      "       [ 0.00837446,  0.00057247,  0.00010577]]), array([[-0.00089546],\n",
      "       [ 0.00404213],\n",
      "       [-0.00080868]])]\n",
      "gradients_biases:  [array([ 1.49208723e-03, -1.78627452e-03, -6.40421555e-05]), array([-0.00092217])]\n",
      "Iteration 360, Cost: 0.2493165560335123\n",
      "gradient_weights:  [array([[ 0.00032011, -0.00185489,  0.00048699],\n",
      "       [ 0.00837895,  0.00057373,  0.00010653]]), array([[-0.00088356],\n",
      "       [ 0.00403939],\n",
      "       [-0.00080875]])]\n",
      "gradients_biases:  [array([ 1.49738531e-03, -1.78906866e-03, -6.41693772e-05]), array([-0.00092358])]\n",
      "Iteration 361, Cost: 0.24931163994405023\n",
      "gradient_weights:  [array([[ 0.00032956, -0.00185731,  0.00048616],\n",
      "       [ 0.00838345,  0.00057499,  0.00010728]]), array([[-0.00087166],\n",
      "       [ 0.00403667],\n",
      "       [-0.00080882]])]\n",
      "gradients_biases:  [array([ 1.50267761e-03, -1.79186120e-03, -6.42970182e-05]), array([-0.000925])]\n",
      "Iteration 362, Cost: 0.2493067200299735\n",
      "gradient_weights:  [array([[ 0.00033901, -0.00185974,  0.00048533],\n",
      "       [ 0.00838798,  0.00057625,  0.00010804]]), array([[-0.00085976],\n",
      "       [ 0.00403395],\n",
      "       [-0.0008089 ]])]\n",
      "gradients_biases:  [array([ 1.50796412e-03, -1.79465216e-03, -6.44250787e-05]), array([-0.00092641])]\n",
      "Iteration 363, Cost: 0.24930179624439286\n",
      "gradient_weights:  [array([[ 0.00034847, -0.00186217,  0.0004845 ],\n",
      "       [ 0.00839251,  0.00057752,  0.00010879]]), array([[-0.00084785],\n",
      "       [ 0.00403124],\n",
      "       [-0.00080898]])]\n",
      "gradients_biases:  [array([ 1.51324485e-03, -1.79744154e-03, -6.45535591e-05]), array([-0.00092783])]\n",
      "Iteration 364, Cost: 0.24929686854041838\n",
      "gradient_weights:  [array([[ 0.00035791, -0.00186459,  0.00048368],\n",
      "       [ 0.00839707,  0.00057879,  0.00010955]]), array([[-0.00083594],\n",
      "       [ 0.00402854],\n",
      "       [-0.00080906]])]\n",
      "gradients_biases:  [array([ 1.51851980e-03, -1.80022933e-03, -6.46824596e-05]), array([-0.00092925])]\n",
      "Iteration 365, Cost: 0.24929193687115953\n",
      "gradient_weights:  [array([[ 0.00036736, -0.00186702,  0.00048285],\n",
      "       [ 0.00840164,  0.00058006,  0.0001103 ]]), array([[-0.00082402],\n",
      "       [ 0.00402585],\n",
      "       [-0.00080914]])]\n",
      "gradients_biases:  [array([ 1.52378896e-03, -1.80301555e-03, -6.48117806e-05]), array([-0.00093068])]\n",
      "Iteration 366, Cost: 0.2492870011897234\n",
      "gradient_weights:  [array([[ 0.00037681, -0.00186944,  0.00048203],\n",
      "       [ 0.00840623,  0.00058133,  0.00011106]]), array([[-0.0008121 ],\n",
      "       [ 0.00402317],\n",
      "       [-0.00080923]])]\n",
      "gradients_biases:  [array([ 1.52905234e-03, -1.80580021e-03, -6.49415224e-05]), array([-0.00093211])]\n",
      "Iteration 367, Cost: 0.24928206144921466\n",
      "gradient_weights:  [array([[ 0.00038625, -0.00187186,  0.00048121],\n",
      "       [ 0.00841083,  0.0005826 ,  0.00011182]]), array([[-0.00080018],\n",
      "       [ 0.00402049],\n",
      "       [-0.00080932]])]\n",
      "gradients_biases:  [array([ 1.53430994e-03, -1.80858329e-03, -6.50716854e-05]), array([-0.00093354])]\n",
      "Iteration 368, Cost: 0.2492771176027347\n",
      "gradient_weights:  [array([[ 0.0003957 , -0.00187429,  0.00048039],\n",
      "       [ 0.00841545,  0.00058388,  0.00011257]]), array([[-0.00078825],\n",
      "       [ 0.00401782],\n",
      "       [-0.00080941]])]\n",
      "gradients_biases:  [array([ 1.53956175e-03, -1.81136481e-03, -6.52022698e-05]), array([-0.00093497])]\n",
      "Iteration 369, Cost: 0.24927216960338083\n",
      "gradient_weights:  [array([[ 0.00040514, -0.00187671,  0.00047957],\n",
      "       [ 0.00842009,  0.00058516,  0.00011333]]), array([[-0.00077632],\n",
      "       [ 0.00401516],\n",
      "       [-0.0008095 ]])]\n",
      "gradients_biases:  [array([ 1.54480778e-03, -1.81414477e-03, -6.53332760e-05]), array([-0.00093641])]\n",
      "Iteration 370, Cost: 0.24926721740424576\n",
      "gradient_weights:  [array([[ 0.00041459, -0.00187913,  0.00047876],\n",
      "       [ 0.00842475,  0.00058644,  0.00011409]]), array([[-0.00076438],\n",
      "       [ 0.00401251],\n",
      "       [-0.0008096 ]])]\n",
      "gradients_biases:  [array([ 1.55004802e-03, -1.81692318e-03, -6.54647044e-05]), array([-0.00093785])]\n",
      "Iteration 371, Cost: 0.24926226095841691\n",
      "gradient_weights:  [array([[ 0.00042403, -0.00188155,  0.00047794],\n",
      "       [ 0.00842942,  0.00058773,  0.00011484]]), array([[-0.00075244],\n",
      "       [ 0.00400987],\n",
      "       [-0.0008097 ]])]\n",
      "gradients_biases:  [array([ 1.55528248e-03, -1.81970003e-03, -6.55965552e-05]), array([-0.0009393])]\n",
      "Iteration 372, Cost: 0.2492573002189753\n",
      "gradient_weights:  [array([[ 0.00043347, -0.00188397,  0.00047713],\n",
      "       [ 0.0084341 ,  0.00058901,  0.0001156 ]]), array([[-0.00074049],\n",
      "       [ 0.00400724],\n",
      "       [-0.00080981]])]\n",
      "gradients_biases:  [array([ 1.56051114e-03, -1.82247534e-03, -6.57288289e-05]), array([-0.00094075])]\n",
      "Iteration 373, Cost: 0.24925233513899542\n",
      "gradient_weights:  [array([[ 0.00044291, -0.00188639,  0.00047631],\n",
      "       [ 0.00843881,  0.0005903 ,  0.00011636]]), array([[-0.00072854],\n",
      "       [ 0.00400461],\n",
      "       [-0.00080991]])]\n",
      "gradients_biases:  [array([ 1.56573402e-03, -1.82524910e-03, -6.58615258e-05]), array([-0.0009422])]\n",
      "Iteration 374, Cost: 0.24924736567154449\n",
      "gradient_weights:  [array([[ 0.00045235, -0.00188881,  0.0004755 ],\n",
      "       [ 0.00844353,  0.00059159,  0.00011712]]), array([[-0.00071658],\n",
      "       [ 0.00400199],\n",
      "       [-0.00081002]])]\n",
      "gradients_biases:  [array([ 1.57095111e-03, -1.82802133e-03, -6.59946462e-05]), array([-0.00094365])]\n",
      "Iteration 375, Cost: 0.24924239176968105\n",
      "gradient_weights:  [array([[ 0.00046179, -0.00189123,  0.00047469],\n",
      "       [ 0.00844827,  0.00059289,  0.00011788]]), array([[-0.00070462],\n",
      "       [ 0.00399938],\n",
      "       [-0.00081013]])]\n",
      "gradients_biases:  [array([ 1.57616240e-03, -1.83079202e-03, -6.61281905e-05]), array([-0.00094511])]\n",
      "Iteration 376, Cost: 0.24923741338645525\n",
      "gradient_weights:  [array([[ 0.00047123, -0.00189364,  0.00047388],\n",
      "       [ 0.00845302,  0.00059418,  0.00011864]]), array([[-0.00069266],\n",
      "       [ 0.00399678],\n",
      "       [-0.00081024]])]\n",
      "gradients_biases:  [array([ 1.58136790e-03, -1.83356118e-03, -6.62621591e-05]), array([-0.00094658])]\n",
      "Iteration 377, Cost: 0.24923243047490753\n",
      "gradient_weights:  [array([[ 0.00048067, -0.00189606,  0.00047307],\n",
      "       [ 0.00845779,  0.00059548,  0.00011939]]), array([[-0.00068069],\n",
      "       [ 0.00399419],\n",
      "       [-0.00081036]])]\n",
      "gradients_biases:  [array([ 1.58656761e-03, -1.83632881e-03, -6.63965523e-05]), array([-0.00094804])]\n",
      "Iteration 378, Cost: 0.24922744298806826\n",
      "gradient_weights:  [array([[ 0.0004901 , -0.00189848,  0.00047226],\n",
      "       [ 0.00846258,  0.00059678,  0.00012015]]), array([[-0.00066871],\n",
      "       [ 0.0039916 ],\n",
      "       [-0.00081048]])]\n",
      "gradients_biases:  [array([ 1.59176151e-03, -1.83909491e-03, -6.65313705e-05]), array([-0.00094951])]\n",
      "Iteration 379, Cost: 0.24922245087895684\n",
      "gradient_weights:  [array([[ 0.00049954, -0.00190089,  0.00047146],\n",
      "       [ 0.00846738,  0.00059809,  0.00012091]]), array([[-0.00065673],\n",
      "       [ 0.00398902],\n",
      "       [-0.0008106 ]])]\n",
      "gradients_biases:  [array([ 1.59694962e-03, -1.84185950e-03, -6.66666141e-05]), array([-0.00095098])]\n",
      "Iteration 380, Cost: 0.2492174541005811\n",
      "gradient_weights:  [array([[ 0.00050898, -0.00190331,  0.00047065],\n",
      "       [ 0.0084722 ,  0.00059939,  0.00012167]]), array([[-0.00064475],\n",
      "       [ 0.00398645],\n",
      "       [-0.00081073]])]\n",
      "gradients_biases:  [array([ 1.60213193e-03, -1.84462257e-03, -6.68022834e-05]), array([-0.00095246])]\n",
      "Iteration 381, Cost: 0.24921245260593677\n",
      "gradient_weights:  [array([[ 0.00051841, -0.00190572,  0.00046985],\n",
      "       [ 0.00847703,  0.0006007 ,  0.00012243]]), array([[-0.00063276],\n",
      "       [ 0.00398389],\n",
      "       [-0.00081085]])]\n",
      "gradients_biases:  [array([ 1.60730843e-03, -1.84738413e-03, -6.69383788e-05]), array([-0.00095394])]\n",
      "Iteration 382, Cost: 0.24920744634800646\n",
      "gradient_weights:  [array([[ 0.00052785, -0.00190813,  0.00046905],\n",
      "       [ 0.00848189,  0.00060201,  0.0001232 ]]), array([[-0.00062076],\n",
      "       [ 0.00398134],\n",
      "       [-0.00081098]])]\n",
      "gradients_biases:  [array([ 1.61247913e-03, -1.85014419e-03, -6.70749008e-05]), array([-0.00095542])]\n",
      "Iteration 383, Cost: 0.2492024352797596\n",
      "gradient_weights:  [array([[ 0.00053729, -0.00191055,  0.00046825],\n",
      "       [ 0.00848675,  0.00060332,  0.00012396]]), array([[-0.00060876],\n",
      "       [ 0.00397879],\n",
      "       [-0.00081112]])]\n",
      "gradients_biases:  [array([ 1.61764402e-03, -1.85290273e-03, -6.72118496e-05]), array([-0.0009569])]\n",
      "Iteration 384, Cost: 0.24919741935415085\n",
      "gradient_weights:  [array([[ 0.00054672, -0.00191296,  0.00046745],\n",
      "       [ 0.00849164,  0.00060464,  0.00012472]]), array([[-0.00059675],\n",
      "       [ 0.00397626],\n",
      "       [-0.00081125]])]\n",
      "gradients_biases:  [array([ 1.62280309e-03, -1.85565978e-03, -6.73492257e-05]), array([-0.00095839])]\n",
      "Iteration 385, Cost: 0.24919239852412045\n",
      "gradient_weights:  [array([[ 0.00055616, -0.00191537,  0.00046665],\n",
      "       [ 0.00849654,  0.00060596,  0.00012548]]), array([[-0.00058474],\n",
      "       [ 0.00397373],\n",
      "       [-0.00081139]])]\n",
      "gradients_biases:  [array([ 1.62795636e-03, -1.85841533e-03, -6.74870294e-05]), array([-0.00095988])]\n",
      "Iteration 386, Cost: 0.2491873727425929\n",
      "gradient_weights:  [array([[ 0.00056559, -0.00191779,  0.00046585],\n",
      "       [ 0.00850146,  0.00060728,  0.00012624]]), array([[-0.00057273],\n",
      "       [ 0.0039712 ],\n",
      "       [-0.00081153]])]\n",
      "gradients_biases:  [array([ 1.63310380e-03, -1.86116939e-03, -6.76252612e-05]), array([-0.00096138])]\n",
      "Iteration 387, Cost: 0.24918234196247646\n",
      "gradient_weights:  [array([[ 0.00057503, -0.0019202 ,  0.00046506],\n",
      "       [ 0.00850639,  0.0006086 ,  0.000127  ]]), array([[-0.0005607 ],\n",
      "       [ 0.00396869],\n",
      "       [-0.00081167]])]\n",
      "gradients_biases:  [array([ 1.63824543e-03, -1.86392196e-03, -6.77639214e-05]), array([-0.00096288])]\n",
      "Iteration 388, Cost: 0.2491773061366624\n",
      "gradient_weights:  [array([[ 0.00058446, -0.00192261,  0.00046426],\n",
      "       [ 0.00851134,  0.00060992,  0.00012777]]), array([[-0.00054868],\n",
      "       [ 0.00396619],\n",
      "       [-0.00081182]])]\n",
      "gradients_biases:  [array([ 1.64338123e-03, -1.86667305e-03, -6.79030105e-05]), array([-0.00096438])]\n",
      "Iteration 389, Cost: 0.24917226521802463\n",
      "gradient_weights:  [array([[ 0.0005939 , -0.00192502,  0.00046347],\n",
      "       [ 0.00851631,  0.00061125,  0.00012853]]), array([[-0.00053664],\n",
      "       [ 0.00396369],\n",
      "       [-0.00081197]])]\n",
      "gradients_biases:  [array([ 1.64851120e-03, -1.86942266e-03, -6.80425287e-05]), array([-0.00096588])]\n",
      "Iteration 390, Cost: 0.24916721915941878\n",
      "gradient_weights:  [array([[ 0.00060333, -0.00192743,  0.00046268],\n",
      "       [ 0.00852129,  0.00061258,  0.00012929]]), array([[-0.0005246 ],\n",
      "       [ 0.0039612 ],\n",
      "       [-0.00081212]])]\n",
      "gradients_biases:  [array([ 1.65363535e-03, -1.87217079e-03, -6.81824767e-05]), array([-0.00096739])]\n",
      "Iteration 391, Cost: 0.24916216791368162\n",
      "gradient_weights:  [array([[ 0.00061277, -0.00192984,  0.00046189],\n",
      "       [ 0.00852629,  0.00061391,  0.00013006]]), array([[-0.00051256],\n",
      "       [ 0.00395872],\n",
      "       [-0.00081227]])]\n",
      "gradients_biases:  [array([ 1.65875366e-03, -1.87491744e-03, -6.83228546e-05]), array([-0.0009689])]\n",
      "Iteration 392, Cost: 0.2491571114336305\n",
      "gradient_weights:  [array([[ 0.00062221, -0.00193225,  0.0004611 ],\n",
      "       [ 0.00853131,  0.00061525,  0.00013082]]), array([[-0.00050051],\n",
      "       [ 0.00395625],\n",
      "       [-0.00081243]])]\n",
      "gradients_biases:  [array([ 1.66386613e-03, -1.87766263e-03, -6.84636630e-05]), array([-0.00097042])]\n",
      "Iteration 393, Cost: 0.2491520496720625\n",
      "gradient_weights:  [array([[ 0.00063164, -0.00193466,  0.00046031],\n",
      "       [ 0.00853634,  0.00061658,  0.00013159]]), array([[-0.00048845],\n",
      "       [ 0.00395378],\n",
      "       [-0.00081259]])]\n",
      "gradients_biases:  [array([ 1.66897277e-03, -1.88040635e-03, -6.86049022e-05]), array([-0.00097194])]\n",
      "Iteration 394, Cost: 0.24914698258175394\n",
      "gradient_weights:  [array([[ 0.00064108, -0.00193706,  0.00045952],\n",
      "       [ 0.00854139,  0.00061792,  0.00013235]]), array([[-0.00047639],\n",
      "       [ 0.00395132],\n",
      "       [-0.00081275]])]\n",
      "gradients_biases:  [array([ 1.67407355e-03, -1.88314861e-03, -6.87465727e-05]), array([-0.00097346])]\n",
      "Iteration 395, Cost: 0.2491419101154597\n",
      "gradient_weights:  [array([[ 0.00065051, -0.00193947,  0.00045874],\n",
      "       [ 0.00854645,  0.00061926,  0.00013312]]), array([[-0.00046432],\n",
      "       [ 0.00394887],\n",
      "       [-0.00081291]])]\n",
      "gradients_biases:  [array([ 1.67916849e-03, -1.88588942e-03, -6.88886748e-05]), array([-0.00097498])]\n",
      "Iteration 396, Cost: 0.24913683222591254\n",
      "gradient_weights:  [array([[ 0.00065995, -0.00194188,  0.00045795],\n",
      "       [ 0.00855153,  0.00062061,  0.00013388]]), array([[-0.00045224],\n",
      "       [ 0.00394643],\n",
      "       [-0.00081308]])]\n",
      "gradients_biases:  [array([ 1.68425758e-03, -1.88862877e-03, -6.90312091e-05]), array([-0.00097651])]\n",
      "Iteration 397, Cost: 0.24913174886582262\n",
      "gradient_weights:  [array([[ 0.00066939, -0.00194429,  0.00045717],\n",
      "       [ 0.00855663,  0.00062196,  0.00013465]]), array([[-0.00044016],\n",
      "       [ 0.003944  ],\n",
      "       [-0.00081325]])]\n",
      "gradients_biases:  [array([ 1.68934080e-03, -1.89136667e-03, -6.91741758e-05]), array([-0.00097804])]\n",
      "Iteration 398, Cost: 0.24912665998787656\n",
      "gradient_weights:  [array([[ 0.00067883, -0.00194669,  0.00045639],\n",
      "       [ 0.00856174,  0.0006233 ,  0.00013542]]), array([[-0.00042807],\n",
      "       [ 0.00394157],\n",
      "       [-0.00081342]])]\n",
      "gradients_biases:  [array([ 1.69441817e-03, -1.89410312e-03, -6.93175754e-05]), array([-0.00097958])]\n",
      "Iteration 399, Cost: 0.24912156554473708\n",
      "gradient_weights:  [array([[ 0.00068826, -0.0019491 ,  0.00045561],\n",
      "       [ 0.00856687,  0.00062466,  0.00013618]]), array([[-0.00041598],\n",
      "       [ 0.00393916],\n",
      "       [-0.0008136 ]])]\n",
      "gradients_biases:  [array([ 1.69948966e-03, -1.89683813e-03, -6.94614084e-05]), array([-0.00098112])]\n",
      "Iteration 400, Cost: 0.24911646548904215\n",
      "gradient_weights:  [array([[ 0.0006977 , -0.00195151,  0.00045483],\n",
      "       [ 0.00857202,  0.00062601,  0.00013695]]), array([[-0.00040388],\n",
      "       [ 0.00393675],\n",
      "       [-0.00081378]])]\n",
      "gradients_biases:  [array([ 1.70455529e-03, -1.89957171e-03, -6.96056752e-05]), array([-0.00098266])]\n",
      "Iteration 401, Cost: 0.2491113597734046\n",
      "gradient_weights:  [array([[ 0.00070714, -0.00195391,  0.00045405],\n",
      "       [ 0.00857718,  0.00062737,  0.00013772]]), array([[-0.00039177],\n",
      "       [ 0.00393434],\n",
      "       [-0.00081396]])]\n",
      "gradients_biases:  [array([ 1.70961503e-03, -1.90230385e-03, -6.97503761e-05]), array([-0.0009842])]\n",
      "Iteration 402, Cost: 0.24910624835041117\n",
      "gradient_weights:  [array([[ 0.00071658, -0.00195632,  0.00045327],\n",
      "       [ 0.00858236,  0.00062872,  0.00013849]]), array([[-0.00037966],\n",
      "       [ 0.00393195],\n",
      "       [-0.00081414]])]\n",
      "gradients_biases:  [array([ 1.71466890e-03, -1.90503456e-03, -6.98955116e-05]), array([-0.00098575])]\n",
      "Iteration 403, Cost: 0.24910113117262228\n",
      "gradient_weights:  [array([[ 0.00072602, -0.00195872,  0.0004525 ],\n",
      "       [ 0.00858755,  0.00063009,  0.00013925]]), array([[-0.00036754],\n",
      "       [ 0.00392956],\n",
      "       [-0.00081433]])]\n",
      "gradients_biases:  [array([ 1.71971688e-03, -1.90776384e-03, -7.00410821e-05]), array([-0.0009873])]\n",
      "Iteration 404, Cost: 0.24909600819257108\n",
      "gradient_weights:  [array([[ 0.00073546, -0.00196113,  0.00045172],\n",
      "       [ 0.00859276,  0.00063145,  0.00014002]]), array([[-0.00035541],\n",
      "       [ 0.00392718],\n",
      "       [-0.00081452]])]\n",
      "gradients_biases:  [array([ 1.72475896e-03, -1.91049170e-03, -7.01870881e-05]), array([-0.00098886])]\n",
      "Iteration 405, Cost: 0.2490908793627628\n",
      "gradient_weights:  [array([[ 0.0007449 , -0.00196353,  0.00045095],\n",
      "       [ 0.00859799,  0.00063281,  0.00014079]]), array([[-0.00034328],\n",
      "       [ 0.00392481],\n",
      "       [-0.00081471]])]\n",
      "gradients_biases:  [array([ 1.72979514e-03, -1.91321813e-03, -7.03335301e-05]), array([-0.00099041])]\n",
      "Iteration 406, Cost: 0.2490857446356745\n",
      "gradient_weights:  [array([[ 0.00075435, -0.00196594,  0.00045018],\n",
      "       [ 0.00860323,  0.00063418,  0.00014156]]), array([[-0.00033114],\n",
      "       [ 0.00392245],\n",
      "       [-0.0008149 ]])]\n",
      "gradients_biases:  [array([ 1.73482543e-03, -1.91594316e-03, -7.04804083e-05]), array([-0.00099198])]\n",
      "Iteration 407, Cost: 0.24908060396375403\n",
      "gradient_weights:  [array([[ 0.00076379, -0.00196834,  0.00044941],\n",
      "       [ 0.00860849,  0.00063555,  0.00014233]]), array([[-0.00031899],\n",
      "       [ 0.0039201 ],\n",
      "       [-0.0008151 ]])]\n",
      "gradients_biases:  [array([ 1.73984980e-03, -1.91866677e-03, -7.06277233e-05]), array([-0.00099354])]\n",
      "Iteration 408, Cost: 0.2490754572994195\n",
      "gradient_weights:  [array([[ 0.00077323, -0.00197075,  0.00044864],\n",
      "       [ 0.00861377,  0.00063693,  0.0001431 ]]), array([[-0.00030684],\n",
      "       [ 0.00391775],\n",
      "       [-0.0008153 ]])]\n",
      "gradients_biases:  [array([ 1.74486825e-03, -1.92138897e-03, -7.07754755e-05]), array([-0.00099511])]\n",
      "Iteration 409, Cost: 0.24907030459505897\n",
      "gradient_weights:  [array([[ 0.00078268, -0.00197315,  0.00044787],\n",
      "       [ 0.00861906,  0.0006383 ,  0.00014387]]), array([[-0.00029467],\n",
      "       [ 0.00391541],\n",
      "       [-0.0008155 ]])]\n",
      "gradients_biases:  [array([ 1.74988079e-03, -1.92410977e-03, -7.09236653e-05]), array([-0.00099668])]\n",
      "Iteration 410, Cost: 0.24906514580302958\n",
      "gradient_weights:  [array([[ 0.00079212, -0.00197556,  0.0004471 ],\n",
      "       [ 0.00862436,  0.00063968,  0.00014464]]), array([[-0.00028251],\n",
      "       [ 0.00391308],\n",
      "       [-0.00081571]])]\n",
      "gradients_biases:  [array([ 1.75488739e-03, -1.92682917e-03, -7.10722933e-05]), array([-0.00099825])]\n",
      "Iteration 411, Cost: 0.2490599808756567\n",
      "gradient_weights:  [array([[ 0.00080157, -0.00197796,  0.00044633],\n",
      "       [ 0.00862969,  0.00064106,  0.00014542]]), array([[-0.00027033],\n",
      "       [ 0.00391075],\n",
      "       [-0.00081592]])]\n",
      "gradients_biases:  [array([ 1.75988806e-03, -1.92954718e-03, -7.12213597e-05]), array([-0.00099983])]\n",
      "Iteration 412, Cost: 0.24905480976523384\n",
      "gradient_weights:  [array([[ 0.00081102, -0.00198036,  0.00044557],\n",
      "       [ 0.00863503,  0.00064245,  0.00014619]]), array([[-0.00025815],\n",
      "       [ 0.00390844],\n",
      "       [-0.00081613]])]\n",
      "gradients_biases:  [array([ 1.76488279e-03, -1.93226379e-03, -7.13708651e-05]), array([-0.00100141])]\n",
      "Iteration 413, Cost: 0.24904963242402173\n",
      "gradient_weights:  [array([[ 0.00082047, -0.00198277,  0.00044481],\n",
      "       [ 0.00864038,  0.00064383,  0.00014696]]), array([[-0.00024596],\n",
      "       [ 0.00390613],\n",
      "       [-0.00081634]])]\n",
      "gradients_biases:  [array([ 1.76987157e-03, -1.93497901e-03, -7.15208099e-05]), array([-0.001003])]\n",
      "Iteration 414, Cost: 0.24904444880424764\n",
      "gradient_weights:  [array([[ 0.00082992, -0.00198517,  0.00044405],\n",
      "       [ 0.00864575,  0.00064522,  0.00014773]]), array([[-0.00023376],\n",
      "       [ 0.00390383],\n",
      "       [-0.00081656]])]\n",
      "gradients_biases:  [array([ 1.77485440e-03, -1.93769285e-03, -7.16711946e-05]), array([-0.00100459])]\n",
      "Iteration 415, Cost: 0.249039258858105\n",
      "gradient_weights:  [array([[ 0.00083937, -0.00198757,  0.00044328],\n",
      "       [ 0.00865114,  0.00064661,  0.00014851]]), array([[-0.00022156],\n",
      "       [ 0.00390154],\n",
      "       [-0.00081677]])]\n",
      "gradients_biases:  [array([ 1.77983127e-03, -1.94040530e-03, -7.18220196e-05]), array([-0.00100618])]\n",
      "Iteration 416, Cost: 0.24903406253775268\n",
      "gradient_weights:  [array([[ 0.00084882, -0.00198998,  0.00044253],\n",
      "       [ 0.00865655,  0.000648  ,  0.00014928]]), array([[-0.00020935],\n",
      "       [ 0.00389925],\n",
      "       [-0.000817  ]])]\n",
      "gradients_biases:  [array([ 1.78480216e-03, -1.94311638e-03, -7.19732853e-05]), array([-0.00100777])]\n",
      "Iteration 417, Cost: 0.2490288597953144\n",
      "gradient_weights:  [array([[ 0.00085828, -0.00199238,  0.00044177],\n",
      "       [ 0.00866197,  0.0006494 ,  0.00015006]]), array([[-0.00019713],\n",
      "       [ 0.00389697],\n",
      "       [-0.00081722]])]\n",
      "gradients_biases:  [array([ 1.78976708e-03, -1.94582608e-03, -7.21249923e-05]), array([-0.00100937])]\n",
      "Iteration 418, Cost: 0.24902365058287795\n",
      "gradient_weights:  [array([[ 0.00086773, -0.00199478,  0.00044101],\n",
      "       [ 0.0086674 ,  0.0006508 ,  0.00015083]]), array([[-0.0001849 ],\n",
      "       [ 0.0038947 ],\n",
      "       [-0.00081745]])]\n",
      "gradients_biases:  [array([ 1.79472602e-03, -1.94853441e-03, -7.22771409e-05]), array([-0.00101097])]\n",
      "Iteration 419, Cost: 0.24901843485249503\n",
      "gradient_weights:  [array([[ 0.00087719, -0.00199718,  0.00044025],\n",
      "       [ 0.00867285,  0.0006522 ,  0.00015161]]), array([[-0.00017267],\n",
      "       [ 0.00389244],\n",
      "       [-0.00081768]])]\n",
      "gradients_biases:  [array([ 1.79967897e-03, -1.95124138e-03, -7.24297317e-05]), array([-0.00101257])]\n",
      "Iteration 420, Cost: 0.24901321255618022\n",
      "gradient_weights:  [array([[ 0.00088665, -0.00199959,  0.0004395 ],\n",
      "       [ 0.00867832,  0.0006536 ,  0.00015238]]), array([[-0.00016043],\n",
      "       [ 0.00389019],\n",
      "       [-0.00081791]])]\n",
      "gradients_biases:  [array([ 1.80462591e-03, -1.95394698e-03, -7.25827650e-05]), array([-0.00101418])]\n",
      "Iteration 421, Cost: 0.2490079836459107\n",
      "gradient_weights:  [array([[ 0.00089611, -0.00200199,  0.00043875],\n",
      "       [ 0.00868381,  0.000655  ,  0.00015316]]), array([[-0.00014818],\n",
      "       [ 0.00388794],\n",
      "       [-0.00081814]])]\n",
      "gradients_biases:  [array([ 1.80956686e-03, -1.95665123e-03, -7.27362414e-05]), array([-0.00101579])]\n",
      "Iteration 422, Cost: 0.2490027480736252\n",
      "gradient_weights:  [array([[ 0.00090557, -0.00200439,  0.000438  ],\n",
      "       [ 0.00868931,  0.00065641,  0.00015393]]), array([[-0.00013592],\n",
      "       [ 0.0038857 ],\n",
      "       [-0.00081838]])]\n",
      "gradients_biases:  [array([ 1.81450178e-03, -1.95935411e-03, -7.28901613e-05]), array([-0.00101741])]\n",
      "Iteration 423, Cost: 0.24899750579122404\n",
      "gradient_weights:  [array([[ 0.00091503, -0.00200679,  0.00043724],\n",
      "       [ 0.00869482,  0.00065782,  0.00015471]]), array([[-0.00012366],\n",
      "       [ 0.00388347],\n",
      "       [-0.00081862]])]\n",
      "gradients_biases:  [array([ 1.81943069e-03, -1.96205565e-03, -7.30445251e-05]), array([-0.00101902])]\n",
      "Iteration 424, Cost: 0.24899225675056808\n",
      "gradient_weights:  [array([[ 0.0009245 , -0.0020092 ,  0.0004365 ],\n",
      "       [ 0.00870035,  0.00065924,  0.00015549]]), array([[-0.00011138],\n",
      "       [ 0.00388125],\n",
      "       [-0.00081886]])]\n",
      "gradients_biases:  [array([ 1.82435356e-03, -1.96475584e-03, -7.31993334e-05]), array([-0.00102064])]\n",
      "Iteration 425, Cost: 0.2489870009034782\n",
      "gradient_weights:  [array([[ 0.00093396, -0.0020116 ,  0.00043575],\n",
      "       [ 0.0087059 ,  0.00066065,  0.00015627]]), array([[-9.91025551e-05],\n",
      "       [ 3.87903223e-03],\n",
      "       [-8.19109273e-04]])]\n",
      "gradients_biases:  [array([ 1.82927039e-03, -1.96745468e-03, -7.33545865e-05]), array([-0.00102227])]\n",
      "Iteration 426, Cost: 0.24898173820173486\n",
      "gradient_weights:  [array([[ 0.00094343, -0.002014  ,  0.000435  ],\n",
      "       [ 0.00871147,  0.00066207,  0.00015705]]), array([[-8.68138123e-05],\n",
      "       [ 3.87682395e-03],\n",
      "       [-8.19357631e-04]])]\n",
      "gradients_biases:  [array([ 1.83418118e-03, -1.97015219e-03, -7.35102850e-05]), array([-0.0010239])]\n",
      "Iteration 427, Cost: 0.2489764685970774\n",
      "gradient_weights:  [array([[ 0.0009529 , -0.0020164 ,  0.00043425],\n",
      "       [ 0.00871705,  0.00066349,  0.00015782]]), array([[-7.45169927e-05],\n",
      "       [ 3.87462329e-03],\n",
      "       [-8.19608697e-04]])]\n",
      "gradients_biases:  [array([ 1.83908591e-03, -1.97284835e-03, -7.36664293e-05]), array([-0.00102553])]\n",
      "Iteration 428, Cost: 0.24897119204120355\n",
      "gradient_weights:  [array([[ 0.00096237, -0.0020188 ,  0.00043351],\n",
      "       [ 0.00872264,  0.00066491,  0.0001586 ]]), array([[-6.22120354e-05],\n",
      "       [ 3.87243026e-03],\n",
      "       [-8.19862473e-04]])]\n",
      "gradients_biases:  [array([ 1.84398457e-03, -1.97554319e-03, -7.38230200e-05]), array([-0.00102716])]\n",
      "Iteration 429, Cost: 0.2489659084857688\n",
      "gradient_weights:  [array([[ 0.00097184, -0.00202121,  0.00043277],\n",
      "       [ 0.00872825,  0.00066634,  0.00015938]]), array([[-4.98988797e-05],\n",
      "       [ 3.87024483e-03],\n",
      "       [-8.20118958e-04]])]\n",
      "gradients_biases:  [array([ 1.84887716e-03, -1.97823669e-03, -7.39800574e-05]), array([-0.0010288])]\n",
      "Iteration 430, Cost: 0.2489606178823856\n",
      "gradient_weights:  [array([[ 0.00098132, -0.00202361,  0.00043202],\n",
      "       [ 0.00873388,  0.00066776,  0.00016017]]), array([[-3.75774649e-05],\n",
      "       [ 3.86806700e-03],\n",
      "       [-8.20378153e-04]])]\n",
      "gradients_biases:  [array([ 1.85376366e-03, -1.98092887e-03, -7.41375420e-05]), array([-0.00103044])]\n",
      "Iteration 431, Cost: 0.24895532018262323\n",
      "gradient_weights:  [array([[ 0.00099079, -0.00202601,  0.00043128],\n",
      "       [ 0.00873952,  0.00066919,  0.00016095]]), array([[-2.52477305e-05],\n",
      "       [ 3.86589677e-03],\n",
      "       [-8.20640060e-04]])]\n",
      "gradients_biases:  [array([ 1.85864407e-03, -1.98361973e-03, -7.42954744e-05]), array([-0.00103208])]\n",
      "Iteration 432, Cost: 0.2489500153380069\n",
      "gradient_weights:  [array([[ 0.00100027, -0.00202841,  0.00043054],\n",
      "       [ 0.00874518,  0.00067063,  0.00016173]]), array([[-1.29096161e-05],\n",
      "       [ 3.86373412e-03],\n",
      "       [-8.20904679e-04]])]\n",
      "gradients_biases:  [array([ 1.86351838e-03, -1.98630927e-03, -7.44538549e-05]), array([-0.00103373])]\n",
      "Iteration 433, Cost: 0.2489447033000174\n",
      "gradient_weights:  [array([[ 0.00100975, -0.00203082,  0.00042981],\n",
      "       [ 0.00875086,  0.00067206,  0.00016251]]), array([[-5.63061538e-07],\n",
      "       [ 3.86157905e-03],\n",
      "       [-8.21172010e-04]])]\n",
      "gradients_biases:  [array([ 1.86838658e-03, -1.98899749e-03, -7.46126842e-05]), array([-0.00103538])]\n",
      "Iteration 434, Cost: 0.24893938402009003\n",
      "gradient_weights:  [array([[ 0.00101923, -0.00203322,  0.00042907],\n",
      "       [ 0.00875655,  0.0006735 ,  0.00016329]]), array([[ 1.17919934e-05],\n",
      "       [ 3.85943155e-03],\n",
      "       [-8.21442055e-04]])]\n",
      "gradients_biases:  [array([ 1.87324865e-03, -1.99168440e-03, -7.47719625e-05]), array([-0.00103703])]\n",
      "Iteration 435, Cost: 0.24893405744961478\n",
      "gradient_weights:  [array([[ 0.00102872, -0.00203562,  0.00042833],\n",
      "       [ 0.00876226,  0.00067494,  0.00016408]]), array([[ 2.41556088e-05],\n",
      "       [ 3.85729160e-03],\n",
      "       [-8.21714814e-04]])]\n",
      "gradients_biases:  [array([ 1.87810459e-03, -1.99437001e-03, -7.49316905e-05]), array([-0.00103869])]\n",
      "Iteration 436, Cost: 0.24892872353993506\n",
      "gradient_weights:  [array([[ 0.0010382 , -0.00203802,  0.0004276 ],\n",
      "       [ 0.00876798,  0.00067638,  0.00016486]]), array([[ 3.65278445e-05],\n",
      "       [ 3.85515920e-03],\n",
      "       [-8.21990288e-04]])]\n",
      "gradients_biases:  [array([ 1.88295439e-03, -1.99705431e-03, -7.50918687e-05]), array([-0.00104035])]\n",
      "Iteration 437, Cost: 0.24892338224234786\n",
      "gradient_weights:  [array([[ 0.00104769, -0.00204043,  0.00042687],\n",
      "       [ 0.00877372,  0.00067783,  0.00016564]]), array([[ 4.89087602e-05],\n",
      "       [ 3.85303434e-03],\n",
      "       [-8.22268478e-04]])]\n",
      "gradients_biases:  [array([ 1.88779804e-03, -1.99973731e-03, -7.52524974e-05]), array([-0.00104201])]\n",
      "Iteration 438, Cost: 0.24891803350810232\n",
      "gradient_weights:  [array([[ 0.00105718, -0.00204283,  0.00042613],\n",
      "       [ 0.00877947,  0.00067927,  0.00016643]]), array([[ 6.12984156e-05],\n",
      "       [ 3.85091702e-03],\n",
      "       [-8.22549385e-04]])]\n",
      "gradients_biases:  [array([ 1.89263552e-03, -2.00241901e-03, -7.54135772e-05]), array([-0.00104368])]\n",
      "Iteration 439, Cost: 0.24891267728839983\n",
      "gradient_weights:  [array([[ 0.00106668, -0.00204523,  0.0004254 ],\n",
      "       [ 0.00878525,  0.00068072,  0.00016721]]), array([[ 7.36968703e-05],\n",
      "       [ 3.84880722e-03],\n",
      "       [-8.22833009e-04]])]\n",
      "gradients_biases:  [array([ 1.89746683e-03, -2.00509942e-03, -7.55751085e-05]), array([-0.00104535])]\n",
      "Iteration 440, Cost: 0.24890731353439333\n",
      "gradient_weights:  [array([[ 0.00107617, -0.00204764,  0.00042467],\n",
      "       [ 0.00879103,  0.00068218,  0.000168  ]]), array([[ 8.61041836e-05],\n",
      "       [ 3.84670493e-03],\n",
      "       [-8.23119351e-04]])]\n",
      "gradients_biases:  [array([ 1.90229196e-03, -2.00777854e-03, -7.57370920e-05]), array([-0.00104702])]\n",
      "Iteration 441, Cost: 0.24890194219718656\n",
      "gradient_weights:  [array([[ 0.00108567, -0.00205004,  0.00042394],\n",
      "       [ 0.00879683,  0.00068363,  0.00016879]]), array([[ 9.85204149e-05],\n",
      "       [ 3.84461015e-03],\n",
      "       [-8.23408412e-04]])]\n",
      "gradients_biases:  [array([ 1.90711089e-03, -2.01045638e-03, -7.58995280e-05]), array([-0.0010487])]\n",
      "Iteration 442, Cost: 0.2488965632278336\n",
      "gradient_weights:  [array([[ 0.00109517, -0.00205244,  0.00042322],\n",
      "       [ 0.00880265,  0.00068509,  0.00016957]]), array([[ 0.00011095],\n",
      "       [ 0.00384252],\n",
      "       [-0.0008237 ]])]\n",
      "gradients_biases:  [array([ 1.91192362e-03, -2.01313293e-03, -7.60624170e-05]), array([-0.00105038])]\n",
      "Iteration 443, Cost: 0.24889117657733845\n",
      "gradient_weights:  [array([[ 0.00110467, -0.00205485,  0.00042249],\n",
      "       [ 0.00880849,  0.00068655,  0.00017036]]), array([[ 0.00012338],\n",
      "       [ 0.00384044],\n",
      "       [-0.00082399]])]\n",
      "gradients_biases:  [array([ 1.91673012e-03, -2.01580821e-03, -7.62257595e-05]), array([-0.00105207])]\n",
      "Iteration 444, Cost: 0.2488857821966543\n",
      "gradient_weights:  [array([[ 0.00111418, -0.00205725,  0.00042177],\n",
      "       [ 0.00881434,  0.00068801,  0.00017115]]), array([[ 0.00013582],\n",
      "       [ 0.00383837],\n",
      "       [-0.00082429]])]\n",
      "gradients_biases:  [array([ 1.92153040e-03, -2.01848221e-03, -7.63895561e-05]), array([-0.00105375])]\n",
      "Iteration 445, Cost: 0.24888038003668303\n",
      "gradient_weights:  [array([[ 0.00112368, -0.00205966,  0.00042104],\n",
      "       [ 0.0088202 ,  0.00068948,  0.00017194]]), array([[ 0.00014828],\n",
      "       [ 0.00383631],\n",
      "       [-0.00082459]])]\n",
      "gradients_biases:  [array([ 1.92632444e-03, -2.02115494e-03, -7.65538072e-05]), array([-0.00105544])]\n",
      "Iteration 446, Cost: 0.2488749700482747\n",
      "gradient_weights:  [array([[ 0.00113319, -0.00206206,  0.00042032],\n",
      "       [ 0.00882608,  0.00069095,  0.00017273]]), array([[ 0.00016074],\n",
      "       [ 0.00383425],\n",
      "       [-0.00082489]])]\n",
      "gradients_biases:  [array([ 1.93111223e-03, -2.02382640e-03, -7.67185133e-05]), array([-0.00105713])]\n",
      "Iteration 447, Cost: 0.24886955218222706\n",
      "gradient_weights:  [array([[ 0.00114271, -0.00206447,  0.0004196 ],\n",
      "       [ 0.00883198,  0.00069242,  0.00017352]]), array([[ 0.00017321],\n",
      "       [ 0.0038322 ],\n",
      "       [-0.0008252 ]])]\n",
      "gradients_biases:  [array([ 1.93589375e-03, -2.02649660e-03, -7.68836749e-05]), array([-0.00105883])]\n",
      "Iteration 448, Cost: 0.24886412638928468\n",
      "gradient_weights:  [array([[ 0.00115222, -0.00206687,  0.00041888],\n",
      "       [ 0.00883789,  0.00069389,  0.00017431]]), array([[ 0.00018569],\n",
      "       [ 0.00383016],\n",
      "       [-0.00082551]])]\n",
      "gradients_biases:  [array([ 1.94066900e-03, -2.02916553e-03, -7.70492926e-05]), array([-0.00106053])]\n",
      "Iteration 449, Cost: 0.24885869262013877\n",
      "gradient_weights:  [array([[ 0.00116174, -0.00206928,  0.00041816],\n",
      "       [ 0.00884382,  0.00069536,  0.0001751 ]]), array([[ 0.00019818],\n",
      "       [ 0.00382812],\n",
      "       [-0.00082582]])]\n",
      "gradients_biases:  [array([ 1.94543796e-03, -2.03183321e-03, -7.72153668e-05]), array([-0.00106223])]\n",
      "Iteration 450, Cost: 0.24885325082542661\n",
      "gradient_weights:  [array([[ 0.00117126, -0.00207168,  0.00041745],\n",
      "       [ 0.00884977,  0.00069684,  0.00017589]]), array([[ 0.00021068],\n",
      "       [ 0.00382609],\n",
      "       [-0.00082613]])]\n",
      "gradients_biases:  [array([ 1.95020062e-03, -2.03449964e-03, -7.73818980e-05]), array([-0.00106394])]\n",
      "Iteration 451, Cost: 0.24884780095573092\n",
      "gradient_weights:  [array([[ 0.00118078, -0.00207409,  0.00041673],\n",
      "       [ 0.00885573,  0.00069832,  0.00017668]]), array([[ 0.00022319],\n",
      "       [ 0.00382407],\n",
      "       [-0.00082645]])]\n",
      "gradients_biases:  [array([ 1.95495697e-03, -2.03716482e-03, -7.75488867e-05]), array([-0.00106565])]\n",
      "Iteration 452, Cost: 0.24884234296157892\n",
      "gradient_weights:  [array([[ 0.00119031, -0.00207649,  0.00041602],\n",
      "       [ 0.0088617 ,  0.0006998 ,  0.00017747]]), array([[ 0.0002357 ],\n",
      "       [ 0.00382206],\n",
      "       [-0.00082677]])]\n",
      "gradients_biases:  [array([ 1.95970699e-03, -2.03982875e-03, -7.77163335e-05]), array([-0.00106736])]\n",
      "Iteration 453, Cost: 0.24883687679344269\n",
      "gradient_weights:  [array([[ 0.00119984, -0.0020789 ,  0.0004153 ],\n",
      "       [ 0.00886769,  0.00070129,  0.00017827]]), array([[ 0.00024823],\n",
      "       [ 0.00382006],\n",
      "       [-0.00082709]])]\n",
      "gradients_biases:  [array([ 1.96445067e-03, -2.04249144e-03, -7.78842388e-05]), array([-0.00106908])]\n",
      "Iteration 454, Cost: 0.24883140240173784\n",
      "gradient_weights:  [array([[ 0.00120937, -0.00208131,  0.00041459],\n",
      "       [ 0.0088737 ,  0.00070278,  0.00017906]]), array([[ 0.00026077],\n",
      "       [ 0.00381806],\n",
      "       [-0.00082741]])]\n",
      "gradients_biases:  [array([ 1.96918800e-03, -2.04515289e-03, -7.80526031e-05]), array([-0.0010708])]\n",
      "Iteration 455, Cost: 0.2488259197368234\n",
      "gradient_weights:  [array([[ 0.0012189 , -0.00208371,  0.00041388],\n",
      "       [ 0.00887972,  0.00070427,  0.00017985]]), array([[ 0.00027332],\n",
      "       [ 0.00381607],\n",
      "       [-0.00082774]])]\n",
      "gradients_biases:  [array([ 1.97391897e-03, -2.04781310e-03, -7.82214270e-05]), array([-0.00107252])]\n",
      "Iteration 456, Cost: 0.24882042874900095\n",
      "gradient_weights:  [array([[ 0.00122844, -0.00208612,  0.00041317],\n",
      "       [ 0.00888576,  0.00070576,  0.00018065]]), array([[ 0.00028587],\n",
      "       [ 0.00381408],\n",
      "       [-0.00082807]])]\n",
      "gradients_biases:  [array([ 1.97864356e-03, -2.05047208e-03, -7.83907110e-05]), array([-0.00107425])]\n",
      "Iteration 457, Cost: 0.24881492938851457\n",
      "gradient_weights:  [array([[ 0.00123798, -0.00208853,  0.00041246],\n",
      "       [ 0.00889181,  0.00070726,  0.00018145]]), array([[ 0.00029844],\n",
      "       [ 0.00381211],\n",
      "       [-0.0008284 ]])]\n",
      "gradients_biases:  [array([ 1.98336176e-03, -2.05312984e-03, -7.85604556e-05]), array([-0.00107598])]\n",
      "Iteration 458, Cost: 0.24880942160554975\n",
      "gradient_weights:  [array([[ 0.00124752, -0.00209094,  0.00041175],\n",
      "       [ 0.00889788,  0.00070875,  0.00018224]]), array([[ 0.00031102],\n",
      "       [ 0.00381014],\n",
      "       [-0.00082874]])]\n",
      "gradients_biases:  [array([ 1.98807355e-03, -2.05578636e-03, -7.87306613e-05]), array([-0.00107771])]\n",
      "Iteration 459, Cost: 0.2488039053502334\n",
      "gradient_weights:  [array([[ 0.00125707, -0.00209334,  0.00041105],\n",
      "       [ 0.00890397,  0.00071025,  0.00018304]]), array([[ 0.0003236 ],\n",
      "       [ 0.00380818],\n",
      "       [-0.00082908]])]\n",
      "gradients_biases:  [array([ 1.99277893e-03, -2.05844167e-03, -7.89013286e-05]), array([-0.00107945])]\n",
      "Iteration 460, Cost: 0.24879838057263282\n",
      "gradient_weights:  [array([[ 0.00126662, -0.00209575,  0.00041034],\n",
      "       [ 0.00891007,  0.00071176,  0.00018384]]), array([[ 0.0003362 ],\n",
      "       [ 0.00380622],\n",
      "       [-0.00082942]])]\n",
      "gradients_biases:  [array([ 1.99747788e-03, -2.06109576e-03, -7.90724581e-05]), array([-0.00108119])]\n",
      "Iteration 461, Cost: 0.24879284722275563\n",
      "gradient_weights:  [array([[ 0.00127617, -0.00209816,  0.00040964],\n",
      "       [ 0.00891619,  0.00071326,  0.00018463]]), array([[ 0.00034881],\n",
      "       [ 0.00380428],\n",
      "       [-0.00082976]])]\n",
      "gradients_biases:  [array([ 2.00217037e-03, -2.06374864e-03, -7.92440502e-05]), array([-0.00108293])]\n",
      "Iteration 462, Cost: 0.248787305250549\n",
      "gradient_weights:  [array([[ 0.00128573, -0.00210057,  0.00040894],\n",
      "       [ 0.00892232,  0.00071477,  0.00018543]]), array([[ 0.00036142],\n",
      "       [ 0.00380234],\n",
      "       [-0.00083011]])]\n",
      "gradients_biases:  [array([ 2.00685641e-03, -2.06640030e-03, -7.94161055e-05]), array([-0.00108468])]\n",
      "Iteration 463, Cost: 0.24878175460589919\n",
      "gradient_weights:  [array([[ 0.00129529, -0.00210298,  0.00040824],\n",
      "       [ 0.00892847,  0.00071628,  0.00018623]]), array([[ 0.00037405],\n",
      "       [ 0.00380041],\n",
      "       [-0.00083046]])]\n",
      "gradients_biases:  [array([ 2.01153597e-03, -2.06905076e-03, -7.95886244e-05]), array([-0.00108642])]\n",
      "Iteration 464, Cost: 0.2487761952386309\n",
      "gradient_weights:  [array([[ 0.00130485, -0.00210539,  0.00040754],\n",
      "       [ 0.00893463,  0.00071779,  0.00018703]]), array([[ 0.00038669],\n",
      "       [ 0.00379848],\n",
      "       [-0.00083081]])]\n",
      "gradients_biases:  [array([ 2.01620905e-03, -2.07170001e-03, -7.97616076e-05]), array([-0.00108818])]\n",
      "Iteration 465, Cost: 0.248770627098507\n",
      "gradient_weights:  [array([[ 0.00131441, -0.0021078 ,  0.00040684],\n",
      "       [ 0.00894081,  0.00071931,  0.00018783]]), array([[ 0.00039934],\n",
      "       [ 0.00379657],\n",
      "       [-0.00083116]])]\n",
      "gradients_biases:  [array([ 2.02087562e-03, -2.07434807e-03, -7.99350556e-05]), array([-0.00108993])]\n",
      "Iteration 466, Cost: 0.24876505013522782\n",
      "gradient_weights:  [array([[ 0.00132398, -0.00211021,  0.00040614],\n",
      "       [ 0.00894701,  0.00072083,  0.00018863]]), array([[ 0.000412  ],\n",
      "       [ 0.00379466],\n",
      "       [-0.00083152]])]\n",
      "gradients_biases:  [array([ 2.02553567e-03, -2.07699492e-03, -8.01089688e-05]), array([-0.00109169])]\n",
      "Iteration 467, Cost: 0.2487594642984308\n",
      "gradient_weights:  [array([[ 0.00133355, -0.00211263,  0.00040544],\n",
      "       [ 0.00895322,  0.00072235,  0.00018944]]), array([[ 0.00042466],\n",
      "       [ 0.00379275],\n",
      "       [-0.00083188]])]\n",
      "gradients_biases:  [array([ 2.03018918e-03, -2.07964059e-03, -8.02833479e-05]), array([-0.00109345])]\n",
      "Iteration 468, Cost: 0.24875386953768985\n",
      "gradient_weights:  [array([[ 0.00134313, -0.00211504,  0.00040475],\n",
      "       [ 0.00895944,  0.00072387,  0.00019024]]), array([[ 0.00043734],\n",
      "       [ 0.00379086],\n",
      "       [-0.00083224]])]\n",
      "gradients_biases:  [array([ 2.03483615e-03, -2.08228506e-03, -8.04581933e-05]), array([-0.00109522])]\n",
      "Iteration 469, Cost: 0.24874826580251488\n",
      "gradient_weights:  [array([[ 0.00135271, -0.00211745,  0.00040406],\n",
      "       [ 0.00896568,  0.0007254 ,  0.00019104]]), array([[ 0.00045003],\n",
      "       [ 0.00378897],\n",
      "       [-0.00083261]])]\n",
      "gradients_biases:  [array([ 2.03947655e-03, -2.08492835e-03, -8.06335055e-05]), array([-0.00109699])]\n",
      "Iteration 470, Cost: 0.24874265304235116\n",
      "gradient_weights:  [array([[ 0.00136229, -0.00211986,  0.00040336],\n",
      "       [ 0.00897194,  0.00072693,  0.00019185]]), array([[ 0.00046273],\n",
      "       [ 0.00378709],\n",
      "       [-0.00083298]])]\n",
      "gradients_biases:  [array([ 2.04411037e-03, -2.08757045e-03, -8.08092852e-05]), array([-0.00109876])]\n",
      "Iteration 471, Cost: 0.2487370312065793\n",
      "gradient_weights:  [array([[ 0.00137188, -0.00212228,  0.00040267],\n",
      "       [ 0.00897821,  0.00072846,  0.00019265]]), array([[ 0.00047544],\n",
      "       [ 0.00378522],\n",
      "       [-0.00083335]])]\n",
      "gradients_biases:  [array([ 2.04873759e-03, -2.09021138e-03, -8.09855328e-05]), array([-0.00110054])]\n",
      "Iteration 472, Cost: 0.2487314002445139\n",
      "gradient_weights:  [array([[ 0.00138147, -0.00212469,  0.00040198],\n",
      "       [ 0.0089845 ,  0.00072999,  0.00019346]]), array([[ 0.00048816],\n",
      "       [ 0.00378335],\n",
      "       [-0.00083372]])]\n",
      "gradients_biases:  [array([ 2.05335820e-03, -2.09285112e-03, -8.11622488e-05]), array([-0.00110232])]\n",
      "Iteration 473, Cost: 0.24872576010540415\n",
      "gradient_weights:  [array([[ 0.00139106, -0.00212711,  0.00040129],\n",
      "       [ 0.00899081,  0.00073153,  0.00019426]]), array([[ 0.0005009 ],\n",
      "       [ 0.00378149],\n",
      "       [-0.0008341 ]])]\n",
      "gradients_biases:  [array([ 2.05797218e-03, -2.09548970e-03, -8.13394339e-05]), array([-0.0011041])]\n",
      "Iteration 474, Cost: 0.2487201107384322\n",
      "gradient_weights:  [array([[ 0.00140066, -0.00212952,  0.00040061],\n",
      "       [ 0.00899712,  0.00073306,  0.00019507]]), array([[ 0.00051364],\n",
      "       [ 0.00377964],\n",
      "       [-0.00083448]])]\n",
      "gradients_biases:  [array([ 2.06257952e-03, -2.09812711e-03, -8.15170885e-05]), array([-0.00110588])]\n",
      "Iteration 475, Cost: 0.24871445209271353\n",
      "gradient_weights:  [array([[ 0.00141026, -0.00213194,  0.00039992],\n",
      "       [ 0.00900346,  0.0007346 ,  0.00019588]]), array([[ 0.00052639],\n",
      "       [ 0.0037778 ],\n",
      "       [-0.00083486]])]\n",
      "gradients_biases:  [array([ 2.06718020e-03, -2.10076335e-03, -8.16952133e-05]), array([-0.00110767])]\n",
      "Iteration 476, Cost: 0.24870878411729586\n",
      "gradient_weights:  [array([[ 0.00141986, -0.00213435,  0.00039923],\n",
      "       [ 0.00900981,  0.00073615,  0.00019668]]), array([[ 0.00053916],\n",
      "       [ 0.00377596],\n",
      "       [-0.00083525]])]\n",
      "gradients_biases:  [array([ 2.07177420e-03, -2.10339843e-03, -8.18738086e-05]), array([-0.00110946])]\n",
      "Iteration 477, Cost: 0.24870310676115912\n",
      "gradient_weights:  [array([[ 0.00142947, -0.00213677,  0.00039855],\n",
      "       [ 0.00901618,  0.00073769,  0.00019749]]), array([[ 0.00055193],\n",
      "       [ 0.00377413],\n",
      "       [-0.00083563]])]\n",
      "gradients_biases:  [array([ 2.07636150e-03, -2.10603235e-03, -8.20528751e-05]), array([-0.00111126])]\n",
      "Iteration 478, Cost: 0.2486974199732147\n",
      "gradient_weights:  [array([[ 0.00143908, -0.00213919,  0.00039787],\n",
      "       [ 0.00902256,  0.00073924,  0.0001983 ]]), array([[ 0.00056472],\n",
      "       [ 0.00377231],\n",
      "       [-0.00083602]])]\n",
      "gradients_biases:  [array([ 2.08094209e-03, -2.10866512e-03, -8.22324134e-05]), array([-0.00111306])]\n",
      "Iteration 479, Cost: 0.24869172370230505\n",
      "gradient_weights:  [array([[ 0.0014487 , -0.00214161,  0.00039719],\n",
      "       [ 0.00902895,  0.00074079,  0.00019911]]), array([[ 0.00057752],\n",
      "       [ 0.00377049],\n",
      "       [-0.00083642]])]\n",
      "gradients_biases:  [array([ 2.08551596e-03, -2.11129673e-03, -8.24124239e-05]), array([-0.00111486])]\n",
      "Iteration 480, Cost: 0.248686017897203\n",
      "gradient_weights:  [array([[ 0.00145832, -0.00214402,  0.00039651],\n",
      "       [ 0.00903537,  0.00074235,  0.00019992]]), array([[ 0.00059033],\n",
      "       [ 0.00376868],\n",
      "       [-0.00083681]])]\n",
      "gradients_biases:  [array([ 2.09008307e-03, -2.11392720e-03, -8.25929072e-05]), array([-0.00111667])]\n",
      "Iteration 481, Cost: 0.2486803025066116\n",
      "gradient_weights:  [array([[ 0.00146794, -0.00214644,  0.00039583],\n",
      "       [ 0.00904179,  0.0007439 ,  0.00020073]]), array([[ 0.00060315],\n",
      "       [ 0.00376688],\n",
      "       [-0.00083721]])]\n",
      "gradients_biases:  [array([ 2.09464343e-03, -2.11655652e-03, -8.27738639e-05]), array([-0.00111847])]\n",
      "Iteration 482, Cost: 0.24867457747916347\n",
      "gradient_weights:  [array([[ 0.00147757, -0.00214886,  0.00039515],\n",
      "       [ 0.00904824,  0.00074546,  0.00020155]]), array([[ 0.00061598],\n",
      "       [ 0.00376509],\n",
      "       [-0.00083761]])]\n",
      "gradients_biases:  [array([ 2.09919700e-03, -2.11918470e-03, -8.29552945e-05]), array([-0.00112029])]\n",
      "Iteration 483, Cost: 0.24866884276342005\n",
      "gradient_weights:  [array([[ 0.0014872 , -0.00215128,  0.00039447],\n",
      "       [ 0.00905469,  0.00074702,  0.00020236]]), array([[ 0.00062882],\n",
      "       [ 0.0037633 ],\n",
      "       [-0.00083802]])]\n",
      "gradients_biases:  [array([ 2.10374377e-03, -2.12181174e-03, -8.31371996e-05]), array([-0.0011221])]\n",
      "Iteration 484, Cost: 0.24866309830787187\n",
      "gradient_weights:  [array([[ 0.00149684, -0.0021537 ,  0.0003938 ],\n",
      "       [ 0.00906117,  0.00074859,  0.00020317]]), array([[ 0.00064168],\n",
      "       [ 0.00376152],\n",
      "       [-0.00083842]])]\n",
      "gradients_biases:  [array([ 2.10828373e-03, -2.12443764e-03, -8.33195797e-05]), array([-0.00112392])]\n",
      "Iteration 485, Cost: 0.24865734406093715\n",
      "gradient_weights:  [array([[ 0.00150648, -0.00215613,  0.00039313],\n",
      "       [ 0.00906766,  0.00075015,  0.00020399]]), array([[ 0.00065454],\n",
      "       [ 0.00375975],\n",
      "       [-0.00083883]])]\n",
      "gradients_biases:  [array([ 2.11281685e-03, -2.12706242e-03, -8.35024354e-05]), array([-0.00112574])]\n",
      "Iteration 486, Cost: 0.24865157997096193\n",
      "gradient_weights:  [array([[ 0.00151612, -0.00215855,  0.00039245],\n",
      "       [ 0.00907416,  0.00075172,  0.0002048 ]]), array([[ 0.00066742],\n",
      "       [ 0.00375799],\n",
      "       [-0.00083924]])]\n",
      "gradients_biases:  [array([ 2.11734312e-03, -2.12968606e-03, -8.36857672e-05]), array([-0.00112757])]\n",
      "Iteration 487, Cost: 0.24864580598621946\n",
      "gradient_weights:  [array([[ 0.00152577, -0.00216097,  0.00039178],\n",
      "       [ 0.00908068,  0.00075329,  0.00020562]]), array([[ 0.00068031],\n",
      "       [ 0.00375623],\n",
      "       [-0.00083966]])]\n",
      "gradients_biases:  [array([ 2.12186252e-03, -2.13230859e-03, -8.38695757e-05]), array([-0.00112939])]\n",
      "Iteration 488, Cost: 0.2486400220549097\n",
      "gradient_weights:  [array([[ 0.00153542, -0.00216339,  0.00039111],\n",
      "       [ 0.00908722,  0.00075487,  0.00020644]]), array([[ 0.00069321],\n",
      "       [ 0.00375448],\n",
      "       [-0.00084008]])]\n",
      "gradients_biases:  [array([ 2.12637502e-03, -2.13492999e-03, -8.40538615e-05]), array([-0.00113123])]\n",
      "Iteration 489, Cost: 0.24863422812515876\n",
      "gradient_weights:  [array([[ 0.00154508, -0.00216582,  0.00039044],\n",
      "       [ 0.00909377,  0.00075644,  0.00020725]]), array([[ 0.00070612],\n",
      "       [ 0.00375274],\n",
      "       [-0.0008405 ]])]\n",
      "gradients_biases:  [array([ 2.13088062e-03, -2.13755027e-03, -8.42386251e-05]), array([-0.00113306])]\n",
      "Iteration 490, Cost: 0.2486284241450186\n",
      "gradient_weights:  [array([[ 0.00155474, -0.00216824,  0.00038978],\n",
      "       [ 0.00910033,  0.00075802,  0.00020807]]), array([[ 0.00071904],\n",
      "       [ 0.003751  ],\n",
      "       [-0.00084092]])]\n",
      "gradients_biases:  [array([ 2.13537928e-03, -2.14016944e-03, -8.44238671e-05]), array([-0.0011349])]\n",
      "Iteration 491, Cost: 0.24862261006246641\n",
      "gradient_weights:  [array([[ 0.0015644 , -0.00217067,  0.00038911],\n",
      "       [ 0.00910692,  0.00075961,  0.00020889]]), array([[ 0.00073198],\n",
      "       [ 0.00374927],\n",
      "       [-0.00084135]])]\n",
      "gradients_biases:  [array([ 2.13987100e-03, -2.14278749e-03, -8.46095880e-05]), array([-0.00113674])]\n",
      "Iteration 492, Cost: 0.24861678582540436\n",
      "gradient_weights:  [array([[ 0.00157407, -0.0021731 ,  0.00038844],\n",
      "       [ 0.00911351,  0.00076119,  0.00020971]]), array([[ 0.00074492],\n",
      "       [ 0.00374755],\n",
      "       [-0.00084178]])]\n",
      "gradients_biases:  [array([ 2.14435575e-03, -2.14540444e-03, -8.47957885e-05]), array([-0.00113858])]\n",
      "Iteration 493, Cost: 0.24861095138165876\n",
      "gradient_weights:  [array([[ 0.00158375, -0.00217552,  0.00038778],\n",
      "       [ 0.00912012,  0.00076278,  0.00021053]]), array([[ 0.00075788],\n",
      "       [ 0.00374583],\n",
      "       [-0.00084221]])]\n",
      "gradients_biases:  [array([ 2.14883352e-03, -2.14802028e-03, -8.49824690e-05]), array([-0.00114043])]\n",
      "Iteration 494, Cost: 0.24860510667898006\n",
      "gradient_weights:  [array([[ 0.00159343, -0.00217795,  0.00038712],\n",
      "       [ 0.00912675,  0.00076437,  0.00021135]]), array([[ 0.00077085],\n",
      "       [ 0.00374412],\n",
      "       [-0.00084264]])]\n",
      "gradients_biases:  [array([ 2.15330427e-03, -2.15063503e-03, -8.51696302e-05]), array([-0.00114228])]\n",
      "Iteration 495, Cost: 0.24859925166504204\n",
      "gradient_weights:  [array([[ 0.00160311, -0.00218038,  0.00038646],\n",
      "       [ 0.00913339,  0.00076596,  0.00021217]]), array([[ 0.00078383],\n",
      "       [ 0.00374242],\n",
      "       [-0.00084308]])]\n",
      "gradients_biases:  [array([ 2.15776801e-03, -2.15324867e-03, -8.53572727e-05]), array([-0.00114414])]\n",
      "Iteration 496, Cost: 0.2485933862874416\n",
      "gradient_weights:  [array([[ 0.0016128 , -0.00218281,  0.0003858 ],\n",
      "       [ 0.00914005,  0.00076755,  0.000213  ]]), array([[ 0.00079683],\n",
      "       [ 0.00374073],\n",
      "       [-0.00084352]])]\n",
      "gradients_biases:  [array([ 2.16222469e-03, -2.15586122e-03, -8.55453969e-05]), array([-0.001146])]\n",
      "Iteration 497, Cost: 0.24858751049369796\n",
      "gradient_weights:  [array([[ 0.00162249, -0.00218524,  0.00038514],\n",
      "       [ 0.00914673,  0.00076915,  0.00021382]]), array([[ 0.00080983],\n",
      "       [ 0.00373904],\n",
      "       [-0.00084396]])]\n",
      "gradients_biases:  [array([ 2.16667430e-03, -2.15847268e-03, -8.57340035e-05]), array([-0.00114786])]\n",
      "Iteration 498, Cost: 0.24858162423125263\n",
      "gradient_weights:  [array([[ 0.00163219, -0.00218767,  0.00038448],\n",
      "       [ 0.00915341,  0.00077075,  0.00021464]]), array([[ 0.00082285],\n",
      "       [ 0.00373736],\n",
      "       [-0.00084441]])]\n",
      "gradients_biases:  [array([ 2.17111683e-03, -2.16108305e-03, -8.59230931e-05]), array([-0.00114972])]\n",
      "Iteration 499, Cost: 0.24857572744746878\n",
      "gradient_weights:  [array([[ 0.00164189, -0.0021901 ,  0.00038382],\n",
      "       [ 0.00916012,  0.00077235,  0.00021547]]), array([[ 0.00083588],\n",
      "       [ 0.00373569],\n",
      "       [-0.00084486]])]\n",
      "gradients_biases:  [array([ 2.17555225e-03, -2.16369234e-03, -8.61126662e-05]), array([-0.00115159])]\n",
      "Iteration 500, Cost: 0.24856982008963063\n",
      "gradient_weights:  [array([[ 0.00165159, -0.00219254,  0.00038317],\n",
      "       [ 0.00916684,  0.00077395,  0.00021629]]), array([[ 0.00084893],\n",
      "       [ 0.00373403],\n",
      "       [-0.00084531]])]\n",
      "gradients_biases:  [array([ 2.17998054e-03, -2.16630054e-03, -8.63027234e-05]), array([-0.00115346])]\n",
      "Iteration 501, Cost: 0.24856390210494328\n",
      "gradient_weights:  [array([[ 0.0016613 , -0.00219497,  0.00038251],\n",
      "       [ 0.00917357,  0.00077556,  0.00021712]]), array([[ 0.00086198],\n",
      "       [ 0.00373237],\n",
      "       [-0.00084576]])]\n",
      "gradients_biases:  [array([ 2.18440168e-03, -2.16890767e-03, -8.64932653e-05]), array([-0.00115533])]\n",
      "Iteration 502, Cost: 0.2485579734405319\n",
      "gradient_weights:  [array([[ 0.00167102, -0.0021974 ,  0.00038186],\n",
      "       [ 0.00918032,  0.00077717,  0.00021795]]), array([[ 0.00087505],\n",
      "       [ 0.00373072],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       [-0.00084622]])]\n",
      "gradients_biases:  [array([ 2.18881565e-03, -2.17151372e-03, -8.66842924e-05]), array([-0.00115721])]\n",
      "Iteration 503, Cost: 0.24855203404344195\n",
      "gradient_weights:  [array([[ 0.00168074, -0.00219984,  0.00038121],\n",
      "       [ 0.00918709,  0.00077878,  0.00021878]]), array([[ 0.00088813],\n",
      "       [ 0.00372907],\n",
      "       [-0.00084668]])]\n",
      "gradients_biases:  [array([ 2.19322243e-03, -2.17411870e-03, -8.68758055e-05]), array([-0.00115909])]\n",
      "Iteration 504, Cost: 0.24854608386063787\n",
      "gradient_weights:  [array([[ 0.00169046, -0.00220228,  0.00038056],\n",
      "       [ 0.00919387,  0.0007804 ,  0.00021961]]), array([[ 0.00090122],\n",
      "       [ 0.00372744],\n",
      "       [-0.00084714]])]\n",
      "gradients_biases:  [array([ 2.19762199e-03, -2.17672261e-03, -8.70678050e-05]), array([-0.00116098])]\n",
      "Iteration 505, Cost: 0.24854012283900329\n",
      "gradient_weights:  [array([[ 0.00170019, -0.00220471,  0.00037991],\n",
      "       [ 0.00920066,  0.00078202,  0.00022044]]), array([[ 0.00091433],\n",
      "       [ 0.00372581],\n",
      "       [-0.00084761]])]\n",
      "gradients_biases:  [array([ 2.20201431e-03, -2.17932546e-03, -8.72602915e-05]), array([-0.00116286])]\n",
      "Iteration 506, Cost: 0.24853415092534045\n",
      "gradient_weights:  [array([[ 0.00170993, -0.00220715,  0.00037926],\n",
      "       [ 0.00920747,  0.00078364,  0.00022127]]), array([[ 0.00092744],\n",
      "       [ 0.00372418],\n",
      "       [-0.00084807]])]\n",
      "gradients_biases:  [array([ 2.20639938e-03, -2.18192725e-03, -8.74532657e-05]), array([-0.00116476])]\n",
      "Iteration 507, Cost: 0.24852816806636957\n",
      "gradient_weights:  [array([[ 0.00171967, -0.00220959,  0.00037862],\n",
      "       [ 0.0092143 ,  0.00078526,  0.0002221 ]]), array([[ 0.00094057],\n",
      "       [ 0.00372257],\n",
      "       [-0.00084854]])]\n",
      "gradients_biases:  [array([ 2.21077716e-03, -2.18452797e-03, -8.76467280e-05]), array([-0.00116665])]\n",
      "Iteration 508, Cost: 0.2485221742087285\n",
      "gradient_weights:  [array([[ 0.00172941, -0.00221203,  0.00037797],\n",
      "       [ 0.00922114,  0.00078688,  0.00022293]]), array([[ 0.00095371],\n",
      "       [ 0.00372096],\n",
      "       [-0.00084902]])]\n",
      "gradients_biases:  [array([ 2.21514765e-03, -2.18712764e-03, -8.78406792e-05]), array([-0.00116855])]\n",
      "Iteration 509, Cost: 0.24851616929897247\n",
      "gradient_weights:  [array([[ 0.00173916, -0.00221447,  0.00037733],\n",
      "       [ 0.009228  ,  0.00078851,  0.00022376]]), array([[ 0.00096687],\n",
      "       [ 0.00371936],\n",
      "       [-0.00084949]])]\n",
      "gradients_biases:  [array([ 2.21951080e-03, -2.18972626e-03, -8.80351198e-05]), array([-0.00117045])]\n",
      "Iteration 510, Cost: 0.24851015328357356\n",
      "gradient_weights:  [array([[ 0.00174892, -0.00221691,  0.00037668],\n",
      "       [ 0.00923487,  0.00079014,  0.0002246 ]]), array([[ 0.00098004],\n",
      "       [ 0.00371776],\n",
      "       [-0.00084997]])]\n",
      "gradients_biases:  [array([ 2.22386661e-03, -2.19232384e-03, -8.82300504e-05]), array([-0.00117235])]\n",
      "Iteration 511, Cost: 0.24850412610892006\n",
      "gradient_weights:  [array([[ 0.00175868, -0.00221935,  0.00037604],\n",
      "       [ 0.00924175,  0.00079177,  0.00022543]]), array([[ 0.00099322],\n",
      "       [ 0.00371617],\n",
      "       [-0.00085045]])]\n",
      "gradients_biases:  [array([ 2.22821505e-03, -2.19492036e-03, -8.84254716e-05]), array([-0.00117426])]\n",
      "Iteration 512, Cost: 0.24849808772131643\n",
      "gradient_weights:  [array([[ 0.00176844, -0.0022218 ,  0.0003754 ],\n",
      "       [ 0.00924865,  0.00079341,  0.00022627]]), array([[ 0.00100641],\n",
      "       [ 0.00371459],\n",
      "       [-0.00085094]])]\n",
      "gradients_biases:  [array([ 2.23255609e-03, -2.19751584e-03, -8.86213840e-05]), array([-0.00117617])]\n",
      "Iteration 513, Cost: 0.2484920380669825\n",
      "gradient_weights:  [array([[ 0.00177821, -0.00222424,  0.00037476],\n",
      "       [ 0.00925557,  0.00079505,  0.0002271 ]]), array([[ 0.00101962],\n",
      "       [ 0.00371302],\n",
      "       [-0.00085143]])]\n",
      "gradients_biases:  [array([ 2.23688972e-03, -2.20011029e-03, -8.88177882e-05]), array([-0.00117808])]\n",
      "Iteration 514, Cost: 0.24848597709205347\n",
      "gradient_weights:  [array([[ 0.00178799, -0.00222669,  0.00037412],\n",
      "       [ 0.0092625 ,  0.00079669,  0.00022794]]), array([[ 0.00103283],\n",
      "       [ 0.00371145],\n",
      "       [-0.00085192]])]\n",
      "gradients_biases:  [array([ 2.24121591e-03, -2.20270370e-03, -8.90146849e-05]), array([-0.00118])]\n",
      "Iteration 515, Cost: 0.24847990474257892\n",
      "gradient_weights:  [array([[ 0.00179777, -0.00222913,  0.00037349],\n",
      "       [ 0.00926945,  0.00079833,  0.00022878]]), array([[ 0.00104607],\n",
      "       [ 0.00370989],\n",
      "       [-0.00085241]])]\n",
      "gradients_biases:  [array([ 2.24553463e-03, -2.20529607e-03, -8.92120745e-05]), array([-0.00118192])]\n",
      "Iteration 516, Cost: 0.24847382096452292\n",
      "gradient_weights:  [array([[ 0.00180756, -0.00223158,  0.00037285],\n",
      "       [ 0.00927641,  0.00079997,  0.00022962]]), array([[ 0.00105931],\n",
      "       [ 0.00370834],\n",
      "       [-0.0008529 ]])]\n",
      "gradients_biases:  [array([ 2.24984586e-03, -2.20788742e-03, -8.94099577e-05]), array([-0.00118384])]\n",
      "Iteration 517, Cost: 0.24846772570376358\n",
      "gradient_weights:  [array([[ 0.00181735, -0.00223403,  0.00037222],\n",
      "       [ 0.00928339,  0.00080162,  0.00023046]]), array([[ 0.00107257],\n",
      "       [ 0.00370679],\n",
      "       [-0.0008534 ]])]\n",
      "gradients_biases:  [array([ 2.25414958e-03, -2.21047774e-03, -8.96083352e-05]), array([-0.00118577])]\n",
      "Iteration 518, Cost: 0.24846161890609206\n",
      "gradient_weights:  [array([[ 0.00182714, -0.00223648,  0.00037159],\n",
      "       [ 0.00929038,  0.00080327,  0.0002313 ]]), array([[ 0.00108584],\n",
      "       [ 0.00370526],\n",
      "       [-0.0008539 ]])]\n",
      "gradients_biases:  [array([ 2.25844576e-03, -2.21306703e-03, -8.98072075e-05]), array([-0.0011877])]\n",
      "Iteration 519, Cost: 0.248455500517213\n",
      "gradient_weights:  [array([[ 0.00183694, -0.00223893,  0.00037095],\n",
      "       [ 0.00929739,  0.00080493,  0.00023214]]), array([[ 0.00109912],\n",
      "       [ 0.00370372],\n",
      "       [-0.00085441]])]\n",
      "gradients_biases:  [array([ 2.26273438e-03, -2.21565531e-03, -9.00065753e-05]), array([-0.00118963])]\n",
      "Iteration 520, Cost: 0.24844937048274346\n",
      "gradient_weights:  [array([[ 0.00184675, -0.00224138,  0.00037032],\n",
      "       [ 0.00930441,  0.00080658,  0.00023299]]), array([[ 0.00111242],\n",
      "       [ 0.0037022 ],\n",
      "       [-0.00085492]])]\n",
      "gradients_biases:  [array([ 2.26701542e-03, -2.21824257e-03, -9.02064391e-05]), array([-0.00119157])]\n",
      "Iteration 521, Cost: 0.24844322874821279\n",
      "gradient_weights:  [array([[ 0.00185657, -0.00224383,  0.00036969],\n",
      "       [ 0.00931145,  0.00080824,  0.00023383]]), array([[ 0.00112573],\n",
      "       [ 0.00370068],\n",
      "       [-0.00085543]])]\n",
      "gradients_biases:  [array([ 2.27128884e-03, -2.22082881e-03, -9.04067996e-05]), array([-0.00119351])]\n",
      "Iteration 522, Cost: 0.24843707525906222\n",
      "gradient_weights:  [array([[ 0.00186638, -0.00224628,  0.00036907],\n",
      "       [ 0.0093185 ,  0.0008099 ,  0.00023467]]), array([[ 0.00113906],\n",
      "       [ 0.00369917],\n",
      "       [-0.00085594]])]\n",
      "gradients_biases:  [array([ 2.27555463e-03, -2.22341405e-03, -9.06076575e-05]), array([-0.00119545])]\n",
      "Iteration 523, Cost: 0.24843090996064443\n",
      "gradient_weights:  [array([[ 0.00187621, -0.00224874,  0.00036844],\n",
      "       [ 0.00932557,  0.00081156,  0.00023552]]), array([[ 0.00115239],\n",
      "       [ 0.00369767],\n",
      "       [-0.00085645]])]\n",
      "gradients_biases:  [array([ 2.27981276e-03, -2.22599828e-03, -9.08090132e-05]), array([-0.0011974])]\n",
      "Iteration 524, Cost: 0.2484247327982231\n",
      "gradient_weights:  [array([[ 0.00188604, -0.00225119,  0.00036781],\n",
      "       [ 0.00933265,  0.00081323,  0.00023637]]), array([[ 0.00116574],\n",
      "       [ 0.00369617],\n",
      "       [-0.00085697]])]\n",
      "gradients_biases:  [array([ 2.28406320e-03, -2.22858150e-03, -9.10108675e-05]), array([-0.00119935])]\n",
      "Iteration 525, Cost: 0.24841854371697258\n",
      "gradient_weights:  [array([[ 0.00189587, -0.00225365,  0.00036719],\n",
      "       [ 0.00933975,  0.0008149 ,  0.00023721]]), array([[ 0.00117911],\n",
      "       [ 0.00369468],\n",
      "       [-0.00085749]])]\n",
      "gradients_biases:  [array([ 2.28830593e-03, -2.23116372e-03, -9.12132209e-05]), array([-0.0012013])]\n",
      "Iteration 526, Cost: 0.24841234266197748\n",
      "gradient_weights:  [array([[ 0.00190571, -0.00225611,  0.00036657],\n",
      "       [ 0.00934686,  0.00081657,  0.00023806]]), array([[ 0.00119249],\n",
      "       [ 0.0036932 ],\n",
      "       [-0.00085802]])]\n",
      "gradients_biases:  [array([ 2.29254093e-03, -2.23374495e-03, -9.14160741e-05]), array([-0.00120326])]\n",
      "Iteration 527, Cost: 0.24840612957823244\n",
      "gradient_weights:  [array([[ 0.00191556, -0.00225856,  0.00036595],\n",
      "       [ 0.00935399,  0.00081824,  0.00023891]]), array([[ 0.00120588],\n",
      "       [ 0.00369172],\n",
      "       [-0.00085854]])]\n",
      "gradients_biases:  [array([ 2.29676816e-03, -2.23632518e-03, -9.16194277e-05]), array([-0.00120522])]\n",
      "Iteration 528, Cost: 0.24839990441064122\n",
      "gradient_weights:  [array([[ 0.00192541, -0.00226102,  0.00036532],\n",
      "       [ 0.00936113,  0.00081992,  0.00023976]]), array([[ 0.00121928],\n",
      "       [ 0.00369025],\n",
      "       [-0.00085907]])]\n",
      "gradients_biases:  [array([ 2.30098760e-03, -2.23890441e-03, -9.18232824e-05]), array([-0.00120718])]\n",
      "Iteration 529, Cost: 0.248393667104017\n",
      "gradient_weights:  [array([[ 0.00193527, -0.00226348,  0.00036471],\n",
      "       [ 0.00936828,  0.0008216 ,  0.00024061]]), array([[ 0.0012327 ],\n",
      "       [ 0.00368879],\n",
      "       [-0.0008596 ]])]\n",
      "gradients_biases:  [array([ 2.30519922e-03, -2.24148266e-03, -9.20276387e-05]), array([-0.00120915])]\n",
      "Iteration 530, Cost: 0.24838741760308142\n",
      "gradient_weights:  [array([[ 0.00194513, -0.00226595,  0.00036409],\n",
      "       [ 0.00937546,  0.00082328,  0.00024146]]), array([[ 0.00124613],\n",
      "       [ 0.00368733],\n",
      "       [-0.00086014]])]\n",
      "gradients_biases:  [array([ 2.30940300e-03, -2.24405993e-03, -9.22324974e-05]), array([-0.00121112])]\n",
      "Iteration 531, Cost: 0.2483811558524647\n",
      "gradient_weights:  [array([[ 0.001955  , -0.00226841,  0.00036347],\n",
      "       [ 0.00938264,  0.00082496,  0.00024231]]), array([[ 0.00125958],\n",
      "       [ 0.00368588],\n",
      "       [-0.00086068]])]\n",
      "gradients_biases:  [array([ 2.31359891e-03, -2.24663621e-03, -9.24378589e-05]), array([-0.00121309])]\n",
      "Iteration 532, Cost: 0.24837488179670472\n",
      "gradient_weights:  [array([[ 0.00196488, -0.00227087,  0.00036286],\n",
      "       [ 0.00938985,  0.00082665,  0.00024317]]), array([[ 0.00127304],\n",
      "       [ 0.00368444],\n",
      "       [-0.00086122]])]\n",
      "gradients_biases:  [array([ 2.31778693e-03, -2.24921151e-03, -9.26437241e-05]), array([-0.00121507])]\n",
      "Iteration 533, Cost: 0.2483685953802472\n",
      "gradient_weights:  [array([[ 0.00197476, -0.00227334,  0.00036224],\n",
      "       [ 0.00939706,  0.00082834,  0.00024402]]), array([[ 0.00128651],\n",
      "       [ 0.00368301],\n",
      "       [-0.00086176]])]\n",
      "gradients_biases:  [array([ 2.32196702e-03, -2.25178584e-03, -9.28500935e-05]), array([-0.00121705])]\n",
      "Iteration 534, Cost: 0.2483622965474449\n",
      "gradient_weights:  [array([[ 0.00198464, -0.00227581,  0.00036163],\n",
      "       [ 0.00940429,  0.00083003,  0.00024488]]), array([[ 0.0013    ],\n",
      "       [ 0.00368158],\n",
      "       [-0.00086231]])]\n",
      "gradients_biases:  [array([ 2.32613916e-03, -2.25435919e-03, -9.30569677e-05]), array([-0.00121903])]\n",
      "Iteration 535, Cost: 0.24835598524255725\n",
      "gradient_weights:  [array([[ 0.00199454, -0.00227827,  0.00036102],\n",
      "       [ 0.00941154,  0.00083172,  0.00024574]]), array([[ 0.0013135 ],\n",
      "       [ 0.00368016],\n",
      "       [-0.00086286]])]\n",
      "gradients_biases:  [array([ 2.33030333e-03, -2.25693157e-03, -9.32643474e-05]), array([-0.00122102])]\n",
      "Iteration 536, Cost: 0.2483496614097505\n",
      "gradient_weights:  [array([[ 0.00200444, -0.00228074,  0.00036041],\n",
      "       [ 0.0094188 ,  0.00083342,  0.00024659]]), array([[ 0.00132701],\n",
      "       [ 0.00367874],\n",
      "       [-0.00086341]])]\n",
      "gradients_biases:  [array([ 2.33445948e-03, -2.25950299e-03, -9.34722332e-05]), array([-0.00122301])]\n",
      "Iteration 537, Cost: 0.24834332499309664\n",
      "gradient_weights:  [array([[ 0.00201434, -0.00228321,  0.0003598 ],\n",
      "       [ 0.00942608,  0.00083512,  0.00024745]]), array([[ 0.00134054],\n",
      "       [ 0.00367734],\n",
      "       [-0.00086396]])]\n",
      "gradients_biases:  [array([ 2.33860761e-03, -2.26207344e-03, -9.36806258e-05]), array([-0.001225])]\n",
      "Iteration 538, Cost: 0.2483369759365735\n",
      "gradient_weights:  [array([[ 0.00202425, -0.00228568,  0.00035919],\n",
      "       [ 0.00943337,  0.00083682,  0.00024831]]), array([[ 0.00135408],\n",
      "       [ 0.00367594],\n",
      "       [-0.00086452]])]\n",
      "gradients_biases:  [array([ 2.34274767e-03, -2.26464293e-03, -9.38895258e-05]), array([-0.00122699])]\n",
      "Iteration 539, Cost: 0.2483306141840642\n",
      "gradient_weights:  [array([[ 0.00203417, -0.00228816,  0.00035858],\n",
      "       [ 0.00944068,  0.00083852,  0.00024917]]), array([[ 0.00136764],\n",
      "       [ 0.00367454],\n",
      "       [-0.00086508]])]\n",
      "gradients_biases:  [array([ 2.34687964e-03, -2.26721146e-03, -9.40989339e-05]), array([-0.00122899])]\n",
      "Iteration 540, Cost: 0.24832423967935702\n",
      "gradient_weights:  [array([[ 0.00204409, -0.00229063,  0.00035798],\n",
      "       [ 0.009448  ,  0.00084023,  0.00025003]]), array([[ 0.00138121],\n",
      "       [ 0.00367316],\n",
      "       [-0.00086564]])]\n",
      "gradients_biases:  [array([ 2.35100349e-03, -2.26977904e-03, -9.43088508e-05]), array([-0.001231])]\n",
      "Iteration 541, Cost: 0.24831785236614456\n",
      "gradient_weights:  [array([[ 0.00205402, -0.0022931 ,  0.00035737],\n",
      "       [ 0.00945533,  0.00084194,  0.0002509 ]]), array([[ 0.0013948 ],\n",
      "       [ 0.00367178],\n",
      "       [-0.00086621]])]\n",
      "gradients_biases:  [array([ 2.35511919e-03, -2.27234567e-03, -9.45192770e-05]), array([-0.001233])]\n",
      "Iteration 542, Cost: 0.24831145218802397\n",
      "gradient_weights:  [array([[ 0.00206396, -0.00229558,  0.00035677],\n",
      "       [ 0.00946268,  0.00084365,  0.00025176]]), array([[ 0.0014084 ],\n",
      "       [ 0.0036704 ],\n",
      "       [-0.00086678]])]\n",
      "gradients_biases:  [array([ 2.35922672e-03, -2.27491134e-03, -9.47302132e-05]), array([-0.00123501])]\n",
      "Iteration 543, Cost: 0.2483050390884961\n",
      "gradient_weights:  [array([[ 0.0020739 , -0.00229806,  0.00035617],\n",
      "       [ 0.00947005,  0.00084536,  0.00025262]]), array([[ 0.00142201],\n",
      "       [ 0.00366904],\n",
      "       [-0.00086735]])]\n",
      "gradients_biases:  [array([ 2.36332604e-03, -2.27747607e-03, -9.49416602e-05]), array([-0.00123702])]\n",
      "Iteration 544, Cost: 0.24829861301096556\n",
      "gradient_weights:  [array([[ 0.00208385, -0.00230054,  0.00035557],\n",
      "       [ 0.00947743,  0.00084708,  0.00025349]]), array([[ 0.00143564],\n",
      "       [ 0.00366768],\n",
      "       [-0.00086792]])]\n",
      "gradients_biases:  [array([ 2.36741713e-03, -2.28003986e-03, -9.51536184e-05]), array([-0.00123904])]\n",
      "Iteration 545, Cost: 0.24829217389873998\n",
      "gradient_weights:  [array([[ 0.0020938 , -0.00230302,  0.00035497],\n",
      "       [ 0.00948482,  0.0008488 ,  0.00025435]]), array([[ 0.00144928],\n",
      "       [ 0.00366633],\n",
      "       [-0.0008685 ]])]\n",
      "gradients_biases:  [array([ 2.37149995e-03, -2.28260271e-03, -9.53660887e-05]), array([-0.00124106])]\n",
      "Iteration 546, Cost: 0.24828572169502997\n",
      "gradient_weights:  [array([[ 0.00210376, -0.0023055 ,  0.00035437],\n",
      "       [ 0.00949223,  0.00085052,  0.00025522]]), array([[ 0.00146294],\n",
      "       [ 0.00366498],\n",
      "       [-0.00086908]])]\n",
      "gradients_biases:  [array([ 2.37557447e-03, -2.28516462e-03, -9.55790717e-05]), array([-0.00124308])]\n",
      "Iteration 547, Cost: 0.24827925634294856\n",
      "gradient_weights:  [array([[ 0.00211373, -0.00230798,  0.00035377],\n",
      "       [ 0.00949966,  0.00085224,  0.00025609]]), array([[ 0.00147661],\n",
      "       [ 0.00366364],\n",
      "       [-0.00086967]])]\n",
      "gradients_biases:  [array([ 2.37964067e-03, -2.28772560e-03, -9.57925679e-05]), array([-0.00124511])]\n",
      "Iteration 548, Cost: 0.248272777785511\n",
      "gradient_weights:  [array([[ 0.00212371, -0.00231047,  0.00035318],\n",
      "       [ 0.00950709,  0.00085397,  0.00025696]]), array([[ 0.0014903 ],\n",
      "       [ 0.00366231],\n",
      "       [-0.00087025]])]\n",
      "gradients_biases:  [array([ 2.38369852e-03, -2.29028564e-03, -9.60065782e-05]), array([-0.00124713])]\n",
      "Iteration 549, Cost: 0.2482662859656345\n",
      "gradient_weights:  [array([[ 0.00213369, -0.00231295,  0.00035258],\n",
      "       [ 0.00951455,  0.0008557 ,  0.00025783]]), array([[ 0.001504  ],\n",
      "       [ 0.00366098],\n",
      "       [-0.00087084]])]\n",
      "gradients_biases:  [array([ 2.38774798e-03, -2.29284476e-03, -9.62211032e-05]), array([-0.00124917])]\n",
      "Iteration 550, Cost: 0.2482597808261376\n",
      "gradient_weights:  [array([[ 0.00214367, -0.00231544,  0.00035199],\n",
      "       [ 0.00952202,  0.00085743,  0.0002587 ]]), array([[ 0.00151771],\n",
      "       [ 0.00365967],\n",
      "       [-0.00087143]])]\n",
      "gradients_biases:  [array([ 2.39178903e-03, -2.29540295e-03, -9.64361435e-05]), array([-0.0012512])]\n",
      "Iteration 551, Cost: 0.24825326230973999\n",
      "gradient_weights:  [array([[ 0.00215367, -0.00231792,  0.0003514 ],\n",
      "       [ 0.0095295 ,  0.00085917,  0.00025957]]), array([[ 0.00153144],\n",
      "       [ 0.00365835],\n",
      "       [-0.00087203]])]\n",
      "gradients_biases:  [array([ 2.39582163e-03, -2.29796023e-03, -9.66516998e-05]), array([-0.00125324])]\n",
      "Iteration 552, Cost: 0.24824673035906225\n",
      "gradient_weights:  [array([[ 0.00216367, -0.00232041,  0.00035081],\n",
      "       [ 0.009537  ,  0.0008609 ,  0.00026045]]), array([[ 0.00154519],\n",
      "       [ 0.00365705],\n",
      "       [-0.00087262]])]\n",
      "gradients_biases:  [array([ 2.39984575e-03, -2.30051658e-03, -9.68677728e-05]), array([-0.00125528])]\n",
      "Iteration 553, Cost: 0.2482401849166255\n",
      "gradient_weights:  [array([[ 0.00217367, -0.0023229 ,  0.00035022],\n",
      "       [ 0.00954451,  0.00086264,  0.00026132]]), array([[ 0.00155895],\n",
      "       [ 0.00365575],\n",
      "       [-0.00087322]])]\n",
      "gradients_biases:  [array([ 2.40386136e-03, -2.30307201e-03, -9.70843631e-05]), array([-0.00125733])]\n",
      "Iteration 554, Cost: 0.248233625924851\n",
      "gradient_weights:  [array([[ 0.00218368, -0.0023254 ,  0.00034963],\n",
      "       [ 0.00955204,  0.00086438,  0.0002622 ]]), array([[ 0.00157272],\n",
      "       [ 0.00365446],\n",
      "       [-0.00087383]])]\n",
      "gradients_biases:  [array([ 2.40786844e-03, -2.30562654e-03, -9.73014715e-05]), array([-0.00125938])]\n",
      "Iteration 555, Cost: 0.24822705332605977\n",
      "gradient_weights:  [array([[ 0.0021937 , -0.00232789,  0.00034904],\n",
      "       [ 0.00955958,  0.00086613,  0.00026308]]), array([[ 0.00158651],\n",
      "       [ 0.00365318],\n",
      "       [-0.00087443]])]\n",
      "gradients_biases:  [array([ 2.41186694e-03, -2.30818015e-03, -9.75190986e-05]), array([-0.00126143])]\n",
      "Iteration 556, Cost: 0.24822046706247253\n",
      "gradient_weights:  [array([[ 0.00220373, -0.00233038,  0.00034846],\n",
      "       [ 0.00956713,  0.00086788,  0.00026395]]), array([[ 0.00160031],\n",
      "       [ 0.0036519 ],\n",
      "       [-0.00087504]])]\n",
      "gradients_biases:  [array([ 2.41585684e-03, -2.31073285e-03, -9.77372451e-05]), array([-0.00126348])]\n",
      "Iteration 557, Cost: 0.24821386707620896\n",
      "gradient_weights:  [array([[ 0.00221376, -0.00233288,  0.00034787],\n",
      "       [ 0.0095747 ,  0.00086963,  0.00026483]]), array([[ 0.00161413],\n",
      "       [ 0.00365063],\n",
      "       [-0.00087565]])]\n",
      "gradients_biases:  [array([ 2.41983810e-03, -2.31328465e-03, -9.79559117e-05]), array([-0.00126554])]\n",
      "Iteration 558, Cost: 0.2482072533092879\n",
      "gradient_weights:  [array([[ 0.0022238 , -0.00233538,  0.00034729],\n",
      "       [ 0.00958229,  0.00087138,  0.00026571]]), array([[ 0.00162796],\n",
      "       [ 0.00364936],\n",
      "       [-0.00087627]])]\n",
      "gradients_biases:  [array([ 2.42381070e-03, -2.31583555e-03, -9.81750991e-05]), array([-0.0012676])]\n",
      "Iteration 559, Cost: 0.24820062570362644\n",
      "gradient_weights:  [array([[ 0.00223385, -0.00233788,  0.00034671],\n",
      "       [ 0.00958989,  0.00087313,  0.00026659]]), array([[ 0.00164181],\n",
      "       [ 0.00364811],\n",
      "       [-0.00087688]])]\n",
      "gradients_biases:  [array([ 2.42777459e-03, -2.31838555e-03, -9.83948079e-05]), array([-0.00126967])]\n",
      "Iteration 560, Cost: 0.24819398420104027\n",
      "gradient_weights:  [array([[ 0.0022439 , -0.00234038,  0.00034613],\n",
      "       [ 0.0095975 ,  0.00087489,  0.00026748]]), array([[ 0.00165567],\n",
      "       [ 0.00364686],\n",
      "       [-0.0008775 ]])]\n",
      "gradients_biases:  [array([ 2.43172975e-03, -2.32093465e-03, -9.86150389e-05]), array([-0.00127174])]\n",
      "Iteration 561, Cost: 0.24818732874324273\n",
      "gradient_weights:  [array([[ 0.00225396, -0.00234288,  0.00034555],\n",
      "       [ 0.00960513,  0.00087665,  0.00026836]]), array([[ 0.00166955],\n",
      "       [ 0.00364561],\n",
      "       [-0.00087813]])]\n",
      "gradients_biases:  [array([ 2.43567615e-03, -2.32348286e-03, -9.88357928e-05]), array([-0.00127381])]\n",
      "Iteration 562, Cost: 0.24818065927184507\n",
      "gradient_weights:  [array([[ 0.00226403, -0.00234538,  0.00034497],\n",
      "       [ 0.00961278,  0.00087841,  0.00026924]]), array([[ 0.00168345],\n",
      "       [ 0.00364437],\n",
      "       [-0.00087875]])]\n",
      "gradients_biases:  [array([ 2.43961375e-03, -2.32603018e-03, -9.90570701e-05]), array([-0.00127588])]\n",
      "Iteration 563, Cost: 0.24817397572835564\n",
      "gradient_weights:  [array([[ 0.0022741 , -0.00234789,  0.00034439],\n",
      "       [ 0.00962043,  0.00088018,  0.00027013]]), array([[ 0.00169736],\n",
      "       [ 0.00364314],\n",
      "       [-0.00087938]])]\n",
      "gradients_biases:  [array([ 2.44354251e-03, -2.32857661e-03, -9.92788718e-05]), array([-0.00127796])]\n",
      "Iteration 564, Cost: 0.2481672780541799\n",
      "gradient_weights:  [array([[ 0.00228419, -0.00235039,  0.00034382],\n",
      "       [ 0.00962811,  0.00088194,  0.00027101]]), array([[ 0.00171128],\n",
      "       [ 0.00364192],\n",
      "       [-0.00088001]])]\n",
      "gradients_biases:  [array([ 2.44746242e-03, -2.33112216e-03, -9.95011983e-05]), array([-0.00128004])]\n",
      "Iteration 565, Cost: 0.2481605661906201\n",
      "gradient_weights:  [array([[ 0.00229427, -0.0023529 ,  0.00034324],\n",
      "       [ 0.00963579,  0.00088371,  0.0002719 ]]), array([[ 0.00172522],\n",
      "       [ 0.0036407 ],\n",
      "       [-0.00088065]])]\n",
      "gradients_biases:  [array([ 2.45137342e-03, -2.33366683e-03, -9.97240505e-05]), array([-0.00128213])]\n",
      "Iteration 566, Cost: 0.24815384007887492\n",
      "gradient_weights:  [array([[ 0.00230437, -0.00235541,  0.00034267],\n",
      "       [ 0.00964349,  0.00088549,  0.00027279]]), array([[ 0.00173917],\n",
      "       [ 0.00363949],\n",
      "       [-0.00088128]])]\n",
      "gradients_biases:  [array([ 2.45527549e-03, -2.33621061e-03, -9.99474291e-05]), array([-0.00128422])]\n",
      "Iteration 567, Cost: 0.24814709966003912\n",
      "gradient_weights:  [array([[ 0.00231447, -0.00235792,  0.0003421 ],\n",
      "       [ 0.00965121,  0.00088726,  0.00027368]]), array([[ 0.00175314],\n",
      "       [ 0.00363829],\n",
      "       [-0.00088192]])]\n",
      "gradients_biases:  [array([ 0.00245917, -0.00233875, -0.00010017]), array([-0.00128631])]\n",
      "Iteration 568, Cost: 0.24814034487510328\n",
      "gradient_weights:  [array([[ 0.00232458, -0.00236043,  0.00034153],\n",
      "       [ 0.00965894,  0.00088904,  0.00027457]]), array([[ 0.00176713],\n",
      "       [ 0.00363709],\n",
      "       [-0.00088257]])]\n",
      "gradients_biases:  [array([ 0.00246305, -0.0023413 , -0.0001004 ]), array([-0.0012884])]\n",
      "Iteration 569, Cost: 0.2481335756649538\n",
      "gradient_weights:  [array([[ 0.0023347 , -0.00236295,  0.00034096],\n",
      "       [ 0.00966668,  0.00089082,  0.00027546]]), array([[ 0.00178113],\n",
      "       [ 0.0036359 ],\n",
      "       [-0.00088321]])]\n",
      "gradients_biases:  [array([ 0.00246693, -0.00234384, -0.00010062]), array([-0.0012905])]\n",
      "Iteration 570, Cost: 0.248126791970372\n",
      "gradient_weights:  [array([[ 0.00234482, -0.00236546,  0.00034039],\n",
      "       [ 0.00967444,  0.0008926 ,  0.00027636]]), array([[ 0.00179514],\n",
      "       [ 0.00363472],\n",
      "       [-0.00088386]])]\n",
      "gradients_biases:  [array([ 0.00247079, -0.00234638, -0.00010085]), array([-0.0012926])]\n",
      "Iteration 571, Cost: 0.24811999373203436\n",
      "gradient_weights:  [array([[ 0.00235495, -0.00236798,  0.00033983],\n",
      "       [ 0.00968222,  0.00089439,  0.00027725]]), array([[ 0.00180918],\n",
      "       [ 0.00363354],\n",
      "       [-0.00088451]])]\n",
      "gradients_biases:  [array([ 0.00247465, -0.00234892, -0.00010107]), array([-0.00129471])]\n",
      "Iteration 572, Cost: 0.2481131808905122\n",
      "gradient_weights:  [array([[ 0.00236509, -0.0023705 ,  0.00033926],\n",
      "       [ 0.00969001,  0.00089617,  0.00027815]]), array([[ 0.00182322],\n",
      "       [ 0.00363238],\n",
      "       [-0.00088517]])]\n",
      "gradients_biases:  [array([ 0.0024785 , -0.00235146, -0.0001013 ]), array([-0.00129681])]\n",
      "Iteration 573, Cost: 0.24810635338627107\n",
      "gradient_weights:  [array([[ 0.00237523, -0.00237302,  0.0003387 ],\n",
      "       [ 0.00969781,  0.00089796,  0.00027904]]), array([[ 0.00183728],\n",
      "       [ 0.00363121],\n",
      "       [-0.00088582]])]\n",
      "gradients_biases:  [array([ 0.00248234, -0.00235399, -0.00010153]), array([-0.00129892])]\n",
      "Iteration 574, Cost: 0.24809951115967077\n",
      "gradient_weights:  [array([[ 0.00238538, -0.00237554,  0.00033813],\n",
      "       [ 0.00970562,  0.00089976,  0.00027994]]), array([[ 0.00185136],\n",
      "       [ 0.00363006],\n",
      "       [-0.00088648]])]\n",
      "gradients_biases:  [array([ 0.00248617, -0.00235653, -0.00010175]), array([-0.00130104])]\n",
      "Iteration 575, Cost: 0.24809265415096496\n",
      "gradient_weights:  [array([[ 0.00239554, -0.00237806,  0.00033757],\n",
      "       [ 0.00971346,  0.00090155,  0.00028084]]), array([[ 0.00186546],\n",
      "       [ 0.00362891],\n",
      "       [-0.00088715]])]\n",
      "gradients_biases:  [array([ 0.00248999, -0.00235907, -0.00010198]), array([-0.00130316])]\n",
      "Iteration 576, Cost: 0.24808578230030104\n",
      "gradient_weights:  [array([[ 0.00240571, -0.00238059,  0.00033701],\n",
      "       [ 0.0097213 ,  0.00090335,  0.00028174]]), array([[ 0.00187957],\n",
      "       [ 0.00362776],\n",
      "       [-0.00088781]])]\n",
      "gradients_biases:  [array([ 0.0024938 , -0.0023616 , -0.00010221]), array([-0.00130528])]\n",
      "Iteration 577, Cost: 0.2480788955477196\n",
      "gradient_weights:  [array([[ 0.00241588, -0.00238311,  0.00033645],\n",
      "       [ 0.00972916,  0.00090515,  0.00028264]]), array([[ 0.00189369],\n",
      "       [ 0.00362663],\n",
      "       [-0.00088848]])]\n",
      "gradients_biases:  [array([ 0.0024976 , -0.00236414, -0.00010244]), array([-0.0013074])]\n",
      "Iteration 578, Cost: 0.24807199383315442\n",
      "gradient_weights:  [array([[ 0.00242607, -0.00238564,  0.00033589],\n",
      "       [ 0.00973703,  0.00090695,  0.00028354]]), array([[ 0.00190783],\n",
      "       [ 0.0036255 ],\n",
      "       [-0.00088915]])]\n",
      "gradients_biases:  [array([ 0.00250139, -0.00236667, -0.00010267]), array([-0.00130953])]\n",
      "Iteration 579, Cost: 0.24806507709643205\n",
      "gradient_weights:  [array([[ 0.00243626, -0.00238817,  0.00033534],\n",
      "       [ 0.00974492,  0.00090876,  0.00028445]]), array([[ 0.00192199],\n",
      "       [ 0.00362438],\n",
      "       [-0.00088983]])]\n",
      "gradients_biases:  [array([ 0.00250517, -0.0023692 , -0.0001029 ]), array([-0.00131166])]\n",
      "Iteration 580, Cost: 0.24805814527727174\n",
      "gradient_weights:  [array([[ 0.00244645, -0.0023907 ,  0.00033478],\n",
      "       [ 0.00975282,  0.00091056,  0.00028535]]), array([[ 0.00193616],\n",
      "       [ 0.00362326],\n",
      "       [-0.00089051]])]\n",
      "gradients_biases:  [array([ 0.00250895, -0.00237173, -0.00010313]), array([-0.00131379])]\n",
      "Iteration 581, Cost: 0.24805119831528472\n",
      "gradient_weights:  [array([[ 0.00245666, -0.00239323,  0.00033423],\n",
      "       [ 0.00976074,  0.00091237,  0.00028626]]), array([[ 0.00195035],\n",
      "       [ 0.00362215],\n",
      "       [-0.00089119]])]\n",
      "gradients_biases:  [array([ 0.00251271, -0.00237426, -0.00010336]), array([-0.00131593])]\n",
      "Iteration 582, Cost: 0.24804423614997478\n",
      "gradient_weights:  [array([[ 0.00246687, -0.00239577,  0.00033368],\n",
      "       [ 0.00976867,  0.00091419,  0.00028717]]), array([[ 0.00196455],\n",
      "       [ 0.00362105],\n",
      "       [-0.00089187]])]\n",
      "gradients_biases:  [array([ 0.00251647, -0.00237679, -0.00010359]), array([-0.00131807])]\n",
      "Iteration 583, Cost: 0.24803725872073723\n",
      "gradient_weights:  [array([[ 0.00247709, -0.0023983 ,  0.00033312],\n",
      "       [ 0.00977662,  0.000916  ,  0.00028807]]), array([[ 0.00197877],\n",
      "       [ 0.00361995],\n",
      "       [-0.00089255]])]\n",
      "gradients_biases:  [array([ 0.00252021, -0.00237932, -0.00010383]), array([-0.00132021])]\n",
      "Iteration 584, Cost: 0.24803026596685881\n",
      "gradient_weights:  [array([[ 0.00248731, -0.00240084,  0.00033257],\n",
      "       [ 0.00978458,  0.00091782,  0.00028898]]), array([[ 0.00199301],\n",
      "       [ 0.00361886],\n",
      "       [-0.00089324]])]\n",
      "gradients_biases:  [array([ 0.00252395, -0.00238185, -0.00010406]), array([-0.00132236])]\n",
      "Iteration 585, Cost: 0.24802325782751794\n",
      "gradient_weights:  [array([[ 0.00249755, -0.00240338,  0.00033203],\n",
      "       [ 0.00979255,  0.00091964,  0.00028989]]), array([[ 0.00200726],\n",
      "       [ 0.00361778],\n",
      "       [-0.00089394]])]\n",
      "gradients_biases:  [array([ 0.00252767, -0.00238438, -0.00010429]), array([-0.00132451])]\n",
      "Iteration 586, Cost: 0.2480162342417839\n",
      "gradient_weights:  [array([[ 0.00250779, -0.00240592,  0.00033148],\n",
      "       [ 0.00980054,  0.00092146,  0.00029081]]), array([[ 0.00202153],\n",
      "       [ 0.0036167 ],\n",
      "       [-0.00089463]])]\n",
      "gradients_biases:  [array([ 0.00253139, -0.00238691, -0.00010453]), array([-0.00132667])]\n",
      "Iteration 587, Cost: 0.24800919514861686\n",
      "gradient_weights:  [array([[ 0.00251804, -0.00240847,  0.00033093],\n",
      "       [ 0.00980854,  0.00092329,  0.00029172]]), array([[ 0.00203581],\n",
      "       [ 0.00361563],\n",
      "       [-0.00089533]])]\n",
      "gradients_biases:  [array([ 0.00253509, -0.00238943, -0.00010476]), array([-0.00132882])]\n",
      "Iteration 588, Cost: 0.24800214048686758\n",
      "gradient_weights:  [array([[ 0.0025283 , -0.00241101,  0.00033039],\n",
      "       [ 0.00981656,  0.00092511,  0.00029263]]), array([[ 0.00205011],\n",
      "       [ 0.00361457],\n",
      "       [-0.00089603]])]\n",
      "gradients_biases:  [array([ 0.00253879, -0.00239196, -0.000105  ]), array([-0.00133098])]\n",
      "Iteration 589, Cost: 0.24799507019527722\n",
      "gradient_weights:  [array([[ 0.00253856, -0.00241356,  0.00032984],\n",
      "       [ 0.00982459,  0.00092694,  0.00029355]]), array([[ 0.00206443],\n",
      "       [ 0.00361352],\n",
      "       [-0.00089673]])]\n",
      "gradients_biases:  [array([ 0.00254248, -0.00239448, -0.00010523]), array([-0.00133315])]\n",
      "Iteration 590, Cost: 0.24798798421247706\n",
      "gradient_weights:  [array([[ 0.00254884, -0.0024161 ,  0.0003293 ],\n",
      "       [ 0.00983263,  0.00092878,  0.00029447]]), array([[ 0.00207876],\n",
      "       [ 0.00361247],\n",
      "       [-0.00089744]])]\n",
      "gradients_biases:  [array([ 0.00254615, -0.00239701, -0.00010547]), array([-0.00133531])]\n",
      "Iteration 591, Cost: 0.24798088247698852\n",
      "gradient_weights:  [array([[ 0.00255912, -0.00241865,  0.00032876],\n",
      "       [ 0.00984069,  0.00093061,  0.00029538]]), array([[ 0.00209311],\n",
      "       [ 0.00361142],\n",
      "       [-0.00089815]])]\n",
      "gradients_biases:  [array([ 0.00254982, -0.00239953, -0.00010571]), array([-0.00133748])]\n",
      "Iteration 592, Cost: 0.24797376492722237\n",
      "gradient_weights:  [array([[ 0.0025694 , -0.0024212 ,  0.00032822],\n",
      "       [ 0.00984876,  0.00093245,  0.0002963 ]]), array([[ 0.00210748],\n",
      "       [ 0.00361039],\n",
      "       [-0.00089886]])]\n",
      "gradients_biases:  [array([ 0.00255348, -0.00240205, -0.00010594]), array([-0.00133966])]\n",
      "Iteration 593, Cost: 0.24796663150147902\n",
      "gradient_weights:  [array([[ 0.0025797 , -0.00242376,  0.00032768],\n",
      "       [ 0.00985685,  0.00093429,  0.00029722]]), array([[ 0.00212186],\n",
      "       [ 0.00360936],\n",
      "       [-0.00089958]])]\n",
      "gradients_biases:  [array([ 0.00255712, -0.00240457, -0.00010618]), array([-0.00134183])]\n",
      "Iteration 594, Cost: 0.24795948213794833\n",
      "gradient_weights:  [array([[ 0.00259001, -0.00242631,  0.00032714],\n",
      "       [ 0.00986495,  0.00093613,  0.00029815]]), array([[ 0.00213626],\n",
      "       [ 0.00360833],\n",
      "       [-0.00090029]])]\n",
      "gradients_biases:  [array([ 0.00256076, -0.00240709, -0.00010642]), array([-0.00134401])]\n",
      "Iteration 595, Cost: 0.24795231677470878\n",
      "gradient_weights:  [array([[ 0.00260032, -0.00242887,  0.00032661],\n",
      "       [ 0.00987306,  0.00093798,  0.00029907]]), array([[ 0.00215067],\n",
      "       [ 0.00360732],\n",
      "       [-0.00090101]])]\n",
      "gradients_biases:  [array([ 0.00256439, -0.00240961, -0.00010666]), array([-0.0013462])]\n",
      "Iteration 596, Cost: 0.24794513534972829\n",
      "gradient_weights:  [array([[ 0.00261064, -0.00243143,  0.00032607],\n",
      "       [ 0.00988119,  0.00093983,  0.00029999]]), array([[ 0.0021651 ],\n",
      "       [ 0.00360631],\n",
      "       [-0.00090174]])]\n",
      "gradients_biases:  [array([ 0.00256801, -0.00241213, -0.0001069 ]), array([-0.00134838])]\n",
      "Iteration 597, Cost: 0.24793793780086282\n",
      "gradient_weights:  [array([[ 0.00262097, -0.00243399,  0.00032554],\n",
      "       [ 0.00988933,  0.00094168,  0.00030092]]), array([[ 0.00217955],\n",
      "       [ 0.00360531],\n",
      "       [-0.00090246]])]\n",
      "gradients_biases:  [array([ 0.00257161, -0.00241465, -0.00010714]), array([-0.00135057])]\n",
      "Iteration 598, Cost: 0.24793072406585714\n",
      "gradient_weights:  [array([[ 0.0026313 , -0.00243655,  0.000325  ],\n",
      "       [ 0.00989749,  0.00094353,  0.00030185]]), array([[ 0.00219402],\n",
      "       [ 0.00360431],\n",
      "       [-0.00090319]])]\n",
      "gradients_biases:  [array([ 0.00257521, -0.00241716, -0.00010738]), array([-0.00135277])]\n",
      "Iteration 599, Cost: 0.2479234940823441\n",
      "gradient_weights:  [array([[ 0.00264165, -0.00243911,  0.00032447],\n",
      "       [ 0.00990566,  0.00094538,  0.00030278]]), array([[ 0.0022085 ],\n",
      "       [ 0.00360332],\n",
      "       [-0.00090393]])]\n",
      "gradients_biases:  [array([ 0.0025788 , -0.00241968, -0.00010762]), array([-0.00135496])]\n",
      "Iteration 600, Cost: 0.24791624778784466\n",
      "gradient_weights:  [array([[ 0.002652  , -0.00244168,  0.00032394],\n",
      "       [ 0.00991384,  0.00094724,  0.00030371]]), array([[ 0.00222299],\n",
      "       [ 0.00360234],\n",
      "       [-0.00090466]])]\n",
      "gradients_biases:  [array([ 0.00258237, -0.0024222 , -0.00010786]), array([-0.00135716])]\n",
      "Iteration 601, Cost: 0.24790898511976742\n",
      "gradient_weights:  [array([[ 0.00266236, -0.00244425,  0.00032341],\n",
      "       [ 0.00992204,  0.0009491 ,  0.00030464]]), array([[ 0.00223751],\n",
      "       [ 0.00360136],\n",
      "       [-0.0009054 ]])]\n",
      "gradients_biases:  [array([ 0.00258594, -0.00242471, -0.0001081 ]), array([-0.00135937])]\n",
      "Iteration 602, Cost: 0.2479017060154089\n",
      "gradient_weights:  [array([[ 0.00267273, -0.00244682,  0.00032289],\n",
      "       [ 0.00993025,  0.00095096,  0.00030557]]), array([[ 0.00225204],\n",
      "       [ 0.00360039],\n",
      "       [-0.00090614]])]\n",
      "gradients_biases:  [array([ 0.0025895 , -0.00242722, -0.00010835]), array([-0.00136157])]\n",
      "Iteration 603, Cost: 0.24789441041195293\n",
      "gradient_weights:  [array([[ 0.0026831 , -0.00244939,  0.00032236],\n",
      "       [ 0.00993848,  0.00095283,  0.0003065 ]]), array([[ 0.00226658],\n",
      "       [ 0.00359943],\n",
      "       [-0.00090689]])]\n",
      "gradients_biases:  [array([ 0.00259304, -0.00242974, -0.00010859]), array([-0.00136378])]\n",
      "Iteration 604, Cost: 0.2478870982464706\n",
      "gradient_weights:  [array([[ 0.00269349, -0.00245196,  0.00032184],\n",
      "       [ 0.00994672,  0.0009547 ,  0.00030744]]), array([[ 0.00228115],\n",
      "       [ 0.00359847],\n",
      "       [-0.00090763]])]\n",
      "gradients_biases:  [array([ 0.00259658, -0.00243225, -0.00010883]), array([-0.001366])]\n",
      "Iteration 605, Cost: 0.2478797694559201\n",
      "gradient_weights:  [array([[ 0.00270388, -0.00245454,  0.00032131],\n",
      "       [ 0.00995497,  0.00095657,  0.00030837]]), array([[ 0.00229573],\n",
      "       [ 0.00359752],\n",
      "       [-0.00090838]])]\n",
      "gradients_biases:  [array([ 0.0026001 , -0.00243476, -0.00010908]), array([-0.00136821])]\n",
      "Iteration 606, Cost: 0.2478724239771466\n",
      "gradient_weights:  [array([[ 0.00271428, -0.00245711,  0.00032079],\n",
      "       [ 0.00996324,  0.00095844,  0.00030931]]), array([[ 0.00231032],\n",
      "       [ 0.00359658],\n",
      "       [-0.00090914]])]\n",
      "gradients_biases:  [array([ 0.00260362, -0.00243727, -0.00010932]), array([-0.00137043])]\n",
      "Iteration 607, Cost: 0.2478650617468819\n",
      "gradient_weights:  [array([[ 0.00272469, -0.00245969,  0.00032027],\n",
      "       [ 0.00997152,  0.00096031,  0.00031025]]), array([[ 0.00232494],\n",
      "       [ 0.00359564],\n",
      "       [-0.00090989]])]\n",
      "gradients_biases:  [array([ 0.00260712, -0.00243978, -0.00010957]), array([-0.00137265])]\n",
      "Iteration 608, Cost: 0.2478576827017444\n",
      "gradient_weights:  [array([[ 0.00273511, -0.00246227,  0.00031975],\n",
      "       [ 0.00997981,  0.00096219,  0.00031119]]), array([[ 0.00233957],\n",
      "       [ 0.00359471],\n",
      "       [-0.00091065]])]\n",
      "gradients_biases:  [array([ 0.00261061, -0.00244229, -0.00010981]), array([-0.00137488])]\n",
      "Iteration 609, Cost: 0.2478502867782389\n",
      "gradient_weights:  [array([[ 0.00274554, -0.00246486,  0.00031923],\n",
      "       [ 0.00998812,  0.00096407,  0.00031213]]), array([[ 0.00235422],\n",
      "       [ 0.00359378],\n",
      "       [-0.00091141]])]\n",
      "gradients_biases:  [array([ 0.0026141 , -0.0024448 , -0.00011006]), array([-0.00137711])]\n",
      "Iteration 610, Cost: 0.24784287391275645\n",
      "gradient_weights:  [array([[ 0.00275597, -0.00246744,  0.00031871],\n",
      "       [ 0.00999644,  0.00096595,  0.00031307]]), array([[ 0.00236888],\n",
      "       [ 0.00359287],\n",
      "       [-0.00091218]])]\n",
      "gradients_biases:  [array([ 0.00261757, -0.00244731, -0.00011031]), array([-0.00137934])]\n",
      "Iteration 611, Cost: 0.24783544404157404\n",
      "gradient_weights:  [array([[ 0.00276642, -0.00247003,  0.0003182 ],\n",
      "       [ 0.01000478,  0.00096784,  0.00031402]]), array([[ 0.00238356],\n",
      "       [ 0.00359196],\n",
      "       [-0.00091295]])]\n",
      "gradients_biases:  [array([ 0.00262103, -0.00244982, -0.00011056]), array([-0.00138158])]\n",
      "Iteration 612, Cost: 0.24782799710085468\n",
      "gradient_weights:  [array([[ 0.00277687, -0.00247262,  0.00031768],\n",
      "       [ 0.01001313,  0.00096973,  0.00031496]]), array([[ 0.00239826],\n",
      "       [ 0.00359105],\n",
      "       [-0.00091372]])]\n",
      "gradients_biases:  [array([ 0.00262448, -0.00245232, -0.0001108 ]), array([-0.00138382])]\n",
      "Iteration 613, Cost: 0.24782053302664714\n",
      "gradient_weights:  [array([[ 0.00278733, -0.00247521,  0.00031717],\n",
      "       [ 0.01002149,  0.00097162,  0.00031591]]), array([[ 0.00241298],\n",
      "       [ 0.00359015],\n",
      "       [-0.00091449]])]\n",
      "gradients_biases:  [array([ 0.00262792, -0.00245483, -0.00011105]), array([-0.00138606])]\n",
      "Iteration 614, Cost: 0.24781305175488572\n",
      "gradient_weights:  [array([[ 0.0027978 , -0.0024778 ,  0.00031666],\n",
      "       [ 0.01002987,  0.00097351,  0.00031686]]), array([[ 0.00242771],\n",
      "       [ 0.00358926],\n",
      "       [-0.00091527]])]\n",
      "gradients_biases:  [array([ 0.00263135, -0.00245733, -0.0001113 ]), array([-0.00138831])]\n",
      "Iteration 615, Cost: 0.2478055532213901\n",
      "gradient_weights:  [array([[ 0.00280827, -0.00248039,  0.00031615],\n",
      "       [ 0.01003826,  0.0009754 ,  0.00031781]]), array([[ 0.00244246],\n",
      "       [ 0.00358838],\n",
      "       [-0.00091605]])]\n",
      "gradients_biases:  [array([ 0.00263477, -0.00245984, -0.00011155]), array([-0.00139055])]\n",
      "Iteration 616, Cost: 0.24779803736186545\n",
      "gradient_weights:  [array([[ 0.00281876, -0.00248299,  0.00031564],\n",
      "       [ 0.01004667,  0.0009773 ,  0.00031876]]), array([[ 0.00245723],\n",
      "       [ 0.0035875 ],\n",
      "       [-0.00091683]])]\n",
      "gradients_biases:  [array([ 0.00263818, -0.00246234, -0.0001118 ]), array([-0.00139281])]\n",
      "Iteration 617, Cost: 0.24779050411190193\n",
      "gradient_weights:  [array([[ 0.00282925, -0.00248559,  0.00031513],\n",
      "       [ 0.01005508,  0.0009792 ,  0.00031971]]), array([[ 0.00247201],\n",
      "       [ 0.00358663],\n",
      "       [-0.00091761]])]\n",
      "gradients_biases:  [array([ 0.00264157, -0.00246484, -0.00011205]), array([-0.00139506])]\n",
      "Iteration 618, Cost: 0.24778295340697476\n",
      "gradient_weights:  [array([[ 0.00283976, -0.00248819,  0.00031462],\n",
      "       [ 0.01006351,  0.0009811 ,  0.00032067]]), array([[ 0.00248681],\n",
      "       [ 0.00358576],\n",
      "       [-0.0009184 ]])]\n",
      "gradients_biases:  [array([ 0.00264496, -0.00246734, -0.00011231]), array([-0.00139732])]\n",
      "Iteration 619, Cost: 0.24777538518244407\n",
      "gradient_weights:  [array([[ 0.00285027, -0.00249079,  0.00031412],\n",
      "       [ 0.01007196,  0.00098301,  0.00032162]]), array([[ 0.00250163],\n",
      "       [ 0.00358491],\n",
      "       [-0.00091919]])]\n",
      "gradients_biases:  [array([ 0.00264833, -0.00246984, -0.00011256]), array([-0.00139958])]\n",
      "Iteration 620, Cost: 0.24776779937355464\n",
      "gradient_weights:  [array([[ 0.00286079, -0.00249339,  0.00031361],\n",
      "       [ 0.01008042,  0.00098491,  0.00032258]]), array([[ 0.00251646],\n",
      "       [ 0.00358405],\n",
      "       [-0.00091999]])]\n",
      "gradients_biases:  [array([ 0.0026517 , -0.00247234, -0.00011281]), array([-0.00140185])]\n",
      "Iteration 621, Cost: 0.24776019591543608\n",
      "gradient_weights:  [array([[ 0.00287131, -0.002496  ,  0.00031311],\n",
      "       [ 0.01008889,  0.00098682,  0.00032354]]), array([[ 0.00253132],\n",
      "       [ 0.00358321],\n",
      "       [-0.00092079]])]\n",
      "gradients_biases:  [array([ 0.00265505, -0.00247484, -0.00011307]), array([-0.00140412])]\n",
      "Iteration 622, Cost: 0.24775257474310208\n",
      "gradient_weights:  [array([[ 0.00288185, -0.00249861,  0.00031261],\n",
      "       [ 0.01009738,  0.00098873,  0.0003245 ]]), array([[ 0.00254619],\n",
      "       [ 0.00358237],\n",
      "       [-0.00092159]])]\n",
      "gradients_biases:  [array([ 0.00265839, -0.00247734, -0.00011332]), array([-0.00140639])]\n",
      "Iteration 623, Cost: 0.2477449357914512\n",
      "gradient_weights:  [array([[ 0.0028924 , -0.00250122,  0.00031211],\n",
      "       [ 0.01010587,  0.00099065,  0.00032546]]), array([[ 0.00256108],\n",
      "       [ 0.00358154],\n",
      "       [-0.00092239]])]\n",
      "gradients_biases:  [array([ 0.00266172, -0.00247984, -0.00011357]), array([-0.00140867])]\n",
      "Iteration 624, Cost: 0.24773727899526593\n",
      "gradient_weights:  [array([[ 0.00290295, -0.00250383,  0.00031161],\n",
      "       [ 0.01011439,  0.00099257,  0.00032642]]), array([[ 0.00257598],\n",
      "       [ 0.00358071],\n",
      "       [-0.0009232 ]])]\n",
      "gradients_biases:  [array([ 0.00266504, -0.00248234, -0.00011383]), array([-0.00141094])]\n",
      "Iteration 625, Cost: 0.24772960428921303\n",
      "gradient_weights:  [array([[ 0.00291352, -0.00250645,  0.00031111],\n",
      "       [ 0.01012291,  0.00099449,  0.00032738]]), array([[ 0.0025909 ],\n",
      "       [ 0.00357989],\n",
      "       [-0.000924  ]])]\n",
      "gradients_biases:  [array([ 0.00266834, -0.00248484, -0.00011409]), array([-0.00141323])]\n",
      "Iteration 626, Cost: 0.24772191160784315\n",
      "gradient_weights:  [array([[ 0.00292409, -0.00250906,  0.00031062],\n",
      "       [ 0.01013145,  0.00099641,  0.00032835]]), array([[ 0.00260584],\n",
      "       [ 0.00357908],\n",
      "       [-0.00092482]])]\n",
      "gradients_biases:  [array([ 0.00267164, -0.00248733, -0.00011434]), array([-0.00141551])]\n",
      "Iteration 627, Cost: 0.24771420088559093\n",
      "gradient_weights:  [array([[ 0.00293467, -0.00251168,  0.00031012],\n",
      "       [ 0.01014   ,  0.00099833,  0.00032931]]), array([[ 0.0026208 ],\n",
      "       [ 0.00357828],\n",
      "       [-0.00092563]])]\n",
      "gradients_biases:  [array([ 0.00267492, -0.00248983, -0.0001146 ]), array([-0.0014178])]\n",
      "Iteration 628, Cost: 0.24770647205677493\n",
      "gradient_weights:  [array([[ 0.00294526, -0.0025143 ,  0.00030963],\n",
      "       [ 0.01014857,  0.00100026,  0.00033028]]), array([[ 0.00263577],\n",
      "       [ 0.00357748],\n",
      "       [-0.00092645]])]\n",
      "gradients_biases:  [array([ 0.0026782 , -0.00249232, -0.00011486]), array([-0.00142009])]\n",
      "Iteration 629, Cost: 0.2476987250555972\n",
      "gradient_weights:  [array([[ 0.00295586, -0.00251693,  0.00030913],\n",
      "       [ 0.01015715,  0.00100219,  0.00033125]]), array([[ 0.00265076],\n",
      "       [ 0.00357668],\n",
      "       [-0.00092727]])]\n",
      "gradients_biases:  [array([ 0.00268146, -0.00249481, -0.00011511]), array([-0.00142239])]\n",
      "Iteration 630, Cost: 0.2476909598161435\n",
      "gradient_weights:  [array([[ 0.00296647, -0.00251955,  0.00030864],\n",
      "       [ 0.01016574,  0.00100412,  0.00033222]]), array([[ 0.00266577],\n",
      "       [ 0.0035759 ],\n",
      "       [-0.00092809]])]\n",
      "gradients_biases:  [array([ 0.00268471, -0.00249731, -0.00011537]), array([-0.00142469])]\n",
      "Iteration 631, Cost: 0.2476831762723832\n",
      "gradient_weights:  [array([[ 0.00297708, -0.00252218,  0.00030815],\n",
      "       [ 0.01017435,  0.00100605,  0.0003332 ]]), array([[ 0.0026808 ],\n",
      "       [ 0.00357512],\n",
      "       [-0.00092892]])]\n",
      "gradients_biases:  [array([ 0.00268795, -0.0024998 , -0.00011563]), array([-0.00142699])]\n",
      "Iteration 632, Cost: 0.247675374358169\n",
      "gradient_weights:  [array([[ 0.00298771, -0.00252481,  0.00030766],\n",
      "       [ 0.01018297,  0.00100799,  0.00033417]]), array([[ 0.00269585],\n",
      "       [ 0.00357434],\n",
      "       [-0.00092975]])]\n",
      "gradients_biases:  [array([ 0.00269117, -0.00250229, -0.00011589]), array([-0.00142929])]\n",
      "Iteration 633, Cost: 0.24766755400723695\n",
      "gradient_weights:  [array([[ 0.00299834, -0.00252744,  0.00030718],\n",
      "       [ 0.0101916 ,  0.00100993,  0.00033514]]), array([[ 0.00271091],\n",
      "       [ 0.00357358],\n",
      "       [-0.00093058]])]\n",
      "gradients_biases:  [array([ 0.00269439, -0.00250478, -0.00011615]), array([-0.0014316])]\n",
      "Iteration 634, Cost: 0.24765971515320623\n",
      "gradient_weights:  [array([[ 0.00300899, -0.00253007,  0.00030669],\n",
      "       [ 0.01020025,  0.00101187,  0.00033612]]), array([[ 0.00272599],\n",
      "       [ 0.00357282],\n",
      "       [-0.00093142]])]\n",
      "gradients_biases:  [array([ 0.00269759, -0.00250727, -0.00011641]), array([-0.00143391])]\n",
      "Iteration 635, Cost: 0.24765185772957943\n",
      "gradient_weights:  [array([[ 0.00301964, -0.00253271,  0.0003062 ],\n",
      "       [ 0.01020891,  0.00101381,  0.0003371 ]]), array([[ 0.00274108],\n",
      "       [ 0.00357206],\n",
      "       [-0.00093226]])]\n",
      "gradients_biases:  [array([ 0.00270078, -0.00250976, -0.00011667]), array([-0.00143623])]\n",
      "Iteration 636, Cost: 0.24764398166974216\n",
      "gradient_weights:  [array([[ 0.0030303 , -0.00253535,  0.00030572],\n",
      "       [ 0.01021758,  0.00101576,  0.00033808]]), array([[ 0.0027562 ],\n",
      "       [ 0.00357131],\n",
      "       [-0.0009331 ]])]\n",
      "gradients_biases:  [array([ 0.00270396, -0.00251225, -0.00011694]), array([-0.00143855])]\n",
      "Iteration 637, Cost: 0.24763608690696298\n",
      "gradient_weights:  [array([[ 0.00304097, -0.00253799,  0.00030524],\n",
      "       [ 0.01022626,  0.00101771,  0.00033906]]), array([[ 0.00277133],\n",
      "       [ 0.00357057],\n",
      "       [-0.00093395]])]\n",
      "gradients_biases:  [array([ 0.00270713, -0.00251474, -0.0001172 ]), array([-0.00144087])]\n",
      "Iteration 638, Cost: 0.2476281733743934\n",
      "gradient_weights:  [array([[ 0.00305165, -0.00254063,  0.00030476],\n",
      "       [ 0.01023496,  0.00101966,  0.00034004]]), array([[ 0.00278648],\n",
      "       [ 0.00356984],\n",
      "       [-0.00093479]])]\n",
      "gradients_biases:  [array([ 0.00271029, -0.00251723, -0.00011746]), array([-0.00144319])]\n",
      "Iteration 639, Cost: 0.2476202410050679\n",
      "gradient_weights:  [array([[ 0.00306234, -0.00254328,  0.00030428],\n",
      "       [ 0.01024367,  0.00102161,  0.00034103]]), array([[ 0.00280165],\n",
      "       [ 0.00356911],\n",
      "       [-0.00093564]])]\n",
      "gradients_biases:  [array([ 0.00271343, -0.00251971, -0.00011773]), array([-0.00144552])]\n",
      "Iteration 640, Cost: 0.24761228973190388\n",
      "gradient_weights:  [array([[ 0.00307304, -0.00254592,  0.0003038 ],\n",
      "       [ 0.0102524 ,  0.00102357,  0.00034201]]), array([[ 0.00281684],\n",
      "       [ 0.00356839],\n",
      "       [-0.0009365 ]])]\n",
      "gradients_biases:  [array([ 0.00271656, -0.0025222 , -0.00011799]), array([-0.00144785])]\n",
      "Iteration 641, Cost: 0.24760431948770134\n",
      "gradient_weights:  [array([[ 0.00308375, -0.00254857,  0.00030332],\n",
      "       [ 0.01026114,  0.00102553,  0.000343  ]]), array([[ 0.00283204],\n",
      "       [ 0.00356767],\n",
      "       [-0.00093735]])]\n",
      "gradients_biases:  [array([ 0.00271968, -0.00252468, -0.00011826]), array([-0.00145019])]\n",
      "Iteration 642, Cost: 0.247596330205143\n",
      "gradient_weights:  [array([[ 0.00309446, -0.00255122,  0.00030285],\n",
      "       [ 0.01026989,  0.00102749,  0.00034399]]), array([[ 0.00284726],\n",
      "       [ 0.00356697],\n",
      "       [-0.00093821]])]\n",
      "gradients_biases:  [array([ 0.00272279, -0.00252717, -0.00011852]), array([-0.00145252])]\n",
      "Iteration 643, Cost: 0.24758832181679435\n",
      "gradient_weights:  [array([[ 0.00310519, -0.00255388,  0.00030237],\n",
      "       [ 0.01027865,  0.00102945,  0.00034498]]), array([[ 0.0028625 ],\n",
      "       [ 0.00356626],\n",
      "       [-0.00093908]])]\n",
      "gradients_biases:  [array([ 0.00272588, -0.00252965, -0.00011879]), array([-0.00145487])]\n",
      "Iteration 644, Cost: 0.24758029425510347\n",
      "gradient_weights:  [array([[ 0.00311592, -0.00255653,  0.0003019 ],\n",
      "       [ 0.01028743,  0.00103142,  0.00034597]]), array([[ 0.00287776],\n",
      "       [ 0.00356557],\n",
      "       [-0.00093994]])]\n",
      "gradients_biases:  [array([ 0.00272897, -0.00253214, -0.00011905]), array([-0.00145721])]\n",
      "Iteration 645, Cost: 0.24757224745240103\n",
      "gradient_weights:  [array([[ 0.00312667, -0.00255919,  0.00030143],\n",
      "       [ 0.01029622,  0.00103338,  0.00034697]]), array([[ 0.00289304],\n",
      "       [ 0.00356488],\n",
      "       [-0.00094081]])]\n",
      "gradients_biases:  [array([ 0.00273204, -0.00253462, -0.00011932]), array([-0.00145956])]\n",
      "Iteration 646, Cost: 0.24756418134090014\n",
      "gradient_weights:  [array([[ 0.00313742, -0.00256185,  0.00030096],\n",
      "       [ 0.01030502,  0.00103535,  0.00034796]]), array([[ 0.00290833],\n",
      "       [ 0.0035642 ],\n",
      "       [-0.00094168]])]\n",
      "gradients_biases:  [array([ 0.00273509, -0.0025371 , -0.00011959]), array([-0.00146191])]\n",
      "Iteration 647, Cost: 0.24755609585269664\n",
      "gradient_weights:  [array([[ 0.00314818, -0.00256451,  0.00030049],\n",
      "       [ 0.01031384,  0.00103733,  0.00034896]]), array([[ 0.00292364],\n",
      "       [ 0.00356352],\n",
      "       [-0.00094256]])]\n",
      "gradients_biases:  [array([ 0.00273814, -0.00253958, -0.00011986]), array([-0.00146426])]\n",
      "Iteration 648, Cost: 0.2475479909197686\n",
      "gradient_weights:  [array([[ 0.00315896, -0.00256718,  0.00030002],\n",
      "       [ 0.01032266,  0.0010393 ,  0.00034996]]), array([[ 0.00293897],\n",
      "       [ 0.00356285],\n",
      "       [-0.00094344]])]\n",
      "gradients_biases:  [array([ 0.00274117, -0.00254206, -0.00012013]), array([-0.00146662])]\n",
      "Iteration 649, Cost: 0.24753986647397674\n",
      "gradient_weights:  [array([[ 0.00316974, -0.00256984,  0.00029955],\n",
      "       [ 0.01033151,  0.00104128,  0.00035096]]), array([[ 0.00295432],\n",
      "       [ 0.00356219],\n",
      "       [-0.00094432]])]\n",
      "gradients_biases:  [array([ 0.00274419, -0.00254454, -0.0001204 ]), array([-0.00146898])]\n",
      "Iteration 650, Cost: 0.247531722447064\n",
      "gradient_weights:  [array([[ 0.00318053, -0.00257251,  0.00029909],\n",
      "       [ 0.01034036,  0.00104326,  0.00035196]]), array([[ 0.00296968],\n",
      "       [ 0.00356153],\n",
      "       [-0.0009452 ]])]\n",
      "gradients_biases:  [array([ 0.0027472 , -0.00254702, -0.00012067]), array([-0.00147135])]\n",
      "Iteration 651, Cost: 0.24752355877065602\n",
      "gradient_weights:  [array([[ 0.00319133, -0.00257519,  0.00029862],\n",
      "       [ 0.01034923,  0.00104524,  0.00035296]]), array([[ 0.00298507],\n",
      "       [ 0.00356088],\n",
      "       [-0.00094609]])]\n",
      "gradients_biases:  [array([ 0.0027502 , -0.0025495 , -0.00012094]), array([-0.00147372])]\n",
      "Iteration 652, Cost: 0.24751537537626045\n",
      "gradient_weights:  [array([[ 0.00320214, -0.00257786,  0.00029816],\n",
      "       [ 0.01035811,  0.00104723,  0.00035397]]), array([[ 0.00300047],\n",
      "       [ 0.00356024],\n",
      "       [-0.00094698]])]\n",
      "gradients_biases:  [array([ 0.00275318, -0.00255198, -0.00012121]), array([-0.00147609])]\n",
      "Iteration 653, Cost: 0.24750717219526752\n",
      "gradient_weights:  [array([[ 0.00321296, -0.00258054,  0.0002977 ],\n",
      "       [ 0.010367  ,  0.00104922,  0.00035497]]), array([[ 0.00301589],\n",
      "       [ 0.0035596 ],\n",
      "       [-0.00094787]])]\n",
      "gradients_biases:  [array([ 0.00275615, -0.00255445, -0.00012148]), array([-0.00147846])]\n",
      "Iteration 654, Cost: 0.24749894915894996\n",
      "gradient_weights:  [array([[ 0.00322379, -0.00258322,  0.00029724],\n",
      "       [ 0.0103759 ,  0.0010512 ,  0.00035598]]), array([[ 0.00303133],\n",
      "       [ 0.00355897],\n",
      "       [-0.00094877]])]\n",
      "gradients_biases:  [array([ 0.00275911, -0.00255693, -0.00012175]), array([-0.00148084])]\n",
      "Iteration 655, Cost: 0.24749070619846264\n",
      "gradient_weights:  [array([[ 0.00323463, -0.0025859 ,  0.00029678],\n",
      "       [ 0.01038482,  0.0010532 ,  0.00035699]]), array([[ 0.00304678],\n",
      "       [ 0.00355834],\n",
      "       [-0.00094967]])]\n",
      "gradients_biases:  [array([ 0.00276205, -0.00255941, -0.00012203]), array([-0.00148322])]\n",
      "Iteration 656, Cost: 0.24748244324484286\n",
      "gradient_weights:  [array([[ 0.00324547, -0.00258858,  0.00029632],\n",
      "       [ 0.01039375,  0.00105519,  0.000358  ]]), array([[ 0.00306226],\n",
      "       [ 0.00355773],\n",
      "       [-0.00095057]])]\n",
      "gradients_biases:  [array([ 0.00276498, -0.00256188, -0.0001223 ]), array([-0.0014856])]\n",
      "Iteration 657, Cost: 0.24747416022901025\n",
      "gradient_weights:  [array([[ 0.00325633, -0.00259127,  0.00029587],\n",
      "       [ 0.0104027 ,  0.00105719,  0.00035902]]), array([[ 0.00307775],\n",
      "       [ 0.00355711],\n",
      "       [-0.00095147]])]\n",
      "gradients_biases:  [array([ 0.0027679 , -0.00256436, -0.00012258]), array([-0.00148799])]\n",
      "Iteration 658, Cost: 0.24746585708176705\n",
      "gradient_weights:  [array([[ 0.0032672 , -0.00259395,  0.00029541],\n",
      "       [ 0.01041165,  0.00105919,  0.00036003]]), array([[ 0.00309326],\n",
      "       [ 0.00355651],\n",
      "       [-0.00095238]])]\n",
      "gradients_biases:  [array([ 0.00277081, -0.00256683, -0.00012285]), array([-0.00149038])]\n",
      "Iteration 659, Cost: 0.24745753373379747\n",
      "gradient_weights:  [array([[ 0.00327807, -0.00259665,  0.00029496],\n",
      "       [ 0.01042062,  0.00106119,  0.00036105]]), array([[ 0.00310879],\n",
      "       [ 0.00355591],\n",
      "       [-0.00095329]])]\n",
      "gradients_biases:  [array([ 0.0027737 , -0.0025693 , -0.00012313]), array([-0.00149278])]\n",
      "Iteration 660, Cost: 0.2474491901156686\n",
      "gradient_weights:  [array([[ 0.00328896, -0.00259934,  0.00029451],\n",
      "       [ 0.0104296 ,  0.00106319,  0.00036206]]), array([[ 0.00312434],\n",
      "       [ 0.00355532],\n",
      "       [-0.00095421]])]\n",
      "gradients_biases:  [array([ 0.00277658, -0.00257178, -0.0001234 ]), array([-0.00149517])]\n",
      "Iteration 661, Cost: 0.24744082615782959\n",
      "gradient_weights:  [array([[ 0.00329986, -0.00260203,  0.00029406],\n",
      "       [ 0.0104386 ,  0.0010652 ,  0.00036308]]), array([[ 0.0031399 ],\n",
      "       [ 0.00355473],\n",
      "       [-0.00095512]])]\n",
      "gradients_biases:  [array([ 0.00277945, -0.00257425, -0.00012368]), array([-0.00149757])]\n",
      "Iteration 662, Cost: 0.24743244179061205\n",
      "gradient_weights:  [array([[ 0.00331076, -0.00260473,  0.00029361],\n",
      "       [ 0.0104476 ,  0.00106721,  0.0003641 ]]), array([[ 0.00315549],\n",
      "       [ 0.00355415],\n",
      "       [-0.00095605]])]\n",
      "gradients_biases:  [array([ 0.0027823 , -0.00257672, -0.00012396]), array([-0.00149998])]\n",
      "Iteration 663, Cost: 0.24742403694423026\n",
      "gradient_weights:  [array([[ 0.00332168, -0.00260743,  0.00029316],\n",
      "       [ 0.01045662,  0.00106922,  0.00036513]]), array([[ 0.00317109],\n",
      "       [ 0.00355358],\n",
      "       [-0.00095697]])]\n",
      "gradients_biases:  [array([ 0.00278514, -0.00257919, -0.00012424]), array([-0.00150239])]\n",
      "Iteration 664, Cost: 0.24741561154878086\n",
      "gradient_weights:  [array([[ 0.0033326 , -0.00261013,  0.00029271],\n",
      "       [ 0.01046565,  0.00107123,  0.00036615]]), array([[ 0.00318671],\n",
      "       [ 0.00355301],\n",
      "       [-0.00095789]])]\n",
      "gradients_biases:  [array([ 0.00278797, -0.00258166, -0.00012452]), array([-0.0015048])]\n",
      "Iteration 665, Cost: 0.2474071655342432\n",
      "gradient_weights:  [array([[ 0.00334354, -0.00261284,  0.00029227],\n",
      "       [ 0.0104747 ,  0.00107325,  0.00036718]]), array([[ 0.00320235],\n",
      "       [ 0.00355245],\n",
      "       [-0.00095882]])]\n",
      "gradients_biases:  [array([ 0.00279078, -0.00258413, -0.00012479]), array([-0.00150721])]\n",
      "Iteration 666, Cost: 0.24739869883047896\n",
      "gradient_weights:  [array([[ 0.00335448, -0.00261555,  0.00029182],\n",
      "       [ 0.01048375,  0.00107527,  0.0003682 ]]), array([[ 0.00321801],\n",
      "       [ 0.0035519 ],\n",
      "       [-0.00095976]])]\n",
      "gradients_biases:  [array([ 0.00279358, -0.0025866 , -0.00012507]), array([-0.00150963])]\n",
      "Iteration 667, Cost: 0.24739021136723255\n",
      "gradient_weights:  [array([[ 0.00336544, -0.00261826,  0.00029138],\n",
      "       [ 0.01049282,  0.00107729,  0.00036923]]), array([[ 0.00323368],\n",
      "       [ 0.00355135],\n",
      "       [-0.00096069]])]\n",
      "gradients_biases:  [array([ 0.00279637, -0.00258906, -0.00012536]), array([-0.00151205])]\n",
      "Iteration 668, Cost: 0.2473817030741311\n",
      "gradient_weights:  [array([[ 0.0033764 , -0.00262097,  0.00029094],\n",
      "       [ 0.0105019 ,  0.00107931,  0.00037026]]), array([[ 0.00324938],\n",
      "       [ 0.00355081],\n",
      "       [-0.00096163]])]\n",
      "gradients_biases:  [array([ 0.00279914, -0.00259153, -0.00012564]), array([-0.00151447])]\n",
      "Iteration 669, Cost: 0.2473731738806844\n",
      "gradient_weights:  [array([[ 0.00338737, -0.00262369,  0.0002905 ],\n",
      "       [ 0.010511  ,  0.00108134,  0.0003713 ]]), array([[ 0.00326509],\n",
      "       [ 0.00355027],\n",
      "       [-0.00096257]])]\n",
      "gradients_biases:  [array([ 0.0028019 , -0.002594  , -0.00012592]), array([-0.0015169])]\n",
      "Iteration 670, Cost: 0.24736462371628495\n",
      "gradient_weights:  [array([[ 0.00339836, -0.0026264 ,  0.00029006],\n",
      "       [ 0.0105201 ,  0.00108336,  0.00037233]]), array([[ 0.00328082],\n",
      "       [ 0.00354975],\n",
      "       [-0.00096352]])]\n",
      "gradients_biases:  [array([ 0.00280464, -0.00259646, -0.0001262 ]), array([-0.00151933])]\n",
      "Iteration 671, Cost: 0.24735605251020837\n",
      "gradient_weights:  [array([[ 0.00340935, -0.00262912,  0.00028962],\n",
      "       [ 0.01052922,  0.00108539,  0.00037337]]), array([[ 0.00329657],\n",
      "       [ 0.00354922],\n",
      "       [-0.00096447]])]\n",
      "gradients_biases:  [array([ 0.00280737, -0.00259893, -0.00012648]), array([-0.00152176])]\n",
      "Iteration 672, Cost: 0.2473474601916129\n",
      "gradient_weights:  [array([[ 0.00342035, -0.00263185,  0.00028918],\n",
      "       [ 0.01053835,  0.00108743,  0.00037441]]), array([[ 0.00331234],\n",
      "       [ 0.00354871],\n",
      "       [-0.00096542]])]\n",
      "gradients_biases:  [array([ 0.00281009, -0.00260139, -0.00012677]), array([-0.0015242])]\n",
      "Iteration 673, Cost: 0.24733884668953965\n",
      "gradient_weights:  [array([[ 0.00343137, -0.00263457,  0.00028875],\n",
      "       [ 0.0105475 ,  0.00108946,  0.00037545]]), array([[ 0.00332812],\n",
      "       [ 0.0035482 ],\n",
      "       [-0.00096637]])]\n",
      "gradients_biases:  [array([ 0.0028128 , -0.00260386, -0.00012705]), array([-0.00152664])]\n",
      "Iteration 674, Cost: 0.2473302119329132\n",
      "gradient_weights:  [array([[ 0.00344239, -0.0026373 ,  0.00028832],\n",
      "       [ 0.01055665,  0.0010915 ,  0.00037649]]), array([[ 0.00334393],\n",
      "       [ 0.0035477 ],\n",
      "       [-0.00096733]])]\n",
      "gradients_biases:  [array([ 0.00281549, -0.00260632, -0.00012734]), array([-0.00152908])]\n",
      "Iteration 675, Cost: 0.2473215558505409\n",
      "gradient_weights:  [array([[ 0.00345342, -0.00264003,  0.00028788],\n",
      "       [ 0.01056582,  0.00109354,  0.00037753]]), array([[ 0.00335975],\n",
      "       [ 0.0035472 ],\n",
      "       [-0.00096829]])]\n",
      "gradients_biases:  [array([ 0.00281816, -0.00260879, -0.00012762]), array([-0.00153153])]\n",
      "Iteration 676, Cost: 0.2473128783711133\n",
      "gradient_weights:  [array([[ 0.00346447, -0.00264276,  0.00028745],\n",
      "       [ 0.010575  ,  0.00109558,  0.00037857]]), array([[ 0.0033756 ],\n",
      "       [ 0.00354671],\n",
      "       [-0.00096925]])]\n",
      "gradients_biases:  [array([ 0.00282082, -0.00261125, -0.00012791]), array([-0.00153398])]\n",
      "Iteration 677, Cost: 0.2473041794232044\n",
      "gradient_weights:  [array([[ 0.00347552, -0.0026455 ,  0.00028702],\n",
      "       [ 0.01058419,  0.00109763,  0.00037962]]), array([[ 0.00339146],\n",
      "       [ 0.00354623],\n",
      "       [-0.00097022]])]\n",
      "gradients_biases:  [array([ 0.00282347, -0.00261371, -0.0001282 ]), array([-0.00153644])]\n",
      "Iteration 678, Cost: 0.24729545893527152\n",
      "gradient_weights:  [array([[ 0.00348658, -0.00264824,  0.00028659],\n",
      "       [ 0.01059339,  0.00109967,  0.00038067]]), array([[ 0.00340734],\n",
      "       [ 0.00354575],\n",
      "       [-0.00097119]])]\n",
      "gradients_biases:  [array([ 0.00282611, -0.00261617, -0.00012848]), array([-0.00153889])]\n",
      "Iteration 679, Cost: 0.24728671683565528\n",
      "gradient_weights:  [array([[ 0.00349766, -0.00265098,  0.00028617],\n",
      "       [ 0.01060261,  0.00110172,  0.00038172]]), array([[ 0.00342324],\n",
      "       [ 0.00354528],\n",
      "       [-0.00097216]])]\n",
      "gradients_biases:  [array([ 0.00282873, -0.00261863, -0.00012877]), array([-0.00154135])]\n",
      "Iteration 680, Cost: 0.24727795305258016\n",
      "gradient_weights:  [array([[ 0.00350874, -0.00265372,  0.00028574],\n",
      "       [ 0.01061183,  0.00110377,  0.00038277]]), array([[ 0.00343915],\n",
      "       [ 0.00354481],\n",
      "       [-0.00097314]])]\n",
      "gradients_biases:  [array([ 0.00283133, -0.00262109, -0.00012906]), array([-0.00154382])]\n",
      "Iteration 681, Cost: 0.247269167514154\n",
      "gradient_weights:  [array([[ 0.00351983, -0.00265647,  0.00028532],\n",
      "       [ 0.01062107,  0.00110583,  0.00038383]]), array([[ 0.00345509],\n",
      "       [ 0.00354436],\n",
      "       [-0.00097411]])]\n",
      "gradients_biases:  [array([ 0.00283392, -0.00262355, -0.00012935]), array([-0.00154628])]\n",
      "Iteration 682, Cost: 0.2472603601483686\n",
      "gradient_weights:  [array([[ 0.00353094, -0.00265921,  0.00028489],\n",
      "       [ 0.01063032,  0.00110788,  0.00038488]]), array([[ 0.00347104],\n",
      "       [ 0.0035439 ],\n",
      "       [-0.0009751 ]])]\n",
      "gradients_biases:  [array([ 0.0028365 , -0.00262601, -0.00012964]), array([-0.00154875])]\n",
      "Iteration 683, Cost: 0.24725153088309954\n",
      "gradient_weights:  [array([[ 0.00354205, -0.00266196,  0.00028447],\n",
      "       [ 0.01063959,  0.00110994,  0.00038594]]), array([[ 0.00348701],\n",
      "       [ 0.00354346],\n",
      "       [-0.00097608]])]\n",
      "gradients_biases:  [array([ 0.00283906, -0.00262847, -0.00012993]), array([-0.00155123])]\n",
      "Iteration 684, Cost: 0.24724267964610647\n",
      "gradient_weights:  [array([[ 0.00355317, -0.00266472,  0.00028405],\n",
      "       [ 0.01064886,  0.001112  ,  0.000387  ]]), array([[ 0.00350301],\n",
      "       [ 0.00354302],\n",
      "       [-0.00097707]])]\n",
      "gradients_biases:  [array([ 0.00284161, -0.00263092, -0.00013022]), array([-0.0015537])]\n",
      "Iteration 685, Cost: 0.24723380636503306\n",
      "gradient_weights:  [array([[ 0.00356431, -0.00266747,  0.00028363],\n",
      "       [ 0.01065815,  0.00111406,  0.00038806]]), array([[ 0.00351902],\n",
      "       [ 0.00354259],\n",
      "       [-0.00097806]])]\n",
      "gradients_biases:  [array([ 0.00284414, -0.00263338, -0.00013051]), array([-0.00155618])]\n",
      "Iteration 686, Cost: 0.24722491096740723\n",
      "gradient_weights:  [array([[ 0.00357545, -0.00267023,  0.00028322],\n",
      "       [ 0.01066745,  0.00111613,  0.00038912]]), array([[ 0.00353505],\n",
      "       [ 0.00354216],\n",
      "       [-0.00097905]])]\n",
      "gradients_biases:  [array([ 0.00284666, -0.00263584, -0.0001308 ]), array([-0.00155867])]\n",
      "Iteration 687, Cost: 0.24721599338064137\n",
      "gradient_weights:  [array([[ 0.0035866 , -0.002673  ,  0.0002828 ],\n",
      "       [ 0.01067676,  0.0011182 ,  0.00039018]]), array([[ 0.00355109],\n",
      "       [ 0.00354174],\n",
      "       [-0.00098005]])]\n",
      "gradients_biases:  [array([ 0.00284916, -0.00263829, -0.0001311 ]), array([-0.00156115])]\n",
      "Iteration 688, Cost: 0.24720705353203215\n",
      "gradient_weights:  [array([[ 0.00359777, -0.00267576,  0.00028238],\n",
      "       [ 0.01068608,  0.00112027,  0.00039125]]), array([[ 0.00356716],\n",
      "       [ 0.00354133],\n",
      "       [-0.00098105]])]\n",
      "gradients_biases:  [array([ 0.00285165, -0.00264075, -0.00013139]), array([-0.00156364])]\n",
      "Iteration 689, Cost: 0.24719809134876103\n",
      "gradient_weights:  [array([[ 0.00360894, -0.00267853,  0.00028197],\n",
      "       [ 0.01069542,  0.00112234,  0.00039232]]), array([[ 0.00358325],\n",
      "       [ 0.00354092],\n",
      "       [-0.00098206]])]\n",
      "gradients_biases:  [array([ 0.00285413, -0.0026432 , -0.00013169]), array([-0.00156614])]\n",
      "Iteration 690, Cost: 0.24718910675789427\n",
      "gradient_weights:  [array([[ 0.00362013, -0.0026813 ,  0.00028156],\n",
      "       [ 0.01070476,  0.00112442,  0.00039339]]), array([[ 0.00359935],\n",
      "       [ 0.00354052],\n",
      "       [-0.00098306]])]\n",
      "gradients_biases:  [array([ 0.00285659, -0.00264565, -0.00013198]), array([-0.00156864])]\n",
      "Iteration 691, Cost: 0.24718009968638277\n",
      "gradient_weights:  [array([[ 0.00363132, -0.00268407,  0.00028115],\n",
      "       [ 0.01071412,  0.00112649,  0.00039446]]), array([[ 0.00361547],\n",
      "       [ 0.00354013],\n",
      "       [-0.00098407]])]\n",
      "gradients_biases:  [array([ 0.00285903, -0.00264811, -0.00013228]), array([-0.00157114])]\n",
      "Iteration 692, Cost: 0.24717107006106287\n",
      "gradient_weights:  [array([[ 0.00364253, -0.00268684,  0.00028074],\n",
      "       [ 0.01072349,  0.00112857,  0.00039553]]), array([[ 0.00363161],\n",
      "       [ 0.00353974],\n",
      "       [-0.00098508]])]\n",
      "gradients_biases:  [array([ 0.00286146, -0.00265056, -0.00013257]), array([-0.00157364])]\n",
      "Iteration 693, Cost: 0.2471620178086558\n",
      "gradient_weights:  [array([[ 0.00365374, -0.00268962,  0.00028033],\n",
      "       [ 0.01073287,  0.00113065,  0.00039661]]), array([[ 0.00364778],\n",
      "       [ 0.00353936],\n",
      "       [-0.0009861 ]])]\n",
      "gradients_biases:  [array([ 0.00286388, -0.00265301, -0.00013287]), array([-0.00157615])]\n",
      "Iteration 694, Cost: 0.24715294285576828\n",
      "gradient_weights:  [array([[ 0.00366497, -0.0026924 ,  0.00027992],\n",
      "       [ 0.01074226,  0.00113274,  0.00039769]]), array([[ 0.00366396],\n",
      "       [ 0.00353898],\n",
      "       [-0.00098712]])]\n",
      "gradients_biases:  [array([ 0.00286628, -0.00265546, -0.00013317]), array([-0.00157866])]\n",
      "Iteration 695, Cost: 0.2471438451288926\n",
      "gradient_weights:  [array([[ 0.0036762 , -0.00269518,  0.00027952],\n",
      "       [ 0.01075167,  0.00113482,  0.00039877]]), array([[ 0.00368015],\n",
      "       [ 0.00353861],\n",
      "       [-0.00098814]])]\n",
      "gradients_biases:  [array([ 0.00286866, -0.00265791, -0.00013346]), array([-0.00158117])]\n",
      "Iteration 696, Cost: 0.24713472455440655\n",
      "gradient_weights:  [array([[ 0.00368745, -0.00269797,  0.00027911],\n",
      "       [ 0.01076108,  0.00113691,  0.00039985]]), array([[ 0.00369637],\n",
      "       [ 0.00353825],\n",
      "       [-0.00098917]])]\n",
      "gradients_biases:  [array([ 0.00287103, -0.00266036, -0.00013376]), array([-0.00158369])]\n",
      "Iteration 697, Cost: 0.24712558105857407\n",
      "gradient_weights:  [array([[ 0.0036987 , -0.00270076,  0.00027871],\n",
      "       [ 0.01077051,  0.001139  ,  0.00040093]]), array([[ 0.00371261],\n",
      "       [ 0.00353789],\n",
      "       [-0.0009902 ]])]\n",
      "gradients_biases:  [array([ 0.00287338, -0.00266281, -0.00013406]), array([-0.00158621])]\n",
      "Iteration 698, Cost: 0.24711641456754493\n",
      "gradient_weights:  [array([[ 0.00370997, -0.00270355,  0.00027831],\n",
      "       [ 0.01077995,  0.0011411 ,  0.00040202]]), array([[ 0.00372886],\n",
      "       [ 0.00353754],\n",
      "       [-0.00099123]])]\n",
      "gradients_biases:  [array([ 0.00287572, -0.00266526, -0.00013436]), array([-0.00158873])]\n",
      "Iteration 699, Cost: 0.24710722500735513\n",
      "gradient_weights:  [array([[ 0.00372125, -0.00270634,  0.00027791],\n",
      "       [ 0.0107894 ,  0.00114319,  0.0004031 ]]), array([[ 0.00374514],\n",
      "       [ 0.0035372 ],\n",
      "       [-0.00099226]])]\n",
      "gradients_biases:  [array([ 0.00287804, -0.00266771, -0.00013466]), array([-0.00159126])]\n",
      "Iteration 700, Cost: 0.24709801230392714\n",
      "gradient_weights:  [array([[ 0.00373254, -0.00270914,  0.00027751],\n",
      "       [ 0.01079886,  0.00114529,  0.00040419]]), array([[ 0.00376143],\n",
      "       [ 0.00353686],\n",
      "       [-0.0009933 ]])]\n",
      "gradients_biases:  [array([ 0.00288035, -0.00267016, -0.00013497]), array([-0.00159379])]\n",
      "Iteration 701, Cost: 0.24708877638306995\n",
      "gradient_weights:  [array([[ 0.00374383, -0.00271194,  0.00027711],\n",
      "       [ 0.01080833,  0.00114739,  0.00040528]]), array([[ 0.00377774],\n",
      "       [ 0.00353653],\n",
      "       [-0.00099434]])]\n",
      "gradients_biases:  [array([ 0.00288264, -0.0026726 , -0.00013527]), array([-0.00159632])]\n",
      "Iteration 702, Cost: 0.24707951717047946\n",
      "gradient_weights:  [array([[ 0.00375514, -0.00271474,  0.00027672],\n",
      "       [ 0.01081782,  0.00114949,  0.00040637]]), array([[ 0.00379407],\n",
      "       [ 0.00353621],\n",
      "       [-0.00099539]])]\n",
      "gradients_biases:  [array([ 0.00288492, -0.00267505, -0.00013557]), array([-0.00159886])]\n",
      "Iteration 703, Cost: 0.2470702345917383\n",
      "gradient_weights:  [array([[ 0.00376646, -0.00271755,  0.00027632],\n",
      "       [ 0.01082731,  0.0011516 ,  0.00040747]]), array([[ 0.00381042],\n",
      "       [ 0.00353589],\n",
      "       [-0.00099643]])]\n",
      "gradients_biases:  [array([ 0.00288718, -0.0026775 , -0.00013587]), array([-0.0016014])]\n",
      "Iteration 704, Cost: 0.24706092857231637\n",
      "gradient_weights:  [array([[ 0.00377779, -0.00272036,  0.00027593],\n",
      "       [ 0.01083682,  0.0011537 ,  0.00040856]]), array([[ 0.00382679],\n",
      "       [ 0.00353558],\n",
      "       [-0.00099749]])]\n",
      "gradients_biases:  [array([ 0.00288942, -0.00267994, -0.00013618]), array([-0.00160394])]\n",
      "Iteration 705, Cost: 0.24705159903757115\n",
      "gradient_weights:  [array([[ 0.00378913, -0.00272317,  0.00027554],\n",
      "       [ 0.01084634,  0.00115581,  0.00040966]]), array([[ 0.00384318],\n",
      "       [ 0.00353527],\n",
      "       [-0.00099854]])]\n",
      "gradients_biases:  [array([ 0.00289165, -0.00268239, -0.00013648]), array([-0.00160649])]\n",
      "Iteration 706, Cost: 0.2470422459127474\n",
      "gradient_weights:  [array([[ 0.00380048, -0.00272598,  0.00027515],\n",
      "       [ 0.01085586,  0.00115793,  0.00041076]]), array([[ 0.00385958],\n",
      "       [ 0.00353497],\n",
      "       [-0.0009996 ]])]\n",
      "gradients_biases:  [array([ 0.00289387, -0.00268483, -0.00013679]), array([-0.00160904])]\n",
      "Iteration 707, Cost: 0.2470328691229778\n",
      "gradient_weights:  [array([[ 0.00381184, -0.0027288 ,  0.00027476],\n",
      "       [ 0.0108654 ,  0.00116004,  0.00041186]]), array([[ 0.00387601],\n",
      "       [ 0.00353468],\n",
      "       [-0.00100066]])]\n",
      "gradients_biases:  [array([ 0.00289606, -0.00268727, -0.00013709]), array([-0.00161159])]\n",
      "Iteration 708, Cost: 0.24702346859328325\n",
      "gradient_weights:  [array([[ 0.00382321, -0.00273162,  0.00027437],\n",
      "       [ 0.01087496,  0.00116216,  0.00041297]]), array([[ 0.00389245],\n",
      "       [ 0.00353439],\n",
      "       [-0.00100172]])]\n",
      "gradients_biases:  [array([ 0.00289824, -0.00268972, -0.0001374 ]), array([-0.00161415])]\n",
      "Iteration 709, Cost: 0.2470140442485725\n",
      "gradient_weights:  [array([[ 0.00383459, -0.00273444,  0.00027398],\n",
      "       [ 0.01088452,  0.00116427,  0.00041407]]), array([[ 0.00390892],\n",
      "       [ 0.00353411],\n",
      "       [-0.00100279]])]\n",
      "gradients_biases:  [array([ 0.00290041, -0.00269216, -0.00013771]), array([-0.00161671])]\n",
      "Iteration 710, Cost: 0.2470045960136431\n",
      "gradient_weights:  [array([[ 0.00384599, -0.00273727,  0.0002736 ],\n",
      "       [ 0.01089409,  0.00116639,  0.00041518]]), array([[ 0.0039254 ],\n",
      "       [ 0.00353384],\n",
      "       [-0.00100386]])]\n",
      "gradients_biases:  [array([ 0.00290256, -0.0026946 , -0.00013801]), array([-0.00161927])]\n",
      "Iteration 711, Cost: 0.24699512381318106\n",
      "gradient_weights:  [array([[ 0.00385739, -0.00274009,  0.00027321],\n",
      "       [ 0.01090368,  0.00116852,  0.00041629]]), array([[ 0.0039419 ],\n",
      "       [ 0.00353357],\n",
      "       [-0.00100493]])]\n",
      "gradients_biases:  [array([ 0.00290469, -0.00269704, -0.00013832]), array([-0.00162184])]\n",
      "Iteration 712, Cost: 0.24698562757176146\n",
      "gradient_weights:  [array([[ 0.0038688 , -0.00274292,  0.00027283],\n",
      "       [ 0.01091327,  0.00117064,  0.0004174 ]]), array([[ 0.00395842],\n",
      "       [ 0.00353331],\n",
      "       [-0.001006  ]])]\n",
      "gradients_biases:  [array([ 0.00290681, -0.00269948, -0.00013863]), array([-0.00162441])]\n",
      "Iteration 713, Cost: 0.2469761072138486\n",
      "gradient_weights:  [array([[ 0.00388023, -0.00274576,  0.00027245],\n",
      "       [ 0.01092288,  0.00117277,  0.00041852]]), array([[ 0.00397496],\n",
      "       [ 0.00353305],\n",
      "       [-0.00100708]])]\n",
      "gradients_biases:  [array([ 0.00290891, -0.00270192, -0.00013894]), array([-0.00162698])]\n",
      "Iteration 714, Cost: 0.24696656266379613\n",
      "gradient_weights:  [array([[ 0.00389166, -0.00274859,  0.00027207],\n",
      "       [ 0.01093249,  0.0011749 ,  0.00041963]]), array([[ 0.00399152],\n",
      "       [ 0.0035328 ],\n",
      "       [-0.00100817]])]\n",
      "gradients_biases:  [array([ 0.00291099, -0.00270436, -0.00013925]), array([-0.00162956])]\n",
      "Iteration 715, Cost: 0.2469569938458473\n",
      "gradient_weights:  [array([[ 0.0039031 , -0.00275143,  0.00027169],\n",
      "       [ 0.01094212,  0.00117703,  0.00042075]]), array([[ 0.0040081 ],\n",
      "       [ 0.00353256],\n",
      "       [-0.00100925]])]\n",
      "gradients_biases:  [array([ 0.00291306, -0.0027068 , -0.00013956]), array([-0.00163214])]\n",
      "Iteration 716, Cost: 0.2469474006841353\n",
      "gradient_weights:  [array([[ 0.00391456, -0.00275428,  0.00027132],\n",
      "       [ 0.01095176,  0.00117917,  0.00042187]]), array([[ 0.00402469],\n",
      "       [ 0.00353232],\n",
      "       [-0.00101034]])]\n",
      "gradients_biases:  [array([ 0.00291511, -0.00270924, -0.00013988]), array([-0.00163472])]\n",
      "Iteration 717, Cost: 0.2469377831026835\n",
      "gradient_weights:  [array([[ 0.00392603, -0.00275712,  0.00027094],\n",
      "       [ 0.01096141,  0.0011813 ,  0.00042299]]), array([[ 0.00404131],\n",
      "       [ 0.00353209],\n",
      "       [-0.00101143]])]\n",
      "gradients_biases:  [array([ 0.00291714, -0.00271168, -0.00014019]), array([-0.00163731])]\n",
      "Iteration 718, Cost: 0.24692814102540597\n",
      "gradient_weights:  [array([[ 0.0039375 , -0.00275997,  0.00027057],\n",
      "       [ 0.01097107,  0.00118344,  0.00042412]]), array([[ 0.00405794],\n",
      "       [ 0.00353187],\n",
      "       [-0.00101253]])]\n",
      "gradients_biases:  [array([ 0.00291916, -0.00271412, -0.0001405 ]), array([-0.0016399])]\n",
      "Iteration 719, Cost: 0.24691847437610726\n",
      "gradient_weights:  [array([[ 0.00394899, -0.00276282,  0.00027019],\n",
      "       [ 0.01098074,  0.00118558,  0.00042524]]), array([[ 0.0040746 ],\n",
      "       [ 0.00353165],\n",
      "       [-0.00101363]])]\n",
      "gradients_biases:  [array([ 0.00292116, -0.00271655, -0.00014081]), array([-0.00164249])]\n",
      "Iteration 720, Cost: 0.24690878307848274\n",
      "gradient_weights:  [array([[ 0.00396049, -0.00276568,  0.00026982],\n",
      "       [ 0.01099042,  0.00118773,  0.00042637]]), array([[ 0.00409127],\n",
      "       [ 0.00353144],\n",
      "       [-0.00101473]])]\n",
      "gradients_biases:  [array([ 0.00292314, -0.00271899, -0.00014113]), array([-0.00164509])]\n",
      "Iteration 721, Cost: 0.24689906705611922\n",
      "gradient_weights:  [array([[ 0.003972  , -0.00276853,  0.00026945],\n",
      "       [ 0.01100012,  0.00118987,  0.0004275 ]]), array([[ 0.00410796],\n",
      "       [ 0.00353124],\n",
      "       [-0.00101583]])]\n",
      "gradients_biases:  [array([ 0.00292511, -0.00272142, -0.00014144]), array([-0.00164769])]\n",
      "Iteration 722, Cost: 0.2468893262324951\n",
      "gradient_weights:  [array([[ 0.00398352, -0.00277139,  0.00026908],\n",
      "       [ 0.01100982,  0.00119202,  0.00042863]]), array([[ 0.00412467],\n",
      "       [ 0.00353104],\n",
      "       [-0.00101694]])]\n",
      "gradients_biases:  [array([ 0.00292706, -0.00272386, -0.00014176]), array([-0.00165029])]\n",
      "Iteration 723, Cost: 0.24687956053098062\n",
      "gradient_weights:  [array([[ 0.00399505, -0.00277426,  0.00026872],\n",
      "       [ 0.01101953,  0.00119417,  0.00042977]]), array([[ 0.0041414 ],\n",
      "       [ 0.00353085],\n",
      "       [-0.00101805]])]\n",
      "gradients_biases:  [array([ 0.00292899, -0.00272629, -0.00014208]), array([-0.0016529])]\n",
      "Iteration 724, Cost: 0.2468697698748379\n",
      "gradient_weights:  [array([[ 0.00400659, -0.00277712,  0.00026835],\n",
      "       [ 0.01102926,  0.00119632,  0.0004309 ]]), array([[ 0.00415815],\n",
      "       [ 0.00353066],\n",
      "       [-0.00101917]])]\n",
      "gradients_biases:  [array([ 0.0029309 , -0.00272873, -0.00014239]), array([-0.00165551])]\n",
      "Iteration 725, Cost: 0.24685995418722173\n",
      "gradient_weights:  [array([[ 0.00401814, -0.00277999,  0.00026799],\n",
      "       [ 0.01103899,  0.00119848,  0.00043204]]), array([[ 0.00417492],\n",
      "       [ 0.00353048],\n",
      "       [-0.00102028]])]\n",
      "gradients_biases:  [array([ 0.0029328 , -0.00273116, -0.00014271]), array([-0.00165812])]\n",
      "Iteration 726, Cost: 0.2468501133911794\n",
      "gradient_weights:  [array([[ 0.0040297 , -0.00278287,  0.00026762],\n",
      "       [ 0.01104874,  0.00120064,  0.00043318]]), array([[ 0.00419171],\n",
      "       [ 0.00353031],\n",
      "       [-0.00102141]])]\n",
      "gradients_biases:  [array([ 0.00293468, -0.00273359, -0.00014303]), array([-0.00166074])]\n",
      "Iteration 727, Cost: 0.24684024740965133\n",
      "gradient_weights:  [array([[ 0.00404128, -0.00278574,  0.00026726],\n",
      "       [ 0.01105849,  0.00120279,  0.00043433]]), array([[ 0.00420851],\n",
      "       [ 0.00353014],\n",
      "       [-0.00102253]])]\n",
      "gradients_biases:  [array([ 0.00293655, -0.00273603, -0.00014335]), array([-0.00166336])]\n",
      "Iteration 728, Cost: 0.24683035616547122\n",
      "gradient_weights:  [array([[ 0.00405286, -0.00278862,  0.0002669 ],\n",
      "       [ 0.01106826,  0.00120496,  0.00043547]]), array([[ 0.00422534],\n",
      "       [ 0.00352998],\n",
      "       [-0.00102366]])]\n",
      "gradients_biases:  [array([ 0.00293839, -0.00273846, -0.00014367]), array([-0.00166598])]\n",
      "Iteration 729, Cost: 0.24682043958136643\n",
      "gradient_weights:  [array([[ 0.00406445, -0.0027915 ,  0.00026654],\n",
      "       [ 0.01107803,  0.00120712,  0.00043662]]), array([[ 0.00424218],\n",
      "       [ 0.00352982],\n",
      "       [-0.00102479]])]\n",
      "gradients_biases:  [array([ 0.00294022, -0.00274089, -0.00014399]), array([-0.00166861])]\n",
      "Iteration 730, Cost: 0.2468104975799581\n",
      "gradient_weights:  [array([[ 0.00407606, -0.00279438,  0.00026618],\n",
      "       [ 0.01108782,  0.00120929,  0.00043777]]), array([[ 0.00425905],\n",
      "       [ 0.00352967],\n",
      "       [-0.00102592]])]\n",
      "gradients_biases:  [array([ 0.00294203, -0.00274332, -0.00014431]), array([-0.00167124])]\n",
      "Iteration 731, Cost: 0.2468005300837619\n",
      "gradient_weights:  [array([[ 0.00408768, -0.00279727,  0.00026583],\n",
      "       [ 0.01109762,  0.00121145,  0.00043892]]), array([[ 0.00427593],\n",
      "       [ 0.00352953],\n",
      "       [-0.00102706]])]\n",
      "gradients_biases:  [array([ 0.00294383, -0.00274575, -0.00014463]), array([-0.00167388])]\n",
      "Iteration 732, Cost: 0.24679053701518777\n",
      "gradient_weights:  [array([[ 0.0040993 , -0.00280016,  0.00026547],\n",
      "       [ 0.01110742,  0.00121362,  0.00044007]]), array([[ 0.00429283],\n",
      "       [ 0.0035294 ],\n",
      "       [-0.0010282 ]])]\n",
      "gradients_biases:  [array([ 0.0029456 , -0.00274818, -0.00014496]), array([-0.00167651])]\n",
      "Iteration 733, Cost: 0.24678051829654088\n",
      "gradient_weights:  [array([[ 0.00411094, -0.00280306,  0.00026512],\n",
      "       [ 0.01111724,  0.0012158 ,  0.00044122]]), array([[ 0.00430975],\n",
      "       [ 0.00352927],\n",
      "       [-0.00102934]])]\n",
      "gradients_biases:  [array([ 0.00294736, -0.00275061, -0.00014528]), array([-0.00167915])]\n",
      "Iteration 734, Cost: 0.24677047385002143\n",
      "gradient_weights:  [array([[ 0.00412259, -0.00280595,  0.00026477],\n",
      "       [ 0.01112707,  0.00121797,  0.00044238]]), array([[ 0.00432669],\n",
      "       [ 0.00352914],\n",
      "       [-0.00103049]])]\n",
      "gradients_biases:  [array([ 0.0029491 , -0.00275303, -0.0001456 ]), array([-0.0016818])]\n",
      "Iteration 735, Cost: 0.2467604035977254\n",
      "gradient_weights:  [array([[ 0.00413425, -0.00280885,  0.00026442],\n",
      "       [ 0.01113691,  0.00122015,  0.00044354]]), array([[ 0.00434365],\n",
      "       [ 0.00352903],\n",
      "       [-0.00103164]])]\n",
      "gradients_biases:  [array([ 0.00295083, -0.00275546, -0.00014593]), array([-0.00168445])]\n",
      "Iteration 736, Cost: 0.2467503074616444\n",
      "gradient_weights:  [array([[ 0.00414592, -0.00281176,  0.00026407],\n",
      "       [ 0.01114675,  0.00122233,  0.0004447 ]]), array([[ 0.00436063],\n",
      "       [ 0.00352892],\n",
      "       [-0.00103279]])]\n",
      "gradients_biases:  [array([ 0.00295253, -0.00275789, -0.00014625]), array([-0.0016871])]\n",
      "Iteration 737, Cost: 0.24674018536366665\n",
      "gradient_weights:  [array([[ 0.0041576 , -0.00281466,  0.00026372],\n",
      "       [ 0.01115661,  0.00122451,  0.00044587]]), array([[ 0.00437763],\n",
      "       [ 0.00352881],\n",
      "       [-0.00103395]])]\n",
      "gradients_biases:  [array([ 0.00295422, -0.00276032, -0.00014658]), array([-0.00168975])]\n",
      "Iteration 738, Cost: 0.24673003722557674\n",
      "gradient_weights:  [array([[ 0.00416929, -0.00281757,  0.00026337],\n",
      "       [ 0.01116648,  0.00122669,  0.00044703]]), array([[ 0.00439464],\n",
      "       [ 0.00352871],\n",
      "       [-0.00103511]])]\n",
      "gradients_biases:  [array([ 0.00295589, -0.00276274, -0.00014691]), array([-0.00169241])]\n",
      "Iteration 739, Cost: 0.2467198629690565\n",
      "gradient_weights:  [array([[ 0.004181  , -0.00282048,  0.00026303],\n",
      "       [ 0.01117636,  0.00122888,  0.0004482 ]]), array([[ 0.00441168],\n",
      "       [ 0.00352862],\n",
      "       [-0.00103627]])]\n",
      "gradients_biases:  [array([ 0.00295754, -0.00276517, -0.00014723]), array([-0.00169507])]\n",
      "Iteration 740, Cost: 0.2467096625156849\n",
      "gradient_weights:  [array([[ 0.00419271, -0.0028234 ,  0.00026268],\n",
      "       [ 0.01118624,  0.00123107,  0.00044937]]), array([[ 0.00442873],\n",
      "       [ 0.00352853],\n",
      "       [-0.00103744]])]\n",
      "gradients_biases:  [array([ 0.00295917, -0.00276759, -0.00014756]), array([-0.00169773])]\n",
      "Iteration 741, Cost: 0.24669943578693876\n",
      "gradient_weights:  [array([[ 0.00420443, -0.00282632,  0.00026234],\n",
      "       [ 0.01119614,  0.00123326,  0.00045054]]), array([[ 0.00444581],\n",
      "       [ 0.00352845],\n",
      "       [-0.00103861]])]\n",
      "gradients_biases:  [array([ 0.00296078, -0.00277002, -0.00014789]), array([-0.0017004])]\n",
      "Iteration 742, Cost: 0.24668918270419282\n",
      "gradient_weights:  [array([[ 0.00421617, -0.00282924,  0.000262  ],\n",
      "       [ 0.01120605,  0.00123545,  0.00045172]]), array([[ 0.0044629 ],\n",
      "       [ 0.00352838],\n",
      "       [-0.00103978]])]\n",
      "gradients_biases:  [array([ 0.00296238, -0.00277244, -0.00014822]), array([-0.00170307])]\n",
      "Iteration 743, Cost: 0.24667890318872054\n",
      "gradient_weights:  [array([[ 0.00422792, -0.00283216,  0.00026166],\n",
      "       [ 0.01121596,  0.00123764,  0.00045289]]), array([[ 0.00448001],\n",
      "       [ 0.00352831],\n",
      "       [-0.00104096]])]\n",
      "gradients_biases:  [array([ 0.00296396, -0.00277486, -0.00014855]), array([-0.00170575])]\n",
      "Iteration 744, Cost: 0.24666859716169387\n",
      "gradient_weights:  [array([[ 0.00423967, -0.00283509,  0.00026132],\n",
      "       [ 0.01122589,  0.00123984,  0.00045407]]), array([[ 0.00449714],\n",
      "       [ 0.00352825],\n",
      "       [-0.00104214]])]\n",
      "gradients_biases:  [array([ 0.00296551, -0.00277729, -0.00014888]), array([-0.00170842])]\n",
      "Iteration 745, Cost: 0.24665826454418438\n",
      "gradient_weights:  [array([[ 0.00425144, -0.00283802,  0.00026099],\n",
      "       [ 0.01123583,  0.00124204,  0.00045525]]), array([[ 0.00451429],\n",
      "       [ 0.0035282 ],\n",
      "       [-0.00104332]])]\n",
      "gradients_biases:  [array([ 0.00296705, -0.00277971, -0.00014921]), array([-0.0017111])]\n",
      "Iteration 746, Cost: 0.24664790525716296\n",
      "gradient_weights:  [array([[ 0.00426322, -0.00284096,  0.00026065],\n",
      "       [ 0.01124577,  0.00124424,  0.00045644]]), array([[ 0.00453146],\n",
      "       [ 0.00352815],\n",
      "       [-0.00104451]])]\n",
      "gradients_biases:  [array([ 0.00296858, -0.00278213, -0.00014955]), array([-0.00171379])]\n",
      "Iteration 747, Cost: 0.24663751922150062\n",
      "gradient_weights:  [array([[ 0.00427501, -0.00284389,  0.00026032],\n",
      "       [ 0.01125573,  0.00124645,  0.00045762]]), array([[ 0.00454865],\n",
      "       [ 0.00352811],\n",
      "       [-0.0010457 ]])]\n",
      "gradients_biases:  [array([ 0.00297008, -0.00278455, -0.00014988]), array([-0.00171648])]\n",
      "Iteration 748, Cost: 0.24662710635796867\n",
      "gradient_weights:  [array([[ 0.00428681, -0.00284684,  0.00025998],\n",
      "       [ 0.01126569,  0.00124865,  0.00045881]]), array([[ 0.00456585],\n",
      "       [ 0.00352807],\n",
      "       [-0.00104689]])]\n",
      "gradients_biases:  [array([ 0.00297156, -0.00278697, -0.00015021]), array([-0.00171917])]\n",
      "Iteration 749, Cost: 0.24661666658723935\n",
      "gradient_weights:  [array([[ 0.00429862, -0.00284978,  0.00025965],\n",
      "       [ 0.01127567,  0.00125086,  0.00046   ]]), array([[ 0.00458308],\n",
      "       [ 0.00352804],\n",
      "       [-0.00104809]])]\n",
      "gradients_biases:  [array([ 0.00297303, -0.00278939, -0.00015055]), array([-0.00172186])]\n",
      "Iteration 750, Cost: 0.24660619982988613\n",
      "gradient_weights:  [array([[ 0.00431045, -0.00285273,  0.00025932],\n",
      "       [ 0.01128565,  0.00125307,  0.00046119]]), array([[ 0.00460033],\n",
      "       [ 0.00352802],\n",
      "       [-0.00104929]])]\n",
      "gradients_biases:  [array([ 0.00297447, -0.00279181, -0.00015088]), array([-0.00172456])]\n",
      "Iteration 751, Cost: 0.24659570600638392\n",
      "gradient_weights:  [array([[ 0.00432228, -0.00285568,  0.00025899],\n",
      "       [ 0.01129564,  0.00125528,  0.00046239]]), array([[ 0.00461759],\n",
      "       [ 0.003528  ],\n",
      "       [-0.00105049]])]\n",
      "gradients_biases:  [array([ 0.0029759 , -0.00279423, -0.00015122]), array([-0.00172726])]\n",
      "Iteration 752, Cost: 0.2465851850371098\n",
      "gradient_weights:  [array([[ 0.00433412, -0.00285863,  0.00025867],\n",
      "       [ 0.01130564,  0.0012575 ,  0.00046359]]), array([[ 0.00463487],\n",
      "       [ 0.00352799],\n",
      "       [-0.0010517 ]])]\n",
      "gradients_biases:  [array([ 0.00297731, -0.00279665, -0.00015156]), array([-0.00172997])]\n",
      "Iteration 753, Cost: 0.24657463684234326\n",
      "gradient_weights:  [array([[ 0.00434598, -0.00286159,  0.00025834],\n",
      "       [ 0.01131566,  0.00125971,  0.00046478]]), array([[ 0.00465217],\n",
      "       [ 0.00352798],\n",
      "       [-0.00105291]])]\n",
      "gradients_biases:  [array([ 0.00297869, -0.00279907, -0.00015189]), array([-0.00173268])]\n",
      "Iteration 754, Cost: 0.2465640613422666\n",
      "gradient_weights:  [array([[ 0.00435785, -0.00286455,  0.00025802],\n",
      "       [ 0.01132568,  0.00126193,  0.00046599]]), array([[ 0.0046695 ],\n",
      "       [ 0.00352799],\n",
      "       [-0.00105412]])]\n",
      "gradients_biases:  [array([ 0.00298006, -0.00280148, -0.00015223]), array([-0.00173539])]\n",
      "Iteration 755, Cost: 0.2465534584569656\n",
      "gradient_weights:  [array([[ 0.00436972, -0.00286751,  0.0002577 ],\n",
      "       [ 0.01133571,  0.00126415,  0.00046719]]), array([[ 0.00468684],\n",
      "       [ 0.00352799],\n",
      "       [-0.00105534]])]\n",
      "gradients_biases:  [array([ 0.00298141, -0.0028039 , -0.00015257]), array([-0.0017381])]\n",
      "Iteration 756, Cost: 0.24654282810642963\n",
      "gradient_weights:  [array([[ 0.00438161, -0.00287048,  0.00025737],\n",
      "       [ 0.01134575,  0.00126637,  0.0004684 ]]), array([[ 0.0047042 ],\n",
      "       [ 0.00352801],\n",
      "       [-0.00105656]])]\n",
      "gradients_biases:  [array([ 0.00298274, -0.00280632, -0.00015291]), array([-0.00174082])]\n",
      "Iteration 757, Cost: 0.24653217021055224\n",
      "gradient_weights:  [array([[ 0.00439351, -0.00287345,  0.00025705],\n",
      "       [ 0.0113558 ,  0.0012686 ,  0.0004696 ]]), array([[ 0.00472157],\n",
      "       [ 0.00352803],\n",
      "       [-0.00105778]])]\n",
      "gradients_biases:  [array([ 0.00298405, -0.00280873, -0.00015325]), array([-0.00174354])]\n",
      "Iteration 758, Cost: 0.24652148468913154\n",
      "gradient_weights:  [array([[ 0.00440542, -0.00287642,  0.00025674],\n",
      "       [ 0.01136585,  0.00127083,  0.00047081]]), array([[ 0.00473897],\n",
      "       [ 0.00352805],\n",
      "       [-0.001059  ]])]\n",
      "gradients_biases:  [array([ 0.00298535, -0.00281115, -0.00015359]), array([-0.00174627])]\n",
      "Iteration 759, Cost: 0.24651077146187095\n",
      "gradient_weights:  [array([[ 0.00441734, -0.0028794 ,  0.00025642],\n",
      "       [ 0.01137592,  0.00127306,  0.00047203]]), array([[ 0.00475639],\n",
      "       [ 0.00352809],\n",
      "       [-0.00106023]])]\n",
      "gradients_biases:  [array([ 0.00298662, -0.00281356, -0.00015393]), array([-0.001749])]\n",
      "Iteration 760, Cost: 0.24650003044837906\n",
      "gradient_weights:  [array([[ 0.00442928, -0.00288238,  0.0002561 ],\n",
      "       [ 0.011386  ,  0.00127529,  0.00047324]]), array([[ 0.00477382],\n",
      "       [ 0.00352812],\n",
      "       [-0.00106147]])]\n",
      "gradients_biases:  [array([ 0.00298787, -0.00281598, -0.00015427]), array([-0.00175173])]\n",
      "Iteration 761, Cost: 0.24648926156817075\n",
      "gradient_weights:  [array([[ 0.00444122, -0.00288536,  0.00025579],\n",
      "       [ 0.01139608,  0.00127752,  0.00047446]]), array([[ 0.00479128],\n",
      "       [ 0.00352817],\n",
      "       [-0.0010627 ]])]\n",
      "gradients_biases:  [array([ 0.0029891 , -0.00281839, -0.00015462]), array([-0.00175447])]\n",
      "Iteration 762, Cost: 0.24647846474066704\n",
      "gradient_weights:  [array([[ 0.00445318, -0.00288835,  0.00025548],\n",
      "       [ 0.01140618,  0.00127976,  0.00047568]]), array([[ 0.00480875],\n",
      "       [ 0.00352822],\n",
      "       [-0.00106394]])]\n",
      "gradients_biases:  [array([ 0.00299031, -0.0028208 , -0.00015496]), array([-0.00175721])]\n",
      "Iteration 763, Cost: 0.24646763988519593\n",
      "gradient_weights:  [array([[ 0.00446514, -0.00289134,  0.00025517],\n",
      "       [ 0.01141628,  0.001282  ,  0.0004769 ]]), array([[ 0.00482624],\n",
      "       [ 0.00352828],\n",
      "       [-0.00106519]])]\n",
      "gradients_biases:  [array([ 0.00299151, -0.00282322, -0.00015531]), array([-0.00175995])]\n",
      "Iteration 764, Cost: 0.24645678692099293\n",
      "gradient_weights:  [array([[ 0.00447712, -0.00289433,  0.00025486],\n",
      "       [ 0.01142639,  0.00128424,  0.00047813]]), array([[ 0.00484375],\n",
      "       [ 0.00352834],\n",
      "       [-0.00106643]])]\n",
      "gradients_biases:  [array([ 0.00299268, -0.00282563, -0.00015565]), array([-0.00176269])]\n",
      "Iteration 765, Cost: 0.2464459057672011\n",
      "gradient_weights:  [array([[ 0.00448911, -0.00289733,  0.00025455],\n",
      "       [ 0.01143651,  0.00128648,  0.00047935]]), array([[ 0.00486128],\n",
      "       [ 0.00352841],\n",
      "       [-0.00106768]])]\n",
      "gradients_biases:  [array([ 0.00299383, -0.00282804, -0.000156  ]), array([-0.00176544])]\n",
      "Iteration 766, Cost: 0.24643499634287197\n",
      "gradient_weights:  [array([[ 0.0045011 , -0.00290033,  0.00025424],\n",
      "       [ 0.01144664,  0.00128872,  0.00048058]]), array([[ 0.00487883],\n",
      "       [ 0.00352848],\n",
      "       [-0.00106894]])]\n",
      "gradients_biases:  [array([ 0.00299496, -0.00283045, -0.00015634]), array([-0.0017682])]\n",
      "Iteration 767, Cost: 0.24642405856696584\n",
      "gradient_weights:  [array([[ 0.00451311, -0.00290333,  0.00025393],\n",
      "       [ 0.01145678,  0.00129097,  0.00048181]]), array([[ 0.0048964 ],\n",
      "       [ 0.00352857],\n",
      "       [-0.00107019]])]\n",
      "gradients_biases:  [array([ 0.00299607, -0.00283286, -0.00015669]), array([-0.00177095])]\n",
      "Iteration 768, Cost: 0.2464130923583523\n",
      "gradient_weights:  [array([[ 0.00452513, -0.00290634,  0.00025363],\n",
      "       [ 0.01146692,  0.00129322,  0.00048305]]), array([[ 0.00491399],\n",
      "       [ 0.00352865],\n",
      "       [-0.00107145]])]\n",
      "gradients_biases:  [array([ 0.00299717, -0.00283527, -0.00015704]), array([-0.00177371])]\n",
      "Iteration 769, Cost: 0.2464020976358105\n",
      "gradient_weights:  [array([[ 0.00453717, -0.00290935,  0.00025333],\n",
      "       [ 0.01147708,  0.00129547,  0.00048429]]), array([[ 0.00493159],\n",
      "       [ 0.00352875],\n",
      "       [-0.00107272]])]\n",
      "gradients_biases:  [array([ 0.00299824, -0.00283768, -0.00015739]), array([-0.00177647])]\n",
      "Iteration 770, Cost: 0.24639107431803015\n",
      "gradient_weights:  [array([[ 0.00454921, -0.00291236,  0.00025303],\n",
      "       [ 0.01148724,  0.00129772,  0.00048552]]), array([[ 0.00494922],\n",
      "       [ 0.00352885],\n",
      "       [-0.00107398]])]\n",
      "gradients_biases:  [array([ 0.00299929, -0.00284009, -0.00015774]), array([-0.00177924])]\n",
      "Iteration 771, Cost: 0.24638002232361142\n",
      "gradient_weights:  [array([[ 0.00456126, -0.00291538,  0.00025273],\n",
      "       [ 0.01149741,  0.00129998,  0.00048677]]), array([[ 0.00496686],\n",
      "       [ 0.00352896],\n",
      "       [-0.00107525]])]\n",
      "gradients_biases:  [array([ 0.00300032, -0.00284249, -0.00015809]), array([-0.00178201])]\n",
      "Iteration 772, Cost: 0.24636894157106587\n",
      "gradient_weights:  [array([[ 0.00457333, -0.0029184 ,  0.00025243],\n",
      "       [ 0.01150759,  0.00130224,  0.00048801]]), array([[ 0.00498452],\n",
      "       [ 0.00352907],\n",
      "       [-0.00107653]])]\n",
      "gradients_biases:  [array([ 0.00300133, -0.0028449 , -0.00015844]), array([-0.00178478])]\n",
      "Iteration 773, Cost: 0.24635783197881678\n",
      "gradient_weights:  [array([[ 0.0045854 , -0.00292142,  0.00025213],\n",
      "       [ 0.01151778,  0.0013045 ,  0.00048925]]), array([[ 0.0050022 ],\n",
      "       [ 0.00352919],\n",
      "       [-0.0010778 ]])]\n",
      "gradients_biases:  [array([ 0.00300231, -0.00284731, -0.00015879]), array([-0.00178756])]\n",
      "Iteration 774, Cost: 0.24634669346519972\n",
      "gradient_weights:  [array([[ 0.00459749, -0.00292445,  0.00025184],\n",
      "       [ 0.01152798,  0.00130676,  0.0004905 ]]), array([[ 0.0050199 ],\n",
      "       [ 0.00352931],\n",
      "       [-0.00107908]])]\n",
      "gradients_biases:  [array([ 0.00300328, -0.00284972, -0.00015914]), array([-0.00179034])]\n",
      "Iteration 775, Cost: 0.24633552594846303\n",
      "gradient_weights:  [array([[ 0.00460959, -0.00292748,  0.00025154],\n",
      "       [ 0.01153819,  0.00130902,  0.00049175]]), array([[ 0.00503762],\n",
      "       [ 0.00352944],\n",
      "       [-0.00108037]])]\n",
      "gradients_biases:  [array([ 0.00300423, -0.00285212, -0.0001595 ]), array([-0.00179313])]\n",
      "Iteration 776, Cost: 0.2463243293467683\n",
      "gradient_weights:  [array([[ 0.0046217 , -0.00293052,  0.00025125],\n",
      "       [ 0.0115484 ,  0.00131129,  0.00049301]]), array([[ 0.00505536],\n",
      "       [ 0.00352958],\n",
      "       [-0.00108165]])]\n",
      "gradients_biases:  [array([ 0.00300515, -0.00285453, -0.00015985]), array([-0.00179591])]\n",
      "Iteration 777, Cost: 0.246313103578191\n",
      "gradient_weights:  [array([[ 0.00463382, -0.00293355,  0.00025096],\n",
      "       [ 0.01155862,  0.00131355,  0.00049426]]), array([[ 0.00507311],\n",
      "       [ 0.00352972],\n",
      "       [-0.00108295]])]\n",
      "gradients_biases:  [array([ 0.00300606, -0.00285693, -0.00016021]), array([-0.0017987])]\n",
      "Iteration 778, Cost: 0.24630184856072085\n",
      "gradient_weights:  [array([[ 0.00464595, -0.00293659,  0.00025067],\n",
      "       [ 0.01156885,  0.00131582,  0.00049552]]), array([[ 0.00509089],\n",
      "       [ 0.00352987],\n",
      "       [-0.00108424]])]\n",
      "gradients_biases:  [array([ 0.00300694, -0.00285934, -0.00016056]), array([-0.0018015])]\n",
      "Iteration 779, Cost: 0.24629056421226259\n",
      "gradient_weights:  [array([[ 0.00465809, -0.00293964,  0.00025038],\n",
      "       [ 0.01157909,  0.0013181 ,  0.00049678]]), array([[ 0.00510868],\n",
      "       [ 0.00353003],\n",
      "       [-0.00108554]])]\n",
      "gradients_biases:  [array([ 0.00300781, -0.00286174, -0.00016092]), array([-0.00180429])]\n",
      "Iteration 780, Cost: 0.24627925045063626\n",
      "gradient_weights:  [array([[ 0.00467024, -0.00294269,  0.00025009],\n",
      "       [ 0.01158934,  0.00132037,  0.00049805]]), array([[ 0.00512649],\n",
      "       [ 0.00353019],\n",
      "       [-0.00108684]])]\n",
      "gradients_biases:  [array([ 0.00300865, -0.00286414, -0.00016128]), array([-0.0018071])]\n",
      "Iteration 781, Cost: 0.24626790719357797\n",
      "gradient_weights:  [array([[ 0.00468241, -0.00294574,  0.00024981],\n",
      "       [ 0.01159959,  0.00132265,  0.00049931]]), array([[ 0.00514433],\n",
      "       [ 0.00353036],\n",
      "       [-0.00108814]])]\n",
      "gradients_biases:  [array([ 0.00300947, -0.00286655, -0.00016163]), array([-0.0018099])]\n",
      "Iteration 782, Cost: 0.24625653435874015\n",
      "gradient_weights:  [array([[ 0.00469458, -0.00294879,  0.00024952],\n",
      "       [ 0.01160986,  0.00132493,  0.00050058]]), array([[ 0.00516218],\n",
      "       [ 0.00353053],\n",
      "       [-0.00108945]])]\n",
      "gradients_biases:  [array([ 0.00301027, -0.00286895, -0.00016199]), array([-0.00181271])]\n",
      "Iteration 783, Cost: 0.24624513186369237\n",
      "gradient_weights:  [array([[ 0.00470677, -0.00295185,  0.00024924],\n",
      "       [ 0.01162013,  0.00132721,  0.00050185]]), array([[ 0.00518004],\n",
      "       [ 0.00353071],\n",
      "       [-0.00109076]])]\n",
      "gradients_biases:  [array([ 0.00301104, -0.00287135, -0.00016235]), array([-0.00181552])]\n",
      "Iteration 784, Cost: 0.24623369962592195\n",
      "gradient_weights:  [array([[ 0.00471897, -0.00295491,  0.00024896],\n",
      "       [ 0.0116304 ,  0.00132949,  0.00050312]]), array([[ 0.00519793],\n",
      "       [ 0.0035309 ],\n",
      "       [-0.00109208]])]\n",
      "gradients_biases:  [array([ 0.0030118 , -0.00287375, -0.00016271]), array([-0.00181833])]\n",
      "Iteration 785, Cost: 0.24622223756283415\n",
      "gradient_weights:  [array([[ 0.00473118, -0.00295798,  0.00024868],\n",
      "       [ 0.01164069,  0.00133177,  0.0005044 ]]), array([[ 0.00521584],\n",
      "       [ 0.00353109],\n",
      "       [-0.00109339]])]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradients_biases:  [array([ 0.00301253, -0.00287615, -0.00016307]), array([-0.00182115])]\n",
      "Iteration 786, Cost: 0.246210745591753\n",
      "gradient_weights:  [array([[ 0.0047434 , -0.00296105,  0.0002484 ],\n",
      "       [ 0.01165098,  0.00133406,  0.00050568]]), array([[ 0.00523376],\n",
      "       [ 0.00353129],\n",
      "       [-0.00109472]])]\n",
      "gradients_biases:  [array([ 0.00301324, -0.00287855, -0.00016343]), array([-0.00182397])]\n",
      "Iteration 787, Cost: 0.2461992236299217\n",
      "gradient_weights:  [array([[ 0.00475563, -0.00296412,  0.00024812],\n",
      "       [ 0.01166129,  0.00133635,  0.00050696]]), array([[ 0.0052517 ],\n",
      "       [ 0.0035315 ],\n",
      "       [-0.00109604]])]\n",
      "gradients_biases:  [array([ 0.00301393, -0.00288095, -0.0001638 ]), array([-0.0018268])]\n",
      "Iteration 788, Cost: 0.24618767159450367\n",
      "gradient_weights:  [array([[ 0.00476787, -0.0029672 ,  0.00024785],\n",
      "       [ 0.0116716 ,  0.00133864,  0.00050824]]), array([[ 0.00526967],\n",
      "       [ 0.00353171],\n",
      "       [-0.00109737]])]\n",
      "gradients_biases:  [array([ 0.0030146 , -0.00288335, -0.00016416]), array([-0.00182963])]\n",
      "Iteration 789, Cost: 0.24617608940258232\n",
      "gradient_weights:  [array([[ 0.00478012, -0.00297028,  0.00024758],\n",
      "       [ 0.01168191,  0.00134093,  0.00050953]]), array([[ 0.00528765],\n",
      "       [ 0.00353193],\n",
      "       [-0.0010987 ]])]\n",
      "gradients_biases:  [array([ 0.00301525, -0.00288575, -0.00016452]), array([-0.00183246])]\n",
      "Iteration 790, Cost: 0.24616447697116228\n",
      "gradient_weights:  [array([[ 0.00479238, -0.00297336,  0.0002473 ],\n",
      "       [ 0.01169224,  0.00134323,  0.00051081]]), array([[ 0.00530564],\n",
      "       [ 0.00353215],\n",
      "       [-0.00110004]])]\n",
      "gradients_biases:  [array([ 0.00301587, -0.00288815, -0.00016489]), array([-0.0018353])]\n",
      "Iteration 791, Cost: 0.24615283421716985\n",
      "gradient_weights:  [array([[ 0.00480466, -0.00297645,  0.00024703],\n",
      "       [ 0.01170257,  0.00134552,  0.00051211]]), array([[ 0.00532366],\n",
      "       [ 0.00353238],\n",
      "       [-0.00110138]])]\n",
      "gradients_biases:  [array([ 0.00301647, -0.00289054, -0.00016525]), array([-0.00183814])]\n",
      "Iteration 792, Cost: 0.24614116105745315\n",
      "gradient_weights:  [array([[ 0.00481694, -0.00297954,  0.00024676],\n",
      "       [ 0.01171291,  0.00134782,  0.0005134 ]]), array([[ 0.0053417 ],\n",
      "       [ 0.00353262],\n",
      "       [-0.00110272]])]\n",
      "gradients_biases:  [array([ 0.00301705, -0.00289294, -0.00016562]), array([-0.00184098])]\n",
      "Iteration 793, Cost: 0.2461294574087834\n",
      "gradient_weights:  [array([[ 0.00482924, -0.00298263,  0.00024649],\n",
      "       [ 0.01172325,  0.00135013,  0.00051469]]), array([[ 0.00535975],\n",
      "       [ 0.00353286],\n",
      "       [-0.00110407]])]\n",
      "gradients_biases:  [array([ 0.00301761, -0.00289534, -0.00016599]), array([-0.00184383])]\n",
      "Iteration 794, Cost: 0.24611772318785496\n",
      "gradient_weights:  [array([[ 0.00484155, -0.00298573,  0.00024623],\n",
      "       [ 0.01173361,  0.00135243,  0.00051599]]), array([[ 0.00537783],\n",
      "       [ 0.00353311],\n",
      "       [-0.00110542]])]\n",
      "gradients_biases:  [array([ 0.00301814, -0.00289773, -0.00016636]), array([-0.00184668])]\n",
      "Iteration 795, Cost: 0.24610595831128623\n",
      "gradient_weights:  [array([[ 0.00485387, -0.00298883,  0.00024596],\n",
      "       [ 0.01174397,  0.00135473,  0.00051729]]), array([[ 0.00539592],\n",
      "       [ 0.00353336],\n",
      "       [-0.00110677]])]\n",
      "gradients_biases:  [array([ 0.00301866, -0.00290013, -0.00016672]), array([-0.00184953])]\n",
      "Iteration 796, Cost: 0.24609416269562\n",
      "gradient_weights:  [array([[ 0.0048662 , -0.00299194,  0.0002457 ],\n",
      "       [ 0.01175434,  0.00135704,  0.0005186 ]]), array([[ 0.00541403],\n",
      "       [ 0.00353362],\n",
      "       [-0.00110813]])]\n",
      "gradients_biases:  [array([ 0.00301915, -0.00290252, -0.00016709]), array([-0.00185239])]\n",
      "Iteration 797, Cost: 0.24608233625732445\n",
      "gradient_weights:  [array([[ 0.00487854, -0.00299505,  0.00024544],\n",
      "       [ 0.01176472,  0.00135935,  0.0005199 ]]), array([[ 0.00543215],\n",
      "       [ 0.00353389],\n",
      "       [-0.00110949]])]\n",
      "gradients_biases:  [array([ 0.00301961, -0.00290492, -0.00016746]), array([-0.00185525])]\n",
      "Iteration 798, Cost: 0.2460704789127933\n",
      "gradient_weights:  [array([[ 0.00489089, -0.00299816,  0.00024518],\n",
      "       [ 0.0117751 ,  0.00136166,  0.00052121]]), array([[ 0.0054503 ],\n",
      "       [ 0.00353416],\n",
      "       [-0.00111085]])]\n",
      "gradients_biases:  [array([ 0.00302006, -0.00290731, -0.00016783]), array([-0.00185811])]\n",
      "Iteration 799, Cost: 0.2460585905783467\n",
      "gradient_weights:  [array([[ 0.00490325, -0.00300127,  0.00024492],\n",
      "       [ 0.01178549,  0.00136398,  0.00052253]]), array([[ 0.00546847],\n",
      "       [ 0.00353444],\n",
      "       [-0.00111222]])]\n",
      "gradients_biases:  [array([ 0.00302048, -0.0029097 , -0.00016821]), array([-0.00186098])]\n",
      "Iteration 800, Cost: 0.24604667117023193\n",
      "gradient_weights:  [array([[ 0.00491563, -0.00300439,  0.00024466],\n",
      "       [ 0.01179589,  0.00136629,  0.00052384]]), array([[ 0.00548665],\n",
      "       [ 0.00353472],\n",
      "       [-0.00111359]])]\n",
      "gradients_biases:  [array([ 0.00302088, -0.00291209, -0.00016858]), array([-0.00186385])]\n",
      "Iteration 801, Cost: 0.24603472060462356\n",
      "gradient_weights:  [array([[ 0.00492801, -0.00300752,  0.0002444 ],\n",
      "       [ 0.01180629,  0.00136861,  0.00052516]]), array([[ 0.00550485],\n",
      "       [ 0.00353501],\n",
      "       [-0.00111496]])]\n",
      "gradients_biases:  [array([ 0.00302125, -0.00291449, -0.00016895]), array([-0.00186672])]\n",
      "Iteration 802, Cost: 0.2460227387976246\n",
      "gradient_weights:  [array([[ 0.00494041, -0.00301064,  0.00024415],\n",
      "       [ 0.0118167 ,  0.00137093,  0.00052648]]), array([[ 0.00552307],\n",
      "       [ 0.00353531],\n",
      "       [-0.00111634]])]\n",
      "gradients_biases:  [array([ 0.00302161, -0.00291688, -0.00016933]), array([-0.0018696])]\n",
      "Iteration 803, Cost: 0.24601072566526683\n",
      "gradient_weights:  [array([[ 0.00495282, -0.00301378,  0.0002439 ],\n",
      "       [ 0.01182712,  0.00137325,  0.0005278 ]]), array([[ 0.00554131],\n",
      "       [ 0.00353561],\n",
      "       [-0.00111772]])]\n",
      "gradients_biases:  [array([ 0.00302194, -0.00291927, -0.0001697 ]), array([-0.00187248])]\n",
      "Iteration 804, Cost: 0.24599868112351153\n",
      "gradient_weights:  [array([[ 0.00496523, -0.00301691,  0.00024364],\n",
      "       [ 0.01183755,  0.00137558,  0.00052912]]), array([[ 0.00555956],\n",
      "       [ 0.00353592],\n",
      "       [-0.00111911]])]\n",
      "gradients_biases:  [array([ 0.00302224, -0.00292166, -0.00017008]), array([-0.00187537])]\n",
      "Iteration 805, Cost: 0.24598660508825015\n",
      "gradient_weights:  [array([[ 0.00497766, -0.00302005,  0.00024339],\n",
      "       [ 0.01184798,  0.0013779 ,  0.00053045]]), array([[ 0.00557784],\n",
      "       [ 0.00353624],\n",
      "       [-0.0011205 ]])]\n",
      "gradients_biases:  [array([ 0.00302253, -0.00292405, -0.00017045]), array([-0.00187825])]\n",
      "Iteration 806, Cost: 0.2459744974753048\n",
      "gradient_weights:  [array([[ 0.0049901 , -0.00302319,  0.00024314],\n",
      "       [ 0.01185842,  0.00138023,  0.00053178]]), array([[ 0.00559613],\n",
      "       [ 0.00353656],\n",
      "       [-0.00112189]])]\n",
      "gradients_biases:  [array([ 0.00302279, -0.00292644, -0.00017083]), array([-0.00188115])]\n",
      "Iteration 807, Cost: 0.24596235820042903\n",
      "gradient_weights:  [array([[ 0.00500255, -0.00302633,  0.0002429 ],\n",
      "       [ 0.01186886,  0.00138256,  0.00053311]]), array([[ 0.00561444],\n",
      "       [ 0.00353688],\n",
      "       [-0.00112328]])]\n",
      "gradients_biases:  [array([ 0.00302302, -0.00292883, -0.00017121]), array([-0.00188404])]\n",
      "Iteration 808, Cost: 0.24595018717930844\n",
      "gradient_weights:  [array([[ 0.00501502, -0.00302948,  0.00024265],\n",
      "       [ 0.01187931,  0.00138489,  0.00053445]]), array([[ 0.00563277],\n",
      "       [ 0.00353722],\n",
      "       [-0.00112468]])]\n",
      "gradients_biases:  [array([ 0.00302323, -0.00293121, -0.00017159]), array([-0.00188694])]\n",
      "Iteration 809, Cost: 0.2459379843275613\n",
      "gradient_weights:  [array([[ 0.00502749, -0.00303264,  0.00024241],\n",
      "       [ 0.01188977,  0.00138723,  0.00053579]]), array([[ 0.00565112],\n",
      "       [ 0.00353756],\n",
      "       [-0.00112609]])]\n",
      "gradients_biases:  [array([ 0.00302342, -0.0029336 , -0.00017197]), array([-0.00188984])]\n",
      "Iteration 810, Cost: 0.24592574956073923\n",
      "gradient_weights:  [array([[ 0.00503997, -0.00303579,  0.00024216],\n",
      "       [ 0.01190024,  0.00138956,  0.00053713]]), array([[ 0.00566949],\n",
      "       [ 0.0035379 ],\n",
      "       [-0.00112749]])]\n",
      "gradients_biases:  [array([ 0.00302359, -0.00293599, -0.00017235]), array([-0.00189275])]\n",
      "Iteration 811, Cost: 0.24591348279432776\n",
      "gradient_weights:  [array([[ 0.00505247, -0.00303895,  0.00024192],\n",
      "       [ 0.01191071,  0.0013919 ,  0.00053847]]), array([[ 0.00568787],\n",
      "       [ 0.00353825],\n",
      "       [-0.0011289 ]])]\n",
      "gradients_biases:  [array([ 0.00302373, -0.00293837, -0.00017273]), array([-0.00189566])]\n",
      "Iteration 812, Cost: 0.24590118394374721\n",
      "gradient_weights:  [array([[ 0.00506497, -0.00304212,  0.00024168],\n",
      "       [ 0.01192119,  0.00139424,  0.00053982]]), array([[ 0.00570627],\n",
      "       [ 0.00353861],\n",
      "       [-0.00113032]])]\n",
      "gradients_biases:  [array([ 0.00302385, -0.00294076, -0.00017311]), array([-0.00189857])]\n",
      "Iteration 813, Cost: 0.24588885292435325\n",
      "gradient_weights:  [array([[ 0.00507749, -0.00304529,  0.00024144],\n",
      "       [ 0.01193167,  0.00139658,  0.00054117]]), array([[ 0.00572469],\n",
      "       [ 0.00353898],\n",
      "       [-0.00113173]])]\n",
      "gradients_biases:  [array([ 0.00302394, -0.00294314, -0.00017349]), array([-0.00190149])]\n",
      "Iteration 814, Cost: 0.24587648965143732\n",
      "gradient_weights:  [array([[ 0.00509002, -0.00304846,  0.00024121],\n",
      "       [ 0.01194216,  0.00139893,  0.00054252]]), array([[ 0.00574313],\n",
      "       [ 0.00353935],\n",
      "       [-0.00113315]])]\n",
      "gradients_biases:  [array([ 0.00302401, -0.00294553, -0.00017388]), array([-0.00190441])]\n",
      "Iteration 815, Cost: 0.2458640940402278\n",
      "gradient_weights:  [array([[ 0.00510256, -0.00305163,  0.00024097],\n",
      "       [ 0.01195266,  0.00140127,  0.00054387]]), array([[ 0.00576158],\n",
      "       [ 0.00353972],\n",
      "       [-0.00113458]])]\n",
      "gradients_biases:  [array([ 0.00302406, -0.00294791, -0.00017426]), array([-0.00190733])]\n",
      "Iteration 816, Cost: 0.2458516660058901\n",
      "gradient_weights:  [array([[ 0.00511511, -0.00305481,  0.00024074],\n",
      "       [ 0.01196316,  0.00140362,  0.00054523]]), array([[ 0.00578006],\n",
      "       [ 0.0035401 ],\n",
      "       [-0.00113601]])]\n",
      "gradients_biases:  [array([ 0.00302408, -0.0029503 , -0.00017465]), array([-0.00191026])]\n",
      "Iteration 817, Cost: 0.24583920546352792\n",
      "gradient_weights:  [array([[ 0.00512767, -0.003058  ,  0.0002405 ],\n",
      "       [ 0.01197367,  0.00140597,  0.00054659]]), array([[ 0.00579855],\n",
      "       [ 0.00354049],\n",
      "       [-0.00113744]])]\n",
      "gradients_biases:  [array([ 0.00302407, -0.00295268, -0.00017503]), array([-0.00191319])]\n",
      "Iteration 818, Cost: 0.24582671232818343\n",
      "gradient_weights:  [array([[ 0.00514024, -0.00306118,  0.00024027],\n",
      "       [ 0.01198418,  0.00140832,  0.00054795]]), array([[ 0.00581706],\n",
      "       [ 0.00354089],\n",
      "       [-0.00113887]])]\n",
      "gradients_biases:  [array([ 0.00302405, -0.00295506, -0.00017542]), array([-0.00191613])]\n",
      "Iteration 819, Cost: 0.24581418651483836\n",
      "gradient_weights:  [array([[ 0.00515282, -0.00306437,  0.00024004],\n",
      "       [ 0.0119947 ,  0.00141068,  0.00054932]]), array([[ 0.00583558],\n",
      "       [ 0.00354129],\n",
      "       [-0.00114031]])]\n",
      "gradients_biases:  [array([ 0.003024  , -0.00295744, -0.00017581]), array([-0.00191907])]\n",
      "Iteration 820, Cost: 0.24580162793841437\n",
      "gradient_weights:  [array([[ 0.00516541, -0.00306757,  0.00023982],\n",
      "       [ 0.01200523,  0.00141303,  0.00055069]]), array([[ 0.00585413],\n",
      "       [ 0.00354169],\n",
      "       [-0.00114176]])]\n",
      "gradients_biases:  [array([ 0.00302392, -0.00295983, -0.0001762 ]), array([-0.00192201])]\n",
      "Iteration 821, Cost: 0.24578903651377385\n",
      "gradient_weights:  [array([[ 0.00517802, -0.00307077,  0.00023959],\n",
      "       [ 0.01201576,  0.00141539,  0.00055206]]), array([[ 0.00587269],\n",
      "       [ 0.0035421 ],\n",
      "       [-0.0011432 ]])]\n",
      "gradients_biases:  [array([ 0.00302382, -0.00296221, -0.00017659]), array([-0.00192495])]\n",
      "Iteration 822, Cost: 0.24577641215572077\n",
      "gradient_weights:  [array([[ 0.00519063, -0.00307397,  0.00023936],\n",
      "       [ 0.0120263 ,  0.00141775,  0.00055343]]), array([[ 0.00589127],\n",
      "       [ 0.00354252],\n",
      "       [-0.00114465]])]\n",
      "gradients_biases:  [array([ 0.00302369, -0.00296459, -0.00017698]), array([-0.0019279])]\n",
      "Iteration 823, Cost: 0.2457637547790012\n",
      "gradient_weights:  [array([[ 0.00520326, -0.00307717,  0.00023914],\n",
      "       [ 0.01203685,  0.00142011,  0.00055481]]), array([[ 0.00590987],\n",
      "       [ 0.00354295],\n",
      "       [-0.00114611]])]\n",
      "gradients_biases:  [array([ 0.00302354, -0.00296697, -0.00017737]), array([-0.00193086])]\n",
      "Iteration 824, Cost: 0.24575106429830398\n",
      "gradient_weights:  [array([[ 0.00521589, -0.00308038,  0.00023892],\n",
      "       [ 0.0120474 ,  0.00142248,  0.00055619]]), array([[ 0.00592849],\n",
      "       [ 0.00354338],\n",
      "       [-0.00114756]])]\n",
      "gradients_biases:  [array([ 0.00302337, -0.00296934, -0.00017776]), array([-0.00193381])]\n",
      "Iteration 825, Cost: 0.24573834062826147\n",
      "gradient_weights:  [array([[ 0.00522854, -0.0030836 ,  0.0002387 ],\n",
      "       [ 0.01205795,  0.00142484,  0.00055757]]), array([[ 0.00594712],\n",
      "       [ 0.00354381],\n",
      "       [-0.00114903]])]\n",
      "gradients_biases:  [array([ 0.00302317, -0.00297172, -0.00017815]), array([-0.00193677])]\n",
      "Iteration 826, Cost: 0.2457255836834506\n",
      "gradient_weights:  [array([[ 0.0052412 , -0.00308682,  0.00023848],\n",
      "       [ 0.01206852,  0.00142721,  0.00055896]]), array([[ 0.00596577],\n",
      "       [ 0.00354426],\n",
      "       [-0.00115049]])]\n",
      "gradients_biases:  [array([ 0.00302294, -0.0029741 , -0.00017855]), array([-0.00193973])]\n",
      "Iteration 827, Cost: 0.24571279337839275\n",
      "gradient_weights:  [array([[ 0.00525387, -0.00309004,  0.00023826],\n",
      "       [ 0.01207908,  0.00142958,  0.00056034]]), array([[ 0.00598444],\n",
      "       [ 0.00354471],\n",
      "       [-0.00115196]])]\n",
      "gradients_biases:  [array([ 0.00302269, -0.00297648, -0.00017894]), array([-0.0019427])]\n",
      "Iteration 828, Cost: 0.24569996962755536\n",
      "gradient_weights:  [array([[ 0.00526655, -0.00309326,  0.00023805],\n",
      "       [ 0.01208966,  0.00143195,  0.00056173]]), array([[ 0.00600313],\n",
      "       [ 0.00354516],\n",
      "       [-0.00115343]])]\n",
      "gradients_biases:  [array([ 0.00302241, -0.00297886, -0.00017934]), array([-0.00194567])]\n",
      "Iteration 829, Cost: 0.24568711234535223\n",
      "gradient_weights:  [array([[ 0.00527924, -0.00309649,  0.00023783],\n",
      "       [ 0.01210023,  0.00143433,  0.00056313]]), array([[ 0.00602183],\n",
      "       [ 0.00354562],\n",
      "       [-0.00115491]])]\n",
      "gradients_biases:  [array([ 0.00302211, -0.00298123, -0.00017973]), array([-0.00194865])]\n",
      "Iteration 830, Cost: 0.2456742214461441\n",
      "gradient_weights:  [array([[ 0.00529194, -0.00309973,  0.00023762],\n",
      "       [ 0.01211082,  0.0014367 ,  0.00056452]]), array([[ 0.00604055],\n",
      "       [ 0.00354609],\n",
      "       [-0.00115639]])]\n",
      "gradients_biases:  [array([ 0.00302179, -0.00298361, -0.00018013]), array([-0.00195162])]\n",
      "Iteration 831, Cost: 0.24566129684423965\n",
      "gradient_weights:  [array([[ 0.00530465, -0.00310296,  0.00023741],\n",
      "       [ 0.01212141,  0.00143908,  0.00056592]]), array([[ 0.00605929],\n",
      "       [ 0.00354656],\n",
      "       [-0.00115787]])]\n",
      "gradients_biases:  [array([ 0.00302143, -0.00298598, -0.00018053]), array([-0.00195461])]\n",
      "Iteration 832, Cost: 0.2456483384538961\n",
      "gradient_weights:  [array([[ 0.00531737, -0.0031062 ,  0.0002372 ],\n",
      "       [ 0.012132  ,  0.00144146,  0.00056733]]), array([[ 0.00607805],\n",
      "       [ 0.00354704],\n",
      "       [-0.00115936]])]\n",
      "gradients_biases:  [array([ 0.00302106, -0.00298836, -0.00018093]), array([-0.00195759])]\n",
      "Iteration 833, Cost: 0.24563534618932004\n",
      "gradient_weights:  [array([[ 0.00533011, -0.00310945,  0.00023699],\n",
      "       [ 0.0121426 ,  0.00144384,  0.00056873]]), array([[ 0.00609682],\n",
      "       [ 0.00354752],\n",
      "       [-0.00116085]])]\n",
      "gradients_biases:  [array([ 0.00302065, -0.00299073, -0.00018133]), array([-0.00196058])]\n",
      "Iteration 834, Cost: 0.24562231996466788\n",
      "gradient_weights:  [array([[ 0.00534285, -0.0031127 ,  0.00023678],\n",
      "       [ 0.0121532 ,  0.00144622,  0.00057014]]), array([[ 0.00611561],\n",
      "       [ 0.00354801],\n",
      "       [-0.00116234]])]\n",
      "gradients_biases:  [array([ 0.00302022, -0.0029931 , -0.00018173]), array([-0.00196357])]\n",
      "Iteration 835, Cost: 0.24560925969404712\n",
      "gradient_weights:  [array([[ 0.00535561, -0.00311595,  0.00023658],\n",
      "       [ 0.01216381,  0.00144861,  0.00057155]]), array([[ 0.00613442],\n",
      "       [ 0.00354851],\n",
      "       [-0.00116384]])]\n",
      "gradients_biases:  [array([ 0.00301977, -0.00299548, -0.00018213]), array([-0.00196657])]\n",
      "Iteration 836, Cost: 0.2455961652915164\n",
      "gradient_weights:  [array([[ 0.00536837, -0.00311921,  0.00023637],\n",
      "       [ 0.01217443,  0.001451  ,  0.00057297]]), array([[ 0.00615324],\n",
      "       [ 0.00354901],\n",
      "       [-0.00116534]])]\n",
      "gradients_biases:  [array([ 0.00301929, -0.00299785, -0.00018253]), array([-0.00196957])]\n",
      "Iteration 837, Cost: 0.24558303667108672\n",
      "gradient_weights:  [array([[ 0.00538115, -0.00312247,  0.00023617],\n",
      "       [ 0.01218505,  0.00145339,  0.00057438]]), array([[ 0.00617209],\n",
      "       [ 0.00354952],\n",
      "       [-0.00116685]])]\n",
      "gradients_biases:  [array([ 0.00301878, -0.00300022, -0.00018293]), array([-0.00197257])]\n",
      "Iteration 838, Cost: 0.2455698737467222\n",
      "gradient_weights:  [array([[ 0.00539394, -0.00312573,  0.00023597],\n",
      "       [ 0.01219567,  0.00145578,  0.0005758 ]]), array([[ 0.00619095],\n",
      "       [ 0.00355004],\n",
      "       [-0.00116836]])]\n",
      "gradients_biases:  [array([ 0.00301825, -0.00300259, -0.00018334]), array([-0.00197558])]\n",
      "Iteration 839, Cost: 0.2455566764323405\n",
      "gradient_weights:  [array([[ 0.00540674, -0.003129  ,  0.00023577],\n",
      "       [ 0.0122063 ,  0.00145817,  0.00057722]]), array([[ 0.00620982],\n",
      "       [ 0.00355056],\n",
      "       [-0.00116987]])]\n",
      "gradients_biases:  [array([ 0.00301769, -0.00300496, -0.00018374]), array([-0.00197859])]\n",
      "Iteration 840, Cost: 0.24554344464181377\n",
      "gradient_weights:  [array([[ 0.00541954, -0.00313227,  0.00023557],\n",
      "       [ 0.01221693,  0.00146057,  0.00057865]]), array([[ 0.00622872],\n",
      "       [ 0.00355109],\n",
      "       [-0.00117139]])]\n",
      "gradients_biases:  [array([ 0.00301711, -0.00300733, -0.00018415]), array([-0.0019816])]\n",
      "Iteration 841, Cost: 0.24553017828896934\n",
      "gradient_weights:  [array([[ 0.00543236, -0.00313555,  0.00023538],\n",
      "       [ 0.01222757,  0.00146296,  0.00058008]]), array([[ 0.00624763],\n",
      "       [ 0.00355162],\n",
      "       [-0.00117291]])]\n",
      "gradients_biases:  [array([ 0.0030165 , -0.0030097 , -0.00018455]), array([-0.00198462])]\n",
      "Iteration 842, Cost: 0.24551687728759075\n",
      "gradient_weights:  [array([[ 0.00544519, -0.00313883,  0.00023518],\n",
      "       [ 0.01223822,  0.00146536,  0.00058151]]), array([[ 0.00626656],\n",
      "       [ 0.00355216],\n",
      "       [-0.00117443]])]\n",
      "gradients_biases:  [array([ 0.00301586, -0.00301207, -0.00018496]), array([-0.00198764])]\n",
      "Iteration 843, Cost: 0.2455035415514179\n",
      "gradient_weights:  [array([[ 0.00545803, -0.00314212,  0.00023499],\n",
      "       [ 0.01224886,  0.00146776,  0.00058295]]), array([[ 0.0062855 ],\n",
      "       [ 0.0035527 ],\n",
      "       [-0.00117596]])]\n",
      "gradients_biases:  [array([ 0.00301519, -0.00301444, -0.00018537]), array([-0.00199067])]\n",
      "Iteration 844, Cost: 0.24549017099414858\n",
      "gradient_weights:  [array([[ 0.00547089, -0.00314541,  0.0002348 ],\n",
      "       [ 0.01225952,  0.00147017,  0.00058438]]), array([[ 0.00630447],\n",
      "       [ 0.00355325],\n",
      "       [-0.00117749]])]\n",
      "gradients_biases:  [array([ 0.0030145 , -0.0030168 , -0.00018578]), array([-0.0019937])]\n",
      "Iteration 845, Cost: 0.24547676552943853\n",
      "gradient_weights:  [array([[ 0.00548375, -0.0031487 ,  0.00023461],\n",
      "       [ 0.01227017,  0.00147257,  0.00058582]]), array([[ 0.00632345],\n",
      "       [ 0.00355381],\n",
      "       [-0.00117903]])]\n",
      "gradients_biases:  [array([ 0.00301379, -0.00301917, -0.00018619]), array([-0.00199673])]\n",
      "Iteration 846, Cost: 0.24546332507090252\n",
      "gradient_weights:  [array([[ 0.00549662, -0.003152  ,  0.00023442],\n",
      "       [ 0.01228083,  0.00147498,  0.00058727]]), array([[ 0.00634244],\n",
      "       [ 0.00355437],\n",
      "       [-0.00118057]])]\n",
      "gradients_biases:  [array([ 0.00301304, -0.00302154, -0.0001866 ]), array([-0.00199977])]\n",
      "Iteration 847, Cost: 0.24544984953211527\n",
      "gradient_weights:  [array([[ 0.0055095 , -0.0031553 ,  0.00023423],\n",
      "       [ 0.0122915 ,  0.00147739,  0.00058871]]), array([[ 0.00636146],\n",
      "       [ 0.00355494],\n",
      "       [-0.00118211]])]\n",
      "gradients_biases:  [array([ 0.00301227, -0.0030239 , -0.00018701]), array([-0.00200281])]\n",
      "Iteration 848, Cost: 0.24543633882661187\n",
      "gradient_weights:  [array([[ 0.0055224 , -0.0031586 ,  0.00023405],\n",
      "       [ 0.01230217,  0.0014798 ,  0.00059016]]), array([[ 0.00638049],\n",
      "       [ 0.00355552],\n",
      "       [-0.00118366]])]\n",
      "gradients_biases:  [array([ 0.00301147, -0.00302627, -0.00018742]), array([-0.00200585])]\n",
      "Iteration 849, Cost: 0.24542279286788887\n",
      "gradient_weights:  [array([[ 0.0055353 , -0.00316191,  0.00023386],\n",
      "       [ 0.01231284,  0.00148221,  0.00059162]]), array([[ 0.00639953],\n",
      "       [ 0.0035561 ],\n",
      "       [-0.00118521]])]\n",
      "gradients_biases:  [array([ 0.00301065, -0.00302863, -0.00018784]), array([-0.0020089])]\n",
      "Iteration 850, Cost: 0.24540921156940487\n",
      "gradient_weights:  [array([[ 0.00554821, -0.00316522,  0.00023368],\n",
      "       [ 0.01232352,  0.00148462,  0.00059307]]), array([[ 0.0064186 ],\n",
      "       [ 0.00355669],\n",
      "       [-0.00118676]])]\n",
      "gradients_biases:  [array([ 0.0030098 , -0.003031  , -0.00018825]), array([-0.00201195])]\n",
      "Iteration 851, Cost: 0.24539559484458132\n",
      "gradient_weights:  [array([[ 0.00556114, -0.00316854,  0.0002335 ],\n",
      "       [ 0.01233421,  0.00148704,  0.00059453]]), array([[ 0.00643768],\n",
      "       [ 0.00355728],\n",
      "       [-0.00118832]])]\n",
      "gradients_biases:  [array([ 0.00300892, -0.00303336, -0.00018866]), array([-0.002015])]\n",
      "Iteration 852, Cost: 0.24538194260680318\n",
      "gradient_weights:  [array([[ 0.00557408, -0.00317186,  0.00023332],\n",
      "       [ 0.01234489,  0.00148946,  0.00059599]]), array([[ 0.00645677],\n",
      "       [ 0.00355788],\n",
      "       [-0.00118989]])]\n",
      "gradients_biases:  [array([ 0.00300801, -0.00303572, -0.00018908]), array([-0.00201806])]\n",
      "Iteration 853, Cost: 0.2453682547694201\n",
      "gradient_weights:  [array([[ 0.00558702, -0.00317519,  0.00023314],\n",
      "       [ 0.01235558,  0.00149188,  0.00059746]]), array([[ 0.00647589],\n",
      "       [ 0.00355848],\n",
      "       [-0.00119145]])]\n",
      "gradients_biases:  [array([ 0.00300708, -0.00303808, -0.0001895 ]), array([-0.00202113])]\n",
      "Iteration 854, Cost: 0.2453545312457468\n",
      "gradient_weights:  [array([[ 0.00559998, -0.00317852,  0.00023297],\n",
      "       [ 0.01236628,  0.0014943 ,  0.00059893]]), array([[ 0.00649502],\n",
      "       [ 0.00355909],\n",
      "       [-0.00119302]])]\n",
      "gradients_biases:  [array([ 0.00300612, -0.00304045, -0.00018992]), array([-0.00202419])]\n",
      "Iteration 855, Cost: 0.24534077194906395\n",
      "gradient_weights:  [array([[ 0.00561294, -0.00318185,  0.00023279],\n",
      "       [ 0.01237698,  0.00149673,  0.0006004 ]]), array([[ 0.00651416],\n",
      "       [ 0.00355971],\n",
      "       [-0.0011946 ]])]\n",
      "gradients_biases:  [array([ 0.00300513, -0.00304281, -0.00019033]), array([-0.00202726])]\n",
      "Iteration 856, Cost: 0.24532697679261917\n",
      "gradient_weights:  [array([[ 0.00562592, -0.00318519,  0.00023262],\n",
      "       [ 0.01238768,  0.00149915,  0.00060187]]), array([[ 0.00653333],\n",
      "       [ 0.00356033],\n",
      "       [-0.00119617]])]\n",
      "gradients_biases:  [array([ 0.00300411, -0.00304517, -0.00019075]), array([-0.00203034])]\n",
      "Iteration 857, Cost: 0.24531314568962725\n",
      "gradient_weights:  [array([[ 0.00563891, -0.00318853,  0.00023245],\n",
      "       [ 0.01239838,  0.00150158,  0.00060335]]), array([[ 0.00655251],\n",
      "       [ 0.00356096],\n",
      "       [-0.00119776]])]\n",
      "gradients_biases:  [array([ 0.00300307, -0.00304753, -0.00019117]), array([-0.00203341])]\n",
      "Iteration 858, Cost: 0.245299278553272\n",
      "gradient_weights:  [array([[ 0.00565191, -0.00319188,  0.00023228],\n",
      "       [ 0.01240909,  0.00150401,  0.00060483]]), array([[ 0.0065717 ],\n",
      "       [ 0.00356159],\n",
      "       [-0.00119934]])]\n",
      "gradients_biases:  [array([ 0.003002  , -0.00304989, -0.0001916 ]), array([-0.00203649])]\n",
      "Iteration 859, Cost: 0.24528537529670577\n",
      "gradient_weights:  [array([[ 0.00566492, -0.00319523,  0.00023211],\n",
      "       [ 0.01241981,  0.00150644,  0.00060631]]), array([[ 0.00659092],\n",
      "       [ 0.00356224],\n",
      "       [-0.00120093]])]\n",
      "gradients_biases:  [array([ 0.0030009 , -0.00305224, -0.00019202]), array([-0.00203958])]\n",
      "Iteration 860, Cost: 0.2452714358330513\n",
      "gradient_weights:  [array([[ 0.00567794, -0.00319858,  0.00023194],\n",
      "       [ 0.01243052,  0.00150887,  0.0006078 ]]), array([[ 0.00661015],\n",
      "       [ 0.00356288],\n",
      "       [-0.00120253]])]\n",
      "gradients_biases:  [array([ 0.00299977, -0.0030546 , -0.00019244]), array([-0.00204267])]\n",
      "Iteration 861, Cost: 0.2452574600754016\n",
      "gradient_weights:  [array([[ 0.00569096, -0.00320194,  0.00023178],\n",
      "       [ 0.01244124,  0.00151131,  0.00060929]]), array([[ 0.00662939],\n",
      "       [ 0.00356353],\n",
      "       [-0.00120412]])]\n",
      "gradients_biases:  [array([ 0.00299862, -0.00305696, -0.00019287]), array([-0.00204576])]\n",
      "Iteration 862, Cost: 0.24524344793682176\n",
      "gradient_weights:  [array([[ 0.005704  , -0.0032053 ,  0.00023161],\n",
      "       [ 0.01245197,  0.00151375,  0.00061078]]), array([[ 0.00664865],\n",
      "       [ 0.00356419],\n",
      "       [-0.00120572]])]\n",
      "gradients_biases:  [array([ 0.00299743, -0.00305931, -0.00019329]), array([-0.00204886])]\n",
      "Iteration 863, Cost: 0.24522939933034893\n",
      "gradient_weights:  [array([[ 0.00571705, -0.00320867,  0.00023145],\n",
      "       [ 0.0124627 ,  0.00151618,  0.00061228]]), array([[ 0.00666793],\n",
      "       [ 0.00356486],\n",
      "       [-0.00120733]])]\n",
      "gradients_biases:  [array([ 0.00299622, -0.00306167, -0.00019372]), array([-0.00205196])]\n",
      "Iteration 864, Cost: 0.24521531416899336\n",
      "gradient_weights:  [array([[ 0.00573011, -0.00321204,  0.00023129],\n",
      "       [ 0.01247343,  0.00151863,  0.00061378]]), array([[ 0.00668722],\n",
      "       [ 0.00356553],\n",
      "       [-0.00120894]])]\n",
      "gradients_biases:  [array([ 0.00299498, -0.00306403, -0.00019414]), array([-0.00205506])]\n",
      "Iteration 865, Cost: 0.24520119236573953\n",
      "gradient_weights:  [array([[ 0.00574318, -0.00321541,  0.00023113],\n",
      "       [ 0.01248416,  0.00152107,  0.00061528]]), array([[ 0.00670653],\n",
      "       [ 0.0035662 ],\n",
      "       [-0.00121055]])]\n",
      "gradients_biases:  [array([ 0.00299372, -0.00306638, -0.00019457]), array([-0.00205817])]\n",
      "Iteration 866, Cost: 0.24518703383354631\n",
      "gradient_weights:  [array([[ 0.00575626, -0.00321879,  0.00023098],\n",
      "       [ 0.0124949 ,  0.00152351,  0.00061678]]), array([[ 0.00672586],\n",
      "       [ 0.00356689],\n",
      "       [-0.00121217]])]\n",
      "gradients_biases:  [array([ 0.00299242, -0.00306873, -0.000195  ]), array([-0.00206128])]\n",
      "Iteration 867, Cost: 0.24517283848534843\n",
      "gradient_weights:  [array([[ 0.00576936, -0.00322218,  0.00023082],\n",
      "       [ 0.01250564,  0.00152596,  0.00061829]]), array([[ 0.0067452 ],\n",
      "       [ 0.00356757],\n",
      "       [-0.00121379]])]\n",
      "gradients_biases:  [array([ 0.00299109, -0.00307109, -0.00019543]), array([-0.00206439])]\n",
      "Iteration 868, Cost: 0.24515860623405694\n",
      "gradient_weights:  [array([[ 0.00578246, -0.00322556,  0.00023067],\n",
      "       [ 0.01251638,  0.00152841,  0.0006198 ]]), array([[ 0.00676456],\n",
      "       [ 0.00356827],\n",
      "       [-0.00121542]])]\n",
      "gradients_biases:  [array([ 0.00298974, -0.00307344, -0.00019586]), array([-0.00206751])]\n",
      "Iteration 869, Cost: 0.24514433699256\n",
      "gradient_weights:  [array([[ 0.00579557, -0.00322896,  0.00023051],\n",
      "       [ 0.01252713,  0.00153086,  0.00062132]]), array([[ 0.00678393],\n",
      "       [ 0.00356897],\n",
      "       [-0.00121704]])]\n",
      "gradients_biases:  [array([ 0.00298836, -0.00307579, -0.00019629]), array([-0.00207063])]\n",
      "Iteration 870, Cost: 0.24513003067372385\n",
      "gradient_weights:  [array([[ 0.00580869, -0.00323235,  0.00023036],\n",
      "       [ 0.01253787,  0.00153331,  0.00062284]]), array([[ 0.00680332],\n",
      "       [ 0.00356967],\n",
      "       [-0.00121868]])]\n",
      "gradients_biases:  [array([ 0.00298695, -0.00307814, -0.00019672]), array([-0.00207376])]\n",
      "Iteration 871, Cost: 0.24511568719039373\n",
      "gradient_weights:  [array([[ 0.00582182, -0.00323575,  0.00023021],\n",
      "       [ 0.01254863,  0.00153576,  0.00062436]]), array([[ 0.00682273],\n",
      "       [ 0.00357039],\n",
      "       [-0.00122031]])]\n",
      "gradients_biases:  [array([ 0.00298551, -0.00308049, -0.00019716]), array([-0.00207689])]\n",
      "Iteration 872, Cost: 0.24510130645539402\n",
      "gradient_weights:  [array([[ 0.00583496, -0.00323916,  0.00023007],\n",
      "       [ 0.01255938,  0.00153822,  0.00062588]]), array([[ 0.00684215],\n",
      "       [ 0.0035711 ],\n",
      "       [-0.00122196]])]\n",
      "gradients_biases:  [array([ 0.00298404, -0.00308284, -0.00019759]), array([-0.00208002])]\n",
      "Iteration 873, Cost: 0.2450868883815301\n",
      "gradient_weights:  [array([[ 0.00584811, -0.00324257,  0.00022992],\n",
      "       [ 0.01257014,  0.00154068,  0.00062741]]), array([[ 0.00686158],\n",
      "       [ 0.00357183],\n",
      "       [-0.0012236 ]])]\n",
      "gradients_biases:  [array([ 0.00298254, -0.00308519, -0.00019803]), array([-0.00208316])]\n",
      "Iteration 874, Cost: 0.2450724328815882\n",
      "gradient_weights:  [array([[ 0.00586128, -0.00324598,  0.00022977],\n",
      "       [ 0.0125809 ,  0.00154314,  0.00062894]]), array([[ 0.00688104],\n",
      "       [ 0.00357256],\n",
      "       [-0.00122525]])]\n",
      "gradients_biases:  [array([ 0.00298102, -0.00308754, -0.00019846]), array([-0.0020863])]\n",
      "Iteration 875, Cost: 0.24505793986833707\n",
      "gradient_weights:  [array([[ 0.00587445, -0.0032494 ,  0.00022963],\n",
      "       [ 0.01259166,  0.0015456 ,  0.00063048]]), array([[ 0.0069005 ],\n",
      "       [ 0.00357329],\n",
      "       [-0.0012269 ]])]\n",
      "gradients_biases:  [array([ 0.00297946, -0.00308989, -0.0001989 ]), array([-0.00208945])]\n",
      "Iteration 876, Cost: 0.245043409254528\n",
      "gradient_weights:  [array([[ 0.00588763, -0.00325282,  0.00022949],\n",
      "       [ 0.01260243,  0.00154806,  0.00063201]]), array([[ 0.00691999],\n",
      "       [ 0.00357403],\n",
      "       [-0.00122856]])]\n",
      "gradients_biases:  [array([ 0.00297788, -0.00309224, -0.00019934]), array([-0.0020926])]\n",
      "Iteration 877, Cost: 0.24502884095289623\n",
      "gradient_weights:  [array([[ 0.00590082, -0.00325625,  0.00022935],\n",
      "       [ 0.01261319,  0.00155052,  0.00063355]]), array([[ 0.00693949],\n",
      "       [ 0.00357478],\n",
      "       [-0.00123022]])]\n",
      "gradients_biases:  [array([ 0.00297626, -0.00309458, -0.00019978]), array([-0.00209575])]\n",
      "Iteration 878, Cost: 0.24501423487616153\n",
      "gradient_weights:  [array([[ 0.00591402, -0.00325968,  0.00022921],\n",
      "       [ 0.01262396,  0.00155299,  0.0006351 ]]), array([[ 0.006959  ],\n",
      "       [ 0.00357553],\n",
      "       [-0.00123188]])]\n",
      "gradients_biases:  [array([ 0.00297462, -0.00309693, -0.00020022]), array([-0.00209891])]\n",
      "Iteration 879, Cost: 0.244999590937029\n",
      "gradient_weights:  [array([[ 0.00592724, -0.00326311,  0.00022908],\n",
      "       [ 0.01263474,  0.00155546,  0.00063665]]), array([[ 0.00697853],\n",
      "       [ 0.00357629],\n",
      "       [-0.00123355]])]\n",
      "gradients_biases:  [array([ 0.00297294, -0.00309928, -0.00020066]), array([-0.00210207])]\n",
      "Iteration 880, Cost: 0.24498490904819004\n",
      "gradient_weights:  [array([[ 0.00594046, -0.00326655,  0.00022894],\n",
      "       [ 0.01264551,  0.00155793,  0.0006382 ]]), array([[ 0.00699808],\n",
      "       [ 0.00357706],\n",
      "       [-0.00123523]])]\n",
      "gradients_biases:  [array([ 0.00297124, -0.00310162, -0.0002011 ]), array([-0.00210523])]\n",
      "Iteration 881, Cost: 0.24497018912232316\n",
      "gradient_weights:  [array([[ 0.00595369, -0.00326999,  0.00022881],\n",
      "       [ 0.01265629,  0.0015604 ,  0.00063975]]), array([[ 0.00701764],\n",
      "       [ 0.00357783],\n",
      "       [-0.0012369 ]])]\n",
      "gradients_biases:  [array([ 0.00296951, -0.00310396, -0.00020154]), array([-0.0021084])]\n",
      "Iteration 882, Cost: 0.24495543107209455\n",
      "gradient_weights:  [array([[ 0.00596693, -0.00327344,  0.00022867],\n",
      "       [ 0.01266707,  0.00156288,  0.00064131]]), array([[ 0.00703721],\n",
      "       [ 0.0035786 ],\n",
      "       [-0.00123859]])]\n",
      "gradients_biases:  [array([ 0.00296775, -0.00310631, -0.00020198]), array([-0.00211158])]\n",
      "Iteration 883, Cost: 0.24494063481015932\n",
      "gradient_weights:  [array([[ 0.00598018, -0.00327689,  0.00022854],\n",
      "       [ 0.01267785,  0.00156535,  0.00064287]]), array([[ 0.0070568 ],\n",
      "       [ 0.00357939],\n",
      "       [-0.00124027]])]\n",
      "gradients_biases:  [array([ 0.00296595, -0.00310865, -0.00020243]), array([-0.00211475])]\n",
      "Iteration 884, Cost: 0.244925800249162\n",
      "gradient_weights:  [array([[ 0.00599345, -0.00328035,  0.00022842],\n",
      "       [ 0.01268863,  0.00156783,  0.00064443]]), array([[ 0.00707641],\n",
      "       [ 0.00358018],\n",
      "       [-0.00124196]])]\n",
      "gradients_biases:  [array([ 0.00296413, -0.00311099, -0.00020287]), array([-0.00211793])]\n",
      "Iteration 885, Cost: 0.24491092730173747\n",
      "gradient_weights:  [array([[ 0.00600672, -0.00328381,  0.00022829],\n",
      "       [ 0.01269942,  0.00157031,  0.000646  ]]), array([[ 0.00709603],\n",
      "       [ 0.00358097],\n",
      "       [-0.00124365]])]\n",
      "gradients_biases:  [array([ 0.00296228, -0.00311333, -0.00020332]), array([-0.00212112])]\n",
      "Iteration 886, Cost: 0.244896015880512\n",
      "gradient_weights:  [array([[ 0.00602   , -0.00328727,  0.00022816],\n",
      "       [ 0.0127102 ,  0.00157279,  0.00064757]]), array([[ 0.00711567],\n",
      "       [ 0.00358177],\n",
      "       [-0.00124535]])]\n",
      "gradients_biases:  [array([ 0.0029604 , -0.00311567, -0.00020377]), array([-0.0021243])]\n",
      "Iteration 887, Cost: 0.24488106589810366\n",
      "gradient_weights:  [array([[ 0.00603329, -0.00329074,  0.00022804],\n",
      "       [ 0.01272099,  0.00157527,  0.00064915]]), array([[ 0.00713532],\n",
      "       [ 0.00358258],\n",
      "       [-0.00124705]])]\n",
      "gradients_biases:  [array([ 0.00295849, -0.00311801, -0.00020422]), array([-0.0021275])]\n",
      "Iteration 888, Cost: 0.24486607726712356\n",
      "gradient_weights:  [array([[ 0.00604659, -0.00329422,  0.00022792],\n",
      "       [ 0.01273178,  0.00157776,  0.00065072]]), array([[ 0.00715498],\n",
      "       [ 0.00358339],\n",
      "       [-0.00124876]])]\n",
      "gradients_biases:  [array([ 0.00295654, -0.00312035, -0.00020467]), array([-0.00213069])]\n",
      "Iteration 889, Cost: 0.2448510499001765\n",
      "gradient_weights:  [array([[ 0.0060599 , -0.00329769,  0.0002278 ],\n",
      "       [ 0.01274258,  0.00158024,  0.0006523 ]]), array([[ 0.00717467],\n",
      "       [ 0.00358421],\n",
      "       [-0.00125047]])]\n",
      "gradients_biases:  [array([ 0.00295457, -0.00312269, -0.00020512]), array([-0.00213389])]\n",
      "Iteration 890, Cost: 0.24483598370986176\n",
      "gradient_weights:  [array([[ 0.00607322, -0.00330118,  0.00022768],\n",
      "       [ 0.01275337,  0.00158273,  0.00065389]]), array([[ 0.00719436],\n",
      "       [ 0.00358503],\n",
      "       [-0.00125218]])]\n",
      "gradients_biases:  [array([ 0.00295257, -0.00312503, -0.00020557]), array([-0.00213709])]\n",
      "Iteration 891, Cost: 0.24482087860877416\n",
      "gradient_weights:  [array([[ 0.00608655, -0.00330466,  0.00022756],\n",
      "       [ 0.01276417,  0.00158522,  0.00065548]]), array([[ 0.00721407],\n",
      "       [ 0.00358586],\n",
      "       [-0.0012539 ]])]\n",
      "gradients_biases:  [array([ 0.00295053, -0.00312737, -0.00020602]), array([-0.0021403])]\n",
      "Iteration 892, Cost: 0.24480573450950457\n",
      "gradient_weights:  [array([[ 0.00609989, -0.00330816,  0.00022744],\n",
      "       [ 0.01277496,  0.00158771,  0.00065707]]), array([[ 0.0072338 ],\n",
      "       [ 0.0035867 ],\n",
      "       [-0.00125562]])]\n",
      "gradients_biases:  [array([ 0.00294847, -0.0031297 , -0.00020647]), array([-0.00214351])]\n",
      "Iteration 893, Cost: 0.244790551324641\n",
      "gradient_weights:  [array([[ 0.00611324, -0.00331165,  0.00022733],\n",
      "       [ 0.01278576,  0.00159021,  0.00065866]]), array([[ 0.00725354],\n",
      "       [ 0.00358754],\n",
      "       [-0.00125735]])]\n",
      "gradients_biases:  [array([ 0.00294637, -0.00313204, -0.00020693]), array([-0.00214673])]\n",
      "Iteration 894, Cost: 0.24477532896676946\n",
      "gradient_weights:  [array([[ 0.0061266 , -0.00331515,  0.00022722],\n",
      "       [ 0.01279656,  0.0015927 ,  0.00066026]]), array([[ 0.00727329],\n",
      "       [ 0.00358838],\n",
      "       [-0.00125908]])]\n",
      "gradients_biases:  [array([ 0.00294425, -0.00313437, -0.00020738]), array([-0.00214995])]\n",
      "Iteration 895, Cost: 0.24476006734847452\n",
      "gradient_weights:  [array([[ 0.00613997, -0.00331866,  0.00022711],\n",
      "       [ 0.01280736,  0.0015952 ,  0.00066186]]), array([[ 0.00729306],\n",
      "       [ 0.00358924],\n",
      "       [-0.00126081]])]\n",
      "gradients_biases:  [array([ 0.00294209, -0.00313671, -0.00020784]), array([-0.00215317])]\n",
      "Iteration 896, Cost: 0.2447447663823405\n",
      "gradient_weights:  [array([[ 0.00615334, -0.00332216,  0.000227  ],\n",
      "       [ 0.01281816,  0.0015977 ,  0.00066346]]), array([[ 0.00731284],\n",
      "       [ 0.0035901 ],\n",
      "       [-0.00126255]])]\n",
      "gradients_biases:  [array([ 0.00293991, -0.00313904, -0.0002083 ]), array([-0.0021564])]\n",
      "Iteration 897, Cost: 0.2447294259809521\n",
      "gradient_weights:  [array([[ 0.00616673, -0.00332568,  0.00022689],\n",
      "       [ 0.01282897,  0.0016002 ,  0.00066507]]), array([[ 0.00733264],\n",
      "       [ 0.00359096],\n",
      "       [-0.0012643 ]])]\n",
      "gradients_biases:  [array([ 0.00293769, -0.00314138, -0.00020876]), array([-0.00215963])]\n",
      "Iteration 898, Cost: 0.24471404605689517\n",
      "gradient_weights:  [array([[ 0.00618013, -0.0033292 ,  0.00022678],\n",
      "       [ 0.01283977,  0.0016027 ,  0.00066668]]), array([[ 0.00735245],\n",
      "       [ 0.00359183],\n",
      "       [-0.00126604]])]\n",
      "gradients_biases:  [array([ 0.00293544, -0.00314371, -0.00020921]), array([-0.00216287])]\n",
      "Iteration 899, Cost: 0.2446986265227578\n",
      "gradient_weights:  [array([[ 0.00619353, -0.00333272,  0.00022668],\n",
      "       [ 0.01285058,  0.0016052 ,  0.0006683 ]]), array([[ 0.00737228],\n",
      "       [ 0.00359271],\n",
      "       [-0.00126779]])]\n",
      "gradients_biases:  [array([ 0.00293316, -0.00314604, -0.00020968]), array([-0.0021661])]\n",
      "Iteration 900, Cost: 0.24468316729113115\n",
      "gradient_weights:  [array([[ 0.00620695, -0.00333624,  0.00022658],\n",
      "       [ 0.01286138,  0.00160771,  0.00066992]]), array([[ 0.00739212],\n",
      "       [ 0.00359359],\n",
      "       [-0.00126955]])]\n",
      "gradients_biases:  [array([ 0.00293085, -0.00314837, -0.00021014]), array([-0.00216935])]\n",
      "Iteration 901, Cost: 0.24466766827460978\n",
      "gradient_weights:  [array([[ 0.00622038, -0.00333978,  0.00022648],\n",
      "       [ 0.01287219,  0.00161022,  0.00067154]]), array([[ 0.00741198],\n",
      "       [ 0.00359448],\n",
      "       [-0.00127131]])]\n",
      "gradients_biases:  [array([ 0.0029285, -0.0031507, -0.0002106]), array([-0.00217259])]\n",
      "Iteration 902, Cost: 0.24465212938579334\n",
      "gradient_weights:  [array([[ 0.00623381, -0.00334331,  0.00022638],\n",
      "       [ 0.012883  ,  0.00161273,  0.00067317]]), array([[ 0.00743184],\n",
      "       [ 0.00359537],\n",
      "       [-0.00127307]])]\n",
      "gradients_biases:  [array([ 0.00292613, -0.00315303, -0.00021106]), array([-0.00217585])]\n",
      "Iteration 903, Cost: 0.24463655053728672\n",
      "gradient_weights:  [array([[ 0.00624726, -0.00334685,  0.00022628],\n",
      "       [ 0.01289381,  0.00161524,  0.00067479]]), array([[ 0.00745173],\n",
      "       [ 0.00359627],\n",
      "       [-0.00127484]])]\n",
      "gradients_biases:  [array([ 0.00292372, -0.00315536, -0.00021153]), array([-0.0021791])]\n",
      "Iteration 904, Cost: 0.24462093164170107\n",
      "gradient_weights:  [array([[ 0.00626071, -0.0033504 ,  0.00022618],\n",
      "       [ 0.01290462,  0.00161775,  0.00067643]]), array([[ 0.00747162],\n",
      "       [ 0.00359718],\n",
      "       [-0.00127661]])]\n",
      "gradients_biases:  [array([ 0.00292128, -0.00315769, -0.00021199]), array([-0.00218236])]\n",
      "Iteration 905, Cost: 0.24460527261165488\n",
      "gradient_weights:  [array([[ 0.00627417, -0.00335394,  0.00022609],\n",
      "       [ 0.01291543,  0.00162027,  0.00067806]]), array([[ 0.00749154],\n",
      "       [ 0.00359809],\n",
      "       [-0.00127839]])]\n",
      "gradients_biases:  [array([ 0.00291882, -0.00316001, -0.00021246]), array([-0.00218562])]\n",
      "Iteration 906, Cost: 0.24458957335977455\n",
      "gradient_weights:  [array([[ 0.00628764, -0.0033575 ,  0.000226  ],\n",
      "       [ 0.01292624,  0.00162278,  0.0006797 ]]), array([[ 0.00751146],\n",
      "       [ 0.003599  ],\n",
      "       [-0.00128017]])]\n",
      "gradients_biases:  [array([ 0.00291631, -0.00316234, -0.00021293]), array([-0.00218889])]\n",
      "Iteration 907, Cost: 0.2445738337986954\n",
      "gradient_weights:  [array([[ 0.00630113, -0.00336106,  0.0002259 ],\n",
      "       [ 0.01293705,  0.0016253 ,  0.00068135]]), array([[ 0.0075314 ],\n",
      "       [ 0.00359993],\n",
      "       [-0.00128196]])]\n",
      "gradients_biases:  [array([ 0.00291378, -0.00316466, -0.00021339]), array([-0.00219216])]\n",
      "Iteration 908, Cost: 0.24455805384106227\n",
      "gradient_weights:  [array([[ 0.00631462, -0.00336462,  0.00022581],\n",
      "       [ 0.01294786,  0.00162782,  0.00068299]]), array([[ 0.00755135],\n",
      "       [ 0.00360086],\n",
      "       [-0.00128375]])]\n",
      "gradients_biases:  [array([ 0.00291122, -0.00316699, -0.00021386]), array([-0.00219544])]\n",
      "Iteration 909, Cost: 0.2445422333995308\n",
      "gradient_weights:  [array([[ 0.00632812, -0.00336819,  0.00022573],\n",
      "       [ 0.01295867,  0.00163034,  0.00068464]]), array([[ 0.00757132],\n",
      "       [ 0.00360179],\n",
      "       [-0.00128554]])]\n",
      "gradients_biases:  [array([ 0.00290862, -0.00316931, -0.00021433]), array([-0.00219872])]\n",
      "Iteration 910, Cost: 0.24452637238676778\n",
      "gradient_weights:  [array([[ 0.00634162, -0.00337176,  0.00022564],\n",
      "       [ 0.01296949,  0.00163287,  0.0006863 ]]), array([[ 0.0075913 ],\n",
      "       [ 0.00360273],\n",
      "       [-0.00128734]])]\n",
      "gradients_biases:  [array([ 0.00290599, -0.00317164, -0.0002148 ]), array([-0.002202])]\n",
      "Iteration 911, Cost: 0.2445104707154524\n",
      "gradient_weights:  [array([[ 0.00635514, -0.00337533,  0.00022556],\n",
      "       [ 0.0129803 ,  0.00163539,  0.00068795]]), array([[ 0.00761129],\n",
      "       [ 0.00360368],\n",
      "       [-0.00128914]])]\n",
      "gradients_biases:  [array([ 0.00290333, -0.00317396, -0.00021528]), array([-0.00220529])]\n",
      "Iteration 912, Cost: 0.24449452829827678\n",
      "gradient_weights:  [array([[ 0.00636867, -0.00337892,  0.00022547],\n",
      "       [ 0.01299111,  0.00163792,  0.00068961]]), array([[ 0.0076313 ],\n",
      "       [ 0.00360463],\n",
      "       [-0.00129095]])]\n",
      "gradients_biases:  [array([ 0.00290064, -0.00317628, -0.00021575]), array([-0.00220858])]\n",
      "Iteration 913, Cost: 0.24447854504794697\n",
      "gradient_weights:  [array([[ 0.00638221, -0.0033825 ,  0.00022539],\n",
      "       [ 0.01300193,  0.00164045,  0.00069128]]), array([[ 0.00765132],\n",
      "       [ 0.00360559],\n",
      "       [-0.00129276]])]\n",
      "gradients_biases:  [array([ 0.00289792, -0.0031786 , -0.00021623]), array([-0.00221187])]\n",
      "Iteration 914, Cost: 0.24446252087718387\n",
      "gradient_weights:  [array([[ 0.00639575, -0.00338609,  0.00022531],\n",
      "       [ 0.01301274,  0.00164298,  0.00069295]]), array([[ 0.00767135],\n",
      "       [ 0.00360655],\n",
      "       [-0.00129457]])]\n",
      "gradients_biases:  [array([ 0.00289516, -0.00318092, -0.0002167 ]), array([-0.00221517])]\n",
      "Iteration 915, Cost: 0.24444645569872392\n",
      "gradient_weights:  [array([[ 0.00640931, -0.00338968,  0.00022523],\n",
      "       [ 0.01302355,  0.00164551,  0.00069462]]), array([[ 0.0076914 ],\n",
      "       [ 0.00360752],\n",
      "       [-0.00129639]])]\n",
      "gradients_biases:  [array([ 0.00289237, -0.00318324, -0.00021718]), array([-0.00221848])]\n",
      "Iteration 916, Cost: 0.24443034942531977\n",
      "gradient_weights:  [array([[ 0.00642287, -0.00339328,  0.00022516],\n",
      "       [ 0.01303436,  0.00164804,  0.00069629]]), array([[ 0.00771146],\n",
      "       [ 0.00360849],\n",
      "       [-0.00129822]])]\n",
      "gradients_biases:  [array([ 0.00288955, -0.00318556, -0.00021766]), array([-0.00222178])]\n",
      "Iteration 917, Cost: 0.24441420196974173\n",
      "gradient_weights:  [array([[ 0.00643644, -0.00339689,  0.00022508],\n",
      "       [ 0.01304518,  0.00165058,  0.00069797]]), array([[ 0.00773153],\n",
      "       [ 0.00360947],\n",
      "       [-0.00130005]])]\n",
      "gradients_biases:  [array([ 0.00288669, -0.00318788, -0.00021813]), array([-0.00222509])]\n",
      "Iteration 918, Cost: 0.24439801324477803\n",
      "gradient_weights:  [array([[ 0.00645002, -0.0034005 ,  0.00022501],\n",
      "       [ 0.01305599,  0.00165311,  0.00069966]]), array([[ 0.00775162],\n",
      "       [ 0.00361046],\n",
      "       [-0.00130188]])]\n",
      "gradients_biases:  [array([ 0.00288381, -0.00319019, -0.00021861]), array([-0.00222841])]\n",
      "Iteration 919, Cost: 0.24438178316323575\n",
      "gradient_weights:  [array([[ 0.00646361, -0.00340411,  0.00022494],\n",
      "       [ 0.0130668 ,  0.00165565,  0.00070134]]), array([[ 0.00777172],\n",
      "       [ 0.00361145],\n",
      "       [-0.00130372]])]\n",
      "gradients_biases:  [array([ 0.00288089, -0.00319251, -0.00021909]), array([-0.00223173])]\n",
      "Iteration 920, Cost: 0.2443655116379419\n",
      "gradient_weights:  [array([[ 0.00647721, -0.00340773,  0.00022487],\n",
      "       [ 0.01307761,  0.00165819,  0.00070303]]), array([[ 0.00779183],\n",
      "       [ 0.00361245],\n",
      "       [-0.00130556]])]\n",
      "gradients_biases:  [array([ 0.00287793, -0.00319483, -0.00021958]), array([-0.00223505])]\n",
      "Iteration 921, Cost: 0.24434919858174417\n",
      "gradient_weights:  [array([[ 0.00649082, -0.00341135,  0.0002248 ],\n",
      "       [ 0.01308842,  0.00166074,  0.00070473]]), array([[ 0.00781195],\n",
      "       [ 0.00361346],\n",
      "       [-0.0013074 ]])]\n",
      "gradients_biases:  [array([ 0.00287495, -0.00319714, -0.00022006]), array([-0.00223838])]\n",
      "Iteration 922, Cost: 0.24433284390751137\n",
      "gradient_weights:  [array([[ 0.00650444, -0.00341498,  0.00022474],\n",
      "       [ 0.01309923,  0.00166328,  0.00070642]]), array([[ 0.00783209],\n",
      "       [ 0.00361446],\n",
      "       [-0.00130925]])]\n",
      "gradients_biases:  [array([ 0.00287193, -0.00319945, -0.00022054]), array([-0.00224171])]\n",
      "Iteration 923, Cost: 0.24431644752813514\n",
      "gradient_weights:  [array([[ 0.00651806, -0.00341861,  0.00022467],\n",
      "       [ 0.01311004,  0.00166582,  0.00070813]]), array([[ 0.00785224],\n",
      "       [ 0.00361548],\n",
      "       [-0.00131111]])]\n",
      "gradients_biases:  [array([ 0.00286888, -0.00320177, -0.00022103]), array([-0.00224505])]\n",
      "Iteration 924, Cost: 0.24430000935652982\n",
      "gradient_weights:  [array([[ 0.0065317 , -0.00342224,  0.00022461],\n",
      "       [ 0.01312085,  0.00166837,  0.00070983]]), array([[ 0.00787241],\n",
      "       [ 0.0036165 ],\n",
      "       [-0.00131297]])]\n",
      "gradients_biases:  [array([ 0.00286579, -0.00320408, -0.00022151]), array([-0.00224839])]\n",
      "Iteration 925, Cost: 0.24428352930563393\n",
      "gradient_weights:  [array([[ 0.00654534, -0.00342588,  0.00022455],\n",
      "       [ 0.01313166,  0.00167092,  0.00071154]]), array([[ 0.00789258],\n",
      "       [ 0.00361753],\n",
      "       [-0.00131483]])]\n",
      "gradients_biases:  [array([ 0.00286267, -0.00320639, -0.000222  ]), array([-0.00225174])]\n",
      "Iteration 926, Cost: 0.24426700728841078\n",
      "gradient_weights:  [array([[ 0.00655899, -0.00342953,  0.00022449],\n",
      "       [ 0.01314247,  0.00167347,  0.00071325]]), array([[ 0.00791277],\n",
      "       [ 0.00361856],\n",
      "       [-0.0013167 ]])]\n",
      "gradients_biases:  [array([ 0.00285952, -0.0032087 , -0.00022249]), array([-0.00225509])]\n",
      "Iteration 927, Cost: 0.24425044321784933\n",
      "gradient_weights:  [array([[ 0.00657265, -0.00343318,  0.00022443],\n",
      "       [ 0.01315327,  0.00167602,  0.00071497]]), array([[ 0.00793297],\n",
      "       [ 0.0036196 ],\n",
      "       [-0.00131857]])]\n",
      "gradients_biases:  [array([ 0.00285634, -0.00321101, -0.00022298]), array([-0.00225844])]\n",
      "Iteration 928, Cost: 0.24423383700696483\n",
      "gradient_weights:  [array([[ 0.00658632, -0.00343683,  0.00022437],\n",
      "       [ 0.01316408,  0.00167858,  0.00071669]]), array([[ 0.00795319],\n",
      "       [ 0.00362064],\n",
      "       [-0.00132045]])]\n",
      "gradients_biases:  [array([ 0.00285312, -0.00321332, -0.00022347]), array([-0.0022618])]\n",
      "Iteration 929, Cost: 0.24421718856880015\n",
      "gradient_weights:  [array([[ 0.0066    , -0.00344049,  0.00022432],\n",
      "       [ 0.01317489,  0.00168113,  0.00071841]]), array([[ 0.00797341],\n",
      "       [ 0.00362169],\n",
      "       [-0.00132233]])]\n",
      "gradients_biases:  [array([ 0.00284987, -0.00321563, -0.00022396]), array([-0.00226516])]\n",
      "Iteration 930, Cost: 0.244200497816426\n",
      "gradient_weights:  [array([[ 0.00661369, -0.00344416,  0.00022427],\n",
      "       [ 0.01318569,  0.00168369,  0.00072014]]), array([[ 0.00799365],\n",
      "       [ 0.00362275],\n",
      "       [-0.00132421]])]\n",
      "gradients_biases:  [array([ 0.00284659, -0.00321793, -0.00022445]), array([-0.00226852])]\n",
      "Iteration 931, Cost: 0.2441837646629423\n",
      "gradient_weights:  [array([[ 0.00662738, -0.00344782,  0.00022422],\n",
      "       [ 0.01319649,  0.00168625,  0.00072187]]), array([[ 0.0080139 ],\n",
      "       [ 0.00362381],\n",
      "       [-0.0013261 ]])]\n",
      "gradients_biases:  [array([ 0.00284327, -0.00322024, -0.00022495]), array([-0.00227189])]\n",
      "Iteration 932, Cost: 0.24416698902147851\n",
      "gradient_weights:  [array([[ 0.00664108, -0.0034515 ,  0.00022417],\n",
      "       [ 0.01320729,  0.00168881,  0.0007236 ]]), array([[ 0.00803417],\n",
      "       [ 0.00362488],\n",
      "       [-0.001328  ]])]\n",
      "gradients_biases:  [array([ 0.00283991, -0.00322255, -0.00022544]), array([-0.00227527])]\n",
      "Iteration 933, Cost: 0.244150170805195\n",
      "gradient_weights:  [array([[ 0.0066548 , -0.00345518,  0.00022412],\n",
      "       [ 0.01321809,  0.00169137,  0.00072534]]), array([[ 0.00805444],\n",
      "       [ 0.00362595],\n",
      "       [-0.0013299 ]])]\n",
      "gradients_biases:  [array([ 0.00283653, -0.00322485, -0.00022594]), array([-0.00227864])]\n",
      "Iteration 934, Cost: 0.2441333099272833\n",
      "gradient_weights:  [array([[ 0.00666852, -0.00345886,  0.00022407],\n",
      "       [ 0.01322889,  0.00169394,  0.00072708]]), array([[ 0.00807473],\n",
      "       [ 0.00362703],\n",
      "       [-0.0013318 ]])]\n",
      "gradients_biases:  [array([ 0.00283311, -0.00322715, -0.00022643]), array([-0.00228203])]\n",
      "Iteration 935, Cost: 0.24411640630096745\n",
      "gradient_weights:  [array([[ 0.00668224, -0.00346255,  0.00022403],\n",
      "       [ 0.01323969,  0.0016965 ,  0.00072883]]), array([[ 0.00809503],\n",
      "       [ 0.00362811],\n",
      "       [-0.00133371]])]\n",
      "gradients_biases:  [array([ 0.00282966, -0.00322946, -0.00022693]), array([-0.00228541])]\n",
      "Iteration 936, Cost: 0.24409945983950448\n",
      "gradient_weights:  [array([[ 0.00669598, -0.00346624,  0.00022399],\n",
      "       [ 0.01325049,  0.00169907,  0.00073058]]), array([[ 0.00811534],\n",
      "       [ 0.0036292 ],\n",
      "       [-0.00133562]])]\n",
      "gradients_biases:  [array([ 0.00282617, -0.00323176, -0.00022743]), array([-0.0022888])]\n",
      "Iteration 937, Cost: 0.24408247045618525\n",
      "gradient_weights:  [array([[ 0.00670973, -0.00346994,  0.00022395],\n",
      "       [ 0.01326128,  0.00170164,  0.00073233]]), array([[ 0.00813566],\n",
      "       [ 0.0036303 ],\n",
      "       [-0.00133754]])]\n",
      "gradients_biases:  [array([ 0.00282265, -0.00323406, -0.00022793]), array([-0.0022922])]\n",
      "Iteration 938, Cost: 0.2440654380643355\n",
      "gradient_weights:  [array([[ 0.00672348, -0.00347364,  0.00022391],\n",
      "       [ 0.01327207,  0.00170421,  0.00073409]]), array([[ 0.00815599],\n",
      "       [ 0.0036314 ],\n",
      "       [-0.00133946]])]\n",
      "gradients_biases:  [array([ 0.00281909, -0.00323636, -0.00022843]), array([-0.0022956])]\n",
      "Iteration 939, Cost: 0.2440483625773165\n",
      "gradient_weights:  [array([[ 0.00673724, -0.00347734,  0.00022387],\n",
      "       [ 0.01328286,  0.00170678,  0.00073585]]), array([[ 0.00817634],\n",
      "       [ 0.00363251],\n",
      "       [-0.00134139]])]\n",
      "gradients_biases:  [array([ 0.0028155 , -0.00323866, -0.00022894]), array([-0.002299])]\n",
      "Iteration 940, Cost: 0.24403124390852582\n",
      "gradient_weights:  [array([[ 0.00675101, -0.00348106,  0.00022383],\n",
      "       [ 0.01329365,  0.00170935,  0.00073762]]), array([[ 0.0081967 ],\n",
      "       [ 0.00363362],\n",
      "       [-0.00134332]])]\n",
      "gradients_biases:  [array([ 0.00281188, -0.00324096, -0.00022944]), array([-0.00230241])]\n",
      "Iteration 941, Cost: 0.2440140819713984\n",
      "gradient_weights:  [array([[ 0.00676479, -0.00348477,  0.0002238 ],\n",
      "       [ 0.01330444,  0.00171193,  0.00073939]]), array([[ 0.00821707],\n",
      "       [ 0.00363474],\n",
      "       [-0.00134526]])]\n",
      "gradients_biases:  [array([ 0.00280822, -0.00324326, -0.00022994]), array([-0.00230582])]\n",
      "Iteration 942, Cost: 0.2439968766794069\n",
      "gradient_weights:  [array([[ 0.00677857, -0.00348849,  0.00022377],\n",
      "       [ 0.01331523,  0.00171451,  0.00074116]]), array([[ 0.00823745],\n",
      "       [ 0.00363586],\n",
      "       [-0.0013472 ]])]\n",
      "gradients_biases:  [array([ 0.00280452, -0.00324555, -0.00023045]), array([-0.00230924])]\n",
      "Iteration 943, Cost: 0.24397962794606293\n",
      "gradient_weights:  [array([[ 0.00679237, -0.00349222,  0.00022374],\n",
      "       [ 0.01332601,  0.00171708,  0.00074294]]), array([[ 0.00825784],\n",
      "       [ 0.00363699],\n",
      "       [-0.00134914]])]\n",
      "gradients_biases:  [array([ 0.0028008 , -0.00324785, -0.00023096]), array([-0.00231266])]\n",
      "Iteration 944, Cost: 0.2439623356849179\n",
      "gradient_weights:  [array([[ 0.00680617, -0.00349595,  0.00022371],\n",
      "       [ 0.01333679,  0.00171966,  0.00074472]]), array([[ 0.00827824],\n",
      "       [ 0.00363813],\n",
      "       [-0.00135109]])]\n",
      "gradients_biases:  [array([ 0.00279703, -0.00325014, -0.00023147]), array([-0.00231608])]\n",
      "Iteration 945, Cost: 0.24394499980956358\n",
      "gradient_weights:  [array([[ 0.00681998, -0.00349968,  0.00022368],\n",
      "       [ 0.01334757,  0.00172225,  0.0007465 ]]), array([[ 0.00829866],\n",
      "       [ 0.00363927],\n",
      "       [-0.00135304]])]\n",
      "gradients_biases:  [array([ 0.00279324, -0.00325244, -0.00023197]), array([-0.00231951])]\n",
      "Iteration 946, Cost: 0.24392762023363268\n",
      "gradient_weights:  [array([[ 0.0068338 , -0.00350342,  0.00022366],\n",
      "       [ 0.01335835,  0.00172483,  0.00074829]]), array([[ 0.00831908],\n",
      "       [ 0.00364041],\n",
      "       [-0.001355  ]])]\n",
      "gradients_biases:  [array([ 0.00278941, -0.00325473, -0.00023248]), array([-0.00232294])]\n",
      "Iteration 947, Cost: 0.2439101968708004\n",
      "gradient_weights:  [array([[ 0.00684763, -0.00350717,  0.00022363],\n",
      "       [ 0.01336912,  0.00172741,  0.00075009]]), array([[ 0.00833952],\n",
      "       [ 0.00364157],\n",
      "       [-0.00135697]])]\n",
      "gradients_biases:  [array([ 0.00278554, -0.00325702, -0.000233  ]), array([-0.00232638])]\n",
      "Iteration 948, Cost: 0.24389272963478456\n",
      "gradient_weights:  [array([[ 0.00686146, -0.00351092,  0.00022361],\n",
      "       [ 0.01337989,  0.00173   ,  0.00075188]]), array([[ 0.00835997],\n",
      "       [ 0.00364272],\n",
      "       [-0.00135893]])]\n",
      "gradients_biases:  [array([ 0.00278164, -0.00325931, -0.00023351]), array([-0.00232982])]\n",
      "Iteration 949, Cost: 0.24387521843934676\n",
      "gradient_weights:  [array([[ 0.0068753 , -0.00351467,  0.00022359],\n",
      "       [ 0.01339066,  0.00173259,  0.00075368]]), array([[ 0.00838043],\n",
      "       [ 0.00364389],\n",
      "       [-0.00136091]])]\n",
      "gradients_biases:  [array([ 0.0027777 , -0.0032616 , -0.00023402]), array([-0.00233327])]\n",
      "Iteration 950, Cost: 0.24385766319829288\n",
      "gradient_weights:  [array([[ 0.00688915, -0.00351843,  0.00022358],\n",
      "       [ 0.01340143,  0.00173518,  0.00075549]]), array([[ 0.0084009 ],\n",
      "       [ 0.00364506],\n",
      "       [-0.00136288]])]\n",
      "gradients_biases:  [array([ 0.00277373, -0.00326389, -0.00023454]), array([-0.00233672])]\n",
      "Iteration 951, Cost: 0.24384006382547424\n",
      "gradient_weights:  [array([[ 0.00690301, -0.00352219,  0.00022356],\n",
      "       [ 0.0134122 ,  0.00173777,  0.0007573 ]]), array([[ 0.00842138],\n",
      "       [ 0.00364623],\n",
      "       [-0.00136486]])]\n",
      "gradients_biases:  [array([ 0.00276973, -0.00326618, -0.00023505]), array([-0.00234017])]\n",
      "Iteration 952, Cost: 0.24382242023478828\n",
      "gradient_weights:  [array([[ 0.00691687, -0.00352596,  0.00022354],\n",
      "       [ 0.01342296,  0.00174036,  0.00075911]]), array([[ 0.00844187],\n",
      "       [ 0.00364741],\n",
      "       [-0.00136685]])]\n",
      "gradients_biases:  [array([ 0.00276569, -0.00326847, -0.00023557]), array([-0.00234363])]\n",
      "Iteration 953, Cost: 0.24380473234017896\n",
      "gradient_weights:  [array([[ 0.00693075, -0.00352974,  0.00022353],\n",
      "       [ 0.01343372,  0.00174296,  0.00076093]]), array([[ 0.00846237],\n",
      "       [ 0.0036486 ],\n",
      "       [-0.00136884]])]\n",
      "gradients_biases:  [array([ 0.00276161, -0.00327076, -0.00023609]), array([-0.0023471])]\n",
      "Iteration 954, Cost: 0.2437870000556383\n",
      "gradient_weights:  [array([[ 0.00694463, -0.00353352,  0.00022352],\n",
      "       [ 0.01344448,  0.00174555,  0.00076275]]), array([[ 0.00848288],\n",
      "       [ 0.00364979],\n",
      "       [-0.00137084]])]\n",
      "gradients_biases:  [array([ 0.0027575 , -0.00327304, -0.00023661]), array([-0.00235057])]\n",
      "Iteration 955, Cost: 0.2437692232952065\n",
      "gradient_weights:  [array([[ 0.00695852, -0.0035373 ,  0.00022351],\n",
      "       [ 0.01345523,  0.00174815,  0.00076457]]), array([[ 0.0085034 ],\n",
      "       [ 0.00365099],\n",
      "       [-0.00137284]])]\n",
      "gradients_biases:  [array([ 0.00275335, -0.00327533, -0.00023713]), array([-0.00235404])]\n",
      "Iteration 956, Cost: 0.24375140197297304\n",
      "gradient_weights:  [array([[ 0.00697241, -0.00354109,  0.0002235 ],\n",
      "       [ 0.01346598,  0.00175075,  0.0007664 ]]), array([[ 0.00852394],\n",
      "       [ 0.00365219],\n",
      "       [-0.00137484]])]\n",
      "gradients_biases:  [array([ 0.00274917, -0.00327761, -0.00023765]), array([-0.00235751])]\n",
      "Iteration 957, Cost: 0.24373353600307746\n",
      "gradient_weights:  [array([[ 0.00698632, -0.00354488,  0.0002235 ],\n",
      "       [ 0.01347673,  0.00175335,  0.00076823]]), array([[ 0.00854448],\n",
      "       [ 0.0036534 ],\n",
      "       [-0.00137686]])]\n",
      "gradients_biases:  [array([ 0.00274495, -0.00327989, -0.00023817]), array([-0.002361])]\n",
      "Iteration 958, Cost: 0.24371562529971008\n",
      "gradient_weights:  [array([[ 0.00700023, -0.00354868,  0.00022349],\n",
      "       [ 0.01348748,  0.00175596,  0.00077007]]), array([[ 0.00856503],\n",
      "       [ 0.00365462],\n",
      "       [-0.00137887]])]\n",
      "gradients_biases:  [array([ 0.0027407 , -0.00328217, -0.0002387 ]), array([-0.00236448])]\n",
      "Iteration 959, Cost: 0.24369766977711277\n",
      "gradient_weights:  [array([[ 0.00701415, -0.00355248,  0.00022349],\n",
      "       [ 0.01349822,  0.00175856,  0.00077191]]), array([[ 0.0085856 ],\n",
      "       [ 0.00365584],\n",
      "       [-0.00138089]])]\n",
      "gradients_biases:  [array([ 0.00273641, -0.00328445, -0.00023922]), array([-0.00236797])]\n",
      "Iteration 960, Cost: 0.2436796693495798\n",
      "gradient_weights:  [array([[ 0.00702807, -0.00355629,  0.00022349],\n",
      "       [ 0.01350896,  0.00176117,  0.00077375]]), array([[ 0.00860617],\n",
      "       [ 0.00365706],\n",
      "       [-0.00138291]])]\n",
      "gradients_biases:  [array([ 0.00273209, -0.00328673, -0.00023975]), array([-0.00237146])]\n",
      "Iteration 961, Cost: 0.24366162393145863\n",
      "gradient_weights:  [array([[ 0.00704201, -0.0035601 ,  0.00022349],\n",
      "       [ 0.0135197 ,  0.00176377,  0.0007756 ]]), array([[ 0.00862675],\n",
      "       [ 0.0036583 ],\n",
      "       [-0.00138494]])]\n",
      "gradients_biases:  [array([ 0.00272773, -0.00328901, -0.00024028]), array([-0.00237496])]\n",
      "Iteration 962, Cost: 0.24364353343715045\n",
      "gradient_weights:  [array([[ 0.00705595, -0.00356392,  0.00022349],\n",
      "       [ 0.01353043,  0.00176638,  0.00077746]]), array([[ 0.00864735],\n",
      "       [ 0.00365953],\n",
      "       [-0.00138698]])]\n",
      "gradients_biases:  [array([ 0.00272333, -0.00329129, -0.00024081]), array([-0.00237847])]\n",
      "Iteration 963, Cost: 0.2436253977811113\n",
      "gradient_weights:  [array([[ 0.0070699 , -0.00356774,  0.0002235 ],\n",
      "       [ 0.01354116,  0.00176899,  0.00077931]]), array([[ 0.00866795],\n",
      "       [ 0.00366077],\n",
      "       [-0.00138902]])]\n",
      "gradients_biases:  [array([ 0.0027189 , -0.00329357, -0.00024134]), array([-0.00238197])]\n",
      "Iteration 964, Cost: 0.24360721687785275\n",
      "gradient_weights:  [array([[ 0.00708385, -0.00357157,  0.00022351],\n",
      "       [ 0.01355189,  0.0017716 ,  0.00078117]]), array([[ 0.00868856],\n",
      "       [ 0.00366202],\n",
      "       [-0.00139106]])]\n",
      "gradients_biases:  [array([ 0.00271443, -0.00329584, -0.00024187]), array([-0.00238549])]\n",
      "Iteration 965, Cost: 0.2435889906419424\n",
      "gradient_weights:  [array([[ 0.00709782, -0.0035754 ,  0.00022351],\n",
      "       [ 0.01356261,  0.00177422,  0.00078304]]), array([[ 0.00870919],\n",
      "       [ 0.00366328],\n",
      "       [-0.00139311]])]\n",
      "gradients_biases:  [array([ 0.00270993, -0.00329812, -0.0002424 ]), array([-0.002389])]\n",
      "Iteration 966, Cost: 0.24357071898800495\n",
      "gradient_weights:  [array([[ 0.00711179, -0.00357924,  0.00022352],\n",
      "       [ 0.01357333,  0.00177683,  0.00078491]]), array([[ 0.00872982],\n",
      "       [ 0.00366454],\n",
      "       [-0.00139516]])]\n",
      "gradients_biases:  [array([ 0.00270539, -0.00330039, -0.00024293]), array([-0.00239252])]\n",
      "Iteration 967, Cost: 0.24355240183072288\n",
      "gradient_weights:  [array([[ 0.00712576, -0.00358308,  0.00022354],\n",
      "       [ 0.01358404,  0.00177945,  0.00078678]]), array([[ 0.00875046],\n",
      "       [ 0.0036658 ],\n",
      "       [-0.00139722]])]\n",
      "gradients_biases:  [array([ 0.00270081, -0.00330266, -0.00024347]), array([-0.00239605])]\n",
      "Iteration 968, Cost: 0.24353403908483706\n",
      "gradient_weights:  [array([[ 0.00713975, -0.00358693,  0.00022355],\n",
      "       [ 0.01359475,  0.00178207,  0.00078866]]), array([[ 0.00877111],\n",
      "       [ 0.00366707],\n",
      "       [-0.00139928]])]\n",
      "gradients_biases:  [array([ 0.0026962 , -0.00330493, -0.00024401]), array([-0.00239958])]\n",
      "Iteration 969, Cost: 0.2435156306651478\n",
      "gradient_weights:  [array([[ 0.00715374, -0.00359078,  0.00022357],\n",
      "       [ 0.01360546,  0.00178469,  0.00079054]]), array([[ 0.00879178],\n",
      "       [ 0.00366835],\n",
      "       [-0.00140135]])]\n",
      "gradients_biases:  [array([ 0.00269155, -0.00330721, -0.00024454]), array([-0.00240311])]\n",
      "Iteration 970, Cost: 0.24349717648651542\n",
      "gradient_weights:  [array([[ 0.00716774, -0.00359464,  0.00022358],\n",
      "       [ 0.01361617,  0.00178731,  0.00079243]]), array([[ 0.00881245],\n",
      "       [ 0.00366963],\n",
      "       [-0.00140342]])]\n",
      "gradients_biases:  [array([ 0.00268687, -0.00330948, -0.00024508]), array([-0.00240665])]\n",
      "Iteration 971, Cost: 0.24347867646386084\n",
      "gradient_weights:  [array([[ 0.00718174, -0.0035985 ,  0.0002236 ],\n",
      "       [ 0.01362687,  0.00178993,  0.00079432]]), array([[ 0.00883312],\n",
      "       [ 0.00367092],\n",
      "       [-0.0014055 ]])]\n",
      "gradients_biases:  [array([ 0.00268215, -0.00331174, -0.00024562]), array([-0.00241019])]\n",
      "Iteration 972, Cost: 0.24346013051216678\n",
      "gradient_weights:  [array([[ 0.00719576, -0.00360237,  0.00022362],\n",
      "       [ 0.01363756,  0.00179255,  0.00079621]]), array([[ 0.00885381],\n",
      "       [ 0.00367221],\n",
      "       [-0.00140758]])]\n",
      "gradients_biases:  [array([ 0.00267739, -0.00331401, -0.00024616]), array([-0.00241374])]\n",
      "Iteration 973, Cost: 0.24344153854647807\n",
      "gradient_weights:  [array([[ 0.00720978, -0.00360624,  0.00022365],\n",
      "       [ 0.01364826,  0.00179518,  0.00079811]]), array([[ 0.00887451],\n",
      "       [ 0.00367351],\n",
      "       [-0.00140967]])]\n",
      "gradients_biases:  [array([ 0.0026726 , -0.00331628, -0.0002467 ]), array([-0.00241729])]\n",
      "Iteration 974, Cost: 0.24342290048190257\n",
      "gradient_weights:  [array([[ 0.0072238 , -0.00361012,  0.00022367],\n",
      "       [ 0.01365895,  0.00179781,  0.00080001]]), array([[ 0.00889522],\n",
      "       [ 0.00367481],\n",
      "       [-0.00141176]])]\n",
      "gradients_biases:  [array([ 0.00266777, -0.00331854, -0.00024725]), array([-0.00242085])]\n",
      "Iteration 975, Cost: 0.24340421623361197\n",
      "gradient_weights:  [array([[ 0.00723784, -0.003614  ,  0.0002237 ],\n",
      "       [ 0.01366963,  0.00180044,  0.00080192]]), array([[ 0.00891593],\n",
      "       [ 0.00367612],\n",
      "       [-0.00141386]])]\n",
      "gradients_biases:  [array([ 0.0026629 , -0.00332081, -0.00024779]), array([-0.00242441])]\n",
      "Iteration 976, Cost: 0.24338548571684246\n",
      "gradient_weights:  [array([[ 0.00725188, -0.00361789,  0.00022372],\n",
      "       [ 0.01368031,  0.00180307,  0.00080383]]), array([[ 0.00893666],\n",
      "       [ 0.00367744],\n",
      "       [-0.00141596]])]\n",
      "gradients_biases:  [array([ 0.002658  , -0.00332307, -0.00024834]), array([-0.00242798])]\n",
      "Iteration 977, Cost: 0.2433667088468955\n",
      "gradient_weights:  [array([[ 0.00726592, -0.00362178,  0.00022375],\n",
      "       [ 0.01369099,  0.0018057 ,  0.00080575]]), array([[ 0.00895739],\n",
      "       [ 0.00367876],\n",
      "       [-0.00141807]])]\n",
      "gradients_biases:  [array([ 0.00265306, -0.00332533, -0.00024888]), array([-0.00243155])]\n",
      "Iteration 978, Cost: 0.24334788553913844\n",
      "gradient_weights:  [array([[ 0.00727998, -0.00362568,  0.00022379],\n",
      "       [ 0.01370166,  0.00180833,  0.00080767]]), array([[ 0.00897813],\n",
      "       [ 0.00368009],\n",
      "       [-0.00142018]])]\n",
      "gradients_biases:  [array([ 0.00264808, -0.00332759, -0.00024943]), array([-0.00243512])]\n",
      "Iteration 979, Cost: 0.24332901570900545\n",
      "gradient_weights:  [array([[ 0.00729404, -0.00362958,  0.00022382],\n",
      "       [ 0.01371232,  0.00181097,  0.00080959]]), array([[ 0.00899888],\n",
      "       [ 0.00368142],\n",
      "       [-0.0014223 ]])]\n",
      "gradients_biases:  [array([ 0.00264307, -0.00332985, -0.00024998]), array([-0.0024387])]\n",
      "Iteration 980, Cost: 0.24331009927199804\n",
      "gradient_weights:  [array([[ 0.0073081 , -0.00363349,  0.00022386],\n",
      "       [ 0.01372299,  0.0018136 ,  0.00081152]]), array([[ 0.00901964],\n",
      "       [ 0.00368276],\n",
      "       [-0.00142442]])]\n",
      "gradients_biases:  [array([ 0.00263802, -0.00333211, -0.00025053]), array([-0.00244229])]\n",
      "Iteration 981, Cost: 0.24329113614368597\n",
      "gradient_weights:  [array([[ 0.00732218, -0.0036374 ,  0.00022389],\n",
      "       [ 0.01373364,  0.00181624,  0.00081345]]), array([[ 0.00904041],\n",
      "       [ 0.0036841 ],\n",
      "       [-0.00142655]])]\n",
      "gradients_biases:  [array([ 0.00263293, -0.00333437, -0.00025109]), array([-0.00244588])]\n",
      "Iteration 982, Cost: 0.2432721262397078\n",
      "gradient_weights:  [array([[ 0.00733626, -0.00364131,  0.00022393],\n",
      "       [ 0.0137443 ,  0.00181888,  0.00081539]]), array([[ 0.00906118],\n",
      "       [ 0.00368545],\n",
      "       [-0.00142868]])]\n",
      "gradients_biases:  [array([ 0.0026278 , -0.00333663, -0.00025164]), array([-0.00244947])]\n",
      "Iteration 983, Cost: 0.2432530694757718\n",
      "gradient_weights:  [array([[ 0.00735035, -0.00364524,  0.00022397],\n",
      "       [ 0.01375495,  0.00182152,  0.00081733]]), array([[ 0.00908197],\n",
      "       [ 0.0036868 ],\n",
      "       [-0.00143082]])]\n",
      "gradients_biases:  [array([ 0.00262264, -0.00333888, -0.00025219]), array([-0.00245307])]\n",
      "Iteration 984, Cost: 0.24323396576765643\n",
      "gradient_weights:  [array([[ 0.00736444, -0.00364916,  0.00022402],\n",
      "       [ 0.01376559,  0.00182416,  0.00081927]]), array([[ 0.00910276],\n",
      "       [ 0.00368816],\n",
      "       [-0.00143296]])]\n",
      "gradients_biases:  [array([ 0.00261744, -0.00334114, -0.00025275]), array([-0.00245667])]\n",
      "Iteration 985, Cost: 0.24321481503121126\n",
      "gradient_weights:  [array([[ 0.00737854, -0.0036531 ,  0.00022406],\n",
      "       [ 0.01377623,  0.00182681,  0.00082122]]), array([[ 0.00912356],\n",
      "       [ 0.00368953],\n",
      "       [-0.00143511]])]\n",
      "gradients_biases:  [array([ 0.00261221, -0.00334339, -0.00025331]), array([-0.00246028])]\n",
      "Iteration 986, Cost: 0.24319561718235772\n",
      "gradient_weights:  [array([[ 0.00739264, -0.00365703,  0.00022411],\n",
      "       [ 0.01378686,  0.00182945,  0.00082318]]), array([[ 0.00914437],\n",
      "       [ 0.0036909 ],\n",
      "       [-0.00143726]])]\n",
      "gradients_biases:  [array([ 0.00260694, -0.00334564, -0.00025386]), array([-0.00246389])]\n",
      "Iteration 987, Cost: 0.24317637213708942\n",
      "gradient_weights:  [array([[ 0.00740676, -0.00366098,  0.00022416],\n",
      "       [ 0.01379749,  0.0018321 ,  0.00082514]]), array([[ 0.00916518],\n",
      "       [ 0.00369228],\n",
      "       [-0.00143942]])]\n",
      "gradients_biases:  [array([ 0.00260163, -0.00334789, -0.00025442]), array([-0.00246751])]\n",
      "Iteration 988, Cost: 0.24315707981147322\n",
      "gradient_weights:  [array([[ 0.00742088, -0.00366493,  0.00022421],\n",
      "       [ 0.01380812,  0.00183475,  0.0008271 ]]), array([[ 0.009186  ],\n",
      "       [ 0.00369366],\n",
      "       [-0.00144158]])]\n",
      "gradients_biases:  [array([ 0.00259628, -0.00335014, -0.00025498]), array([-0.00247113])]\n",
      "Iteration 989, Cost: 0.24313774012165007\n",
      "gradient_weights:  [array([[ 0.007435  , -0.00366888,  0.00022426],\n",
      "       [ 0.01381873,  0.0018374 ,  0.00082907]]), array([[ 0.00920684],\n",
      "       [ 0.00369505],\n",
      "       [-0.00144375]])]\n",
      "gradients_biases:  [array([ 0.00259089, -0.00335239, -0.00025555]), array([-0.00247475])]\n",
      "Iteration 990, Cost: 0.24311835298383522\n",
      "gradient_weights:  [array([[ 0.00744913, -0.00367284,  0.00022431],\n",
      "       [ 0.01382935,  0.00184005,  0.00083104]]), array([[ 0.00922767],\n",
      "       [ 0.00369644],\n",
      "       [-0.00144592]])]\n",
      "gradients_biases:  [array([ 0.00258547, -0.00335464, -0.00025611]), array([-0.00247838])]\n",
      "Iteration 991, Cost: 0.2430989183143191\n",
      "gradient_weights:  [array([[ 0.00746327, -0.0036768 ,  0.00022437],\n",
      "       [ 0.01383996,  0.0018427 ,  0.00083301]]), array([[ 0.00924852],\n",
      "       [ 0.00369784],\n",
      "       [-0.0014481 ]])]\n",
      "gradients_biases:  [array([ 0.00258001, -0.00335688, -0.00025668]), array([-0.00248202])]\n",
      "Iteration 992, Cost: 0.2430794360294683\n",
      "gradient_weights:  [array([[ 0.00747741, -0.00368077,  0.00022442],\n",
      "       [ 0.01385056,  0.00184536,  0.00083499]]), array([[ 0.00926937],\n",
      "       [ 0.00369924],\n",
      "       [-0.00145028]])]\n",
      "gradients_biases:  [array([ 0.00257452, -0.00335913, -0.00025724]), array([-0.00248566])]\n",
      "Iteration 993, Cost: 0.24305990604572597\n",
      "gradient_weights:  [array([[ 0.00749156, -0.00368474,  0.00022448],\n",
      "       [ 0.01386116,  0.00184801,  0.00083698]]), array([[ 0.00929023],\n",
      "       [ 0.00370065],\n",
      "       [-0.00145247]])]\n",
      "gradients_biases:  [array([ 0.00256898, -0.00336137, -0.00025781]), array([-0.0024893])]\n",
      "Iteration 994, Cost: 0.2430403282796125\n",
      "gradient_weights:  [array([[ 0.00750572, -0.00368872,  0.00022455],\n",
      "       [ 0.01387175,  0.00185067,  0.00083897]]), array([[ 0.0093111 ],\n",
      "       [ 0.00370207],\n",
      "       [-0.00145467]])]\n",
      "gradients_biases:  [array([ 0.00256341, -0.00336362, -0.00025838]), array([-0.00249295])]\n",
      "Iteration 995, Cost: 0.24302070264772635\n",
      "gradient_weights:  [array([[ 0.00751988, -0.0036927 ,  0.00022461],\n",
      "       [ 0.01388234,  0.00185333,  0.00084096]]), array([[ 0.00933198],\n",
      "       [ 0.00370349],\n",
      "       [-0.00145687]])]\n",
      "gradients_biases:  [array([ 0.0025578 , -0.00336586, -0.00025895]), array([-0.0024966])]\n",
      "Iteration 996, Cost: 0.2430010290667445\n",
      "gradient_weights:  [array([[ 0.00753405, -0.00369669,  0.00022467],\n",
      "       [ 0.01389292,  0.00185599,  0.00084296]]), array([[ 0.00935286],\n",
      "       [ 0.00370491],\n",
      "       [-0.00145907]])]\n",
      "gradients_biases:  [array([ 0.00255215, -0.0033681 , -0.00025952]), array([-0.00250026])]\n",
      "Iteration 997, Cost: 0.2429813074534234\n",
      "gradient_weights:  [array([[ 0.00754822, -0.00370069,  0.00022474],\n",
      "       [ 0.0139035 ,  0.00185865,  0.00084496]]), array([[ 0.00937375],\n",
      "       [ 0.00370634],\n",
      "       [-0.00146128]])]\n",
      "gradients_biases:  [array([ 0.00254647, -0.00337034, -0.00026009]), array([-0.00250393])]\n",
      "Iteration 998, Cost: 0.2429615377245995\n",
      "gradient_weights:  [array([[ 0.0075624 , -0.00370468,  0.00022481],\n",
      "       [ 0.01391407,  0.00186131,  0.00084697]]), array([[ 0.00939465],\n",
      "       [ 0.00370778],\n",
      "       [-0.00146349]])]\n",
      "gradients_biases:  [array([ 0.00254074, -0.00337257, -0.00026066]), array([-0.00250759])]\n",
      "Iteration 999, Cost: 0.24294171979718993\n",
      "gradient_weights:  [array([[ 0.00757658, -0.00370869,  0.00022488],\n",
      "       [ 0.01392463,  0.00186398,  0.00084898]]), array([[ 0.00941555],\n",
      "       [ 0.00370922],\n",
      "       [-0.00146571]])]\n",
      "gradients_biases:  [array([ 0.00253498, -0.00337481, -0.00026124]), array([-0.00251127])]\n",
      "Iteration 1000, Cost: 0.24292185358819318\n",
      "Accuracy: 0.5\n",
      "F1 Score: 0.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        self.weights = [np.random.randn(layers[i], layers[i+1]) for i in range(len(layers)-1)]\n",
    "        self.biases = [np.zeros((1, layers[i+1])) for i in range(len(layers)-1)]\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    def forward_pass(self, X):\n",
    "        activations = [X]\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(activations[-1], w) + b\n",
    "            activations.append(self.sigmoid(z))\n",
    "        return activations\n",
    "    \n",
    "    def backward_pass(self, X, Y, activations):\n",
    "        deltas = [(activations[-1] - Y) * self.sigmoid_derivative(activations[-1])]\n",
    "        for i in range(len(self.layers) - 2, 0, -1):\n",
    "            delta = np.dot(deltas[0], self.weights[i].T) * self.sigmoid_derivative(activations[i])\n",
    "            deltas.insert(0, delta)\n",
    "        return deltas\n",
    "    \n",
    "    def compute_gradients(self, activations, deltas):\n",
    "        gradients_weights = [np.dot(activations[i].T, deltas[i]) for i in range(len(self.layers) - 1)]\n",
    "        gradients_biases = [np.sum(deltas[i], axis=0) for i in range(len(self.layers) - 1)]\n",
    "        print(\"gradient_weights: \",gradients_weights)\n",
    "        print(\"gradients_biases: \",gradients_biases)\n",
    "        return gradients_weights, gradients_biases\n",
    "    \n",
    "    def update_weights(self, gradients_weights, gradients_biases, learning_rate):\n",
    "        self.weights = [w - learning_rate * gw for w, gw in zip(self.weights, gradients_weights)]\n",
    "        self.biases = [b - learning_rate * gb for b, gb in zip(self.biases, gradients_biases)]\n",
    "    \n",
    "    def train(self, X, Y, learning_rate, lam, max_iterations, epsilon):\n",
    "        for iteration in range(max_iterations):\n",
    "            activations = self.forward_pass(X)\n",
    "            deltas = self.backward_pass(X, Y, activations)\n",
    "            gradients_weights, gradients_biases = self.compute_gradients(activations, deltas)\n",
    "            self.update_weights(gradients_weights, gradients_biases, learning_rate)\n",
    "            # Compute cost function\n",
    "            J = np.mean(np.square(activations[-1] - Y))\n",
    "            print(f\"Iteration {iteration+1}, Cost: {J}\")\n",
    "            # Check for convergence\n",
    "            if J < epsilon:\n",
    "                print(\"Converged!\")\n",
    "                break\n",
    "\n",
    "    def accuracy(self, y_true, y_pred):\n",
    "        correct = np.sum(y_true == y_pred)\n",
    "        return correct / len(y_true)\n",
    "\n",
    "    def f1_score(self, y_true, y_pred):\n",
    "        tp = np.sum(y_true & y_pred)\n",
    "        fp = np.sum((~y_true) & y_pred)\n",
    "        fn = np.sum(y_true & (~y_pred))\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        return f1\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        activations = self.forward_pass(X_test)[-1]\n",
    "        y_pred = (activations > 0.5).astype(int)\n",
    "        acc = self.accuracy(y_test, y_pred)\n",
    "        f1 = self.f1_score(y_test, y_pred)\n",
    "        return acc, f1\n",
    "\n",
    "# Example usage\n",
    "# Create a neural network with 2 input neurons, 3 hidden neurons, and 1 output neuron\n",
    "nn = NeuralNetwork([2, 3, 1])\n",
    "\n",
    "# Generate some dummy data for training\n",
    "X_train = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "Y_train = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# Train the neural network\n",
    "nn.train(X_train, Y_train, learning_rate=0.1, lam=0.0, max_iterations=1000, epsilon=0.005)\n",
    "\n",
    "# Generate some dummy data for testing\n",
    "X_test = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "Y_test = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# Evaluate the trained model\n",
    "accuracy, f1_score = nn.evaluate(X_test, Y_test)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1 Score:\", f1_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb86324",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0559ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "75b06333",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        self.weights = [np.random.randn(layers[i], layers[i+1]) for i in range(len(layers)-1)]\n",
    "        self.biases = [np.zeros((1, layers[i+1])) for i in range(len(layers)-1)]\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    def forward_pass(self, X):\n",
    "        activations = [X]\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(activations[-1], w) + b\n",
    "            activations.append(self.sigmoid(z))\n",
    "        return activations\n",
    "    \n",
    "    def backward_pass(self, X, Y, activations):\n",
    "        deltas = [(activations[-1] - Y) * self.sigmoid_derivative(activations[-1])]\n",
    "        for i in range(len(self.layers) - 2, 0, -1):\n",
    "            delta = np.dot(deltas[0], self.weights[i].T) * self.sigmoid_derivative(activations[i])\n",
    "            deltas.insert(0, delta)\n",
    "        return deltas\n",
    "    \n",
    "    def compute_gradients(self, activations, deltas):\n",
    "        gradients_weights = [np.dot(activations[i].T, deltas[i]) for i in range(len(self.layers) - 1)]\n",
    "        gradients_biases = [np.sum(deltas[i], axis=0) for i in range(len(self.layers) - 1)]\n",
    "        print(\"gradient_weights: \",gradients_weights)\n",
    "        print(\"gradients_biases: \",gradients_biases)\n",
    "        return gradients_weights, gradients_biases\n",
    "    \n",
    "    def update_weights(self, gradients_weights, gradients_biases, learning_rate):\n",
    "        self.weights = [w - learning_rate * gw for w, gw in zip(self.weights, gradients_weights)]\n",
    "        self.biases = [b - learning_rate * gb for b, gb in zip(self.biases, gradients_biases)]\n",
    "    \n",
    "    def train(self, X, Y, learning_rate, lam, max_iterations, epsilon):\n",
    "        for iteration in range(max_iterations):\n",
    "            activations = self.forward_pass(X)\n",
    "            deltas = self.backward_pass(X, Y, activations)\n",
    "            gradients_weights, gradients_biases = self.compute_gradients(activations, deltas)\n",
    "            self.update_weights(gradients_weights, gradients_biases, learning_rate)\n",
    "            # Compute cost function\n",
    "            J = np.mean(np.square(activations[-1] - Y))\n",
    "            print(f\"Iteration {iteration+1}, Cost: {J}\")\n",
    "            # Check for convergence\n",
    "            if J < epsilon:\n",
    "                print(\"Converged!\")\n",
    "                break\n",
    "\n",
    "    def accuracy(self, y_true, y_pred):\n",
    "        correct = np.sum(y_true == y_pred)\n",
    "        return correct / len(y_true)\n",
    "\n",
    "\n",
    "    def f1_score(self, y_true, y_pred):\n",
    "        tp = np.sum(np.logical_and(y_true, y_pred))\n",
    "        fp = np.sum(np.logical_and(np.logical_not(y_true), y_pred))\n",
    "        fn = np.sum(np.logical_and(y_true, np.logical_not(y_pred)))\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        return f1\n",
    "\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        activations = self.forward_pass(X_test)[-1]\n",
    "        y_pred = (activations > 0.5).astype(int)\n",
    "        acc = self.accuracy(y_test, y_pred)\n",
    "        f1 = self.f1_score(y_test, y_pred)\n",
    "        return acc, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c13da784",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient_weights:  [array([[ 0.000402  , -0.00265863,  0.01294115],\n",
      "       [-0.00080759, -0.00144509,  0.00444915]]), array([[0.00093006],\n",
      "       [0.00027098],\n",
      "       [0.00792345]])]\n",
      "gradients_biases:  [array([ 0.00020471, -0.00166134,  0.00378406]), array([-0.00391733])]\n",
      "Iteration 1, Cost: 0.24675711498480743\n",
      "gradient_weights:  [array([[ 0.00040247, -0.00265482,  0.01295189],\n",
      "       [-0.00080593, -0.00144155,  0.0044568 ]]), array([[0.00097211],\n",
      "       [0.00028531],\n",
      "       [0.00796393]])]\n",
      "gradients_biases:  [array([ 0.00020595, -0.00165727,  0.00377606]), array([-0.00385331])]\n",
      "Iteration 2, Cost: 0.24674243792757755\n",
      "gradient_weights:  [array([[ 0.0004029 , -0.00265107,  0.01296282],\n",
      "       [-0.00080426, -0.00143808,  0.00446456]]), array([[0.00101252],\n",
      "       [0.00029873],\n",
      "       [0.00800364]])]\n",
      "gradients_biases:  [array([ 0.00020712, -0.00165336,  0.00376837]), array([-0.00379192])]\n",
      "Iteration 3, Cost: 0.24672773681870974\n",
      "gradient_weights:  [array([[ 0.00040329, -0.00264738,  0.01297392],\n",
      "       [-0.0008026 , -0.00143466,  0.00447243]]), array([[0.00105136],\n",
      "       [0.00031127],\n",
      "       [0.00804263]])]\n",
      "gradients_biases:  [array([ 0.00020822, -0.0016496 ,  0.00376098]), array([-0.00373307])]\n",
      "Iteration 4, Cost: 0.24671301023902775\n",
      "gradient_weights:  [array([[ 0.00040364, -0.00264374,  0.01298518],\n",
      "       [-0.00080095, -0.00143129,  0.00448041]]), array([[0.00108869],\n",
      "       [0.00032297],\n",
      "       [0.00808093]])]\n",
      "gradients_biases:  [array([ 0.00020924, -0.00164598,  0.00375387]), array([-0.00367664])]\n",
      "Iteration 5, Cost: 0.24669825687222174\n",
      "gradient_weights:  [array([[ 0.00040396, -0.00264016,  0.01299661],\n",
      "       [-0.0007993 , -0.00142797,  0.00448848]]), array([[0.00112458],\n",
      "       [0.00033386],\n",
      "       [0.00811856]])]\n",
      "gradients_biases:  [array([ 0.00021019, -0.00164251,  0.00374703]), array([-0.00362255])]\n",
      "Iteration 6, Cost: 0.2466834754967069\n",
      "gradient_weights:  [array([[ 0.00040424, -0.00263664,  0.01300818],\n",
      "       [-0.00079765, -0.0014247 ,  0.00449666]]), array([[0.00115908],\n",
      "       [0.00034398],\n",
      "       [0.00815555]])]\n",
      "gradients_biases:  [array([ 0.00021108, -0.00163917,  0.00374044]), array([-0.00357068])]\n",
      "Iteration 7, Cost: 0.24666866497812753\n",
      "gradient_weights:  [array([[ 0.00040449, -0.00263316,  0.0130199 ],\n",
      "       [-0.000796  , -0.00142148,  0.00450493]]), array([[0.00119225],\n",
      "       [0.00035336],\n",
      "       [0.00819193]])]\n",
      "gradients_biases:  [array([ 0.00021191, -0.00163596,  0.0037341 ]), array([-0.00352097])]\n",
      "Iteration 8, Cost: 0.2466538242624549\n",
      "gradient_weights:  [array([[ 0.0004047 , -0.00262973,  0.01303176],\n",
      "       [-0.00079436, -0.0014183 ,  0.0045133 ]]), array([[0.00122415],\n",
      "       [0.00036203],\n",
      "       [0.00822773]])]\n",
      "gradients_biases:  [array([ 0.00021268, -0.00163287,  0.003728  ]), array([-0.00347331])]\n",
      "Iteration 9, Cost: 0.24663895236963246\n",
      "gradient_weights:  [array([[ 0.00040488, -0.00262636,  0.01304376],\n",
      "       [-0.00079272, -0.00141516,  0.00452175]]), array([[0.00125482],\n",
      "       [0.00037001],\n",
      "       [0.00826297]])]\n",
      "gradients_biases:  [array([ 0.00021339, -0.0016299 ,  0.00372212]), array([-0.00342763])]\n",
      "Iteration 10, Cost: 0.2466240483877239\n",
      "gradient_weights:  [array([[ 0.00040504, -0.00262302,  0.01305588],\n",
      "       [-0.00079109, -0.00141207,  0.00453029]]), array([[0.00128431],\n",
      "       [0.00037733],\n",
      "       [0.00829767]])]\n",
      "gradients_biases:  [array([ 0.00021404, -0.00162705,  0.00371645]), array([-0.00338385])]\n",
      "Iteration 11, Cost: 0.24660911146752595\n",
      "gradient_weights:  [array([[ 0.00040516, -0.00261974,  0.01306813],\n",
      "       [-0.00078946, -0.00140902,  0.00453892]]), array([[0.00131269],\n",
      "       [0.00038403],\n",
      "       [0.00833187]])]\n",
      "gradients_biases:  [array([ 0.00021464, -0.0016243 ,  0.00371099]), array([-0.00334188])]\n",
      "Iteration 12, Cost: 0.24659414081760755\n",
      "gradient_weights:  [array([[ 0.00040526, -0.00261649,  0.01308051],\n",
      "       [-0.00078783, -0.00140601,  0.00454762]]), array([[0.00133998],\n",
      "       [0.00039012],\n",
      "       [0.00836557]])]\n",
      "gradients_biases:  [array([ 0.00021519, -0.00162166,  0.00370572]), array([-0.00330166])]\n",
      "Iteration 13, Cost: 0.24657913569974232\n",
      "gradient_weights:  [array([[ 0.00040533, -0.00261329,  0.01309299],\n",
      "       [-0.0007862 , -0.00140303,  0.0045564 ]]), array([[0.00136623],\n",
      "       [0.00039562],\n",
      "       [0.0083988 ]])]\n",
      "gradients_biases:  [array([ 0.00021568, -0.00161913,  0.00370064]), array([-0.00326311])]\n",
      "Iteration 14, Cost: 0.24656409542470345\n",
      "gradient_weights:  [array([[ 0.00040538, -0.00261013,  0.01310559],\n",
      "       [-0.00078457, -0.0014001 ,  0.00456526]]), array([[0.00139149],\n",
      "       [0.00040058],\n",
      "       [0.00843158]])]\n",
      "gradients_biases:  [array([ 0.00021614, -0.00161669,  0.00369573]), array([-0.00322617])]\n",
      "Iteration 15, Cost: 0.2465490193483914\n",
      "gradient_weights:  [array([[ 0.0004054 , -0.002607  ,  0.01311829],\n",
      "       [-0.00078295, -0.00139719,  0.00457419]]), array([[0.0014158 ],\n",
      "       [0.00040499],\n",
      "       [0.00846393]])]\n",
      "gradients_biases:  [array([ 0.00021654, -0.00161435,  0.00369099]), array([-0.00319077])]\n",
      "Iteration 16, Cost: 0.24653390686826945\n",
      "gradient_weights:  [array([[ 0.0004054 , -0.00260392,  0.0131311 ],\n",
      "       [-0.00078133, -0.00139432,  0.00458319]]), array([[0.00143919],\n",
      "       [0.00040889],\n",
      "       [0.00849586]])]\n",
      "gradients_biases:  [array([ 0.00021691, -0.00161209,  0.00368642]), array([-0.00315686])]\n",
      "Iteration 17, Cost: 0.24651875742008042\n",
      "gradient_weights:  [array([[ 0.00040538, -0.00260086,  0.013144  ],\n",
      "       [-0.00077971, -0.00139149,  0.00459226]]), array([[0.00146171],\n",
      "       [0.0004123 ],\n",
      "       [0.00852741]])]\n",
      "gradients_biases:  [array([ 0.00021723, -0.00160993,  0.00368199]), array([-0.00312436])]\n",
      "Iteration 18, Cost: 0.24650357047482555\n",
      "gradient_weights:  [array([[ 0.00040533, -0.00259785,  0.013157  ],\n",
      "       [-0.0007781 , -0.00138868,  0.0046014 ]]), array([[0.00148338],\n",
      "       [0.00041524],\n",
      "       [0.00855857]])]\n",
      "gradients_biases:  [array([ 0.00021751, -0.00160785,  0.00367771]), array([-0.00309322])]\n",
      "Iteration 19, Cost: 0.2464883455359811\n",
      "gradient_weights:  [array([[ 0.00040527, -0.00259487,  0.01317009],\n",
      "       [-0.00077648, -0.00138591,  0.0046106 ]]), array([[0.00150425],\n",
      "       [0.00041772],\n",
      "       [0.00858938]])]\n",
      "gradients_biases:  [array([ 0.00021775, -0.00160584,  0.00367357]), array([-0.00306339])]\n",
      "Iteration 20, Cost: 0.24647308213693708\n",
      "gradient_weights:  [array([[ 0.00040518, -0.00259192,  0.01318327],\n",
      "       [-0.00077487, -0.00138316,  0.00461986]]), array([[0.00152434],\n",
      "       [0.00041976],\n",
      "       [0.00861984]])]\n",
      "gradients_biases:  [array([ 0.00021796, -0.00160392,  0.00366956]), array([-0.00303482])]\n",
      "Iteration 21, Cost: 0.24645777983863842\n",
      "gradient_weights:  [array([[ 0.00040508, -0.002589  ,  0.01319653],\n",
      "       [-0.00077326, -0.00138044,  0.00462919]]), array([[0.00154369],\n",
      "       [0.00042139],\n",
      "       [0.00864996]])]\n",
      "gradients_biases:  [array([ 0.00021813, -0.00160207,  0.00366567]), array([-0.00300745])]\n",
      "Iteration 22, Cost: 0.2464424382274129\n",
      "gradient_weights:  [array([[ 0.00040496, -0.00258612,  0.01320987],\n",
      "       [-0.00077165, -0.00137775,  0.00463857]]), array([[0.00156233],\n",
      "       [0.00042261],\n",
      "       [0.00867978]])]\n",
      "gradients_biases:  [array([ 0.00021826, -0.00160029,  0.00366191]), array([-0.00298123])]\n",
      "Iteration 23, Cost: 0.24642705691297237\n",
      "gradient_weights:  [array([[ 0.00040482, -0.00258326,  0.01322329],\n",
      "       [-0.00077005, -0.00137509,  0.00464801]]), array([[0.00158029],\n",
      "       [0.00042345],\n",
      "       [0.00870929]])]\n",
      "gradients_biases:  [array([ 0.00021837, -0.00159858,  0.00365825]), array([-0.00295613])]\n",
      "Iteration 24, Cost: 0.24641163552657136\n",
      "gradient_weights:  [array([[ 0.00040466, -0.00258043,  0.01323678],\n",
      "       [-0.00076844, -0.00137245,  0.00465751]]), array([[0.00159759],\n",
      "       [0.00042392],\n",
      "       [0.00873851]])]\n",
      "gradients_biases:  [array([ 0.00021844, -0.00159694,  0.0036547 ]), array([-0.00293208])]\n",
      "Iteration 25, Cost: 0.24639617371931227\n",
      "gradient_weights:  [array([[ 0.00040449, -0.00257763,  0.01325035],\n",
      "       [-0.00076684, -0.00136983,  0.00466706]]), array([[0.00161426],\n",
      "       [0.00042403],\n",
      "       [0.00876745]])]\n",
      "gradients_biases:  [array([ 0.00021849, -0.00159536,  0.00365125]), array([-0.00290906])]\n",
      "Iteration 26, Cost: 0.24638067116058504\n",
      "gradient_weights:  [array([[ 0.0004043 , -0.00257486,  0.01326398],\n",
      "       [-0.00076523, -0.00136724,  0.00467667]]), array([[0.00163033],\n",
      "       [0.0004238 ],\n",
      "       [0.00879613]])]\n",
      "gradients_biases:  [array([ 0.0002185 , -0.00159384,  0.0036479 ]), array([-0.00288702])]\n",
      "Iteration 27, Cost: 0.24636512753663065\n",
      "gradient_weights:  [array([[ 0.00040409, -0.00257211,  0.01327768],\n",
      "       [-0.00076363, -0.00136468,  0.00468632]]), array([[0.00164583],\n",
      "       [0.00042324],\n",
      "       [0.00882455]])]\n",
      "gradients_biases:  [array([ 0.00021849, -0.00159238,  0.00364464]), array([-0.00286591])]\n",
      "Iteration 28, Cost: 0.24634954254921815\n",
      "gradient_weights:  [array([[ 0.00040388, -0.00256939,  0.01329145],\n",
      "       [-0.00076203, -0.00136213,  0.00469602]]), array([[0.00166077],\n",
      "       [0.00042237],\n",
      "       [0.00885274]])]\n",
      "gradients_biases:  [array([ 0.00021845, -0.00159098,  0.00364146]), array([-0.00284571])]\n",
      "Iteration 29, Cost: 0.246333915914427\n",
      "gradient_weights:  [array([[ 0.00040364, -0.00256669,  0.01330527],\n",
      "       [-0.00076043, -0.00135961,  0.00470578]]), array([[0.00167518],\n",
      "       [0.0004212 ],\n",
      "       [0.00888069]])]\n",
      "gradients_biases:  [array([ 0.00021839, -0.00158964,  0.00363836]), array([-0.00282637])]\n",
      "Iteration 30, Cost: 0.24631824736152585\n",
      "gradient_weights:  [array([[ 0.0004034 , -0.00256402,  0.01331915],\n",
      "       [-0.00075883, -0.0013571 ,  0.00471557]]), array([[0.00168908],\n",
      "       [0.00041973],\n",
      "       [0.00890842]])]\n",
      "gradients_biases:  [array([ 0.0002183 , -0.00158834,  0.00363534]), array([-0.00280787])]\n",
      "Iteration 31, Cost: 0.24630253663194066\n",
      "gradient_weights:  [array([[ 0.00040314, -0.00256137,  0.01333309],\n",
      "       [-0.00075723, -0.00135462,  0.00472542]]), array([[0.00170249],\n",
      "       [0.00041799],\n",
      "       [0.00893594]])]\n",
      "gradients_biases:  [array([ 0.00021819, -0.0015871 ,  0.00363239]), array([-0.00279016])]\n",
      "Iteration 32, Cost: 0.2462867834783039\n",
      "gradient_weights:  [array([[ 0.00040287, -0.00255874,  0.01334709],\n",
      "       [-0.00075564, -0.00135215,  0.00473531]]), array([[0.00171543],\n",
      "       [0.00041598],\n",
      "       [0.00896326]])]\n",
      "gradients_biases:  [array([ 0.00021806, -0.0015859 ,  0.00362951]), array([-0.00277321])]\n",
      "Iteration 33, Cost: 0.24627098766358024\n",
      "gradient_weights:  [array([[ 0.00040259, -0.00255614,  0.01336113],\n",
      "       [-0.00075404, -0.00134971,  0.00474524]]), array([[0.00172793],\n",
      "       [0.00041371],\n",
      "       [0.00899038]])]\n",
      "gradients_biases:  [array([ 0.00021791, -0.00158476,  0.00362669]), array([-0.002757])]\n",
      "Iteration 34, Cost: 0.24625514896026057\n",
      "gradient_weights:  [array([[ 0.0004023 , -0.00255355,  0.01337523],\n",
      "       [-0.00075245, -0.00134728,  0.00475522]]), array([[0.00173999],\n",
      "       [0.0004112 ],\n",
      "       [0.00901732]])]\n",
      "gradients_biases:  [array([ 0.00021773, -0.00158365,  0.00362393]), array([-0.00274149])]\n",
      "Iteration 35, Cost: 0.24623926714962102\n",
      "gradient_weights:  [array([[ 0.00040199, -0.00255099,  0.01338938],\n",
      "       [-0.00075085, -0.00134487,  0.00476523]]), array([[0.00175165],\n",
      "       [0.00040845],\n",
      "       [0.00904409]])]\n",
      "gradients_biases:  [array([ 0.00021754, -0.0015826 ,  0.00362122]), array([-0.00272666])]\n",
      "Iteration 36, Cost: 0.24622334202103985\n",
      "gradient_weights:  [array([[ 0.00040168, -0.00254845,  0.01340357],\n",
      "       [-0.00074926, -0.00134248,  0.00477529]]), array([[0.0017629 ],\n",
      "       [0.00040548],\n",
      "       [0.00907068]])]\n",
      "gradients_biases:  [array([ 0.00021733, -0.00158158,  0.00361857]), array([-0.00271247])]\n",
      "Iteration 37, Cost: 0.2462073733713694\n",
      "gradient_weights:  [array([[ 0.00040135, -0.00254592,  0.0134178 ],\n",
      "       [-0.00074766, -0.0013401 ,  0.00478539]]), array([[0.00177378],\n",
      "       [0.00040228],\n",
      "       [0.00909712]])]\n",
      "gradients_biases:  [array([ 0.0002171 , -0.0015806 ,  0.00361596]), array([-0.00269891])]\n",
      "Iteration 38, Cost: 0.24619136100435737\n",
      "gradient_weights:  [array([[ 0.00040102, -0.00254342,  0.01343208],\n",
      "       [-0.00074607, -0.00133774,  0.00479552]]), array([[0.0017843 ],\n",
      "       [0.00039888],\n",
      "       [0.0091234 ]])]\n",
      "gradients_biases:  [array([ 0.00021685, -0.00157967,  0.0036134 ]), array([-0.00268595])]\n",
      "Iteration 39, Cost: 0.24617530473011404\n",
      "gradient_weights:  [array([[ 0.00040068, -0.00254093,  0.0134464 ],\n",
      "       [-0.00074448, -0.00133539,  0.00480569]]), array([[0.00179446],\n",
      "       [0.00039527],\n",
      "       [0.00914954]])]\n",
      "gradients_biases:  [array([ 0.00021659, -0.00157877,  0.00361088]), array([-0.00267356])]\n",
      "Iteration 40, Cost: 0.24615920436462246\n",
      "gradient_weights:  [array([[ 0.00040033, -0.00253846,  0.01346076],\n",
      "       [-0.00074289, -0.00133306,  0.0048159 ]]), array([[0.0018043 ],\n",
      "       [0.00039147],\n",
      "       [0.00917554]])]\n",
      "gradients_biases:  [array([ 0.00021631, -0.00157791,  0.0036084 ]), array([-0.00266171])]\n",
      "Iteration 41, Cost: 0.2461430597292868\n",
      "gradient_weights:  [array([[ 0.00039996, -0.00253601,  0.01347515],\n",
      "       [-0.0007413 , -0.00133075,  0.00482615]]), array([[0.00181381],\n",
      "       [0.00038749],\n",
      "       [0.0092014 ]])]\n",
      "gradients_biases:  [array([ 0.00021602, -0.00157708,  0.00360596]), array([-0.0026504])]\n",
      "Iteration 42, Cost: 0.24612687065051683\n",
      "gradient_weights:  [array([[ 0.0003996 , -0.00253357,  0.01348958],\n",
      "       [-0.0007397 , -0.00132844,  0.00483642]]), array([[0.00182301],\n",
      "       [0.00038333],\n",
      "       [0.00922714]])]\n",
      "gradients_biases:  [array([ 0.00021571, -0.00157628,  0.00360355]), array([-0.00263959])]\n",
      "Iteration 43, Cost: 0.2461106369593456\n",
      "gradient_weights:  [array([[ 0.00039922, -0.00253115,  0.01350405],\n",
      "       [-0.00073811, -0.00132615,  0.00484674]]), array([[0.00183192],\n",
      "       [0.000379  ],\n",
      "       [0.00925276]])]\n",
      "gradients_biases:  [array([ 0.00021539, -0.00157552,  0.00360116]), array([-0.00262926])]\n",
      "Iteration 44, Cost: 0.24609435849107753\n",
      "gradient_weights:  [array([[ 0.00039884, -0.00252875,  0.01351855],\n",
      "       [-0.00073652, -0.00132388,  0.00485708]]), array([[0.00184055],\n",
      "       [0.0003745 ],\n",
      "       [0.00927826]])]\n",
      "gradients_biases:  [array([ 0.00021506, -0.00157479,  0.00359881]), array([-0.00261941])]\n",
      "Iteration 45, Cost: 0.24607803508496423\n",
      "gradient_weights:  [array([[ 0.00039844, -0.00252636,  0.01353308],\n",
      "       [-0.00073493, -0.00132161,  0.00486746]]), array([[0.00184891],\n",
      "       [0.00036985],\n",
      "       [0.00930365]])]\n",
      "gradients_biases:  [array([ 0.00021471, -0.00157409,  0.00359648]), array([-0.00261])]\n",
      "Iteration 46, Cost: 0.2460616665839062\n",
      "gradient_weights:  [array([[ 0.00039805, -0.00252399,  0.01354765],\n",
      "       [-0.00073334, -0.00131936,  0.00487788]]), array([[0.00185702],\n",
      "       [0.00036504],\n",
      "       [0.00932894]])]\n",
      "gradients_biases:  [array([ 0.00021435, -0.00157342,  0.00359417]), array([-0.00260102])]\n",
      "Iteration 47, Cost: 0.2460452528341785\n",
      "gradient_weights:  [array([[ 0.00039764, -0.00252163,  0.01356224],\n",
      "       [-0.00073175, -0.00131712,  0.00488832]]), array([[0.00186487],\n",
      "       [0.00036009],\n",
      "       [0.00935412]])]\n",
      "gradients_biases:  [array([ 0.00021398, -0.00157278,  0.00359188]), array([-0.00259245])]\n",
      "Iteration 48, Cost: 0.24602879368517785\n",
      "gradient_weights:  [array([[ 0.00039723, -0.00251929,  0.01357686],\n",
      "       [-0.00073016, -0.0013149 ,  0.00489879]]), array([[0.00187248],\n",
      "       [0.000355  ],\n",
      "       [0.00937921]])]\n",
      "gradients_biases:  [array([ 0.0002136 , -0.00157217,  0.00358961]), array([-0.00258428])]\n",
      "Iteration 49, Cost: 0.24601228898919036\n",
      "gradient_weights:  [array([[ 0.00039681, -0.00251696,  0.01359151],\n",
      "       [-0.00072857, -0.00131268,  0.0049093 ]]), array([[0.00187987],\n",
      "       [0.00034977],\n",
      "       [0.00940422]])]\n",
      "gradients_biases:  [array([ 0.00021321, -0.00157158,  0.00358735]), array([-0.00257648])]\n",
      "Iteration 50, Cost: 0.2459957386011773\n",
      "gradient_weights:  [array([[ 0.00039639, -0.00251465,  0.01360618],\n",
      "       [-0.00072698, -0.00131048,  0.00491983]]), array([[0.00188704],\n",
      "       [0.00034442],\n",
      "       [0.00942913]])]\n",
      "gradients_biases:  [array([ 0.00021281, -0.00157102,  0.00358511]), array([-0.00256905])]\n",
      "Iteration 51, Cost: 0.24597914237857832\n",
      "gradient_weights:  [array([[ 0.00039596, -0.00251235,  0.01362088],\n",
      "       [-0.00072539, -0.00130828,  0.00493039]]), array([[0.001894  ],\n",
      "       [0.00033894],\n",
      "       [0.00945396]])]\n",
      "gradients_biases:  [array([ 0.0002124 , -0.00157048,  0.00358288]), array([-0.00256197])]\n",
      "Iteration 52, Cost: 0.24596250018113006\n",
      "gradient_weights:  [array([[ 0.00039552, -0.00251006,  0.0136356 ],\n",
      "       [-0.0007238 , -0.0013061 ,  0.00494098]]), array([[0.00190076],\n",
      "       [0.00033334],\n",
      "       [0.00947872]])]\n",
      "gradients_biases:  [array([ 0.00021198, -0.00156997,  0.00358066]), array([-0.00255523])]\n",
      "Iteration 53, Cost: 0.24594581187069978\n",
      "gradient_weights:  [array([[ 0.00039508, -0.00250778,  0.01365035],\n",
      "       [-0.00072221, -0.00130392,  0.0049516 ]]), array([[0.00190733],\n",
      "       [0.00032763],\n",
      "       [0.0095034 ]])]\n",
      "gradients_biases:  [array([ 0.00021155, -0.00156947,  0.00357844]), array([-0.00254881])]\n",
      "Iteration 54, Cost: 0.24592907731113114\n",
      "gradient_weights:  [array([[ 0.00039464, -0.00250552,  0.01366511],\n",
      "       [-0.00072062, -0.00130176,  0.00496225]]), array([[0.00191372],\n",
      "       [0.00032181],\n",
      "       [0.00952801]])]\n",
      "gradients_biases:  [array([ 0.00021111, -0.00156901,  0.00357623]), array([-0.00254269])]\n",
      "Iteration 55, Cost: 0.24591229636810402\n",
      "gradient_weights:  [array([[ 0.00039419, -0.00250327,  0.0136799 ],\n",
      "       [-0.00071903, -0.0012996 ,  0.00497292]]), array([[0.00191993],\n",
      "       [0.00031589],\n",
      "       [0.00955255]])]\n",
      "gradients_biases:  [array([ 0.00021066, -0.00156856,  0.00357403]), array([-0.00253687])]\n",
      "Iteration 56, Cost: 0.2458954689090039\n",
      "gradient_weights:  [array([[ 0.00039373, -0.00250103,  0.01369471],\n",
      "       [-0.00071744, -0.00129746,  0.00498362]]), array([[0.00192597],\n",
      "       [0.00030986],\n",
      "       [0.00957703]])]\n",
      "gradients_biases:  [array([ 0.00021021, -0.00156813,  0.00357182]), array([-0.00253134])]\n",
      "Iteration 57, Cost: 0.2458785948028026\n",
      "gradient_weights:  [array([[ 0.00039327, -0.0024988 ,  0.01370954],\n",
      "       [-0.00071585, -0.00129532,  0.00499435]]), array([[0.00193185],\n",
      "       [0.00030373],\n",
      "       [0.00960144]])]\n",
      "gradients_biases:  [array([ 0.00020975, -0.00156773,  0.00356962]), array([-0.00252608])]\n",
      "Iteration 58, Cost: 0.24586167391994845\n",
      "gradient_weights:  [array([[ 0.00039281, -0.00249659,  0.01372438],\n",
      "       [-0.00071425, -0.00129319,  0.0050051 ]]), array([[0.00193758],\n",
      "       [0.00029752],\n",
      "       [0.00962581]])]\n",
      "gradients_biases:  [array([ 0.00020928, -0.00156734,  0.00356741]), array([-0.00252108])]\n",
      "Iteration 59, Cost: 0.24584470613226536\n",
      "gradient_weights:  [array([[ 0.00039234, -0.00249438,  0.01373925],\n",
      "       [-0.00071266, -0.00129107,  0.00501587]]), array([[0.00194316],\n",
      "       [0.00029121],\n",
      "       [0.00965011]])]\n",
      "gradients_biases:  [array([ 0.00020881, -0.00156697,  0.00356521]), array([-0.00251634])]\n",
      "Iteration 60, Cost: 0.24582769131285942\n",
      "gradient_weights:  [array([[ 0.00039187, -0.00249219,  0.01375413],\n",
      "       [-0.00071107, -0.00128896,  0.00502667]]), array([[0.00194859],\n",
      "       [0.00028481],\n",
      "       [0.00967437]])]\n",
      "gradients_biases:  [array([ 0.00020833, -0.00156662,  0.003563  ]), array([-0.00251183])]\n",
      "Iteration 61, Cost: 0.2458106293360338\n",
      "gradient_weights:  [array([[ 0.00039139, -0.00249001,  0.01376902],\n",
      "       [-0.00070948, -0.00128686,  0.0050375 ]]), array([[0.00195389],\n",
      "       [0.00027833],\n",
      "       [0.00969857]])]\n",
      "gradients_biases:  [array([ 0.00020784, -0.00156629,  0.00356078]), array([-0.00250756])]\n",
      "Iteration 62, Cost: 0.24579352007720995\n",
      "gradient_weights:  [array([[ 0.00039091, -0.00248783,  0.01378393],\n",
      "       [-0.00070789, -0.00128476,  0.00504835]]), array([[0.00195907],\n",
      "       [0.00027178],\n",
      "       [0.00972273]])]\n",
      "gradients_biases:  [array([ 0.00020735, -0.00156598,  0.00355855]), array([-0.00250351])]\n",
      "Iteration 63, Cost: 0.24577636341285564\n",
      "gradient_weights:  [array([[ 0.00039043, -0.00248567,  0.01379886],\n",
      "       [-0.00070629, -0.00128267,  0.00505922]]), array([[0.00196411],\n",
      "       [0.00026514],\n",
      "       [0.00974685]])]\n",
      "gradients_biases:  [array([ 0.00020685, -0.00156568,  0.00355632]), array([-0.00249967])]\n",
      "Iteration 64, Cost: 0.24575915922041824\n",
      "gradient_weights:  [array([[ 0.00038994, -0.00248352,  0.0138138 ],\n",
      "       [-0.0007047 , -0.00128059,  0.00507012]]), array([[0.00196904],\n",
      "       [0.00025843],\n",
      "       [0.00977092]])]\n",
      "gradients_biases:  [array([ 0.00020635, -0.0015654 ,  0.00355408]), array([-0.00249604])]\n",
      "Iteration 65, Cost: 0.2457419073782638\n",
      "gradient_weights:  [array([[ 0.00038945, -0.00248138,  0.01382875],\n",
      "       [-0.00070311, -0.00127852,  0.00508104]]), array([[0.00197386],\n",
      "       [0.00025165],\n",
      "       [0.00979495]])]\n",
      "gradients_biases:  [array([ 0.00020584, -0.00156514,  0.00355183]), array([-0.00249261])]\n",
      "Iteration 66, Cost: 0.24572460776562083\n",
      "gradient_weights:  [array([[ 0.00038895, -0.00247925,  0.01384371],\n",
      "       [-0.00070151, -0.00127646,  0.00509198]]), array([[0.00197857],\n",
      "       [0.00024481],\n",
      "       [0.00981895]])]\n",
      "gradients_biases:  [array([ 0.00020532, -0.00156489,  0.00354957]), array([-0.00248936])]\n",
      "Iteration 67, Cost: 0.24570726026252934\n",
      "gradient_weights:  [array([[ 0.00038845, -0.00247713,  0.01385869],\n",
      "       [-0.00069992, -0.0012744 ,  0.00510295]]), array([[0.00198317],\n",
      "       [0.00023789],\n",
      "       [0.00984291]])]\n",
      "gradients_biases:  [array([ 0.00020481, -0.00156465,  0.00354729]), array([-0.0024863])]\n",
      "Iteration 68, Cost: 0.24568986474979276\n",
      "gradient_weights:  [array([[ 0.00038795, -0.00247501,  0.01387368],\n",
      "       [-0.00069833, -0.00127235,  0.00511394]]), array([[0.00198768],\n",
      "       [0.00023092],\n",
      "       [0.00986684]])]\n",
      "gradients_biases:  [array([ 0.00020428, -0.00156443,  0.003545  ]), array([-0.0024834])]\n",
      "Iteration 69, Cost: 0.245672421108935\n",
      "gradient_weights:  [array([[ 0.00038745, -0.00247291,  0.01388867],\n",
      "       [-0.00069673, -0.0012703 ,  0.00512495]]), array([[0.00199209],\n",
      "       [0.00022388],\n",
      "       [0.00989074]])]\n",
      "gradients_biases:  [array([ 0.00020376, -0.00156422,  0.00354269]), array([-0.00248068])]\n",
      "Iteration 70, Cost: 0.24565492922216042\n",
      "gradient_weights:  [array([[ 0.00038694, -0.00247082,  0.01390368],\n",
      "       [-0.00069514, -0.00126826,  0.00513598]]), array([[0.00199641],\n",
      "       [0.00021679],\n",
      "       [0.00991461]])]\n",
      "gradients_biases:  [array([ 0.00020323, -0.00156403,  0.00354037]), array([-0.00247812])]\n",
      "Iteration 71, Cost: 0.24563738897231707\n",
      "gradient_weights:  [array([[ 0.00038643, -0.00246873,  0.0139187 ],\n",
      "       [-0.00069354, -0.00126623,  0.00514703]]), array([[0.00200064],\n",
      "       [0.00020964],\n",
      "       [0.00993845]])]\n",
      "gradients_biases:  [array([ 0.00020269, -0.00156385,  0.00353803]), array([-0.00247571])]\n",
      "Iteration 72, Cost: 0.24561980024286345\n",
      "gradient_weights:  [array([[ 0.00038592, -0.00246666,  0.01393373],\n",
      "       [-0.00069194, -0.0012642 ,  0.0051581 ]]), array([[0.00200479],\n",
      "       [0.00020243],\n",
      "       [0.00996227]])]\n",
      "gradients_biases:  [array([ 0.00020216, -0.00156368,  0.00353568]), array([-0.00247345])]\n",
      "Iteration 73, Cost: 0.24560216291783762\n",
      "gradient_weights:  [array([[ 0.0003854 , -0.00246459,  0.01394876],\n",
      "       [-0.00069035, -0.00126218,  0.0051692 ]]), array([[0.00200886],\n",
      "       [0.00019517],\n",
      "       [0.00998606]])]\n",
      "gradients_biases:  [array([ 0.00020161, -0.00156353,  0.0035333 ]), array([-0.00247133])]\n",
      "Iteration 74, Cost: 0.24558447688182855\n",
      "gradient_weights:  [array([[ 0.00038489, -0.00246253,  0.01396381],\n",
      "       [-0.00068875, -0.00126017,  0.00518031]]), array([[0.00201285],\n",
      "       [0.00018787],\n",
      "       [0.01000982]])]\n",
      "gradients_biases:  [array([ 0.00020107, -0.00156339,  0.00353091]), array([-0.00246934])]\n",
      "Iteration 75, Cost: 0.24556674201995066\n",
      "gradient_weights:  [array([[ 0.00038437, -0.00246048,  0.01397886],\n",
      "       [-0.00068715, -0.00125816,  0.00519145]]), array([[0.00201678],\n",
      "       [0.00018051],\n",
      "       [0.01003357]])]\n",
      "gradients_biases:  [array([ 0.00020052, -0.00156326,  0.00352849]), array([-0.00246749])]\n",
      "Iteration 76, Cost: 0.24554895821782016\n",
      "gradient_weights:  [array([[ 0.00038384, -0.00245844,  0.01399391],\n",
      "       [-0.00068555, -0.00125616,  0.0052026 ]]), array([[0.00202063],\n",
      "       [0.00017311],\n",
      "       [0.0100573 ]])]\n",
      "gradients_biases:  [array([ 0.00019997, -0.00156314,  0.00352605]), array([-0.00246576])]\n",
      "Iteration 77, Cost: 0.24553112536153318\n",
      "gradient_weights:  [array([[ 0.00038332, -0.0024564 ,  0.01400898],\n",
      "       [-0.00068396, -0.00125416,  0.00521378]]), array([[0.00202442],\n",
      "       [0.00016567],\n",
      "       [0.01008101]])]\n",
      "gradients_biases:  [array([ 0.00019942, -0.00156303,  0.00352359]), array([-0.00246415])]\n",
      "Iteration 78, Cost: 0.24551324333764568\n",
      "gradient_weights:  [array([[ 0.00038279, -0.00245438,  0.01402405],\n",
      "       [-0.00068236, -0.00125217,  0.00522497]]), array([[0.00202814],\n",
      "       [0.00015818],\n",
      "       [0.0101047 ]])]\n",
      "gradients_biases:  [array([ 0.00019886, -0.00156294,  0.00352111]), array([-0.00246266])]\n",
      "Iteration 79, Cost: 0.2454953120331559\n",
      "gradient_weights:  [array([[ 0.00038226, -0.00245236,  0.01403912],\n",
      "       [-0.00068076, -0.00125018,  0.00523618]]), array([[0.00203181],\n",
      "       [0.00015065],\n",
      "       [0.01012837]])]\n",
      "gradients_biases:  [array([ 0.0001983 , -0.00156285,  0.00351861]), array([-0.00246128])]\n",
      "Iteration 80, Cost: 0.24547733133548708\n",
      "gradient_weights:  [array([[ 0.00038173, -0.00245035,  0.0140542 ],\n",
      "       [-0.00067916, -0.0012482 ,  0.00524742]]), array([[0.00203541],\n",
      "       [0.00014308],\n",
      "       [0.01015203]])]\n",
      "gradients_biases:  [array([ 0.00019774, -0.00156278,  0.00351607]), array([-0.00246001])]\n",
      "Iteration 81, Cost: 0.24545930113247283\n",
      "gradient_weights:  [array([[ 0.0003812 , -0.00244835,  0.01406929],\n",
      "       [-0.00067756, -0.00124622,  0.00525867]]), array([[0.00203897],\n",
      "       [0.00013547],\n",
      "       [0.01017567]])]\n",
      "gradients_biases:  [array([ 0.00019718, -0.00156271,  0.00351352]), array([-0.00245884])]\n",
      "Iteration 82, Cost: 0.24544122131234297\n",
      "gradient_weights:  [array([[ 0.00038066, -0.00244635,  0.01408438],\n",
      "       [-0.00067595, -0.00124425,  0.00526994]]), array([[0.00204247],\n",
      "       [0.00012782],\n",
      "       [0.0101993 ]])]\n",
      "gradients_biases:  [array([ 0.00019661, -0.00156266,  0.00351094]), array([-0.00245777])]\n",
      "Iteration 83, Cost: 0.24542309176371116\n",
      "gradient_weights:  [array([[ 0.00038012, -0.00244437,  0.01409947],\n",
      "       [-0.00067435, -0.00124228,  0.00528123]]), array([[0.00204592],\n",
      "       [0.00012014],\n",
      "       [0.01022292]])]\n",
      "gradients_biases:  [array([ 0.00019604, -0.00156261,  0.00350833]), array([-0.00245679])]\n",
      "Iteration 84, Cost: 0.24540491237556317\n",
      "gradient_weights:  [array([[ 0.00037958, -0.00244239,  0.01411457],\n",
      "       [-0.00067275, -0.00124032,  0.00529254]]), array([[0.00204933],\n",
      "       [0.00011243],\n",
      "       [0.01024653]])]\n",
      "gradients_biases:  [array([ 0.00019547, -0.00156258,  0.00350569]), array([-0.0024559])]\n",
      "Iteration 85, Cost: 0.2453866830372467\n",
      "gradient_weights:  [array([[ 0.00037904, -0.00244041,  0.01412967],\n",
      "       [-0.00067115, -0.00123837,  0.00530387]]), array([[0.00205269],\n",
      "       [0.00010468],\n",
      "       [0.01027013]])]\n",
      "gradients_biases:  [array([ 0.0001949 , -0.00156255,  0.00350303]), array([-0.0024551])]\n",
      "Iteration 86, Cost: 0.2453684036384615\n",
      "gradient_weights:  [array([[ 0.0003785 , -0.00243845,  0.01414478],\n",
      "       [-0.00066954, -0.00123641,  0.00531521]]), array([[2.05600284e-03],\n",
      "       [9.69007731e-05],\n",
      "       [1.02937195e-02]])]\n",
      "gradients_biases:  [array([ 0.00019433, -0.00156254,  0.00350034]), array([-0.00245439])]\n",
      "Iteration 87, Cost: 0.24535007406925116\n",
      "gradient_weights:  [array([[ 0.00037795, -0.00243649,  0.01415988],\n",
      "       [-0.00066794, -0.00123447,  0.00532658]]), array([[2.05927913e-03],\n",
      "       [8.90901924e-05],\n",
      "       [1.03173001e-02]])]\n",
      "gradients_biases:  [array([ 0.00019375, -0.00156253,  0.00349762]), array([-0.00245375])]\n",
      "Iteration 88, Cost: 0.24533169421999518\n",
      "gradient_weights:  [array([[ 0.0003774 , -0.00243454,  0.01417499],\n",
      "       [-0.00066633, -0.00123252,  0.00533796]]), array([[2.06251669e-03],\n",
      "       [8.12499756e-05],\n",
      "       [1.03408729e-02]])]\n",
      "gradients_biases:  [array([ 0.00019318, -0.00156253,  0.00349487]), array([-0.00245319])]\n",
      "Iteration 89, Cost: 0.2453132639814018\n",
      "gradient_weights:  [array([[ 0.00037685, -0.00243259,  0.0141901 ],\n",
      "       [-0.00066473, -0.00123058,  0.00534936]]), array([[2.06571744e-03],\n",
      "       [7.33811693e-05],\n",
      "       [1.03644387e-02]])]\n",
      "gradients_biases:  [array([ 0.0001926 , -0.00156254,  0.00349209]), array([-0.00245271])]\n",
      "Iteration 90, Cost: 0.24529478324450146\n",
      "gradient_weights:  [array([[ 0.0003763 , -0.00243066,  0.01420521],\n",
      "       [-0.00066312, -0.00122865,  0.00536078]]), array([[2.06888320e-03],\n",
      "       [6.54847764e-05],\n",
      "       [1.03879983e-02]])]\n",
      "gradients_biases:  [array([ 0.00019202, -0.00156256,  0.00348928]), array([-0.0024523])]\n",
      "Iteration 91, Cost: 0.2452762519006416\n",
      "gradient_weights:  [array([[ 0.00037575, -0.00242873,  0.01422033],\n",
      "       [-0.00066152, -0.00122672,  0.00537221]]), array([[2.07201574e-03],\n",
      "       [5.75617588e-05],\n",
      "       [1.04115523e-02]])]\n",
      "gradients_biases:  [array([ 0.00019144, -0.00156259,  0.00348644]), array([-0.00245195])]\n",
      "Iteration 92, Cost: 0.245257669841481\n",
      "gradient_weights:  [array([[ 0.0003752 , -0.0024268 ,  0.01423544],\n",
      "       [-0.00065991, -0.00122479,  0.00538366]]), array([[2.07511672e-03],\n",
      "       [4.96130386e-05],\n",
      "       [1.04351013e-02]])]\n",
      "gradients_biases:  [array([ 0.00019086, -0.00156263,  0.00348357]), array([-0.00245168])]\n",
      "Iteration 93, Cost: 0.24523903695898525\n",
      "gradient_weights:  [array([[ 0.00037464, -0.00242489,  0.01425055],\n",
      "       [-0.0006583 , -0.00122287,  0.00539513]]), array([[2.07818778e-03],\n",
      "       [4.16394998e-05],\n",
      "       [1.04586460e-02]])]\n",
      "gradients_biases:  [array([ 0.00019027, -0.00156267,  0.00348066]), array([-0.00245146])]\n",
      "Iteration 94, Cost: 0.24522035314542273\n",
      "gradient_weights:  [array([[ 0.00037408, -0.00242298,  0.01426567],\n",
      "       [-0.00065669, -0.00122095,  0.00540662]]), array([[2.08123045e-03],\n",
      "       [3.36419898e-05],\n",
      "       [1.04821869e-02]])]\n",
      "gradients_biases:  [array([ 0.00018969, -0.00156272,  0.00347773]), array([-0.00245131])]\n",
      "Iteration 95, Cost: 0.2452016182933609\n",
      "gradient_weights:  [array([[ 0.00037352, -0.00242107,  0.01428078],\n",
      "       [-0.00065508, -0.00121904,  0.00541812]]), array([[2.08424624e-03],\n",
      "       [2.56213209e-05],\n",
      "       [1.05057245e-02]])]\n",
      "gradients_biases:  [array([ 0.0001891 , -0.00156278,  0.00347476]), array([-0.00245121])]\n",
      "Iteration 96, Cost: 0.24518283229566284\n",
      "gradient_weights:  [array([[ 0.00037296, -0.00241918,  0.0142959 ],\n",
      "       [-0.00065347, -0.00121713,  0.00542964]]), array([[2.08723655e-03],\n",
      "       [1.75782718e-05],\n",
      "       [1.05292594e-02]])]\n",
      "gradients_biases:  [array([ 0.00018852, -0.00156285,  0.00347175]), array([-0.00245117])]\n",
      "Iteration 97, Cost: 0.24516399504548475\n",
      "gradient_weights:  [array([[ 0.0003724 , -0.00241728,  0.01431101],\n",
      "       [-0.00065186, -0.00121522,  0.00544118]]), array([[2.09020278e-03],\n",
      "       [9.51358880e-06],\n",
      "       [1.05527921e-02]])]\n",
      "gradients_biases:  [array([ 0.00018793, -0.00156292,  0.00346872]), array([-0.00245119])]\n",
      "Iteration 98, Cost: 0.24514510643627305\n",
      "gradient_weights:  [array([[ 0.00037184, -0.0024154 ,  0.01432612],\n",
      "       [-0.00065025, -0.00121332,  0.00545273]]), array([[2.09314621e-03],\n",
      "       [1.42798724e-06],\n",
      "       [1.05763230e-02]])]\n",
      "gradients_biases:  [array([ 0.00018734, -0.00156301,  0.00346565]), array([-0.00245126])]\n",
      "Iteration 99, Cost: 0.2451261663617622\n",
      "gradient_weights:  [array([[ 0.00037127, -0.00241352,  0.01434124],\n",
      "       [-0.00064864, -0.00121142,  0.0054643 ]]), array([[ 2.09606813e-03],\n",
      "       [-6.67784734e-06],\n",
      "       [ 1.05998526e-02]])]\n",
      "gradients_biases:  [array([ 0.00018675, -0.0015631 ,  0.00346254]), array([-0.00245137])]\n",
      "Iteration 100, Cost: 0.245107174715973\n",
      "gradient_weights:  [array([[ 0.00037071, -0.00241165,  0.01435635],\n",
      "       [-0.00064703, -0.00120953,  0.00547589]]), array([[ 2.09896974e-03],\n",
      "       [-1.48032580e-05],\n",
      "       [ 1.06233813e-02]])]\n",
      "gradients_biases:  [array([ 0.00018616, -0.00156319,  0.0034594 ]), array([-0.00245154])]\n",
      "Iteration 101, Cost: 0.24508813139321098\n",
      "gradient_weights:  [array([[ 0.00037014, -0.00240978,  0.01437146],\n",
      "       [-0.00064542, -0.00120763,  0.00548749]]), array([[ 2.10185220e-03],\n",
      "       [-2.29476151e-05],\n",
      "       [ 1.06469094e-02]])]\n",
      "gradients_biases:  [array([ 0.00018557, -0.0015633 ,  0.00345623]), array([-0.00245175])]\n",
      "Iteration 102, Cost: 0.24506903628806465\n",
      "gradient_weights:  [array([[ 0.00036957, -0.00240792,  0.01438656],\n",
      "       [-0.0006438 , -0.00120575,  0.00549911]]), array([[ 2.10471663e-03],\n",
      "       [-3.11103154e-05],\n",
      "       [ 1.06704374e-02]])]\n",
      "gradients_biases:  [array([ 0.00018498, -0.00156341,  0.00345301]), array([-0.002452])]\n",
      "Iteration 103, Cost: 0.24504988929540478\n",
      "gradient_weights:  [array([[ 0.000369  , -0.00240607,  0.01440167],\n",
      "       [-0.00064219, -0.00120386,  0.00551075]]), array([[ 2.10756408e-03],\n",
      "       [-3.92907810e-05],\n",
      "       [ 1.06939656e-02]])]\n",
      "gradients_biases:  [array([ 0.00018439, -0.00156353,  0.00344977]), array([-0.0024523])]\n",
      "Iteration 104, Cost: 0.24503069031038321\n",
      "gradient_weights:  [array([[ 0.00036843, -0.00240422,  0.01441677],\n",
      "       [-0.00064057, -0.00120198,  0.0055224 ]]), array([[ 2.11039560e-03],\n",
      "       [-4.74884580e-05],\n",
      "       [ 1.07174943e-02]])]\n",
      "gradients_biases:  [array([ 0.0001838 , -0.00156365,  0.00344649]), array([-0.00245264])]\n",
      "Iteration 105, Cost: 0.2450114392284321\n",
      "gradient_weights:  [array([[ 0.00036786, -0.00240238,  0.01443187],\n",
      "       [-0.00063896, -0.0012001 ,  0.00553407]]), array([[ 2.11321216e-03],\n",
      "       [-5.57028159e-05],\n",
      "       [ 1.07410238e-02]])]\n",
      "gradients_biases:  [array([ 0.00018321, -0.00156378,  0.00344317]), array([-0.00245302])]\n",
      "Iteration 106, Cost: 0.2449921359452637\n",
      "gradient_weights:  [array([[ 0.00036728, -0.00240055,  0.01444697],\n",
      "       [-0.00063734, -0.00119823,  0.00554575]]), array([[ 2.11601471e-03],\n",
      "       [-6.39333465e-05],\n",
      "       [ 1.07645545e-02]])]\n",
      "gradients_biases:  [array([ 0.00018261, -0.00156392,  0.00343981]), array([-0.00245344])]\n",
      "Iteration 107, Cost: 0.24497278035686937\n",
      "gradient_weights:  [array([[ 0.00036671, -0.00239872,  0.01446206],\n",
      "       [-0.00063572, -0.00119636,  0.00555745]]), array([[ 2.11880415e-03],\n",
      "       [-7.21795632e-05],\n",
      "       [ 1.07880865e-02]])]\n",
      "gradients_biases:  [array([ 0.00018202, -0.00156406,  0.00343642]), array([-0.00245389])]\n",
      "Iteration 108, Cost: 0.24495337235951994\n",
      "gradient_weights:  [array([[ 0.00036613, -0.00239689,  0.01447715],\n",
      "       [-0.00063411, -0.00119449,  0.00556916]]), array([[ 2.12158135e-03],\n",
      "       [-8.04409995e-05],\n",
      "       [ 1.08116202e-02]])]\n",
      "gradients_biases:  [array([ 0.00018143, -0.00156421,  0.00343298]), array([-0.00245438])]\n",
      "Iteration 109, Cost: 0.24493391184976512\n",
      "gradient_weights:  [array([[ 0.00036555, -0.00239507,  0.01449224],\n",
      "       [-0.00063249, -0.00119262,  0.00558089]]), array([[ 2.12434715e-03],\n",
      "       [-8.87172090e-05],\n",
      "       [ 1.08351558e-02]])]\n",
      "gradients_biases:  [array([ 0.00018083, -0.00156437,  0.00342951]), array([-0.0024549])]\n",
      "Iteration 110, Cost: 0.244914398724434\n",
      "gradient_weights:  [array([[ 0.00036497, -0.00239326,  0.01450732],\n",
      "       [-0.00063087, -0.00119076,  0.00559264]]), array([[ 2.12710235e-03],\n",
      "       [-9.70077642e-05],\n",
      "       [ 1.08586935e-02]])]\n",
      "gradients_biases:  [array([ 0.00018024, -0.00156454,  0.00342601]), array([-0.00245545])]\n",
      "Iteration 111, Cost: 0.2448948328806348\n",
      "gradient_weights:  [array([[ 0.00036439, -0.00239145,  0.0145224 ],\n",
      "       [-0.00062925, -0.00118891,  0.0056044 ]]), array([[ 0.00212985],\n",
      "       [-0.00010531],\n",
      "       [ 0.01088223]])]\n",
      "gradients_biases:  [array([ 0.00017964, -0.00156471,  0.00342246]), array([-0.00245604])]\n",
      "Iteration 112, Cost: 0.24487521421575514\n",
      "gradient_weights:  [array([[ 0.00036381, -0.00238965,  0.01453747],\n",
      "       [-0.00062763, -0.00118705,  0.00561617]]), array([[ 0.00213258],\n",
      "       [-0.00011363],\n",
      "       [ 0.01090578]])]\n",
      "gradients_biases:  [array([ 0.00017905, -0.00156488,  0.00341888]), array([-0.00245666])]\n",
      "Iteration 113, Cost: 0.24485554262746267\n",
      "gradient_weights:  [array([[ 0.00036323, -0.00238786,  0.01455254],\n",
      "       [-0.00062601, -0.0011852 ,  0.00562797]]), array([[ 0.00213531],\n",
      "       [-0.00012196],\n",
      "       [ 0.01092932]])]\n",
      "gradients_biases:  [array([ 0.00017845, -0.00156506,  0.00341525]), array([-0.0024573])]\n",
      "Iteration 114, Cost: 0.24483581801370535\n",
      "gradient_weights:  [array([[ 0.00036264, -0.00238607,  0.01456761],\n",
      "       [-0.00062439, -0.00118335,  0.00563977]]), array([[ 0.00213803],\n",
      "       [-0.00013031],\n",
      "       [ 0.01095287]])]\n",
      "gradients_biases:  [array([ 0.00017786, -0.00156525,  0.00341159]), array([-0.00245798])]\n",
      "Iteration 115, Cost: 0.2448160402727118\n",
      "gradient_weights:  [array([[ 0.00036206, -0.00238428,  0.01458267],\n",
      "       [-0.00062277, -0.0011815 ,  0.00565159]]), array([[ 0.00214075],\n",
      "       [-0.00013866],\n",
      "       [ 0.01097642]])]\n",
      "gradients_biases:  [array([ 0.00017726, -0.00156545,  0.00340789]), array([-0.00245868])]\n",
      "Iteration 116, Cost: 0.24479620930299206\n",
      "gradient_weights:  [array([[ 0.00036147, -0.0023825 ,  0.01459773],\n",
      "       [-0.00062114, -0.00117966,  0.00566343]]), array([[ 0.00214345],\n",
      "       [-0.00014703],\n",
      "       [ 0.01099997]])]\n",
      "gradients_biases:  [array([ 0.00017667, -0.00156565,  0.00340415]), array([-0.0024594])]\n",
      "Iteration 117, Cost: 0.24477632500333846\n",
      "gradient_weights:  [array([[ 0.00036088, -0.00238073,  0.01461278],\n",
      "       [-0.00061952, -0.00117782,  0.00567528]]), array([[ 0.00214615],\n",
      "       [-0.00015541],\n",
      "       [ 0.01102353]])]\n",
      "gradients_biases:  [array([ 0.00017607, -0.00156585,  0.00340036]), array([-0.00246015])]\n",
      "Iteration 118, Cost: 0.2447563872728258\n",
      "gradient_weights:  [array([[ 0.00036029, -0.00237896,  0.01462782],\n",
      "       [-0.00061789, -0.00117598,  0.00568715]]), array([[ 0.00214885],\n",
      "       [-0.0001638 ],\n",
      "       [ 0.01104709]])]\n",
      "gradients_biases:  [array([ 0.00017548, -0.00156607,  0.00339654]), array([-0.00246093])]\n",
      "Iteration 119, Cost: 0.24473639601081254\n",
      "gradient_weights:  [array([[ 0.0003597 , -0.0023772 ,  0.01464286],\n",
      "       [-0.00061627, -0.00117414,  0.00569903]]), array([[ 0.00215154],\n",
      "       [-0.00017221],\n",
      "       [ 0.01107066]])]\n",
      "gradients_biases:  [array([ 0.00017488, -0.00156629,  0.00339268]), array([-0.00246173])]\n",
      "Iteration 120, Cost: 0.24471635111694165\n",
      "gradient_weights:  [array([[ 0.00035911, -0.00237544,  0.0146579 ],\n",
      "       [-0.00061464, -0.00117231,  0.00571093]]), array([[ 0.00215423],\n",
      "       [-0.00018062],\n",
      "       [ 0.01109422]])]\n",
      "gradients_biases:  [array([ 0.00017429, -0.00156651,  0.00338878]), array([-0.00246255])]\n",
      "Iteration 121, Cost: 0.24469625249114121\n",
      "gradient_weights:  [array([[ 0.00035852, -0.00237369,  0.01467292],\n",
      "       [-0.00061302, -0.00117048,  0.00572284]]), array([[ 0.00215691],\n",
      "       [-0.00018905],\n",
      "       [ 0.0111178 ]])]\n",
      "gradients_biases:  [array([ 0.00017369, -0.00156674,  0.00338483]), array([-0.00246339])]\n",
      "Iteration 122, Cost: 0.24467610003362547\n",
      "gradient_weights:  [array([[ 0.00035792, -0.00237194,  0.01468795],\n",
      "       [-0.00061139, -0.00116866,  0.00573476]]), array([[ 0.00215959],\n",
      "       [-0.00019748],\n",
      "       [ 0.01114137]])]\n",
      "gradients_biases:  [array([ 0.00017309, -0.00156697,  0.00338085]), array([-0.00246426])]\n",
      "Iteration 123, Cost: 0.24465589364489587\n",
      "gradient_weights:  [array([[ 0.00035733, -0.0023702 ,  0.01470296],\n",
      "       [-0.00060976, -0.00116683,  0.0057467 ]]), array([[ 0.00216227],\n",
      "       [-0.00020593],\n",
      "       [ 0.01116495]])]\n",
      "gradients_biases:  [array([ 0.0001725 , -0.00156722,  0.00337682]), array([-0.00246514])]\n",
      "Iteration 124, Cost: 0.2446356332257419\n",
      "gradient_weights:  [array([[ 0.00035673, -0.00236847,  0.01471797],\n",
      "       [-0.00060813, -0.00116501,  0.00575865]]), array([[ 0.00216494],\n",
      "       [-0.00021438],\n",
      "       [ 0.01118853]])]\n",
      "gradients_biases:  [array([ 0.0001719 , -0.00156746,  0.00337275]), array([-0.00246604])]\n",
      "Iteration 125, Cost: 0.24461531867724223\n",
      "gradient_weights:  [array([[ 0.00035613, -0.00236673,  0.01473297],\n",
      "       [-0.0006065 , -0.00116319,  0.00577062]]), array([[ 0.00216761],\n",
      "       [-0.00022285],\n",
      "       [ 0.01121212]])]\n",
      "gradients_biases:  [array([ 0.00017131, -0.00156772,  0.00336864]), array([-0.00246697])]\n",
      "Iteration 126, Cost: 0.24459494990076558\n",
      "gradient_weights:  [array([[ 0.00035554, -0.00236501,  0.01474797],\n",
      "       [-0.00060487, -0.00116138,  0.0057826 ]]), array([[ 0.00217029],\n",
      "       [-0.00023132],\n",
      "       [ 0.01123571]])]\n",
      "gradients_biases:  [array([ 0.00017071, -0.00156797,  0.00336449]), array([-0.00246791])]\n",
      "Iteration 127, Cost: 0.24457452679797204\n",
      "gradient_weights:  [array([[ 0.00035494, -0.00236329,  0.01476295],\n",
      "       [-0.00060324, -0.00115956,  0.0057946 ]]), array([[ 0.00217296],\n",
      "       [-0.00023981],\n",
      "       [ 0.01125931]])]\n",
      "gradients_biases:  [array([ 0.00017012, -0.00156824,  0.00336029]), array([-0.00246886])]\n",
      "Iteration 128, Cost: 0.24455404927081376\n",
      "gradient_weights:  [array([[ 0.00035434, -0.00236157,  0.01477793],\n",
      "       [-0.00060161, -0.00115775,  0.00580661]]), array([[ 0.00217563],\n",
      "       [-0.0002483 ],\n",
      "       [ 0.01128291]])]\n",
      "gradients_biases:  [array([ 0.00016952, -0.00156851,  0.00335606]), array([-0.00246984])]\n",
      "Iteration 129, Cost: 0.24453351722153663\n",
      "gradient_weights:  [array([[ 0.00035373, -0.00235986,  0.01479291],\n",
      "       [-0.00059998, -0.00115594,  0.00581864]]), array([[ 0.0021783 ],\n",
      "       [-0.0002568 ],\n",
      "       [ 0.01130651]])]\n",
      "gradients_biases:  [array([ 0.00016893, -0.00156878,  0.00335178]), array([-0.00247083])]\n",
      "Iteration 130, Cost: 0.24451293055268086\n",
      "gradient_weights:  [array([[ 0.00035313, -0.00235815,  0.01480787],\n",
      "       [-0.00059835, -0.00115413,  0.00583067]]), array([[ 0.00218096],\n",
      "       [-0.00026531],\n",
      "       [ 0.01133012]])]\n",
      "gradients_biases:  [array([ 0.00016833, -0.00156906,  0.00334746]), array([-0.00247184])]\n",
      "Iteration 131, Cost: 0.2444922891670823\n",
      "gradient_weights:  [array([[ 0.00035253, -0.00235645,  0.01482283],\n",
      "       [-0.00059671, -0.00115233,  0.00584273]]), array([[ 0.00218364],\n",
      "       [-0.00027383],\n",
      "       [ 0.01135373]])]\n",
      "gradients_biases:  [array([ 0.00016774, -0.00156935,  0.00334309]), array([-0.00247286])]\n",
      "Iteration 132, Cost: 0.24447159296787363\n",
      "gradient_weights:  [array([[ 0.00035192, -0.00235476,  0.01483778],\n",
      "       [-0.00059508, -0.00115053,  0.00585479]]), array([[ 0.00218631],\n",
      "       [-0.00028236],\n",
      "       [ 0.01137734]])]\n",
      "gradients_biases:  [array([ 0.00016714, -0.00156964,  0.00333868]), array([-0.0024739])]\n",
      "Iteration 133, Cost: 0.2444508418584857\n",
      "gradient_weights:  [array([[ 0.00035131, -0.00235306,  0.01485272],\n",
      "       [-0.00059345, -0.00114873,  0.00586687]]), array([[ 0.00218898],\n",
      "       [-0.0002909 ],\n",
      "       [ 0.01140096]])]\n",
      "gradients_biases:  [array([ 0.00016655, -0.00156993,  0.00333423]), array([-0.00247495])]\n",
      "Iteration 134, Cost: 0.2444300357426482\n",
      "gradient_weights:  [array([[ 0.0003507 , -0.00235138,  0.01486765],\n",
      "       [-0.00059181, -0.00114693,  0.00587897]]), array([[ 0.00219165],\n",
      "       [-0.00029944],\n",
      "       [ 0.01142458]])]\n",
      "gradients_biases:  [array([ 0.00016595, -0.00157023,  0.00332974]), array([-0.00247601])]\n",
      "Iteration 135, Cost: 0.24440917452439118\n",
      "gradient_weights:  [array([[ 0.0003501 , -0.0023497 ,  0.01488258],\n",
      "       [-0.00059017, -0.00114514,  0.00589108]]), array([[ 0.00219433],\n",
      "       [-0.00030799],\n",
      "       [ 0.01144821]])]\n",
      "gradients_biases:  [array([ 0.00016536, -0.00157054,  0.0033252 ]), array([-0.00247709])]\n",
      "Iteration 136, Cost: 0.2443882581080462\n",
      "gradient_weights:  [array([[ 0.00034949, -0.00234802,  0.01489749],\n",
      "       [-0.00058854, -0.00114335,  0.0059032 ]]), array([[ 0.002197  ],\n",
      "       [-0.00031655],\n",
      "       [ 0.01147184]])]\n",
      "gradients_biases:  [array([ 0.00016477, -0.00157085,  0.00332062]), array([-0.00247818])]\n",
      "Iteration 137, Cost: 0.24436728639824756\n",
      "gradient_weights:  [array([[ 0.00034887, -0.00234635,  0.0149124 ],\n",
      "       [-0.0005869 , -0.00114155,  0.00591533]]), array([[ 0.00219968],\n",
      "       [-0.00032512],\n",
      "       [ 0.01149548]])]\n",
      "gradients_biases:  [array([ 0.00016417, -0.00157117,  0.00331599]), array([-0.00247928])]\n",
      "Iteration 138, Cost: 0.24434625929993314\n",
      "gradient_weights:  [array([[ 0.00034826, -0.00234468,  0.0149273 ],\n",
      "       [-0.00058526, -0.00113977,  0.00592748]]), array([[ 0.00220236],\n",
      "       [-0.0003337 ],\n",
      "       [ 0.01151912]])]\n",
      "gradients_biases:  [array([ 0.00016358, -0.00157149,  0.00331132]), array([-0.0024804])]\n",
      "Iteration 139, Cost: 0.24432517671834594\n",
      "gradient_weights:  [array([[ 0.00034765, -0.00234302,  0.01494219],\n",
      "       [-0.00058362, -0.00113798,  0.00593964]]), array([[ 0.00220505],\n",
      "       [-0.00034228],\n",
      "       [ 0.01154276]])]\n",
      "gradients_biases:  [array([ 0.00016299, -0.00157182,  0.00330661]), array([-0.00248153])]\n",
      "Iteration 140, Cost: 0.2443040385590351\n",
      "gradient_weights:  [array([[ 0.00034703, -0.00234136,  0.01495707],\n",
      "       [-0.00058198, -0.0011362 ,  0.00595182]]), array([[ 0.00220773],\n",
      "       [-0.00035087],\n",
      "       [ 0.01156641]])]\n",
      "gradients_biases:  [array([ 0.00016239, -0.00157216,  0.00330185]), array([-0.00248267])]\n",
      "Iteration 141, Cost: 0.24428284472785722\n",
      "gradient_weights:  [array([[ 0.00034642, -0.00233971,  0.01497194],\n",
      "       [-0.00058034, -0.00113442,  0.005964  ]]), array([[ 0.00221042],\n",
      "       [-0.00035947],\n",
      "       [ 0.01159006]])]\n",
      "gradients_biases:  [array([ 0.0001618 , -0.00157249,  0.00329705]), array([-0.00248382])]\n",
      "Iteration 142, Cost: 0.24426159513097712\n",
      "gradient_weights:  [array([[ 0.0003458 , -0.00233806,  0.0149868 ],\n",
      "       [-0.0005787 , -0.00113264,  0.00597621]]), array([[ 0.00221312],\n",
      "       [-0.00036808],\n",
      "       [ 0.01161371]])]\n",
      "gradients_biases:  [array([ 0.00016121, -0.00157284,  0.0032922 ]), array([-0.00248498])]\n",
      "Iteration 143, Cost: 0.24424028967486955\n",
      "gradient_weights:  [array([[ 0.00034518, -0.00233642,  0.01500165],\n",
      "       [-0.00057706, -0.00113086,  0.00598842]]), array([[ 0.00221581],\n",
      "       [-0.00037669],\n",
      "       [ 0.01163737]])]\n",
      "gradients_biases:  [array([ 0.00016062, -0.00157319,  0.00328731]), array([-0.00248615])]\n",
      "Iteration 144, Cost: 0.24421892826632013\n",
      "gradient_weights:  [array([[ 0.00034456, -0.00233478,  0.01501649],\n",
      "       [-0.00057542, -0.00112908,  0.00600065]]), array([[ 0.00221851],\n",
      "       [-0.00038531],\n",
      "       [ 0.01166103]])]\n",
      "gradients_biases:  [array([ 0.00016003, -0.00157354,  0.00328237]), array([-0.00248733])]\n",
      "Iteration 145, Cost: 0.24419751081242635\n",
      "gradient_weights:  [array([[ 0.00034394, -0.00233314,  0.01503133],\n",
      "       [-0.00057377, -0.00112731,  0.00601289]]), array([[ 0.00222122],\n",
      "       [-0.00039393],\n",
      "       [ 0.01168469]])]\n",
      "gradients_biases:  [array([ 0.00015944, -0.0015739 ,  0.00327739]), array([-0.00248852])]\n",
      "Iteration 146, Cost: 0.24417603722059883\n",
      "gradient_weights:  [array([[ 0.00034332, -0.00233151,  0.01504615],\n",
      "       [-0.00057213, -0.00112554,  0.00602514]]), array([[ 0.00222393],\n",
      "       [-0.00040257],\n",
      "       [ 0.01170836]])]\n",
      "gradients_biases:  [array([ 0.00015885, -0.00157426,  0.00327236]), array([-0.00248972])]\n",
      "Iteration 147, Cost: 0.24415450739856265\n",
      "gradient_weights:  [array([[ 0.00034269, -0.00232989,  0.01506096],\n",
      "       [-0.00057048, -0.00112377,  0.00603741]]), array([[ 0.00222664],\n",
      "       [-0.00041121],\n",
      "       [ 0.01173203]])]\n",
      "gradients_biases:  [array([ 0.00015826, -0.00157463,  0.00326729]), array([-0.00249093])]\n",
      "Iteration 148, Cost: 0.24413292125435823\n",
      "gradient_weights:  [array([[ 0.00034207, -0.00232827,  0.01507576],\n",
      "       [-0.00056884, -0.001122  ,  0.00604969]]), array([[ 0.00222936],\n",
      "       [-0.00041986],\n",
      "       [ 0.01175571]])]\n",
      "gradients_biases:  [array([ 0.00015767, -0.00157501,  0.00326217]), array([-0.00249215])]\n",
      "Iteration 149, Cost: 0.24411127869634253\n",
      "gradient_weights:  [array([[ 0.00034144, -0.00232665,  0.01509055],\n",
      "       [-0.00056719, -0.00112024,  0.00606198]]), array([[ 0.00223208],\n",
      "       [-0.00042851],\n",
      "       [ 0.01177939]])]\n",
      "gradients_biases:  [array([ 0.00015708, -0.00157539,  0.003257  ]), array([-0.00249338])]\n",
      "Iteration 150, Cost: 0.24408957963319033\n",
      "gradient_weights:  [array([[ 0.00034081, -0.00232504,  0.01510533],\n",
      "       [-0.00056555, -0.00111847,  0.00607429]]), array([[ 0.0022348 ],\n",
      "       [-0.00043717],\n",
      "       [ 0.01180307]])]\n",
      "gradients_biases:  [array([ 0.00015649, -0.00157577,  0.00325179]), array([-0.00249462])]\n",
      "Iteration 151, Cost: 0.24406782397389504\n",
      "gradient_weights:  [array([[ 0.00034019, -0.00232343,  0.01512009],\n",
      "       [-0.0005639 , -0.00111671,  0.0060866 ]]), array([[ 0.00223753],\n",
      "       [-0.00044584],\n",
      "       [ 0.01182675]])]\n",
      "gradients_biases:  [array([ 0.0001559 , -0.00157616,  0.00324654]), array([-0.00249586])]\n",
      "Iteration 152, Cost: 0.24404601162777018\n",
      "gradient_weights:  [array([[ 0.00033956, -0.00232183,  0.01513485],\n",
      "       [-0.00056225, -0.00111495,  0.00609893]]), array([[ 0.00224027],\n",
      "       [-0.00045451],\n",
      "       [ 0.01185044]])]\n",
      "gradients_biases:  [array([ 0.00015531, -0.00157656,  0.00324124]), array([-0.00249712])]\n",
      "Iteration 153, Cost: 0.24402414250445015\n",
      "gradient_weights:  [array([[ 0.00033893, -0.00232023,  0.0151496 ],\n",
      "       [-0.0005606 , -0.00111319,  0.00611128]]), array([[ 0.00224301],\n",
      "       [-0.00046319],\n",
      "       [ 0.01187413]])]\n",
      "gradients_biases:  [array([ 0.00015472, -0.00157696,  0.00323589]), array([-0.00249838])]\n",
      "Iteration 154, Cost: 0.2440022165138914\n",
      "gradient_weights:  [array([[ 0.00033829, -0.00231864,  0.01516433],\n",
      "       [-0.00055895, -0.00111144,  0.00612363]]), array([[ 0.00224576],\n",
      "       [-0.00047187],\n",
      "       [ 0.01189782]])]\n",
      "gradients_biases:  [array([ 0.00015414, -0.00157737,  0.00323049]), array([-0.00249965])]\n",
      "Iteration 155, Cost: 0.2439802335663737\n",
      "gradient_weights:  [array([[ 0.00033766, -0.00231705,  0.01517905],\n",
      "       [-0.0005573 , -0.00110968,  0.006136  ]]), array([[ 0.00224851],\n",
      "       [-0.00048057],\n",
      "       [ 0.01192152]])]\n",
      "gradients_biases:  [array([ 0.00015355, -0.00157778,  0.00322505]), array([-0.00250093])]\n",
      "Iteration 156, Cost: 0.24395819357250087\n",
      "gradient_weights:  [array([[ 0.00033703, -0.00231546,  0.01519376],\n",
      "       [-0.00055565, -0.00110793,  0.00614838]]), array([[ 0.00225127],\n",
      "       [-0.00048927],\n",
      "       [ 0.01194522]])]\n",
      "gradients_biases:  [array([ 0.00015296, -0.00157819,  0.00321957]), array([-0.00250221])]\n",
      "Iteration 157, Cost: 0.24393609644320208\n",
      "gradient_weights:  [array([[ 0.00033639, -0.00231388,  0.01520846],\n",
      "       [-0.000554  , -0.00110618,  0.00616077]]), array([[ 0.00225403],\n",
      "       [-0.00049797],\n",
      "       [ 0.01196892]])]\n",
      "gradients_biases:  [array([ 0.00015238, -0.00157861,  0.00321403]), array([-0.0025035])]\n",
      "Iteration 158, Cost: 0.24391394208973283\n",
      "gradient_weights:  [array([[ 0.00033575, -0.0023123 ,  0.01522315],\n",
      "       [-0.00055235, -0.00110443,  0.00617318]]), array([[ 0.0022568 ],\n",
      "       [-0.00050668],\n",
      "       [ 0.01199262]])]\n",
      "gradients_biases:  [array([ 0.00015179, -0.00157904,  0.00320846]), array([-0.0025048])]\n",
      "Iteration 159, Cost: 0.24389173042367576\n",
      "gradient_weights:  [array([[ 0.00033511, -0.00231073,  0.01523782],\n",
      "       [-0.00055069, -0.00110268,  0.00618559]]), array([[ 0.00225958],\n",
      "       [-0.0005154 ],\n",
      "       [ 0.01201633]])]\n",
      "gradients_biases:  [array([ 0.0001512 , -0.00157947,  0.00320283]), array([-0.0025061])]\n",
      "Iteration 160, Cost: 0.24386946135694226\n",
      "gradient_weights:  [array([[ 0.00033447, -0.00230916,  0.01525249],\n",
      "       [-0.00054904, -0.00110093,  0.00619802]]), array([[ 0.00226236],\n",
      "       [-0.00052412],\n",
      "       [ 0.01204003]])]\n",
      "gradients_biases:  [array([ 0.00015062, -0.0015799 ,  0.00319716]), array([-0.00250742])]\n",
      "Iteration 161, Cost: 0.24384713480177256\n",
      "gradient_weights:  [array([[ 0.00033383, -0.0023076 ,  0.01526714],\n",
      "       [-0.00054739, -0.00109919,  0.00621046]]), array([[ 0.00226514],\n",
      "       [-0.00053285],\n",
      "       [ 0.01206374]])]\n",
      "gradients_biases:  [array([ 0.00015003, -0.00158035,  0.00319144]), array([-0.00250874])]\n",
      "Iteration 162, Cost: 0.2438247506707375\n",
      "gradient_weights:  [array([[ 0.00033319, -0.00230604,  0.01528178],\n",
      "       [-0.00054573, -0.00109745,  0.00622292]]), array([[ 0.00226794],\n",
      "       [-0.00054159],\n",
      "       [ 0.01208746]])]\n",
      "gradients_biases:  [array([ 0.00014945, -0.00158079,  0.00318567]), array([-0.00251006])]\n",
      "Iteration 163, Cost: 0.2438023088767392\n",
      "gradient_weights:  [array([[ 0.00033255, -0.00230448,  0.0152964 ],\n",
      "       [-0.00054408, -0.00109571,  0.00623538]]), array([[ 0.00227074],\n",
      "       [-0.00055033],\n",
      "       [ 0.01211117]])]\n",
      "gradients_biases:  [array([ 0.00014887, -0.00158124,  0.00317985]), array([-0.00251139])]\n",
      "Iteration 164, Cost: 0.24377980933301208\n",
      "gradient_weights:  [array([[ 0.0003319 , -0.00230293,  0.01531101],\n",
      "       [-0.00054242, -0.00109397,  0.00624786]]), array([[ 0.00227354],\n",
      "       [-0.00055908],\n",
      "       [ 0.01213488]])]\n",
      "gradients_biases:  [array([ 0.00014828, -0.0015817 ,  0.00317399]), array([-0.00251273])]\n",
      "Iteration 165, Cost: 0.24375725195312353\n",
      "gradient_weights:  [array([[ 0.00033125, -0.00230138,  0.01532561],\n",
      "       [-0.00054076, -0.00109223,  0.00626035]]), array([[ 0.00227635],\n",
      "       [-0.00056783],\n",
      "       [ 0.0121586 ]])]\n",
      "gradients_biases:  [array([ 0.0001477 , -0.00158216,  0.00316808]), array([-0.00251407])]\n",
      "Iteration 166, Cost: 0.24373463665097517\n",
      "gradient_weights:  [array([[ 0.00033061, -0.00229984,  0.0153402 ],\n",
      "       [-0.0005391 , -0.00109049,  0.00627285]]), array([[ 0.00227917],\n",
      "       [-0.00057659],\n",
      "       [ 0.01218232]])]\n",
      "gradients_biases:  [array([ 0.00014712, -0.00158263,  0.00316213]), array([-0.00251542])]\n",
      "Iteration 167, Cost: 0.24371196334080386\n",
      "gradient_weights:  [array([[ 0.00032996, -0.0022983 ,  0.01535477],\n",
      "       [-0.00053745, -0.00108876,  0.00628536]]), array([[ 0.002282  ],\n",
      "       [-0.00058536],\n",
      "       [ 0.01220604]])]\n",
      "gradients_biases:  [array([ 0.00014654, -0.0015831 ,  0.00315612]), array([-0.00251678])]\n",
      "Iteration 168, Cost: 0.24368923193718217\n",
      "gradient_weights:  [array([[ 0.00032931, -0.00229676,  0.01536933],\n",
      "       [-0.00053579, -0.00108702,  0.00629789]]), array([[ 0.00228483],\n",
      "       [-0.00059413],\n",
      "       [ 0.01222976]])]\n",
      "gradients_biases:  [array([ 0.00014596, -0.00158357,  0.00315007]), array([-0.00251814])]\n",
      "Iteration 169, Cost: 0.2436664423550196\n",
      "gradient_weights:  [array([[ 0.00032866, -0.00229523,  0.01538388],\n",
      "       [-0.00053413, -0.00108529,  0.00631042]]), array([[ 0.00228767],\n",
      "       [-0.00060291],\n",
      "       [ 0.01225348]])]\n",
      "gradients_biases:  [array([ 0.00014537, -0.00158405,  0.00314397]), array([-0.00251951])]\n",
      "Iteration 170, Cost: 0.24364359450956327\n",
      "gradient_weights:  [array([[ 0.000328  , -0.0022937 ,  0.01539841],\n",
      "       [-0.00053247, -0.00108356,  0.00632297]]), array([[ 0.00229051],\n",
      "       [-0.00061169],\n",
      "       [ 0.0122772 ]])]\n",
      "gradients_biases:  [array([ 0.00014479, -0.00158454,  0.00313782]), array([-0.00252088])]\n",
      "Iteration 171, Cost: 0.243620688316399\n",
      "gradient_weights:  [array([[ 0.00032735, -0.00229218,  0.01541293],\n",
      "       [-0.0005308 , -0.00108183,  0.00633553]]), array([[ 0.00229336],\n",
      "       [-0.00062048],\n",
      "       [ 0.01230093]])]\n",
      "gradients_biases:  [array([ 0.00014421, -0.00158503,  0.00313163]), array([-0.00252226])]\n",
      "Iteration 172, Cost: 0.24359772369145186\n",
      "gradient_weights:  [array([[ 0.00032669, -0.00229066,  0.01542744],\n",
      "       [-0.00052914, -0.0010801 ,  0.0063481 ]]), array([[ 0.00229622],\n",
      "       [-0.00062927],\n",
      "       [ 0.01232465]])]\n",
      "gradients_biases:  [array([ 0.00014363, -0.00158553,  0.00312539]), array([-0.00252365])]\n",
      "Iteration 173, Cost: 0.2435747005509873\n",
      "gradient_weights:  [array([[ 0.00032603, -0.00228915,  0.01544193],\n",
      "       [-0.00052748, -0.00107838,  0.00636068]]), array([[ 0.00229909],\n",
      "       [-0.00063808],\n",
      "       [ 0.01234837]])]\n",
      "gradients_biases:  [array([ 0.00014306, -0.00158603,  0.00311909]), array([-0.00252503])]\n",
      "Iteration 174, Cost: 0.24355161881161158\n",
      "gradient_weights:  [array([[ 0.00032538, -0.00228763,  0.01545641],\n",
      "       [-0.00052582, -0.00107665,  0.00637327]]), array([[ 0.00230196],\n",
      "       [-0.00064688],\n",
      "       [ 0.0123721 ]])]\n",
      "gradients_biases:  [array([ 0.00014248, -0.00158654,  0.00311276]), array([-0.00252643])]\n",
      "Iteration 175, Cost: 0.2435284783902732\n",
      "gradient_weights:  [array([[ 0.00032472, -0.00228613,  0.01547087],\n",
      "       [-0.00052415, -0.00107493,  0.00638588]]), array([[ 0.00230484],\n",
      "       [-0.00065569],\n",
      "       [ 0.01239582]])]\n",
      "gradients_biases:  [array([ 0.0001419 , -0.00158705,  0.00310637]), array([-0.00252783])]\n",
      "Iteration 176, Cost: 0.24350527920426285\n",
      "gradient_weights:  [array([[ 0.00032405, -0.00228462,  0.01548532],\n",
      "       [-0.00052249, -0.0010732 ,  0.00639849]]), array([[ 0.00230772],\n",
      "       [-0.00066451],\n",
      "       [ 0.01241955]])]\n",
      "gradients_biases:  [array([ 0.00014132, -0.00158757,  0.00309993]), array([-0.00252923])]\n",
      "Iteration 177, Cost: 0.2434820211712148\n",
      "gradient_weights:  [array([[ 0.00032339, -0.00228312,  0.01549975],\n",
      "       [-0.00052082, -0.00107148,  0.00641112]]), array([[ 0.00231062],\n",
      "       [-0.00067334],\n",
      "       [ 0.01244328]])]\n",
      "gradients_biases:  [array([ 0.00014074, -0.00158809,  0.00309345]), array([-0.00253064])]\n",
      "Iteration 178, Cost: 0.2434587042091074\n",
      "gradient_weights:  [array([[ 0.00032273, -0.00228163,  0.01551417],\n",
      "       [-0.00051916, -0.00106976,  0.00642376]]), array([[ 0.00231352],\n",
      "       [-0.00068217],\n",
      "       [ 0.012467  ]])]\n",
      "gradients_biases:  [array([ 0.00014017, -0.00158861,  0.00308692]), array([-0.00253206])]\n",
      "Iteration 179, Cost: 0.2434353282362639\n",
      "gradient_weights:  [array([[ 0.00032206, -0.00228014,  0.01552858],\n",
      "       [-0.00051749, -0.00106804,  0.0064364 ]]), array([[ 0.00231643],\n",
      "       [-0.000691  ],\n",
      "       [ 0.01249073]])]\n",
      "gradients_biases:  [array([ 0.00013959, -0.00158915,  0.00308033]), array([-0.00253348])]\n",
      "Iteration 180, Cost: 0.2434118931713531\n",
      "gradient_weights:  [array([[ 0.00032139, -0.00227865,  0.01554297],\n",
      "       [-0.00051582, -0.00106633,  0.00644906]]), array([[ 0.00231934],\n",
      "       [-0.00069985],\n",
      "       [ 0.01251445]])]\n",
      "gradients_biases:  [array([ 0.00013902, -0.00158968,  0.0030737 ]), array([-0.00253491])]\n",
      "Iteration 181, Cost: 0.24338839893338998\n",
      "gradient_weights:  [array([[ 0.00032072, -0.00227716,  0.01555734],\n",
      "       [-0.00051415, -0.00106461,  0.00646174]]), array([[ 0.00232226],\n",
      "       [-0.00070869],\n",
      "       [ 0.01253818]])]\n",
      "gradients_biases:  [array([ 0.00013844, -0.00159022,  0.00306703]), array([-0.00253633])]\n",
      "Iteration 182, Cost: 0.2433648454417368\n",
      "gradient_weights:  [array([[ 0.00032005, -0.00227568,  0.0155717 ],\n",
      "       [-0.00051249, -0.0010629 ,  0.00647442]]), array([[ 0.00232519],\n",
      "       [-0.00071755],\n",
      "       [ 0.0125619 ]])]\n",
      "gradients_biases:  [array([ 0.00013787, -0.00159077,  0.0030603 ]), array([-0.00253777])]\n",
      "Iteration 183, Cost: 0.24334123261610308\n",
      "gradient_weights:  [array([[ 0.00031938, -0.00227421,  0.01558605],\n",
      "       [-0.00051082, -0.00106118,  0.00648711]]), array([[ 0.00232813],\n",
      "       [-0.00072641],\n",
      "       [ 0.01258562]])]\n",
      "gradients_biases:  [array([ 0.00013729, -0.00159132,  0.00305352]), array([-0.00253921])]\n",
      "Iteration 184, Cost: 0.24331756037654678\n",
      "gradient_weights:  [array([[ 0.00031871, -0.00227273,  0.01560038],\n",
      "       [-0.00050915, -0.00105947,  0.00649981]]), array([[ 0.00233108],\n",
      "       [-0.00073527],\n",
      "       [ 0.01260934]])]\n",
      "gradients_biases:  [array([ 0.00013672, -0.00159188,  0.0030467 ]), array([-0.00254065])]\n",
      "Iteration 185, Cost: 0.2432938286434749\n",
      "gradient_weights:  [array([[ 0.00031804, -0.00227126,  0.01561469],\n",
      "       [-0.00050747, -0.00105776,  0.00651253]]), array([[ 0.00233403],\n",
      "       [-0.00074414],\n",
      "       [ 0.01263307]])]\n",
      "gradients_biases:  [array([ 0.00013615, -0.00159244,  0.00303982]), array([-0.0025421])]\n",
      "Iteration 186, Cost: 0.24327003733764377\n",
      "gradient_weights:  [array([[ 0.00031736, -0.0022698 ,  0.01562899],\n",
      "       [-0.0005058 , -0.00105605,  0.00652525]]), array([[ 0.00233699],\n",
      "       [-0.00075302],\n",
      "       [ 0.01265679]])]\n",
      "gradients_biases:  [array([ 0.00013557, -0.00159301,  0.0030329 ]), array([-0.00254356])]\n",
      "Iteration 187, Cost: 0.24324618638015988\n",
      "gradient_weights:  [array([[ 0.00031668, -0.00226833,  0.01564327],\n",
      "       [-0.00050413, -0.00105434,  0.00653798]]), array([[ 0.00233996],\n",
      "       [-0.0007619 ],\n",
      "       [ 0.0126805 ]])]\n",
      "gradients_biases:  [array([ 0.000135  , -0.00159358,  0.00302593]), array([-0.00254501])]\n",
      "Iteration 188, Cost: 0.24322227569248062\n",
      "gradient_weights:  [array([[ 0.000316  , -0.00226688,  0.01565754],\n",
      "       [-0.00050246, -0.00105263,  0.00655073]]), array([[ 0.00234293],\n",
      "       [-0.00077079],\n",
      "       [ 0.01270422]])]\n",
      "gradients_biases:  [array([ 0.00013443, -0.00159416,  0.00301891]), array([-0.00254648])]\n",
      "Iteration 189, Cost: 0.24319830519641447\n",
      "gradient_weights:  [array([[ 0.00031532, -0.00226542,  0.01567179],\n",
      "       [-0.00050079, -0.00105092,  0.00656348]]), array([[ 0.00234591],\n",
      "       [-0.00077968],\n",
      "       [ 0.01272794]])]\n",
      "gradients_biases:  [array([ 0.00013386, -0.00159474,  0.00301184]), array([-0.00254794])]\n",
      "Iteration 190, Cost: 0.24317427481412202\n",
      "gradient_weights:  [array([[ 0.00031464, -0.00226397,  0.01568602],\n",
      "       [-0.00049911, -0.00104921,  0.00657625]]), array([[ 0.0023489 ],\n",
      "       [-0.00078858],\n",
      "       [ 0.01275165]])]\n",
      "gradients_biases:  [array([ 0.00013329, -0.00159533,  0.00300472]), array([-0.00254941])]\n",
      "Iteration 191, Cost: 0.24315018446811598\n",
      "gradient_weights:  [array([[ 0.00031395, -0.00226252,  0.01570024],\n",
      "       [-0.00049744, -0.00104751,  0.00658903]]), array([[ 0.0023519 ],\n",
      "       [-0.00079749],\n",
      "       [ 0.01277536]])]\n",
      "gradients_biases:  [array([ 0.00013272, -0.00159592,  0.00299755]), array([-0.00255089])]\n",
      "Iteration 192, Cost: 0.2431260340812621\n",
      "gradient_weights:  [array([[ 0.00031327, -0.00226108,  0.01571444],\n",
      "       [-0.00049576, -0.0010458 ,  0.00660181]]), array([[ 0.00235491],\n",
      "       [-0.0008064 ],\n",
      "       [ 0.01279907]])]\n",
      "gradients_biases:  [array([ 0.00013215, -0.00159652,  0.00299033]), array([-0.00255237])]\n",
      "Iteration 193, Cost: 0.2431018235767795\n",
      "gradient_weights:  [array([[ 0.00031258, -0.00225964,  0.01572863],\n",
      "       [-0.00049408, -0.0010441 ,  0.00661461]]), array([[ 0.00235792],\n",
      "       [-0.00081532],\n",
      "       [ 0.01282278]])]\n",
      "gradients_biases:  [array([ 0.00013158, -0.00159712,  0.00298306]), array([-0.00255386])]\n",
      "Iteration 194, Cost: 0.2430775528782414\n",
      "gradient_weights:  [array([[ 0.00031189, -0.0022582 ,  0.0157428 ],\n",
      "       [-0.00049241, -0.0010424 ,  0.00662742]]), array([[ 0.00236094],\n",
      "       [-0.00082424],\n",
      "       [ 0.01284648]])]\n",
      "gradients_biases:  [array([ 0.00013101, -0.00159773,  0.00297575]), array([-0.00255534])]\n",
      "Iteration 195, Cost: 0.24305322190957507\n",
      "gradient_weights:  [array([[ 0.0003112 , -0.00225677,  0.01575695],\n",
      "       [-0.00049073, -0.0010407 ,  0.00664023]]), array([[ 0.00236397],\n",
      "       [-0.00083317],\n",
      "       [ 0.01287019]])]\n",
      "gradients_biases:  [array([ 0.00013044, -0.00159834,  0.00296838]), array([-0.00255684])]\n",
      "Iteration 196, Cost: 0.2430288305950628\n",
      "gradient_weights:  [array([[ 0.00031051, -0.00225534,  0.01577108],\n",
      "       [-0.00048905, -0.00103899,  0.00665306]]), array([[ 0.002367  ],\n",
      "       [-0.00084211],\n",
      "       [ 0.01289388]])]\n",
      "gradients_biases:  [array([ 0.00012988, -0.00159896,  0.00296097]), array([-0.00255834])]\n",
      "Iteration 197, Cost: 0.24300437885934217\n",
      "gradient_weights:  [array([[ 0.00030982, -0.00225391,  0.0157852 ],\n",
      "       [-0.00048737, -0.00103729,  0.0066659 ]]), array([[ 0.00237005],\n",
      "       [-0.00085105],\n",
      "       [ 0.01291758]])]\n",
      "gradients_biases:  [array([ 0.00012931, -0.00159958,  0.0029535 ]), array([-0.00255984])]\n",
      "Iteration 198, Cost: 0.24297986662740634\n",
      "gradient_weights:  [array([[ 0.00030912, -0.00225249,  0.0157993 ],\n",
      "       [-0.00048569, -0.00103559,  0.00667874]]), array([[ 0.0023731 ],\n",
      "       [-0.00086   ],\n",
      "       [ 0.01294128]])]\n",
      "gradients_biases:  [array([ 0.00012874, -0.00160021,  0.00294598]), array([-0.00256134])]\n",
      "Iteration 199, Cost: 0.24295529382460465\n",
      "gradient_weights:  [array([[ 0.00030843, -0.00225107,  0.01581339],\n",
      "       [-0.00048401, -0.0010339 ,  0.0066916 ]]), array([[ 0.00237616],\n",
      "       [-0.00086895],\n",
      "       [ 0.01296497]])]\n",
      "gradients_biases:  [array([ 0.00012818, -0.00160085,  0.00293842]), array([-0.00256286])]\n",
      "Iteration 200, Cost: 0.2429306603766429\n",
      "gradient_weights:  [array([[ 0.00030773, -0.00224965,  0.01582745],\n",
      "       [-0.00048233, -0.0010322 ,  0.00670447]]), array([[ 0.00237923],\n",
      "       [-0.00087791],\n",
      "       [ 0.01298865]])]\n",
      "gradients_biases:  [array([ 0.00012761, -0.00160148,  0.0029308 ]), array([-0.00256437])]\n",
      "Iteration 201, Cost: 0.24290596620958377\n",
      "gradient_weights:  [array([[ 0.00030703, -0.00224824,  0.0158415 ],\n",
      "       [-0.00048065, -0.0010305 ,  0.00671734]]), array([[ 0.0023823 ],\n",
      "       [-0.00088687],\n",
      "       [ 0.01301234]])]\n",
      "gradients_biases:  [array([ 0.00012705, -0.00160213,  0.00292314]), array([-0.00256589])]\n",
      "Iteration 202, Cost: 0.24288121124984702\n",
      "gradient_weights:  [array([[ 0.00030633, -0.00224683,  0.01585554],\n",
      "       [-0.00047897, -0.0010288 ,  0.00673023]]), array([[ 0.00238538],\n",
      "       [-0.00089584],\n",
      "       [ 0.01303602]])]\n",
      "gradients_biases:  [array([ 0.00012649, -0.00160278,  0.00291543]), array([-0.00256741])]\n",
      "Iteration 203, Cost: 0.24285639542421017\n",
      "gradient_weights:  [array([[ 0.00030562, -0.00224542,  0.01586955],\n",
      "       [-0.00047729, -0.00102711,  0.00674312]]), array([[ 0.00238847],\n",
      "       [-0.00090482],\n",
      "       [ 0.0130597 ]])]\n",
      "gradients_biases:  [array([ 0.00012592, -0.00160343,  0.00290766]), array([-0.00256894])]\n",
      "Iteration 204, Cost: 0.24283151865980848\n",
      "gradient_weights:  [array([[ 0.00030492, -0.00224402,  0.01588355],\n",
      "       [-0.00047561, -0.00102541,  0.00675603]]), array([[ 0.00239157],\n",
      "       [-0.0009138 ],\n",
      "       [ 0.01308337]])]\n",
      "gradients_biases:  [array([ 0.00012536, -0.00160409,  0.00289985]), array([-0.00257047])]\n",
      "Iteration 205, Cost: 0.2428065808841354\n",
      "gradient_weights:  [array([[ 0.00030421, -0.00224262,  0.01589752],\n",
      "       [-0.00047392, -0.00102372,  0.00676894]]), array([[ 0.00239468],\n",
      "       [-0.00092279],\n",
      "       [ 0.01310704]])]\n",
      "gradients_biases:  [array([ 0.0001248 , -0.00160475,  0.00289198]), array([-0.00257201])]\n",
      "Iteration 206, Cost: 0.24278158202504285\n",
      "gradient_weights:  [array([[ 0.0003035 , -0.00224122,  0.01591148],\n",
      "       [-0.00047224, -0.00102203,  0.00678187]]), array([[ 0.00239779],\n",
      "       [-0.00093179],\n",
      "       [ 0.0131307 ]])]\n",
      "gradients_biases:  [array([ 0.00012424, -0.00160542,  0.00288407]), array([-0.00257355])]\n",
      "Iteration 207, Cost: 0.24275652201074155\n",
      "gradient_weights:  [array([[ 0.00030279, -0.00223983,  0.01592543],\n",
      "       [-0.00047055, -0.00102033,  0.0067948 ]]), array([[ 0.00240092],\n",
      "       [-0.00094079],\n",
      "       [ 0.01315436]])]\n",
      "gradients_biases:  [array([ 0.00012367, -0.0016061 ,  0.0028761 ]), array([-0.00257509])]\n",
      "Iteration 208, Cost: 0.2427314007698012\n",
      "gradient_weights:  [array([[ 0.00030208, -0.00223844,  0.01593935],\n",
      "       [-0.00046887, -0.00101864,  0.00680774]]), array([[ 0.00240405],\n",
      "       [-0.0009498 ],\n",
      "       [ 0.01317802]])]\n",
      "gradients_biases:  [array([ 0.00012311, -0.00160678,  0.00286809]), array([-0.00257664])]\n",
      "Iteration 209, Cost: 0.24270621823115077\n",
      "gradient_weights:  [array([[ 0.00030137, -0.00223705,  0.01595326],\n",
      "       [-0.00046718, -0.00101695,  0.00682069]]), array([[ 0.00240718],\n",
      "       [-0.00095881],\n",
      "       [ 0.01320167]])]\n",
      "gradients_biases:  [array([ 0.00012255, -0.00160746,  0.00286002]), array([-0.00257819])]\n",
      "Iteration 210, Cost: 0.24268097432407842\n",
      "gradient_weights:  [array([[ 0.00030065, -0.00223567,  0.01596714],\n",
      "       [-0.00046549, -0.00101526,  0.00683365]]), array([[ 0.00241033],\n",
      "       [-0.00096783],\n",
      "       [ 0.01322532]])]\n",
      "gradients_biases:  [array([ 0.000122  , -0.00160815,  0.00285191]), array([-0.00257975])]\n",
      "Iteration 211, Cost: 0.24265566897823249\n",
      "gradient_weights:  [array([[ 0.00029994, -0.00223429,  0.01598101],\n",
      "       [-0.00046381, -0.00101357,  0.00684662]]), array([[ 0.00241348],\n",
      "       [-0.00097685],\n",
      "       [ 0.01324896]])]\n",
      "gradients_biases:  [array([ 0.00012144, -0.00160885,  0.00284374]), array([-0.00258131])]\n",
      "Iteration 212, Cost: 0.2426303021236208\n",
      "gradient_weights:  [array([[ 0.00029922, -0.00223291,  0.01599486],\n",
      "       [-0.00046212, -0.00101188,  0.0068596 ]]), array([[ 0.00241665],\n",
      "       [-0.00098588],\n",
      "       [ 0.01327259]])]\n",
      "gradients_biases:  [array([ 0.00012088, -0.00160955,  0.00283553]), array([-0.00258288])]\n",
      "Iteration 213, Cost: 0.24260487369061134\n",
      "gradient_weights:  [array([[ 0.0002985 , -0.00223153,  0.0160087 ],\n",
      "       [-0.00046043, -0.00101019,  0.00687259]]), array([[ 0.00241982],\n",
      "       [-0.00099492],\n",
      "       [ 0.01329623]])]\n",
      "gradients_biases:  [array([ 0.00012032, -0.00161025,  0.00282726]), array([-0.00258445])]\n",
      "Iteration 214, Cost: 0.24257938360993223\n",
      "gradient_weights:  [array([[ 0.00029778, -0.00223016,  0.01602251],\n",
      "       [-0.00045874, -0.0010085 ,  0.00688558]]), array([[ 0.00242299],\n",
      "       [-0.00100396],\n",
      "       [ 0.01331985]])]\n",
      "gradients_biases:  [array([ 0.00011976, -0.00161096,  0.00281895]), array([-0.00258602])]\n",
      "Iteration 215, Cost: 0.24255383181267218\n",
      "gradient_weights:  [array([[ 0.00029705, -0.0022288 ,  0.0160363 ],\n",
      "       [-0.00045705, -0.00100681,  0.00689859]]), array([[ 0.00242618],\n",
      "       [-0.00101301],\n",
      "       [ 0.01334347]])]\n",
      "gradients_biases:  [array([ 0.00011921, -0.00161168,  0.00281058]), array([-0.0025876])]\n",
      "Iteration 216, Cost: 0.24252821823028006\n",
      "gradient_weights:  [array([[ 0.00029633, -0.00222743,  0.01605008],\n",
      "       [-0.00045536, -0.00100512,  0.0069116 ]]), array([[ 0.00242937],\n",
      "       [-0.00102207],\n",
      "       [ 0.01336709]])]\n",
      "gradients_biases:  [array([ 0.00011865, -0.0016124 ,  0.00280217]), array([-0.00258918])]\n",
      "Iteration 217, Cost: 0.2425025427945655\n",
      "gradient_weights:  [array([[ 0.0002956 , -0.00222607,  0.01606383],\n",
      "       [-0.00045367, -0.00100343,  0.00692462]]), array([[ 0.00243258],\n",
      "       [-0.00103113],\n",
      "       [ 0.01339069]])]\n",
      "gradients_biases:  [array([ 0.0001181 , -0.00161312,  0.0027937 ]), array([-0.00259076])]\n",
      "Iteration 218, Cost: 0.24247680543769895\n",
      "gradient_weights:  [array([[ 0.00029487, -0.00222471,  0.01607757],\n",
      "       [-0.00045198, -0.00100175,  0.00693766]]), array([[ 0.00243579],\n",
      "       [-0.0010402 ],\n",
      "       [ 0.0134143 ]])]\n",
      "gradients_biases:  [array([ 0.00011754, -0.00161386,  0.00278518]), array([-0.00259235])]\n",
      "Iteration 219, Cost: 0.24245100609221137\n",
      "gradient_weights:  [array([[ 0.00029414, -0.00222335,  0.01609128],\n",
      "       [-0.00045028, -0.00100006,  0.0069507 ]]), array([[ 0.002439  ],\n",
      "       [-0.00104927],\n",
      "       [ 0.01343789]])]\n",
      "gradients_biases:  [array([ 0.00011699, -0.00161459,  0.00277662]), array([-0.00259395])]\n",
      "Iteration 220, Cost: 0.24242514469099463\n",
      "gradient_weights:  [array([[ 0.00029341, -0.002222  ,  0.01610498],\n",
      "       [-0.00044859, -0.00099837,  0.00696374]]), array([[ 0.00244223],\n",
      "       [-0.00105835],\n",
      "       [ 0.01346148]])]\n",
      "gradients_biases:  [array([ 0.00011643, -0.00161533,  0.002768  ]), array([-0.00259555])]\n",
      "Iteration 221, Cost: 0.24239922116730167\n",
      "gradient_weights:  [array([[ 0.00029267, -0.00222065,  0.01611866],\n",
      "       [-0.0004469 , -0.00099669,  0.0069768 ]]), array([[ 0.00244546],\n",
      "       [-0.00106744],\n",
      "       [ 0.01348507]])]\n",
      "gradients_biases:  [array([ 0.00011588, -0.00161608,  0.00275933]), array([-0.00259715])]\n",
      "Iteration 222, Cost: 0.2423732354547461\n",
      "gradient_weights:  [array([[ 0.00029193, -0.0022193 ,  0.01613232],\n",
      "       [-0.0004452 , -0.000995  ,  0.00698986]]), array([[ 0.0024487 ],\n",
      "       [-0.00107653],\n",
      "       [ 0.01350865]])]\n",
      "gradients_biases:  [array([ 0.00011533, -0.00161683,  0.00275061]), array([-0.00259875])]\n",
      "Iteration 223, Cost: 0.24234718748730266\n",
      "gradient_weights:  [array([[ 0.0002912 , -0.00221796,  0.01614596],\n",
      "       [-0.00044351, -0.00099332,  0.00700294]]), array([[ 0.00245195],\n",
      "       [-0.00108563],\n",
      "       [ 0.01353222]])]\n",
      "gradients_biases:  [array([ 0.00011478, -0.00161759,  0.00274184]), array([-0.00260037])]\n",
      "Iteration 224, Cost: 0.24232107719930696\n",
      "gradient_weights:  [array([[ 0.00029046, -0.00221662,  0.01615957],\n",
      "       [-0.00044181, -0.00099163,  0.00701602]]), array([[ 0.00245521],\n",
      "       [-0.00109474],\n",
      "       [ 0.01355578]])]\n",
      "gradients_biases:  [array([ 0.00011422, -0.00161835,  0.00273302]), array([-0.00260198])]\n",
      "Iteration 225, Cost: 0.24229490452545555\n",
      "gradient_weights:  [array([[ 0.00028971, -0.00221528,  0.01617317],\n",
      "       [-0.00044012, -0.00098994,  0.00702911]]), array([[ 0.00245848],\n",
      "       [-0.00110385],\n",
      "       [ 0.01357934]])]\n",
      "gradients_biases:  [array([ 0.00011367, -0.00161912,  0.00272415]), array([-0.0026036])]\n",
      "Iteration 226, Cost: 0.24226866940080588\n",
      "gradient_weights:  [array([[ 0.00028897, -0.00221394,  0.01618675],\n",
      "       [-0.00043842, -0.00098826,  0.00704221]]), array([[ 0.00246175],\n",
      "       [-0.00111297],\n",
      "       [ 0.01360288]])]\n",
      "gradients_biases:  [array([ 0.00011312, -0.0016199 ,  0.00271523]), array([-0.00260522])]\n",
      "Iteration 227, Cost: 0.2422423717607763\n",
      "gradient_weights:  [array([[ 0.00028822, -0.00221261,  0.01620031],\n",
      "       [-0.00043672, -0.00098657,  0.00705531]]), array([[ 0.00246503],\n",
      "       [-0.00112209],\n",
      "       [ 0.01362643]])]\n",
      "gradients_biases:  [array([ 0.00011257, -0.00162067,  0.00270626]), array([-0.00260685])]\n",
      "Iteration 228, Cost: 0.242216011541146\n",
      "gradient_weights:  [array([[ 0.00028747, -0.00221128,  0.01621385],\n",
      "       [-0.00043503, -0.00098489,  0.00706843]]), array([[ 0.00246832],\n",
      "       [-0.00113122],\n",
      "       [ 0.01364996]])]\n",
      "gradients_biases:  [array([ 0.00011203, -0.00162146,  0.00269724]), array([-0.00260848])]\n",
      "Iteration 229, Cost: 0.2421895886780549\n",
      "gradient_weights:  [array([[ 0.00028672, -0.00220995,  0.01622736],\n",
      "       [-0.00043333, -0.00098321,  0.00708155]]), array([[ 0.00247162],\n",
      "       [-0.00114036],\n",
      "       [ 0.01367349]])]\n",
      "gradients_biases:  [array([ 0.00011148, -0.00162225,  0.00268817]), array([-0.00261012])]\n",
      "Iteration 230, Cost: 0.24216310310800349\n",
      "gradient_weights:  [array([[ 0.00028597, -0.00220863,  0.01624086],\n",
      "       [-0.00043163, -0.00098152,  0.00709468]]), array([[ 0.00247493],\n",
      "       [-0.00114951],\n",
      "       [ 0.013697  ]])]\n",
      "gradients_biases:  [array([ 0.00011093, -0.00162304,  0.00267905]), array([-0.00261176])]\n",
      "Iteration 231, Cost: 0.24213655476785295\n",
      "gradient_weights:  [array([[ 0.00028522, -0.00220731,  0.01625434],\n",
      "       [-0.00042993, -0.00097984,  0.00710781]]), array([[ 0.00247824],\n",
      "       [-0.00115866],\n",
      "       [ 0.01372052]])]\n",
      "gradients_biases:  [array([ 0.00011038, -0.00162384,  0.00266987]), array([-0.0026134])]\n",
      "Iteration 232, Cost: 0.24210994359482482\n",
      "gradient_weights:  [array([[ 0.00028446, -0.00220599,  0.01626779],\n",
      "       [-0.00042823, -0.00097815,  0.00712096]]), array([[ 0.00248156],\n",
      "       [-0.00116781],\n",
      "       [ 0.01374402]])]\n",
      "gradients_biases:  [array([ 0.00010984, -0.00162465,  0.00266065]), array([-0.00261505])]\n",
      "Iteration 233, Cost: 0.242083269526501\n",
      "gradient_weights:  [array([[ 0.0002837 , -0.00220467,  0.01628123],\n",
      "       [-0.00042653, -0.00097647,  0.00713411]]), array([[ 0.00248489],\n",
      "       [-0.00117698],\n",
      "       [ 0.01376751]])]\n",
      "gradients_biases:  [array([ 0.00010929, -0.00162546,  0.00265138]), array([-0.0026167])]\n",
      "Iteration 234, Cost: 0.24205653250082354\n",
      "gradient_weights:  [array([[ 0.00028294, -0.00220336,  0.01629464],\n",
      "       [-0.00042483, -0.00097478,  0.00714727]]), array([[ 0.00248823],\n",
      "       [-0.00118615],\n",
      "       [ 0.013791  ]])]\n",
      "gradients_biases:  [array([ 0.00010874, -0.00162627,  0.00264205]), array([-0.00261836])]\n",
      "Iteration 235, Cost: 0.24202973245609455\n",
      "gradient_weights:  [array([[ 0.00028218, -0.00220205,  0.01630803],\n",
      "       [-0.00042313, -0.0009731 ,  0.00716044]]), array([[ 0.00249158],\n",
      "       [-0.00119532],\n",
      "       [ 0.01381447]])]\n",
      "gradients_biases:  [array([ 0.0001082 , -0.00162709,  0.00263268]), array([-0.00262002])]\n",
      "Iteration 236, Cost: 0.24200286933097592\n",
      "gradient_weights:  [array([[ 0.00028142, -0.00220074,  0.01632141],\n",
      "       [-0.00042143, -0.00097142,  0.00717362]]), array([[ 0.00249493],\n",
      "       [-0.0012045 ],\n",
      "       [ 0.01383794]])]\n",
      "gradients_biases:  [array([ 0.00010766, -0.00162792,  0.00262325]), array([-0.00262168])]\n",
      "Iteration 237, Cost: 0.24197594306448922\n",
      "gradient_weights:  [array([[ 0.00028065, -0.00219943,  0.01633476],\n",
      "       [-0.00041972, -0.00096973,  0.0071868 ]]), array([[ 0.00249829],\n",
      "       [-0.00121369],\n",
      "       [ 0.0138614 ]])]\n",
      "gradients_biases:  [array([ 0.00010711, -0.00162875,  0.00261377]), array([-0.00262335])]\n",
      "Iteration 238, Cost: 0.2419489535960156\n",
      "gradient_weights:  [array([[ 0.00027989, -0.00219813,  0.01634809],\n",
      "       [-0.00041802, -0.00096805,  0.00719999]]), array([[ 0.00250166],\n",
      "       [-0.00122289],\n",
      "       [ 0.01388485]])]\n",
      "gradients_biases:  [array([ 0.00010657, -0.00162959,  0.00260425]), array([-0.00262502])]\n",
      "Iteration 239, Cost: 0.2419219008652953\n",
      "gradient_weights:  [array([[ 0.00027912, -0.00219683,  0.01636139],\n",
      "       [-0.00041632, -0.00096636,  0.00721319]]), array([[ 0.00250504],\n",
      "       [-0.00123209],\n",
      "       [ 0.01390829]])]\n",
      "gradients_biases:  [array([ 0.00010603, -0.00163043,  0.00259467]), array([-0.0026267])]\n",
      "Iteration 240, Cost: 0.24189478481242774\n",
      "gradient_weights:  [array([[ 0.00027834, -0.00219553,  0.01637468],\n",
      "       [-0.00041461, -0.00096468,  0.00722639]]), array([[ 0.00250843],\n",
      "       [-0.0012413 ],\n",
      "       [ 0.01393172]])]\n",
      "gradients_biases:  [array([ 0.00010549, -0.00163128,  0.00258504]), array([-0.00262838])]\n",
      "Iteration 241, Cost: 0.2418676053778709\n",
      "gradient_weights:  [array([[ 0.00027757, -0.00219424,  0.01638795],\n",
      "       [-0.00041291, -0.00096299,  0.0072396 ]]), array([[ 0.00251182],\n",
      "       [-0.00125052],\n",
      "       [ 0.01395515]])]\n",
      "gradients_biases:  [array([ 0.00010495, -0.00163213,  0.00257536]), array([-0.00263007])]\n",
      "Iteration 242, Cost: 0.2418403625024415\n",
      "gradient_weights:  [array([[ 0.00027679, -0.00219294,  0.01640119],\n",
      "       [-0.0004112 , -0.00096131,  0.00725282]]), array([[ 0.00251522],\n",
      "       [-0.00125974],\n",
      "       [ 0.01397856]])]\n",
      "gradients_biases:  [array([ 0.00010441, -0.00163299,  0.00256564]), array([-0.00263176])]\n",
      "Iteration 243, Cost: 0.2418130561273144\n",
      "gradient_weights:  [array([[ 0.00027601, -0.00219165,  0.01641441],\n",
      "       [-0.0004095 , -0.00095962,  0.00726605]]), array([[ 0.00251863],\n",
      "       [-0.00126897],\n",
      "       [ 0.01400196]])]\n",
      "gradients_biases:  [array([ 0.00010387, -0.00163385,  0.00255586]), array([-0.00263345])]\n",
      "Iteration 244, Cost: 0.2417856861940226\n",
      "gradient_weights:  [array([[ 0.00027523, -0.00219037,  0.01642761],\n",
      "       [-0.00040779, -0.00095794,  0.00727928]]), array([[ 0.00252205],\n",
      "       [-0.0012782 ],\n",
      "       [ 0.01402535]])]\n",
      "gradients_biases:  [array([ 0.00010333, -0.00163472,  0.00254603]), array([-0.00263515])]\n",
      "Iteration 245, Cost: 0.24175825264445666\n",
      "gradient_weights:  [array([[ 0.00027445, -0.00218908,  0.01644079],\n",
      "       [-0.00040608, -0.00095625,  0.00729252]]), array([[ 0.00252548],\n",
      "       [-0.00128744],\n",
      "       [ 0.01404873]])]\n",
      "gradients_biases:  [array([ 0.00010279, -0.00163559,  0.00253615]), array([-0.00263685])]\n",
      "Iteration 246, Cost: 0.24173075542086458\n",
      "gradient_weights:  [array([[ 0.00027367, -0.0021878 ,  0.01645395],\n",
      "       [-0.00040437, -0.00095457,  0.00730576]]), array([[ 0.00252891],\n",
      "       [-0.00129669],\n",
      "       [ 0.01407211]])]\n",
      "gradients_biases:  [array([ 0.00010225, -0.00163647,  0.00252622]), array([-0.00263856])]\n",
      "Iteration 247, Cost: 0.24170319446585148\n",
      "gradient_weights:  [array([[ 0.00027288, -0.00218652,  0.01646708],\n",
      "       [-0.00040267, -0.00095288,  0.00731902]]), array([[ 0.00253236],\n",
      "       [-0.00130595],\n",
      "       [ 0.01409547]])]\n",
      "gradients_biases:  [array([ 0.00010171, -0.00163736,  0.00251623]), array([-0.00264027])]\n",
      "Iteration 248, Cost: 0.2416755697223791\n",
      "gradient_weights:  [array([[ 0.00027209, -0.00218524,  0.01648019],\n",
      "       [-0.00040096, -0.0009512 ,  0.00733228]]), array([[ 0.00253581],\n",
      "       [-0.00131521],\n",
      "       [ 0.01411882]])]\n",
      "gradients_biases:  [array([ 0.00010118, -0.00163825,  0.0025062 ]), array([-0.00264199])]\n",
      "Iteration 249, Cost: 0.2416478811337657\n",
      "gradient_weights:  [array([[ 0.0002713 , -0.00218396,  0.01649328],\n",
      "       [-0.00039925, -0.00094951,  0.00734554]]), array([[ 0.00253927],\n",
      "       [-0.00132448],\n",
      "       [ 0.01414216]])]\n",
      "gradients_biases:  [array([ 0.00010064, -0.00163914,  0.00249612]), array([-0.00264371])]\n",
      "Iteration 250, Cost: 0.24162012864368562\n",
      "gradient_weights:  [array([[ 0.00027051, -0.00218269,  0.01650635],\n",
      "       [-0.00039754, -0.00094782,  0.00735882]]), array([[ 0.00254273],\n",
      "       [-0.00133376],\n",
      "       [ 0.01416549]])]\n",
      "gradients_biases:  [array([ 0.00010011, -0.00164005,  0.00248599]), array([-0.00264543])]\n",
      "Iteration 251, Cost: 0.24159231219616878\n",
      "gradient_weights:  [array([[ 0.00026971, -0.00218142,  0.01651939],\n",
      "       [-0.00039583, -0.00094614,  0.0073721 ]]), array([[ 0.00254621],\n",
      "       [-0.00134304],\n",
      "       [ 0.0141888 ]])]\n",
      "gradients_biases:  [array([ 9.95725527e-05, -1.64095191e-03,  2.47580652e-03]), array([-0.00264716])]\n",
      "Iteration 252, Cost: 0.2415644317356004\n",
      "gradient_weights:  [array([[ 0.00026891, -0.00218015,  0.01653242],\n",
      "       [-0.00039412, -0.00094445,  0.00738538]]), array([[ 0.00254969],\n",
      "       [-0.00135233],\n",
      "       [ 0.01421211]])]\n",
      "gradients_biases:  [array([ 9.90387737e-05, -1.64186396e-03,  2.46557367e-03]), array([-0.00264889])]\n",
      "Iteration 253, Cost: 0.24153648720672077\n",
      "gradient_weights:  [array([[ 0.00026812, -0.00217888,  0.01654542],\n",
      "       [-0.00039241, -0.00094276,  0.00739867]]), array([[ 0.00255318],\n",
      "       [-0.00136163],\n",
      "       [ 0.0142354 ]])]\n",
      "gradients_biases:  [array([ 9.85056491e-05, -1.64278145e-03,  2.45529053e-03]), array([-0.00265063])]\n",
      "Iteration 254, Cost: 0.24150847855462457\n",
      "gradient_weights:  [array([[ 0.00026731, -0.00217762,  0.01655839],\n",
      "       [-0.0003907 , -0.00094107,  0.00741197]]), array([[ 0.00255668],\n",
      "       [-0.00137093],\n",
      "       [ 0.01425869]])]\n",
      "gradients_biases:  [array([ 9.79731812e-05, -1.64370437e-03,  2.44495711e-03]), array([-0.00265237])]\n",
      "Iteration 255, Cost: 0.24148040572476065\n",
      "gradient_weights:  [array([[ 0.00026651, -0.00217636,  0.01657135],\n",
      "       [-0.00038899, -0.00093939,  0.00742528]]), array([[ 0.00256019],\n",
      "       [-0.00138024],\n",
      "       [ 0.01428196]])]\n",
      "gradients_biases:  [array([ 9.74413722e-05, -1.64463273e-03,  2.43457345e-03]), array([-0.00265412])]\n",
      "Iteration 256, Cost: 0.2414522686629314\n",
      "gradient_weights:  [array([[ 0.0002657 , -0.0021751 ,  0.01658428],\n",
      "       [-0.00038727, -0.0009377 ,  0.00743859]]), array([[ 0.0025637 ],\n",
      "       [-0.00138956],\n",
      "       [ 0.01430522]])]\n",
      "gradients_biases:  [array([ 9.69102244e-05, -1.64556654e-03,  2.42413957e-03]), array([-0.00265587])]\n",
      "Iteration 257, Cost: 0.24142406731529265\n",
      "gradient_weights:  [array([[ 0.00026489, -0.00217384,  0.01659719],\n",
      "       [-0.00038556, -0.00093601,  0.0074519 ]]), array([[ 0.00256722],\n",
      "       [-0.00139888],\n",
      "       [ 0.01432847]])]\n",
      "gradients_biases:  [array([ 9.63797401e-05, -1.64650581e-03,  2.41365550e-03]), array([-0.00265762])]\n",
      "Iteration 258, Cost: 0.2413958016283529\n",
      "gradient_weights:  [array([[ 0.00026408, -0.00217258,  0.01661008],\n",
      "       [-0.00038385, -0.00093432,  0.00746523]]), array([[ 0.00257075],\n",
      "       [-0.00140821],\n",
      "       [ 0.0143517 ]])]\n",
      "gradients_biases:  [array([ 9.58499215e-05, -1.64745055e-03,  2.40312128e-03]), array([-0.00265938])]\n",
      "Iteration 259, Cost: 0.24136747154897317\n",
      "gradient_weights:  [array([[ 0.00026327, -0.00217133,  0.01662294],\n",
      "       [-0.00038213, -0.00093263,  0.00747856]]), array([[ 0.00257429],\n",
      "       [-0.00141755],\n",
      "       [ 0.01437493]])]\n",
      "gradients_biases:  [array([ 9.53207709e-05, -1.64840075e-03,  2.39253694e-03]), array([-0.00266115])]\n",
      "Iteration 260, Cost: 0.24133907702436608\n",
      "gradient_weights:  [array([[ 0.00026245, -0.00217008,  0.01663578],\n",
      "       [-0.00038042, -0.00093093,  0.00749189]]), array([[ 0.00257784],\n",
      "       [-0.0014269 ],\n",
      "       [ 0.01439814]])]\n",
      "gradients_biases:  [array([ 9.47922906e-05, -1.64935644e-03,  2.38190250e-03]), array([-0.00266291])]\n",
      "Iteration 261, Cost: 0.24131061800209588\n",
      "gradient_weights:  [array([[ 0.00026164, -0.00216883,  0.0166486 ],\n",
      "       [-0.0003787 , -0.00092924,  0.00750523]]), array([[ 0.00258139],\n",
      "       [-0.00143625],\n",
      "       [ 0.01442134]])]\n",
      "gradients_biases:  [array([ 9.42644831e-05, -1.65031761e-03,  2.37121802e-03]), array([-0.00266469])]\n",
      "Iteration 262, Cost: 0.2412820944300777\n",
      "gradient_weights:  [array([[ 0.00026082, -0.00216758,  0.01666139],\n",
      "       [-0.00037699, -0.00092755,  0.00751858]]), array([[ 0.00258496],\n",
      "       [-0.00144561],\n",
      "       [ 0.01444452]])]\n",
      "gradients_biases:  [array([ 9.37373506e-05, -1.65128427e-03,  2.36048351e-03]), array([-0.00266646])]\n",
      "Iteration 263, Cost: 0.24125350625657693\n",
      "gradient_weights:  [array([[ 0.00025999, -0.00216634,  0.01667416],\n",
      "       [-0.00037527, -0.00092586,  0.00753193]]), array([[ 0.00258853],\n",
      "       [-0.00145498],\n",
      "       [ 0.01446769]])]\n",
      "gradients_biases:  [array([ 9.32108954e-05, -1.65225643e-03,  2.34969903e-03]), array([-0.00266825])]\n",
      "Iteration 264, Cost: 0.24122485343020916\n",
      "gradient_weights:  [array([[ 0.00025917, -0.00216509,  0.01668691],\n",
      "       [-0.00037356, -0.00092416,  0.00754529]]), array([[ 0.00259211],\n",
      "       [-0.00146435],\n",
      "       [ 0.01449085]])]\n",
      "gradients_biases:  [array([ 9.26851200e-05, -1.65323410e-03,  2.33886461e-03]), array([-0.00267003])]\n",
      "Iteration 265, Cost: 0.24119613589993905\n",
      "gradient_weights:  [array([[ 0.00025834, -0.00216385,  0.01669963],\n",
      "       [-0.00037184, -0.00092247,  0.00755865]]), array([[ 0.00259569],\n",
      "       [-0.00147373],\n",
      "       [ 0.014514  ]])]\n",
      "gradients_biases:  [array([ 9.21600267e-05, -1.65421729e-03,  2.32798029e-03]), array([-0.00267182])]\n",
      "Iteration 266, Cost: 0.2411673536150803\n",
      "gradient_weights:  [array([[ 0.00025751, -0.00216261,  0.01671233],\n",
      "       [-0.00037012, -0.00092077,  0.00757202]]), array([[ 0.00259929],\n",
      "       [-0.00148312],\n",
      "       [ 0.01453713]])]\n",
      "gradients_biases:  [array([ 9.16356179e-05, -1.65520599e-03,  2.31704611e-03]), array([-0.00267362])]\n",
      "Iteration 267, Cost: 0.241138506525295\n",
      "gradient_weights:  [array([[ 0.00025668, -0.00216137,  0.016725  ],\n",
      "       [-0.00036841, -0.00091908,  0.00758539]]), array([[ 0.00260289],\n",
      "       [-0.00149251],\n",
      "       [ 0.01456025]])]\n",
      "gradients_biases:  [array([ 9.11118961e-05, -1.65620023e-03,  2.30606212e-03]), array([-0.00267542])]\n",
      "Iteration 268, Cost: 0.24110959458059283\n",
      "gradient_weights:  [array([[ 0.00025585, -0.00216014,  0.01673766],\n",
      "       [-0.00036669, -0.00091738,  0.00759877]]), array([[ 0.0026065 ],\n",
      "       [-0.00150191],\n",
      "       [ 0.01458336]])]\n",
      "gradients_biases:  [array([ 9.05888636e-05, -1.65720000e-03,  2.29502836e-03]), array([-0.00267722])]\n",
      "Iteration 269, Cost: 0.241080617731331\n",
      "gradient_weights:  [array([[ 0.00025501, -0.00215891,  0.01675028],\n",
      "       [-0.00036497, -0.00091568,  0.00761216]]), array([[ 0.00261012],\n",
      "       [-0.00151132],\n",
      "       [ 0.01460645]])]\n",
      "gradients_biases:  [array([ 9.00665230e-05, -1.65820532e-03,  2.28394488e-03]), array([-0.00267903])]\n",
      "Iteration 270, Cost: 0.24105157592821294\n",
      "gradient_weights:  [array([[ 0.00025417, -0.00215767,  0.01676289],\n",
      "       [-0.00036325, -0.00091399,  0.00762555]]), array([[ 0.00261375],\n",
      "       [-0.00152074],\n",
      "       [ 0.01462953]])]\n",
      "gradients_biases:  [array([ 8.95448767e-05, -1.65921618e-03,  2.27281172e-03]), array([-0.00268084])]\n",
      "Iteration 271, Cost: 0.2410224691222887\n",
      "gradient_weights:  [array([[ 0.00025333, -0.00215644,  0.01677547],\n",
      "       [-0.00036153, -0.00091229,  0.00763894]]), array([[ 0.00261738],\n",
      "       [-0.00153016],\n",
      "       [ 0.01465259]])]\n",
      "gradients_biases:  [array([ 8.90239271e-05, -1.66023261e-03,  2.26162894e-03]), array([-0.00268266])]\n",
      "Iteration 272, Cost: 0.2409932972649534\n",
      "gradient_weights:  [array([[ 0.00025249, -0.00215522,  0.01678803],\n",
      "       [-0.00035981, -0.00091059,  0.00765234]]), array([[ 0.00262102],\n",
      "       [-0.0015396 ],\n",
      "       [ 0.01467564]])]\n",
      "gradients_biases:  [array([ 8.85036767e-05, -1.66125460e-03,  2.25039658e-03]), array([-0.00268448])]\n",
      "Iteration 273, Cost: 0.24096406030794743\n",
      "gradient_weights:  [array([[ 0.00025164, -0.00215399,  0.01680056],\n",
      "       [-0.00035809, -0.00090889,  0.00766575]]), array([[ 0.00262467],\n",
      "       [-0.00154903],\n",
      "       [ 0.01469868]])]\n",
      "gradients_biases:  [array([ 8.79841282e-05, -1.66228216e-03,  2.23911470e-03]), array([-0.00268631])]\n",
      "Iteration 274, Cost: 0.24093475820335514\n",
      "gradient_weights:  [array([[ 0.00025079, -0.00215277,  0.01681307],\n",
      "       [-0.00035637, -0.00090718,  0.00767916]]), array([[ 0.00262833],\n",
      "       [-0.00155848],\n",
      "       [ 0.0147217 ]])]\n",
      "gradients_biases:  [array([ 8.74652838e-05, -1.66331530e-03,  2.22778334e-03]), array([-0.00268814])]\n",
      "Iteration 275, Cost: 0.24090539090360505\n",
      "gradient_weights:  [array([[ 0.00024994, -0.00215154,  0.01682555],\n",
      "       [-0.00035465, -0.00090548,  0.00769257]]), array([[ 0.002632  ],\n",
      "       [-0.00156793],\n",
      "       [ 0.0147447 ]])]\n",
      "gradients_biases:  [array([ 8.69471463e-05, -1.66435403e-03,  2.21640257e-03]), array([-0.00268997])]\n",
      "Iteration 276, Cost: 0.24087595836146847\n",
      "gradient_weights:  [array([[ 0.00024909, -0.00215032,  0.01683801],\n",
      "       [-0.00035293, -0.00090378,  0.00770599]]), array([[ 0.00263567],\n",
      "       [-0.00157739],\n",
      "       [ 0.01476769]])]\n",
      "gradients_biases:  [array([ 8.64297182e-05, -1.66539836e-03,  2.20497244e-03]), array([-0.00269181])]\n",
      "Iteration 277, Cost: 0.24084646053005937\n",
      "gradient_weights:  [array([[ 0.00024823, -0.0021491 ,  0.01685044],\n",
      "       [-0.00035121, -0.00090207,  0.00771941]]), array([[ 0.00263936],\n",
      "       [-0.00158686],\n",
      "       [ 0.01479067]])]\n",
      "gradients_biases:  [array([ 8.59130020e-05, -1.66644829e-03,  2.19349301e-03]), array([-0.00269366])]\n",
      "Iteration 278, Cost: 0.2408168973628334\n",
      "gradient_weights:  [array([[ 0.00024737, -0.00214789,  0.01686286],\n",
      "       [-0.00034949, -0.00090037,  0.00773284]]), array([[ 0.00264305],\n",
      "       [-0.00159634],\n",
      "       [ 0.01481363]])]\n",
      "gradients_biases:  [array([ 8.53970003e-05, -1.66750383e-03,  2.18196433e-03]), array([-0.00269551])]\n",
      "Iteration 279, Cost: 0.2407872688135877\n",
      "gradient_weights:  [array([[ 0.00024651, -0.00214667,  0.01687524],\n",
      "       [-0.00034776, -0.00089866,  0.00774628]]), array([[ 0.00264675],\n",
      "       [-0.00160582],\n",
      "       [ 0.01483657]])]\n",
      "gradients_biases:  [array([ 8.48817158e-05, -1.66856499e-03,  2.17038647e-03]), array([-0.00269736])]\n",
      "Iteration 280, Cost: 0.2407575748364599\n",
      "gradient_weights:  [array([[ 0.00024565, -0.00214546,  0.0168876 ],\n",
      "       [-0.00034604, -0.00089695,  0.00775972]]), array([[ 0.00265045],\n",
      "       [-0.00161531],\n",
      "       [ 0.0148595 ]])]\n",
      "gradients_biases:  [array([ 8.43671509e-05, -1.66963177e-03,  2.15875948e-03]), array([-0.00269922])]\n",
      "Iteration 281, Cost: 0.2407278153859274\n",
      "gradient_weights:  [array([[ 0.00024478, -0.00214425,  0.01689994],\n",
      "       [-0.00034432, -0.00089524,  0.00777316]]), array([[ 0.00265417],\n",
      "       [-0.00162481],\n",
      "       [ 0.01488242]])]\n",
      "gradients_biases:  [array([ 8.38533085e-05, -1.67070419e-03,  2.14708343e-03]), array([-0.00270108])]\n",
      "Iteration 282, Cost: 0.2406979904168073\n",
      "gradient_weights:  [array([[ 0.00024391, -0.00214303,  0.01691225],\n",
      "       [-0.00034259, -0.00089354,  0.0077866 ]]), array([[ 0.00265789],\n",
      "       [-0.00163431],\n",
      "       [ 0.01490532]])]\n",
      "gradients_biases:  [array([ 8.33401910e-05, -1.67178225e-03,  2.13535838e-03]), array([-0.00270295])]\n",
      "Iteration 283, Cost: 0.24066809988425503\n",
      "gradient_weights:  [array([[ 0.00024304, -0.00214183,  0.01692454],\n",
      "       [-0.00034087, -0.00089182,  0.00780006]]), array([[ 0.00266162],\n",
      "       [-0.00164382],\n",
      "       [ 0.0149282 ]])]\n",
      "gradients_biases:  [array([ 8.28278013e-05, -1.67286595e-03,  2.12358440e-03]), array([-0.00270482])]\n",
      "Iteration 284, Cost: 0.2406381437437639\n",
      "gradient_weights:  [array([[ 0.00024217, -0.00214062,  0.0169368 ],\n",
      "       [-0.00033915, -0.00089011,  0.00781351]]), array([[ 0.00266536],\n",
      "       [-0.00165334],\n",
      "       [ 0.01495107]])]\n",
      "gradients_biases:  [array([ 8.23161418e-05, -1.67395532e-03,  2.11176156e-03]), array([-0.0027067])]\n",
      "Iteration 285, Cost: 0.2406081219511648\n",
      "gradient_weights:  [array([[ 0.00024129, -0.00213941,  0.01694904],\n",
      "       [-0.00033742, -0.0008884 ,  0.00782697]]), array([[ 0.0026691 ],\n",
      "       [-0.00166287],\n",
      "       [ 0.01497392]])]\n",
      "gradients_biases:  [array([ 8.18052155e-05, -1.67505034e-03,  2.09988992e-03]), array([-0.00270858])]\n",
      "Iteration 286, Cost: 0.24057803446262513\n",
      "gradient_weights:  [array([[ 0.00024041, -0.00213821,  0.01696126],\n",
      "       [-0.0003357 , -0.00088668,  0.00784044]]), array([[ 0.00267286],\n",
      "       [-0.00167241],\n",
      "       [ 0.01499675]])]\n",
      "gradients_biases:  [array([ 8.12950249e-05, -1.67615104e-03,  2.08796955e-03]), array([-0.00271047])]\n",
      "Iteration 287, Cost: 0.24054788123464818\n",
      "gradient_weights:  [array([[ 0.00023953, -0.002137  ,  0.01697344],\n",
      "       [-0.00033397, -0.00088497,  0.0078539 ]]), array([[ 0.00267662],\n",
      "       [-0.00168195],\n",
      "       [ 0.01501957]])]\n",
      "gradients_biases:  [array([ 8.07855728e-05, -1.67725741e-03,  2.07600052e-03]), array([-0.00271236])]\n",
      "Iteration 288, Cost: 0.2405176622240725\n",
      "gradient_weights:  [array([[ 0.00023865, -0.0021358 ,  0.01698561],\n",
      "       [-0.00033225, -0.00088325,  0.00786738]]), array([[ 0.00268039],\n",
      "       [-0.0016915 ],\n",
      "       [ 0.01504237]])]\n",
      "gradients_biases:  [array([ 8.02768619e-05, -1.67836948e-03,  2.06398291e-03]), array([-0.00271426])]\n",
      "Iteration 289, Cost: 0.24048737738807135\n",
      "gradient_weights:  [array([[ 0.00023776, -0.0021346 ,  0.01699775],\n",
      "       [-0.00033052, -0.00088153,  0.00788085]]), array([[ 0.00268417],\n",
      "       [-0.00170106],\n",
      "       [ 0.01506515]])]\n",
      "gradients_biases:  [array([ 7.97688951e-05, -1.67948723e-03,  2.05191678e-03]), array([-0.00271616])]\n",
      "Iteration 290, Cost: 0.24045702668415148\n",
      "gradient_weights:  [array([[ 0.00023687, -0.0021334 ,  0.01700986],\n",
      "       [-0.00032879, -0.00087981,  0.00789433]]), array([[ 0.00268795],\n",
      "       [-0.00171062],\n",
      "       [ 0.01508792]])]\n",
      "gradients_biases:  [array([ 7.92616750e-05, -1.68061069e-03,  2.03980222e-03]), array([-0.00271807])]\n",
      "Iteration 291, Cost: 0.24042661007015326\n",
      "gradient_weights:  [array([[ 0.00023598, -0.00213221,  0.01702195],\n",
      "       [-0.00032707, -0.00087809,  0.00790782]]), array([[ 0.00269175],\n",
      "       [-0.00172019],\n",
      "       [ 0.01511067]])]\n",
      "gradients_biases:  [array([ 7.87552045e-05, -1.68173986e-03,  2.02763930e-03]), array([-0.00271998])]\n",
      "Iteration 292, Cost: 0.24039612750424913\n",
      "gradient_weights:  [array([[ 0.00023508, -0.00213101,  0.01703401],\n",
      "       [-0.00032534, -0.00087637,  0.0079213 ]]), array([[ 0.00269555],\n",
      "       [-0.00172977],\n",
      "       [ 0.0151334 ]])]\n",
      "gradients_biases:  [array([ 7.82494863e-05, -1.68287475e-03,  2.01542810e-03]), array([-0.00272189])]\n",
      "Iteration 293, Cost: 0.24036557894494345\n",
      "gradient_weights:  [array([[ 0.00023418, -0.00212982,  0.01704605],\n",
      "       [-0.00032361, -0.00087465,  0.00793479]]), array([[ 0.00269936],\n",
      "       [-0.00173936],\n",
      "       [ 0.01515612]])]\n",
      "gradients_biases:  [array([ 7.77445234e-05, -1.68401536e-03,  2.00316869e-03]), array([-0.00272381])]\n",
      "Iteration 294, Cost: 0.2403349643510716\n",
      "gradient_weights:  [array([[ 0.00023328, -0.00212862,  0.01705806],\n",
      "       [-0.00032189, -0.00087292,  0.00794829]]), array([[ 0.00270317],\n",
      "       [-0.00174896],\n",
      "       [ 0.01517882]])]\n",
      "gradients_biases:  [array([ 7.72403185e-05, -1.68516171e-03,  1.99086116e-03]), array([-0.00272574])]\n",
      "Iteration 295, Cost: 0.24030428368179918\n",
      "gradient_weights:  [array([[ 0.00023238, -0.00212743,  0.01707004],\n",
      "       [-0.00032016, -0.0008712 ,  0.00796179]]), array([[ 0.002707  ],\n",
      "       [-0.00175856],\n",
      "       [ 0.0152015 ]])]\n",
      "gradients_biases:  [array([ 7.67368745e-05, -1.68631380e-03,  1.97850559e-03]), array([-0.00272766])]\n",
      "Iteration 296, Cost: 0.24027353689662131\n",
      "gradient_weights:  [array([[ 0.00023148, -0.00212624,  0.017082  ],\n",
      "       [-0.00031843, -0.00086947,  0.00797529]]), array([[ 0.00271083],\n",
      "       [-0.00176817],\n",
      "       [ 0.01522416]])]\n",
      "gradients_biases:  [array([ 7.62341943e-05, -1.68747164e-03,  1.96610206e-03]), array([-0.0027296])]\n",
      "Iteration 297, Cost: 0.24024272395536217\n",
      "gradient_weights:  [array([[ 0.00023057, -0.00212505,  0.01709394],\n",
      "       [-0.0003167 , -0.00086774,  0.00798879]]), array([[ 0.00271467],\n",
      "       [-0.00177779],\n",
      "       [ 0.01524681]])]\n",
      "gradients_biases:  [array([ 7.57322808e-05, -1.68863523e-03,  1.95365065e-03]), array([-0.00273154])]\n",
      "Iteration 298, Cost: 0.24021184481817393\n",
      "gradient_weights:  [array([[ 0.00022966, -0.00212386,  0.01710585],\n",
      "       [-0.00031497, -0.00086601,  0.0080023 ]]), array([[ 0.00271852],\n",
      "       [-0.00178742],\n",
      "       [ 0.01526943]])]\n",
      "gradients_biases:  [array([ 7.52311368e-05, -1.68980459e-03,  1.94115146e-03]), array([-0.00273348])]\n",
      "Iteration 299, Cost: 0.24018089944553617\n",
      "gradient_weights:  [array([[ 0.00022874, -0.00212268,  0.01711773],\n",
      "       [-0.00031325, -0.00086428,  0.00801581]]), array([[ 0.00272238],\n",
      "       [-0.00179706],\n",
      "       [ 0.01529204]])]\n",
      "gradients_biases:  [array([ 7.47307653e-05, -1.69097972e-03,  1.92860456e-03]), array([-0.00273543])]\n",
      "Iteration 300, Cost: 0.24014988779825502\n",
      "gradient_weights:  [array([[ 0.00022783, -0.00212149,  0.01712959],\n",
      "       [-0.00031152, -0.00086254,  0.00802933]]), array([[ 0.00272624],\n",
      "       [-0.0018067 ],\n",
      "       [ 0.01531463]])]\n",
      "gradients_biases:  [array([ 7.42311693e-05, -1.69216064e-03,  1.91601005e-03]), array([-0.00273738])]\n",
      "Iteration 301, Cost: 0.2401188098374627\n",
      "gradient_weights:  [array([[ 0.00022691, -0.00212031,  0.01714142],\n",
      "       [-0.00030979, -0.00086081,  0.00804285]]), array([[ 0.00273012],\n",
      "       [-0.00181635],\n",
      "       [ 0.0153372 ]])]\n",
      "gradients_biases:  [array([ 7.37323516e-05, -1.69334734e-03,  1.90336802e-03]), array([-0.00273934])]\n",
      "Iteration 302, Cost: 0.24008766552461644\n",
      "gradient_weights:  [array([[ 0.00022598, -0.00211912,  0.01715323],\n",
      "       [-0.00030806, -0.00085907,  0.00805637]]), array([[ 0.002734  ],\n",
      "       [-0.00182601],\n",
      "       [ 0.01535976]])]\n",
      "gradients_biases:  [array([ 7.32343154e-05, -1.69453984e-03,  1.89067856e-03]), array([-0.00274131])]\n",
      "Iteration 303, Cost: 0.24005645482149804\n",
      "gradient_weights:  [array([[ 0.00022506, -0.00211794,  0.01716501],\n",
      "       [-0.00030633, -0.00085733,  0.00806989]]), array([[ 0.00273789],\n",
      "       [-0.00183567],\n",
      "       [ 0.01538229]])]\n",
      "gradients_biases:  [array([ 7.27370635e-05, -1.69573815e-03,  1.87794176e-03]), array([-0.00274327])]\n",
      "Iteration 304, Cost: 0.24002517769021284\n",
      "gradient_weights:  [array([[ 0.00022413, -0.00211676,  0.01717677],\n",
      "       [-0.0003046 , -0.00085559,  0.00808342]]), array([[ 0.00274178],\n",
      "       [-0.00184535],\n",
      "       [ 0.01540481]])]\n",
      "gradients_biases:  [array([ 7.22405990e-05, -1.69694226e-03,  1.86515771e-03]), array([-0.00274525])]\n",
      "Iteration 305, Cost: 0.2399938340931892\n",
      "gradient_weights:  [array([[ 0.0002232 , -0.00211558,  0.0171885 ],\n",
      "       [-0.00030287, -0.00085385,  0.00809695]]), array([[ 0.00274569],\n",
      "       [-0.00185503],\n",
      "       [ 0.0154273 ]])]\n",
      "gradients_biases:  [array([ 7.17449248e-05, -1.69815220e-03,  1.85232651e-03]), array([-0.00274722])]\n",
      "Iteration 306, Cost: 0.23996242399317777\n",
      "gradient_weights:  [array([[ 0.00022227, -0.0021144 ,  0.0172002 ],\n",
      "       [-0.00030114, -0.00085211,  0.00811048]]), array([[ 0.0027496 ],\n",
      "       [-0.00186472],\n",
      "       [ 0.01544978]])]\n",
      "gradients_biases:  [array([ 7.12500441e-05, -1.69936797e-03,  1.83944826e-03]), array([-0.00274921])]\n",
      "Iteration 307, Cost: 0.23993094735325043\n",
      "gradient_weights:  [array([[ 0.00022133, -0.00211322,  0.01721188],\n",
      "       [-0.0002994 , -0.00085036,  0.00812402]]), array([[ 0.00275352],\n",
      "       [-0.00187442],\n",
      "       [ 0.01547224]])]\n",
      "gradients_biases:  [array([ 7.07559599e-05, -1.70058958e-03,  1.82652305e-03]), array([-0.0027512])]\n",
      "Iteration 308, Cost: 0.23989940413679983\n",
      "gradient_weights:  [array([[ 0.00022039, -0.00211205,  0.01722353],\n",
      "       [-0.00029767, -0.00084861,  0.00813755]]), array([[ 0.00275745],\n",
      "       [-0.00188413],\n",
      "       [ 0.01549468]])]\n",
      "gradients_biases:  [array([ 7.02626752e-05, -1.70181703e-03,  1.81355099e-03]), array([-0.00275319])]\n",
      "Iteration 309, Cost: 0.23986779430753863\n",
      "gradient_weights:  [array([[ 0.00021945, -0.00211087,  0.01723515],\n",
      "       [-0.00029594, -0.00084686,  0.0081511 ]]), array([[ 0.00276138],\n",
      "       [-0.00189384],\n",
      "       [ 0.01551709]])]\n",
      "gradients_biases:  [array([ 6.97701932e-05, -1.70305034e-03,  1.80053217e-03]), array([-0.00275519])]\n",
      "Iteration 310, Cost: 0.23983611782949874\n",
      "gradient_weights:  [array([[ 0.0002185 , -0.0021097 ,  0.01724675],\n",
      "       [-0.00029421, -0.00084511,  0.00816464]]), array([[ 0.00276533],\n",
      "       [-0.00190356],\n",
      "       [ 0.01553949]])]\n",
      "gradients_biases:  [array([ 6.9278517e-05, -1.7042895e-03,  1.7874667e-03]), array([-0.00275719])]\n",
      "Iteration 311, Cost: 0.23980437466703025\n",
      "gradient_weights:  [array([[ 0.00021755, -0.00210852,  0.01725832],\n",
      "       [-0.00029248, -0.00084336,  0.00817818]]), array([[ 0.00276928],\n",
      "       [-0.00191329],\n",
      "       [ 0.01556187]])]\n",
      "gradients_biases:  [array([ 6.87876496e-05, -1.70553454e-03,  1.77435467e-03]), array([-0.00275919])]\n",
      "Iteration 312, Cost: 0.23977256478480113\n",
      "gradient_weights:  [array([[ 0.0002166 , -0.00210735,  0.01726987],\n",
      "       [-0.00029075, -0.00084161,  0.00819173]]), array([[ 0.00277324],\n",
      "       [-0.00192303],\n",
      "       [ 0.01558423]])]\n",
      "gradients_biases:  [array([ 6.82975943e-05, -1.70678546e-03,  1.76119620e-03]), array([-0.00276121])]\n",
      "Iteration 313, Cost: 0.2397406881477962\n",
      "gradient_weights:  [array([[ 0.00021565, -0.00210618,  0.01728139],\n",
      "       [-0.00028901, -0.00083985,  0.00820528]]), array([[ 0.0027772 ],\n",
      "       [-0.00193278],\n",
      "       [ 0.01560657]])]\n",
      "gradients_biases:  [array([ 6.78083541e-05, -1.70804226e-03,  1.74799140e-03]), array([-0.00276322])]\n",
      "Iteration 314, Cost: 0.23970874472131656\n",
      "gradient_weights:  [array([[ 0.00021469, -0.002105  ,  0.01729288],\n",
      "       [-0.00028728, -0.00083809,  0.00821884]]), array([[ 0.00278118],\n",
      "       [-0.00194253],\n",
      "       [ 0.01562889]])]\n",
      "gradients_biases:  [array([ 6.73199323e-05, -1.70930496e-03,  1.73474036e-03]), array([-0.00276525])]\n",
      "Iteration 315, Cost: 0.23967673447097865\n",
      "gradient_weights:  [array([[ 0.00021373, -0.00210383,  0.01730435],\n",
      "       [-0.00028555, -0.00083633,  0.00823239]]), array([[ 0.00278516],\n",
      "       [-0.0019523 ],\n",
      "       [ 0.01565119]])]\n",
      "gradients_biases:  [array([ 6.68323320e-05, -1.71057355e-03,  1.72144319e-03]), array([-0.00276727])]\n",
      "Iteration 316, Cost: 0.23964465736271368\n",
      "gradient_weights:  [array([[ 0.00021277, -0.00210266,  0.01731579],\n",
      "       [-0.00028382, -0.00083457,  0.00824595]]), array([[ 0.00278915],\n",
      "       [-0.00196207],\n",
      "       [ 0.01567347]])]\n",
      "gradients_biases:  [array([ 6.63455565e-05, -1.71184806e-03,  1.70810001e-03]), array([-0.00276931])]\n",
      "Iteration 317, Cost: 0.23961251336276662\n",
      "gradient_weights:  [array([[ 0.0002118 , -0.00210149,  0.01732721],\n",
      "       [-0.00028208, -0.0008328 ,  0.00825951]]), array([[ 0.00279315],\n",
      "       [-0.00197185],\n",
      "       [ 0.01569572]])]\n",
      "gradients_biases:  [array([ 6.58596090e-05, -1.71312849e-03,  1.69471093e-03]), array([-0.00277134])]\n",
      "Iteration 318, Cost: 0.23958030243769574\n",
      "gradient_weights:  [array([[ 0.00021084, -0.00210033,  0.01733859],\n",
      "       [-0.00028035, -0.00083104,  0.00827307]]), array([[ 0.00279716],\n",
      "       [-0.00198163],\n",
      "       [ 0.01571796]])]\n",
      "gradients_biases:  [array([ 6.53744926e-05, -1.71441485e-03,  1.68127606e-03]), array([-0.00277338])]\n",
      "Iteration 319, Cost: 0.23954802455437169\n",
      "gradient_weights:  [array([[ 0.00020986, -0.00209916,  0.01734995],\n",
      "       [-0.00027862, -0.00082927,  0.00828663]]), array([[ 0.00280117],\n",
      "       [-0.00199143],\n",
      "       [ 0.01574017]])]\n",
      "gradients_biases:  [array([ 6.48902108e-05, -1.71570715e-03,  1.66779551e-03]), array([-0.00277543])]\n",
      "Iteration 320, Cost: 0.239515679679977\n",
      "gradient_weights:  [array([[ 0.00020889, -0.00209799,  0.01736129],\n",
      "       [-0.00027688, -0.0008275 ,  0.0083002 ]]), array([[ 0.0028052 ],\n",
      "       [-0.00200123],\n",
      "       [ 0.01576237]])]\n",
      "gradients_biases:  [array([ 6.44067666e-05, -1.71700538e-03,  1.65426941e-03]), array([-0.00277748])]\n",
      "Iteration 321, Cost: 0.23948326778200466\n",
      "gradient_weights:  [array([[ 0.00020791, -0.00209682,  0.0173726 ],\n",
      "       [-0.00027515, -0.00082573,  0.00831376]]), array([[ 0.00280923],\n",
      "       [-0.00201105],\n",
      "       [ 0.01578454]])]\n",
      "gradients_biases:  [array([ 6.39241636e-05, -1.71830957e-03,  1.64069786e-03]), array([-0.00277954])]\n",
      "Iteration 322, Cost: 0.2394507888282584\n",
      "gradient_weights:  [array([[ 0.00020693, -0.00209566,  0.01738388],\n",
      "       [-0.00027342, -0.00082395,  0.00832733]]), array([[ 0.00281326],\n",
      "       [-0.00202087],\n",
      "       [ 0.01580669]])]\n",
      "gradients_biases:  [array([ 6.34424048e-05, -1.71961973e-03,  1.62708098e-03]), array([-0.0027816])]\n",
      "Iteration 323, Cost: 0.2394182427868512\n",
      "gradient_weights:  [array([[ 0.00020595, -0.00209449,  0.01739513],\n",
      "       [-0.00027168, -0.00082217,  0.0083409 ]]), array([[ 0.00281731],\n",
      "       [-0.0020307 ],\n",
      "       [ 0.01582882]])]\n",
      "gradients_biases:  [array([ 6.29614937e-05, -1.72093585e-03,  1.61341890e-03]), array([-0.00278367])]\n",
      "Iteration 324, Cost: 0.23938562962620458\n",
      "gradient_weights:  [array([[ 0.00020496, -0.00209333,  0.01740636],\n",
      "       [-0.00026995, -0.0008204 ,  0.00835447]]), array([[ 0.00282136],\n",
      "       [-0.00204053],\n",
      "       [ 0.01585093]])]\n",
      "gradients_biases:  [array([ 6.24814337e-05, -1.72225795e-03,  1.59971173e-03]), array([-0.00278574])]\n",
      "Iteration 325, Cost: 0.2393529493150483\n",
      "gradient_weights:  [array([[ 0.00020397, -0.00209217,  0.01741756],\n",
      "       [-0.00026821, -0.00081861,  0.00836805]]), array([[ 0.00282542],\n",
      "       [-0.00205038],\n",
      "       [ 0.01587302]])]\n",
      "gradients_biases:  [array([ 6.20022280e-05, -1.72358603e-03,  1.58595960e-03]), array([-0.00278782])]\n",
      "Iteration 326, Cost: 0.2393202018224192\n",
      "gradient_weights:  [array([[ 0.00020297, -0.002091  ,  0.01742874],\n",
      "       [-0.00026648, -0.00081683,  0.00838162]]), array([[ 0.00282949],\n",
      "       [-0.00206023],\n",
      "       [ 0.01589508]])]\n",
      "gradients_biases:  [array([ 6.15238800e-05, -1.72492011e-03,  1.57216262e-03]), array([-0.0027899])]\n",
      "Iteration 327, Cost: 0.23928738711766084\n",
      "gradient_weights:  [array([[ 0.00020198, -0.00208984,  0.01743988],\n",
      "       [-0.00026475, -0.00081504,  0.0083952 ]]), array([[ 0.00283357],\n",
      "       [-0.0020701 ],\n",
      "       [ 0.01591712]])]\n",
      "gradients_biases:  [array([ 6.10463932e-05, -1.72626020e-03,  1.55832093e-03]), array([-0.00279199])]\n",
      "Iteration 328, Cost: 0.2392545051704223\n",
      "gradient_weights:  [array([[ 0.00020098, -0.00208868,  0.01745101],\n",
      "       [-0.00026301, -0.00081326,  0.00840877]]), array([[ 0.00283766],\n",
      "       [-0.00207997],\n",
      "       [ 0.01593914]])]\n",
      "gradients_biases:  [array([ 6.05697709e-05, -1.72760629e-03,  1.54443465e-03]), array([-0.00279408])]\n",
      "Iteration 329, Cost: 0.23922155595065786\n",
      "gradient_weights:  [array([[ 0.00019997, -0.00208751,  0.0174621 ],\n",
      "       [-0.00026128, -0.00081147,  0.00842235]]), array([[ 0.00284175],\n",
      "       [-0.00208984],\n",
      "       [ 0.01596114]])]\n",
      "gradients_biases:  [array([ 6.00940166e-05, -1.72895841e-03,  1.53050390e-03]), array([-0.00279618])]\n",
      "Iteration 330, Cost: 0.23918853942862625\n",
      "gradient_weights:  [array([[ 0.00019897, -0.00208635,  0.01747317],\n",
      "       [-0.00025954, -0.00080967,  0.00843593]]), array([[ 0.00284585],\n",
      "       [-0.00209973],\n",
      "       [ 0.01598312]])]\n",
      "gradients_biases:  [array([ 5.96191337e-05, -1.73031655e-03,  1.51652882e-03]), array([-0.00279828])]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 331, Cost: 0.23915545557488976\n",
      "gradient_weights:  [array([[ 0.00019796, -0.00208519,  0.01748421],\n",
      "       [-0.00025781, -0.00080788,  0.00844951]]), array([[ 0.00284996],\n",
      "       [-0.00210963],\n",
      "       [ 0.01600507]])]\n",
      "gradients_biases:  [array([ 5.91451256e-05, -1.73168073e-03,  1.50250954e-03]), array([-0.00280039])]\n",
      "Iteration 332, Cost: 0.23912230436031356\n",
      "gradient_weights:  [array([[ 0.00019695, -0.00208403,  0.01749522],\n",
      "       [-0.00025607, -0.00080608,  0.0084631 ]]), array([[ 0.00285407],\n",
      "       [-0.00211953],\n",
      "       [ 0.016027  ]])]\n",
      "gradients_biases:  [array([ 5.86719959e-05, -1.73305096e-03,  1.48844618e-03]), array([-0.0028025])]\n",
      "Iteration 333, Cost: 0.23908908575606508\n",
      "gradient_weights:  [array([[ 0.00019593, -0.00208287,  0.01750621],\n",
      "       [-0.00025434, -0.00080428,  0.00847668]]), array([[ 0.0028582 ],\n",
      "       [-0.00212945],\n",
      "       [ 0.01604891]])]\n",
      "gradients_biases:  [array([ 5.81997481e-05, -1.73442724e-03,  1.47433888e-03]), array([-0.00280462])]\n",
      "Iteration 334, Cost: 0.23905579973361324\n",
      "gradient_weights:  [array([[ 0.00019491, -0.00208171,  0.01751717],\n",
      "       [-0.0002526 , -0.00080248,  0.00849026]]), array([[ 0.00286233],\n",
      "       [-0.00213937],\n",
      "       [ 0.01607079]])]\n",
      "gradients_biases:  [array([ 5.77283855e-05, -1.73580959e-03,  1.46018777e-03]), array([-0.00280674])]\n",
      "Iteration 335, Cost: 0.23902244626472774\n",
      "gradient_weights:  [array([[ 0.00019389, -0.00208055,  0.0175281 ],\n",
      "       [-0.00025087, -0.00080067,  0.00850385]]), array([[ 0.00286647],\n",
      "       [-0.0021493 ],\n",
      "       [ 0.01609266]])]\n",
      "gradients_biases:  [array([ 5.72579119e-05, -1.73719801e-03,  1.44599298e-03]), array([-0.00280887])]\n",
      "Iteration 336, Cost: 0.23898902532147853\n",
      "gradient_weights:  [array([[ 0.00019286, -0.00207939,  0.017539  ],\n",
      "       [-0.00024913, -0.00079887,  0.00851743]]), array([[ 0.00287062],\n",
      "       [-0.00215924],\n",
      "       [ 0.01611449]])]\n",
      "gradients_biases:  [array([ 5.67883306e-05, -1.73859250e-03,  1.43175466e-03]), array([-0.002811])]\n",
      "Iteration 337, Cost: 0.2389555368762349\n",
      "gradient_weights:  [array([[ 0.00019184, -0.00207823,  0.01754988],\n",
      "       [-0.0002474 , -0.00079706,  0.00853102]]), array([[ 0.00287477],\n",
      "       [-0.00216918],\n",
      "       [ 0.01613631]])]\n",
      "gradients_biases:  [array([ 5.63196453e-05, -1.73999309e-03,  1.41747293e-03]), array([-0.00281314])]\n",
      "Iteration 338, Cost: 0.23892198090166483\n",
      "gradient_weights:  [array([[ 0.0001908 , -0.00207707,  0.01756073],\n",
      "       [-0.00024566, -0.00079524,  0.00854461]]), array([[ 0.00287894],\n",
      "       [-0.00217914],\n",
      "       [ 0.0161581 ]])]\n",
      "gradients_biases:  [array([ 5.58518596e-05, -1.74139977e-03,  1.40314795e-03]), array([-0.00281528])]\n",
      "Iteration 339, Cost: 0.2388883573707346\n",
      "gradient_weights:  [array([[ 0.00018977, -0.00207591,  0.01757155],\n",
      "       [-0.00024393, -0.00079343,  0.0085582 ]]), array([[ 0.00288311],\n",
      "       [-0.0021891 ],\n",
      "       [ 0.01617987]])]\n",
      "gradients_biases:  [array([ 5.53849770e-05, -1.74281256e-03,  1.38877984e-03]), array([-0.00281743])]\n",
      "Iteration 340, Cost: 0.23885466625670762\n",
      "gradient_weights:  [array([[ 0.00018873, -0.00207476,  0.01758235],\n",
      "       [-0.00024219, -0.00079161,  0.00857178]]), array([[ 0.00288729],\n",
      "       [-0.00219908],\n",
      "       [ 0.01620161]])]\n",
      "gradients_biases:  [array([ 5.49190012e-05, -1.74423147e-03,  1.37436876e-03]), array([-0.00281958])]\n",
      "Iteration 341, Cost: 0.23882090753314425\n",
      "gradient_weights:  [array([[ 0.00018769, -0.0020736 ,  0.01759312],\n",
      "       [-0.00024046, -0.00078979,  0.00858537]]), array([[ 0.00289147],\n",
      "       [-0.00220906],\n",
      "       [ 0.01622333]])]\n",
      "gradients_biases:  [array([ 5.44539357e-05, -1.74565649e-03,  1.35991483e-03]), array([-0.00282174])]\n",
      "Iteration 342, Cost: 0.23878708117390068\n",
      "gradient_weights:  [array([[ 0.00018664, -0.00207244,  0.01760386],\n",
      "       [-0.00023872, -0.00078797,  0.00859896]]), array([[ 0.00289567],\n",
      "       [-0.00221905],\n",
      "       [ 0.01624503]])]\n",
      "gradients_biases:  [array([ 5.39897842e-05, -1.74708766e-03,  1.34541821e-03]), array([-0.0028239])]\n",
      "Iteration 343, Cost: 0.23875318715312882\n",
      "gradient_weights:  [array([[ 0.00018559, -0.00207128,  0.01761458],\n",
      "       [-0.00023699, -0.00078614,  0.00861255]]), array([[ 0.00289987],\n",
      "       [-0.00222905],\n",
      "       [ 0.0162667 ]])]\n",
      "gradients_biases:  [array([ 5.35265504e-05, -1.74852496e-03,  1.33087905e-03]), array([-0.00282607])]\n",
      "Iteration 344, Cost: 0.2387192254452751\n",
      "gradient_weights:  [array([[ 0.00018454, -0.00207012,  0.01762526],\n",
      "       [-0.00023525, -0.00078431,  0.00862614]]), array([[ 0.00290408],\n",
      "       [-0.00223906],\n",
      "       [ 0.01628835]])]\n",
      "gradients_biases:  [array([ 5.30642380e-05, -1.74996841e-03,  1.31629747e-03]), array([-0.00282825])]\n",
      "Iteration 345, Cost: 0.23868519602508026\n",
      "gradient_weights:  [array([[ 0.00018349, -0.00206896,  0.01763592],\n",
      "       [-0.00023352, -0.00078248,  0.00863973]]), array([[ 0.0029083 ],\n",
      "       [-0.00224907],\n",
      "       [ 0.01630998]])]\n",
      "gradients_biases:  [array([ 5.26028506e-05, -1.75141802e-03,  1.30167365e-03]), array([-0.00283043])]\n",
      "Iteration 346, Cost: 0.23865109886757835\n",
      "gradient_weights:  [array([[ 0.00018243, -0.00206781,  0.01764656],\n",
      "       [-0.00023178, -0.00078065,  0.00865332]]), array([[ 0.00291253],\n",
      "       [-0.0022591 ],\n",
      "       [ 0.01633158]])]\n",
      "gradients_biases:  [array([ 5.21423920e-05, -1.75287380e-03,  1.28700771e-03]), array([-0.00283261])]\n",
      "Iteration 347, Cost: 0.23861693394809647\n",
      "gradient_weights:  [array([[ 0.00018137, -0.00206665,  0.01765716],\n",
      "       [-0.00023005, -0.00077881,  0.00866691]]), array([[ 0.00291676],\n",
      "       [-0.00226913],\n",
      "       [ 0.01635315]])]\n",
      "gradients_biases:  [array([ 5.16828659e-05, -1.75433575e-03,  1.27229982e-03]), array([-0.0028348])]\n",
      "Iteration 348, Cost: 0.238582701242254\n",
      "gradient_weights:  [array([[ 0.0001803 , -0.00206549,  0.01766774],\n",
      "       [-0.00022831, -0.00077697,  0.0086805 ]]), array([[ 0.002921  ],\n",
      "       [-0.00227917],\n",
      "       [ 0.0163747 ]])]\n",
      "gradients_biases:  [array([ 5.12242760e-05, -1.75580389e-03,  1.25755013e-03]), array([-0.00283699])]\n",
      "Iteration 349, Cost: 0.23854840072596173\n",
      "gradient_weights:  [array([[ 0.00017923, -0.00206433,  0.01767829],\n",
      "       [-0.00022658, -0.00077513,  0.00869409]]), array([[ 0.00292525],\n",
      "       [-0.00228923],\n",
      "       [ 0.01639623]])]\n",
      "gradients_biases:  [array([ 5.07666260e-05, -1.75727822e-03,  1.24275878e-03]), array([-0.00283919])]\n",
      "Iteration 350, Cost: 0.23851403237542163\n",
      "gradient_weights:  [array([[ 0.00017816, -0.00206317,  0.01768881],\n",
      "       [-0.00022484, -0.00077328,  0.00870768]]), array([[ 0.00292951],\n",
      "       [-0.00229929],\n",
      "       [ 0.01641773]])]\n",
      "gradients_biases:  [array([ 5.03099199e-05, -1.75875875e-03,  1.22792593e-03]), array([-0.0028414])]\n",
      "Iteration 351, Cost: 0.23847959616712616\n",
      "gradient_weights:  [array([[ 0.00017708, -0.00206201,  0.01769931],\n",
      "       [-0.00022311, -0.00077143,  0.00872127]]), array([[ 0.00293377],\n",
      "       [-0.00230935],\n",
      "       [ 0.0164392 ]])]\n",
      "gradients_biases:  [array([ 4.98541613e-05, -1.76024550e-03,  1.21305174e-03]), array([-0.00284361])]\n",
      "Iteration 352, Cost: 0.23844509207785752\n",
      "gradient_weights:  [array([[ 0.000176  , -0.00206086,  0.01770978],\n",
      "       [-0.00022137, -0.00076958,  0.00873486]]), array([[ 0.00293805],\n",
      "       [-0.00231943],\n",
      "       [ 0.01646065]])]\n",
      "gradients_biases:  [array([ 4.93993540e-05, -1.76173847e-03,  1.19813635e-03]), array([-0.00284582])]\n",
      "Iteration 353, Cost: 0.2384105200846873\n",
      "gradient_weights:  [array([[ 0.00017492, -0.0020597 ,  0.01772022],\n",
      "       [-0.00021964, -0.00076772,  0.00874845]]), array([[ 0.00294233],\n",
      "       [-0.00232952],\n",
      "       [ 0.01648208]])]\n",
      "gradients_biases:  [array([ 4.89455020e-05, -1.76323767e-03,  1.18317994e-03]), array([-0.00284804])]\n",
      "Iteration 354, Cost: 0.23837588016497563\n",
      "gradient_weights:  [array([[ 0.00017383, -0.00205854,  0.01773063],\n",
      "       [-0.0002179 , -0.00076587,  0.00876204]]), array([[ 0.00294662],\n",
      "       [-0.00233962],\n",
      "       [ 0.01650347]])]\n",
      "gradients_biases:  [array([ 4.84926090e-05, -1.76474310e-03,  1.16818266e-03]), array([-0.00285027])]\n",
      "Iteration 355, Cost: 0.23834117229637083\n",
      "gradient_weights:  [array([[ 0.00017274, -0.00205738,  0.01774102],\n",
      "       [-0.00021617, -0.00076401,  0.00877563]]), array([[ 0.00295091],\n",
      "       [-0.00234972],\n",
      "       [ 0.01652485]])]\n",
      "gradients_biases:  [array([ 4.80406789e-05, -1.76625478e-03,  1.15314466e-03]), array([-0.0028525])]\n",
      "Iteration 356, Cost: 0.23830639645680887\n",
      "gradient_weights:  [array([[ 0.00017165, -0.00205622,  0.01775138],\n",
      "       [-0.00021443, -0.00076214,  0.00878922]]), array([[ 0.00295522],\n",
      "       [-0.00235983],\n",
      "       [ 0.0165462 ]])]\n",
      "gradients_biases:  [array([ 4.75897156e-05, -1.76777272e-03,  1.13806611e-03]), array([-0.00285473])]\n",
      "Iteration 357, Cost: 0.2382715526245127\n",
      "gradient_weights:  [array([[ 0.00017055, -0.00205506,  0.01776171],\n",
      "       [-0.0002127 , -0.00076027,  0.00880281]]), array([[ 0.00295953],\n",
      "       [-0.00236996],\n",
      "       [ 0.01656752]])]\n",
      "gradients_biases:  [array([ 4.71397229e-05, -1.76929692e-03,  1.12294717e-03]), array([-0.00285698])]\n",
      "Iteration 358, Cost: 0.23823664077799173\n",
      "gradient_weights:  [array([[ 0.00016945, -0.0020539 ,  0.01777201],\n",
      "       [-0.00021097, -0.0007584 ,  0.0088164 ]]), array([[ 0.00296385],\n",
      "       [-0.00238009],\n",
      "       [ 0.01658881]])]\n",
      "gradients_biases:  [array([ 4.66907049e-05, -1.77082740e-03,  1.10778800e-03]), array([-0.00285922])]\n",
      "Iteration 359, Cost: 0.23820166089604136\n",
      "gradient_weights:  [array([[ 0.00016835, -0.00205274,  0.01778229],\n",
      "       [-0.00020923, -0.00075653,  0.00882998]]), array([[ 0.00296818],\n",
      "       [-0.00239023],\n",
      "       [ 0.01661008]])]\n",
      "gradients_biases:  [array([ 4.62426653e-05, -1.77236416e-03,  1.09258878e-03]), array([-0.00286147])]\n",
      "Iteration 360, Cost: 0.2381666129577425\n",
      "gradient_weights:  [array([[ 0.00016724, -0.00205158,  0.01779254],\n",
      "       [-0.0002075 , -0.00075465,  0.00884357]]), array([[ 0.00297251],\n",
      "       [-0.00240038],\n",
      "       [ 0.01663133]])]\n",
      "gradients_biases:  [array([ 4.57956082e-05, -1.77390721e-03,  1.07734965e-03]), array([-0.00286373])]\n",
      "Iteration 361, Cost: 0.23813149694246089\n",
      "gradient_weights:  [array([[ 0.00016613, -0.00205042,  0.01780276],\n",
      "       [-0.00020577, -0.00075277,  0.00885715]]), array([[ 0.00297686],\n",
      "       [-0.00241054],\n",
      "       [ 0.01665255]])]\n",
      "gradients_biases:  [array([ 4.53495375e-05, -1.77545657e-03,  1.06207080e-03]), array([-0.00286599])]\n",
      "Iteration 362, Cost: 0.23809631282984678\n",
      "gradient_weights:  [array([[ 0.00016501, -0.00204926,  0.01781295],\n",
      "       [-0.00020403, -0.00075089,  0.00887074]]), array([[ 0.00298121],\n",
      "       [-0.0024207 ],\n",
      "       [ 0.01667374]])]\n",
      "gradients_biases:  [array([ 4.49044572e-05, -1.77701223e-03,  1.04675239e-03]), array([-0.00286826])]\n",
      "Iteration 363, Cost: 0.23806106059983434\n",
      "gradient_weights:  [array([[ 0.00016389, -0.00204809,  0.01782312],\n",
      "       [-0.0002023 , -0.000749  ,  0.00888432]]), array([[ 0.00298557],\n",
      "       [-0.00243088],\n",
      "       [ 0.0166949 ]])]\n",
      "gradients_biases:  [array([ 4.44603712e-05, -1.77857421e-03,  1.03139459e-03]), array([-0.00287053])]\n",
      "Iteration 364, Cost: 0.23802574023264117\n",
      "gradient_weights:  [array([[ 0.00016277, -0.00204693,  0.01783325],\n",
      "       [-0.00020057, -0.00074711,  0.0088979 ]]), array([[ 0.00298994],\n",
      "       [-0.00244107],\n",
      "       [ 0.01671604]])]\n",
      "gradients_biases:  [array([ 4.40172837e-05, -1.78014252e-03,  1.01599756e-03]), array([-0.00287281])]\n",
      "Iteration 365, Cost: 0.23799035170876792\n",
      "gradient_weights:  [array([[ 0.00016164, -0.00204577,  0.01784337],\n",
      "       [-0.00019883, -0.00074522,  0.00891149]]), array([[ 0.00299431],\n",
      "       [-0.00245126],\n",
      "       [ 0.01673715]])]\n",
      "gradients_biases:  [array([ 4.35751985e-05, -1.78171717e-03,  1.00056149e-03]), array([-0.00287509])]\n",
      "Iteration 366, Cost: 0.23795489500899786\n",
      "gradient_weights:  [array([[ 0.00016051, -0.0020446 ,  0.01785345],\n",
      "       [-0.0001971 , -0.00074332,  0.00892507]]), array([[ 0.00299869],\n",
      "       [-0.00246147],\n",
      "       [ 0.01675824]])]\n",
      "gradients_biases:  [array([ 4.31341199e-05, -1.78329817e-03,  9.85086536e-04]), array([-0.00287738])]\n",
      "Iteration 367, Cost: 0.2379193701143962\n",
      "gradient_weights:  [array([[ 0.00015938, -0.00204344,  0.0178635 ],\n",
      "       [-0.00019537, -0.00074142,  0.00893864]]), array([[ 0.00300308],\n",
      "       [-0.00247168],\n",
      "       [ 0.01677929]])]\n",
      "gradients_biases:  [array([ 4.26940517e-05, -1.78488552e-03,  9.69572883e-04]), array([-0.00287967])]\n",
      "Iteration 368, Cost: 0.23788377700630986\n",
      "gradient_weights:  [array([[ 0.00015824, -0.00204227,  0.01787353],\n",
      "       [-0.00019363, -0.00073952,  0.00895222]]), array([[ 0.00300748],\n",
      "       [-0.0024819 ],\n",
      "       [ 0.01680032]])]\n",
      "gradients_biases:  [array([ 4.22549982e-05, -1.78647923e-03,  9.54020703e-04]), array([-0.00288197])]\n",
      "Iteration 369, Cost: 0.237848115666367\n",
      "gradient_weights:  [array([[ 0.0001571 , -0.00204111,  0.01788353],\n",
      "       [-0.0001919 , -0.00073761,  0.0089658 ]]), array([[ 0.00301189],\n",
      "       [-0.00249213],\n",
      "       [ 0.01682133]])]\n",
      "gradients_biases:  [array([ 4.18169633e-05, -1.78807932e-03,  9.38430172e-04]), array([-0.00288427])]\n",
      "Iteration 370, Cost: 0.2378123860764766\n",
      "gradient_weights:  [array([[ 0.00015596, -0.00203994,  0.0178935 ],\n",
      "       [-0.00019017, -0.0007357 ,  0.00897937]]), array([[ 0.0030163 ],\n",
      "       [-0.00250237],\n",
      "       [ 0.0168423 ]])]\n",
      "gradients_biases:  [array([ 4.13799512e-05, -1.78968579e-03,  9.22801467e-04]), array([-0.00288658])]\n",
      "Iteration 371, Cost: 0.23777658821882802\n",
      "gradient_weights:  [array([[ 0.00015481, -0.00203878,  0.01790345],\n",
      "       [-0.00018844, -0.00073379,  0.00899295]]), array([[ 0.00302073],\n",
      "       [-0.00251262],\n",
      "       [ 0.01686325]])]\n",
      "gradients_biases:  [array([ 4.09439660e-05, -1.79129866e-03,  9.07134767e-04]), array([-0.00288889])]\n",
      "Iteration 372, Cost: 0.23774072207589073\n",
      "gradient_weights:  [array([[ 0.00015366, -0.00203761,  0.01791337],\n",
      "       [-0.00018671, -0.00073187,  0.00900652]]), array([[ 0.00302516],\n",
      "       [-0.00252288],\n",
      "       [ 0.01688417]])]\n",
      "gradients_biases:  [array([ 4.05090120e-05, -1.79291792e-03,  8.91430251e-04]), array([-0.00289121])]\n",
      "Iteration 373, Cost: 0.23770478763041375\n",
      "gradient_weights:  [array([[ 0.0001525 , -0.00203644,  0.01792326],\n",
      "       [-0.00018498, -0.00072995,  0.00902009]]), array([[ 0.00302959],\n",
      "       [-0.00253315],\n",
      "       [ 0.01690506]])]\n",
      "gradients_biases:  [array([ 4.00750931e-05, -1.79454360e-03,  8.75688101e-04]), array([-0.00289353])]\n",
      "Iteration 374, Cost: 0.23766878486542525\n",
      "gradient_weights:  [array([[ 0.00015134, -0.00203527,  0.01793312],\n",
      "       [-0.00018324, -0.00072802,  0.00903366]]), array([[ 0.00303404],\n",
      "       [-0.00254343],\n",
      "       [ 0.01692593]])]\n",
      "gradients_biases:  [array([ 3.96422136e-05, -1.79617570e-03,  8.59908498e-04]), array([-0.00289586])]\n",
      "Iteration 375, Cost: 0.2376327137642326\n",
      "gradient_weights:  [array([[ 0.00015017, -0.0020341 ,  0.01794295],\n",
      "       [-0.00018151, -0.0007261 ,  0.00904723]]), array([[ 0.00303849],\n",
      "       [-0.00255372],\n",
      "       [ 0.01694677]])]\n",
      "gradients_biases:  [array([ 3.92103777e-05, -1.79781422e-03,  8.44091626e-04]), array([-0.0028982])]\n",
      "Iteration 376, Cost: 0.23759657431042153\n",
      "gradient_weights:  [array([[ 0.00014901, -0.00203293,  0.01795276],\n",
      "       [-0.00017978, -0.00072416,  0.00906079]]), array([[ 0.00304296],\n",
      "       [-0.00256401],\n",
      "       [ 0.01696758]])]\n",
      "gradients_biases:  [array([ 3.87795896e-05, -1.79945918e-03,  8.28237669e-04]), array([-0.00290054])]\n",
      "Iteration 377, Cost: 0.23756036648785586\n",
      "gradient_weights:  [array([[ 0.00014784, -0.00203176,  0.01796254],\n",
      "       [-0.00017805, -0.00072223,  0.00907436]]), array([[ 0.00304742],\n",
      "       [-0.00257432],\n",
      "       [ 0.01698836]])]\n",
      "gradients_biases:  [array([ 3.83498535e-05, -1.80111059e-03,  8.12346811e-04]), array([-0.00290288])]\n",
      "Iteration 378, Cost: 0.23752409028067767\n",
      "gradient_weights:  [array([[ 0.00014666, -0.00203059,  0.01797229],\n",
      "       [-0.00017632, -0.00072029,  0.00908792]]), array([[ 0.0030519 ],\n",
      "       [-0.00258463],\n",
      "       [ 0.01700911]])]\n",
      "gradients_biases:  [array([ 3.79211737e-05, -1.80276846e-03,  7.96419240e-04]), array([-0.00290523])]\n",
      "Iteration 379, Cost: 0.2374877456733065\n",
      "gradient_weights:  [array([[ 0.00014548, -0.00202941,  0.01798201],\n",
      "       [-0.00017459, -0.00071834,  0.00910148]]), array([[ 0.00305639],\n",
      "       [-0.00259495],\n",
      "       [ 0.01702983]])]\n",
      "gradients_biases:  [array([ 3.74935544e-05, -1.80443280e-03,  7.80455143e-04]), array([-0.00290758])]\n",
      "Iteration 380, Cost: 0.23745133265043894\n",
      "gradient_weights:  [array([[ 0.0001443 , -0.00202824,  0.01799171],\n",
      "       [-0.00017286, -0.0007164 ,  0.00911504]]), array([[ 0.00306088],\n",
      "       [-0.00260529],\n",
      "       [ 0.01705053]])]\n",
      "gradients_biases:  [array([ 3.70669999e-05, -1.80610360e-03,  7.64454708e-04]), array([-0.00290994])]\n",
      "Iteration 381, Cost: 0.2374148511970489\n",
      "gradient_weights:  [array([[ 0.00014311, -0.00202707,  0.01800138],\n",
      "       [-0.00017114, -0.00071445,  0.00912859]]), array([[ 0.00306538],\n",
      "       [-0.00261563],\n",
      "       [ 0.0170712 ]])]\n",
      "gradients_biases:  [array([ 3.66415144e-05, -1.80778090e-03,  7.48418125e-04]), array([-0.00291231])]\n",
      "Iteration 382, Cost: 0.23737830129838694\n",
      "gradient_weights:  [array([[ 0.00014192, -0.00202589,  0.01801102],\n",
      "       [-0.00016941, -0.00071249,  0.00914215]]), array([[ 0.00306989],\n",
      "       [-0.00262598],\n",
      "       [ 0.01709184]])]\n",
      "gradients_biases:  [array([ 3.62171023e-05, -1.80946469e-03,  7.32345585e-04]), array([-0.00291468])]\n",
      "Iteration 383, Cost: 0.23734168293997998\n",
      "gradient_weights:  [array([[ 0.00014072, -0.00202471,  0.01802063],\n",
      "       [-0.00016768, -0.00071053,  0.0091557 ]]), array([[ 0.00307441],\n",
      "       [-0.00263634],\n",
      "       [ 0.01711245]])]\n",
      "gradients_biases:  [array([ 3.57937680e-05, -1.81115498e-03,  7.16237280e-04]), array([-0.00291706])]\n",
      "Iteration 384, Cost: 0.23730499610763145\n",
      "gradient_weights:  [array([[ 0.00013952, -0.00202353,  0.01803021],\n",
      "       [-0.00016595, -0.00070857,  0.00916925]]), array([[ 0.00307893],\n",
      "       [-0.00264672],\n",
      "       [ 0.01713303]])]\n",
      "gradients_biases:  [array([ 3.53715157e-05, -1.81285178e-03,  7.00093403e-04]), array([-0.00291944])]\n",
      "Iteration 385, Cost: 0.2372682407874205\n",
      "gradient_weights:  [array([[ 0.00013832, -0.00202235,  0.01803977],\n",
      "       [-0.00016422, -0.0007066 ,  0.0091828 ]]), array([[ 0.00308346],\n",
      "       [-0.0026571 ],\n",
      "       [ 0.01715358]])]\n",
      "gradients_biases:  [array([ 3.49503497e-05, -1.81455511e-03,  6.83914147e-04]), array([-0.00292182])]\n",
      "Iteration 386, Cost: 0.23723141696570227\n",
      "gradient_weights:  [array([[ 0.00013711, -0.00202117,  0.0180493 ],\n",
      "       [-0.0001625 , -0.00070463,  0.00919634]]), array([[ 0.003088  ],\n",
      "       [-0.00266749],\n",
      "       [ 0.0171741 ]])]\n",
      "gradients_biases:  [array([ 3.45302746e-05, -1.81626496e-03,  6.67699708e-04]), array([-0.00292421])]\n",
      "Iteration 387, Cost: 0.23719452462910726\n",
      "gradient_weights:  [array([[ 0.0001359 , -0.00201999,  0.0180588 ],\n",
      "       [-0.00016077, -0.00070265,  0.00920988]]), array([[ 0.00309255],\n",
      "       [-0.00267788],\n",
      "       [ 0.0171946 ]])]\n",
      "gradients_biases:  [array([ 3.41112945e-05, -1.81798136e-03,  6.51450282e-04]), array([-0.00292661])]\n",
      "Iteration 388, Cost: 0.2371575637645416\n",
      "gradient_weights:  [array([[ 0.00013469, -0.00201881,  0.01806828],\n",
      "       [-0.00015904, -0.00070068,  0.00922342]]), array([[ 0.00309711],\n",
      "       [-0.00268829],\n",
      "       [ 0.01721506]])]\n",
      "gradients_biases:  [array([ 3.36934141e-05, -1.81970431e-03,  6.35166065e-04]), array([-0.00292901])]\n",
      "Iteration 389, Cost: 0.23712053435918656\n",
      "gradient_weights:  [array([[ 0.00013347, -0.00201763,  0.01807773],\n",
      "       [-0.00015732, -0.00069869,  0.00923696]]), array([[ 0.00310167],\n",
      "       [-0.00269871],\n",
      "       [ 0.01723549]])]\n",
      "gradients_biases:  [array([ 3.32766376e-05, -1.82143382e-03,  6.18847257e-04]), array([-0.00293142])]\n",
      "Iteration 390, Cost: 0.23708343640049828\n",
      "gradient_weights:  [array([[ 0.00013225, -0.00201644,  0.01808714],\n",
      "       [-0.00015559, -0.0006967 ,  0.0092505 ]]), array([[ 0.00310624],\n",
      "       [-0.00270914],\n",
      "       [ 0.0172559 ]])]\n",
      "gradients_biases:  [array([ 3.28609695e-05, -1.82316989e-03,  6.02494056e-04]), array([-0.00293383])]\n",
      "Iteration 391, Cost: 0.23704626987620797\n",
      "gradient_weights:  [array([[ 0.00013102, -0.00201525,  0.01809654],\n",
      "       [-0.00015387, -0.00069471,  0.00926403]]), array([[ 0.00311082],\n",
      "       [-0.00271958],\n",
      "       [ 0.01727627]])]\n",
      "gradients_biases:  [array([ 3.24464143e-05, -1.82491255e-03,  5.86106663e-04]), array([-0.00293625])]\n",
      "Iteration 392, Cost: 0.23700903477432167\n",
      "gradient_weights:  [array([[ 0.00012979, -0.00201407,  0.0181059 ],\n",
      "       [-0.00015214, -0.00069272,  0.00927756]]), array([[ 0.00311541],\n",
      "       [-0.00273002],\n",
      "       [ 0.01729662]])]\n",
      "gradients_biases:  [array([ 3.20329764e-05, -1.82666179e-03,  5.69685279e-04]), array([-0.00293867])]\n",
      "Iteration 393, Cost: 0.23697173108311984\n",
      "gradient_weights:  [array([[ 0.00012855, -0.00201288,  0.01811524],\n",
      "       [-0.00015042, -0.00069072,  0.00929108]]), array([[ 0.00312001],\n",
      "       [-0.00274048],\n",
      "       [ 0.01731694]])]\n",
      "gradients_biases:  [array([ 3.16206603e-05, -1.82841763e-03,  5.53230106e-04]), array([-0.00294109])]\n",
      "Iteration 394, Cost: 0.23693435879115748\n",
      "gradient_weights:  [array([[ 0.00012731, -0.00201169,  0.01812454],\n",
      "       [-0.00014869, -0.00068871,  0.00930461]]), array([[ 0.00312461],\n",
      "       [-0.00275094],\n",
      "       [ 0.01733722]])]\n",
      "gradients_biases:  [array([ 3.12094705e-05, -1.83018008e-03,  5.36741348e-04]), array([-0.00294353])]\n",
      "Iteration 395, Cost: 0.23689691788726414\n",
      "gradient_weights:  [array([[ 0.00012607, -0.00201049,  0.01813383],\n",
      "       [-0.00014697, -0.0006867 ,  0.00931813]]), array([[ 0.00312922],\n",
      "       [-0.00276142],\n",
      "       [ 0.01735748]])]\n",
      "gradients_biases:  [array([ 3.07994115e-05, -1.83194915e-03,  5.20219209e-04]), array([-0.00294596])]\n",
      "Iteration 396, Cost: 0.23685940836054367\n",
      "gradient_weights:  [array([[ 0.00012482, -0.0020093 ,  0.01814308],\n",
      "       [-0.00014525, -0.00068469,  0.00933165]]), array([[ 0.00313384],\n",
      "       [-0.0027719 ],\n",
      "       [ 0.0173777 ]])]\n",
      "gradients_biases:  [array([ 3.03904879e-05, -1.83372484e-03,  5.03663894e-04]), array([-0.00294841])]\n",
      "Iteration 397, Cost: 0.23682183020037423\n",
      "gradient_weights:  [array([[ 0.00012357, -0.00200811,  0.0181523 ],\n",
      "       [-0.00014352, -0.00068267,  0.00934516]]), array([[ 0.00313847],\n",
      "       [-0.0027824 ],\n",
      "       [ 0.0173979 ]])]\n",
      "gradients_biases:  [array([ 2.99827042e-05, -1.83550716e-03,  4.87075610e-04]), array([-0.00295085])]\n",
      "Iteration 398, Cost: 0.2367841833964081\n",
      "gradient_weights:  [array([[ 0.00012231, -0.00200691,  0.0181615 ],\n",
      "       [-0.0001418 , -0.00068065,  0.00935867]]), array([[ 0.00314311],\n",
      "       [-0.0027929 ],\n",
      "       [ 0.01741806]])]\n",
      "gradients_biases:  [array([ 2.95760649e-05, -1.83729613e-03,  4.70454564e-04]), array([-0.00295331])]\n",
      "Iteration 399, Cost: 0.23674646793857185\n",
      "gradient_weights:  [array([[ 0.00012105, -0.00200571,  0.01817067],\n",
      "       [-0.00014008, -0.00067863,  0.00937218]]), array([[ 0.00314775],\n",
      "       [-0.00280342],\n",
      "       [ 0.0174382 ]])]\n",
      "gradients_biases:  [array([ 2.91705747e-05, -1.83909176e-03,  4.53800965e-04]), array([-0.00295576])]\n",
      "Iteration 400, Cost: 0.23670868381706622\n",
      "gradient_weights:  [array([[ 0.00011979, -0.00200451,  0.01817981],\n",
      "       [-0.00013836, -0.0006766 ,  0.00938569]]), array([[ 0.0031524 ],\n",
      "       [-0.00281394],\n",
      "       [ 0.0174583 ]])]\n",
      "gradients_biases:  [array([ 2.87662382e-05, -1.84089405e-03,  4.37115021e-04]), array([-0.00295823])]\n",
      "Iteration 401, Cost: 0.236670831022366\n",
      "gradient_weights:  [array([[ 0.00011852, -0.00200331,  0.01818893],\n",
      "       [-0.00013664, -0.00067456,  0.00939919]]), array([[ 0.00315706],\n",
      "       [-0.00282447],\n",
      "       [ 0.01747837]])]\n",
      "gradients_biases:  [array([ 2.83630599e-05, -1.84270301e-03,  4.20396944e-04]), array([-0.00296069])]\n",
      "Iteration 402, Cost: 0.23663290954522032\n",
      "gradient_weights:  [array([[ 0.00011724, -0.00200211,  0.01819802],\n",
      "       [-0.00013492, -0.00067252,  0.00941269]]), array([[ 0.00316173],\n",
      "       [-0.00283502],\n",
      "       [ 0.01749841]])]\n",
      "gradients_biases:  [array([ 2.79610444e-05, -1.84451865e-03,  4.03646944e-04]), array([-0.00296317])]\n",
      "Iteration 403, Cost: 0.23659491937665228\n",
      "gradient_weights:  [array([[ 0.00011597, -0.00200091,  0.01820708],\n",
      "       [-0.0001332 , -0.00067048,  0.00942618]]), array([[ 0.00316641],\n",
      "       [-0.00284557],\n",
      "       [ 0.01751843]])]\n",
      "gradients_biases:  [array([ 2.75601965e-05, -1.84634099e-03,  3.86865233e-04]), array([-0.00296565])]\n",
      "Iteration 404, Cost: 0.23655686050795943\n",
      "gradient_weights:  [array([[ 0.00011468, -0.0019997 ,  0.01821611],\n",
      "       [-0.00013148, -0.00066843,  0.00943968]]), array([[ 0.00317109],\n",
      "       [-0.00285613],\n",
      "       [ 0.01753841]])]\n",
      "gradients_biases:  [array([ 2.71605207e-05, -1.84817004e-03,  3.70052025e-04]), array([-0.00296813])]\n",
      "Iteration 405, Cost: 0.2365187329307135\n",
      "gradient_weights:  [array([[ 0.0001134 , -0.00199849,  0.01822511],\n",
      "       [-0.00012976, -0.00066637,  0.00945317]]), array([[ 0.00317578],\n",
      "       [-0.0028667 ],\n",
      "       [ 0.01755835]])]\n",
      "gradients_biases:  [array([ 2.67620218e-05, -1.85000579e-03,  3.53207535e-04]), array([-0.00297062])]\n",
      "Iteration 406, Cost: 0.23648053663676052\n",
      "gradient_weights:  [array([[ 0.00011211, -0.00199728,  0.01823409],\n",
      "       [-0.00012804, -0.00066432,  0.00946665]]), array([[ 0.00318048],\n",
      "       [-0.00287729],\n",
      "       [ 0.01757827]])]\n",
      "gradients_biases:  [array([ 2.63647045e-05, -1.85184826e-03,  3.36331976e-04]), array([-0.00297311])]\n",
      "Iteration 407, Cost: 0.23644227161822096\n",
      "gradient_weights:  [array([[ 0.00011081, -0.00199607,  0.01824304],\n",
      "       [-0.00012633, -0.00066225,  0.00948013]]), array([[ 0.00318519],\n",
      "       [-0.00288788],\n",
      "       [ 0.01759816]])]\n",
      "gradients_biases:  [array([ 2.59685733e-05, -1.85369747e-03,  3.19425566e-04]), array([-0.00297561])]\n",
      "Iteration 408, Cost: 0.23640393786748995\n",
      "gradient_weights:  [array([[ 0.00010951, -0.00199486,  0.01825196],\n",
      "       [-0.00012461, -0.00066019,  0.00949361]]), array([[ 0.0031899 ],\n",
      "       [-0.00289848],\n",
      "       [ 0.01761801]])]\n",
      "gradients_biases:  [array([ 2.55736332e-05, -1.85555342e-03,  3.02488520e-04]), array([-0.00297811])]\n",
      "Iteration 409, Cost: 0.2363655353772371\n",
      "gradient_weights:  [array([[ 0.00010821, -0.00199365,  0.01826086],\n",
      "       [-0.0001229 , -0.00065811,  0.00950708]]), array([[ 0.00319463],\n",
      "       [-0.00290909],\n",
      "       [ 0.01763784]])]\n",
      "gradients_biases:  [array([ 2.51798887e-05, -1.85741611e-03,  2.85521058e-04]), array([-0.00298062])]\n",
      "Iteration 410, Cost: 0.23632706414040688\n",
      "gradient_weights:  [array([[ 0.0001069 , -0.00199243,  0.01826973],\n",
      "       [-0.00012118, -0.00065604,  0.00952055]]), array([[ 0.00319936],\n",
      "       [-0.00291971],\n",
      "       [ 0.01765763]])]\n",
      "gradients_biases:  [array([ 2.47873447e-05, -1.85928557e-03,  2.68523397e-04]), array([-0.00298313])]\n",
      "Iteration 411, Cost: 0.23628852415021862\n",
      "gradient_weights:  [array([[ 0.00010559, -0.00199121,  0.01827857],\n",
      "       [-0.00011947, -0.00065395,  0.00953402]]), array([[ 0.0032041 ],\n",
      "       [-0.00293034],\n",
      "       [ 0.01767739]])]\n",
      "gradients_biases:  [array([ 2.43960060e-05, -1.86116180e-03,  2.51495758e-04]), array([-0.00298565])]\n",
      "Iteration 412, Cost: 0.23624991540016677\n",
      "gradient_weights:  [array([[ 0.00010427, -0.00198999,  0.01828738],\n",
      "       [-0.00011775, -0.00065187,  0.00954748]]), array([[ 0.00320885],\n",
      "       [-0.00294098],\n",
      "       [ 0.01769712]])]\n",
      "gradients_biases:  [array([ 2.40058772e-05, -1.86304481e-03,  2.34438361e-04]), array([-0.00298818])]\n",
      "Iteration 413, Cost: 0.23621123788402099\n",
      "gradient_weights:  [array([[ 0.00010295, -0.00198877,  0.01829616],\n",
      "       [-0.00011604, -0.00064978,  0.00956094]]), array([[ 0.0032136 ],\n",
      "       [-0.00295163],\n",
      "       [ 0.01771681]])]\n",
      "gradients_biases:  [array([ 2.36169633e-05, -1.86493460e-03,  2.17351428e-04]), array([-0.0029907])]\n",
      "Iteration 414, Cost: 0.23617249159582637\n",
      "gradient_weights:  [array([[ 0.00010162, -0.00198754,  0.01830492],\n",
      "       [-0.00011432, -0.00064768,  0.00957439]]), array([[ 0.00321837],\n",
      "       [-0.00296229],\n",
      "       [ 0.01773648]])]\n",
      "gradients_biases:  [array([ 2.32292689e-05, -1.86683120e-03,  2.00235182e-04]), array([-0.00299324])]\n",
      "Iteration 415, Cost: 0.23613367652990375\n",
      "gradient_weights:  [array([[ 0.00010029, -0.00198632,  0.01831365],\n",
      "       [-0.00011261, -0.00064558,  0.00958785]]), array([[ 0.00322314],\n",
      "       [-0.00297296],\n",
      "       [ 0.01775611]])]\n",
      "gradients_biases:  [array([ 2.28427991e-05, -1.86873460e-03,  1.83089844e-04]), array([-0.00299578])]\n",
      "Iteration 416, Cost: 0.23609479268084974\n",
      "gradient_weights:  [array([[ 9.89585354e-05, -1.98509065e-03,  1.83223519e-02],\n",
      "       [-1.10901087e-04, -6.43471412e-04,  9.60129132e-03]]), array([[ 0.00322792],\n",
      "       [-0.00298364],\n",
      "       [ 0.01777571]])]\n",
      "gradients_biases:  [array([ 2.24575585e-05, -1.87064483e-03,  1.65915641e-04]), array([-0.00299832])]\n",
      "Iteration 417, Cost: 0.23605584004353697\n",
      "gradient_weights:  [array([[ 9.76192123e-05, -1.98386048e-03,  1.83310269e-02],\n",
      "       [-1.09190311e-04, -6.41359852e-04,  9.61473319e-03]]), array([[ 0.00323271],\n",
      "       [-0.00299433],\n",
      "       [ 0.01779528]])]\n",
      "gradients_biases:  [array([ 2.20735521e-05, -1.87256188e-03,  1.48712796e-04]), array([-0.00300087])]\n",
      "Iteration 418, Cost: 0.23601681861311471\n",
      "gradient_weights:  [array([[ 9.62753580e-05, -1.98262821e-03,  1.83396745e-02],\n",
      "       [-1.07480233e-04, -6.39243230e-04,  9.62817078e-03]]), array([[ 0.0032375 ],\n",
      "       [-0.00300503],\n",
      "       [ 0.01781481]])]\n",
      "gradients_biases:  [array([ 2.16907847e-05, -1.87448577e-03,  1.31481537e-04]), array([-0.00300342])]\n",
      "Iteration 419, Cost: 0.23597772838500863\n",
      "gradient_weights:  [array([[ 9.49269555e-05, -1.98139380e-03,  1.83482947e-02],\n",
      "       [-1.05770864e-04, -6.37121513e-04,  9.64160404e-03]]), array([[ 0.00324231],\n",
      "       [-0.00301574],\n",
      "       [ 0.01783431]])]\n",
      "gradients_biases:  [array([ 2.13092612e-05, -1.87641650e-03,  1.14222089e-04]), array([-0.00300598])]\n",
      "Iteration 420, Cost: 0.23593856935492136\n",
      "gradient_weights:  [array([[ 9.35739877e-05, -1.98015723e-03,  1.83568875e-02],\n",
      "       [-1.04062215e-04, -6.34994667e-04,  9.65503291e-03]]), array([[ 0.00324712],\n",
      "       [-0.00302646],\n",
      "       [ 0.01785378]])]\n",
      "gradients_biases:  [array([ 2.09289866e-05, -1.87835410e-03,  9.69346800e-05]), array([-0.00300854])]\n",
      "Iteration 421, Cost: 0.2358993415188328\n",
      "gradient_weights:  [array([[ 9.22164375e-05, -1.97891846e-03,  1.83654530e-02],\n",
      "       [-1.02354298e-04, -6.32862658e-04,  9.66845733e-03]]), array([[ 0.00325194],\n",
      "       [-0.00303719],\n",
      "       [ 0.01787322]])]\n",
      "gradients_biases:  [array([ 2.05499658e-05, -1.88029856e-03,  7.96195393e-05]), array([-0.00301111])]\n",
      "Iteration 422, Cost: 0.23586004487300039\n",
      "gradient_weights:  [array([[ 9.08542875e-05, -1.97767746e-03,  1.83739911e-02],\n",
      "       [-1.00647123e-04, -6.30725452e-04,  9.68187724e-03]]), array([[ 0.00325677],\n",
      "       [-0.00304793],\n",
      "       [ 0.01789262]])]\n",
      "gradients_biases:  [array([ 2.01722036e-05, -1.88224990e-03,  6.22768958e-05]), array([-0.00301368])]\n",
      "Iteration 423, Cost: 0.23582067941395934\n",
      "gradient_weights:  [array([[ 8.94875206e-05, -1.97643420e-03,  1.83825019e-02],\n",
      "       [-9.89407026e-05, -6.28583014e-04,  9.69529257e-03]]), array([[ 0.0032616 ],\n",
      "       [-0.00305868],\n",
      "       [ 0.01791199]])]\n",
      "gradients_biases:  [array([ 1.97957051e-05, -1.88420812e-03,  4.49069800e-05]), array([-0.00301626])]\n",
      "Iteration 424, Cost: 0.23578124513852317\n",
      "gradient_weights:  [array([[ 8.81161195e-05, -1.97518864e-03,  1.83909855e-02],\n",
      "       [-9.72350473e-05, -6.26435311e-04,  9.70870328e-03]]), array([[ 0.00326645],\n",
      "       [-0.00306944],\n",
      "       [ 0.01793133]])]\n",
      "gradients_biases:  [array([ 1.94204752e-05, -1.88617325e-03,  2.75100227e-05]), array([-0.00301884])]\n",
      "Iteration 425, Cost: 0.23574174204378406\n",
      "gradient_weights:  [array([[ 8.67400667e-05, -1.97394075e-03,  1.83994417e-02],\n",
      "       [-9.55301688e-05, -6.24282308e-04,  9.72210931e-03]]), array([[ 0.0032713 ],\n",
      "       [-0.00308021],\n",
      "       [ 0.01795064]])]\n",
      "gradients_biases:  [array([ 1.90465189e-05, -1.88814527e-03,  1.00862559e-05]), array([-0.00302143])]\n",
      "Iteration 426, Cost: 0.23570217012711298\n",
      "gradient_weights:  [array([[ 8.53593451e-05, -1.97269050e-03,  1.84078706e-02],\n",
      "       [-9.38260783e-05, -6.22123970e-04,  9.73551060e-03]]), array([[ 0.00327616],\n",
      "       [-0.00309099],\n",
      "       [ 0.01796991]])]\n",
      "gradients_biases:  [array([ 1.86738413e-05, -1.89012422e-03, -7.36408774e-06]), array([-0.00302402])]\n",
      "Iteration 427, Cost: 0.23566252938616045\n",
      "gradient_weights:  [array([[ 8.39739370e-05, -1.97143787e-03,  1.84162723e-02],\n",
      "       [-9.21227876e-05, -6.19960264e-04,  9.74890708e-03]]), array([[ 0.00328103],\n",
      "       [-0.00310178],\n",
      "       [ 0.01798915]])]\n",
      "gradients_biases:  [array([ 1.83024472e-05, -1.89211009e-03, -2.48407747e-05]), array([-0.00302662])]\n",
      "Iteration 428, Cost: 0.23562281981885658\n",
      "gradient_weights:  [array([[ 8.25838252e-05, -1.97018280e-03,  1.84246468e-02],\n",
      "       [-9.04203080e-05, -6.17791155e-04,  9.76229870e-03]]), array([[ 0.00328591],\n",
      "       [-0.00311258],\n",
      "       [ 0.01800835]])]\n",
      "gradients_biases:  [array([ 1.79323418e-05, -1.89410290e-03, -4.23435706e-05]), array([-0.00302922])]\n",
      "Iteration 429, Cost: 0.23558304142341185\n",
      "gradient_weights:  [array([[ 8.11889920e-05, -1.96892528e-03,  1.84329940e-02],\n",
      "       [-8.87186513e-05, -6.15616607e-04,  9.77568541e-03]]), array([[ 0.00329079],\n",
      "       [-0.00312338],\n",
      "       [ 0.01802753]])]\n",
      "gradients_biases:  [array([ 1.75635301e-05, -1.89610265e-03, -5.98722404e-05]), array([-0.00303183])]\n",
      "Iteration 430, Cost: 0.23554319419831737\n",
      "gradient_weights:  [array([[ 7.97894200e-05, -1.96766527e-03,  1.84413141e-02],\n",
      "       [-8.70178290e-05, -6.13436587e-04,  9.78906715e-03]]), array([[ 0.00329569],\n",
      "       [-0.0031342 ],\n",
      "       [ 0.01804666]])]\n",
      "gradients_biases:  [array([ 1.71960171e-05, -1.89810936e-03, -7.74265482e-05]), array([-0.00303444])]\n",
      "Iteration 431, Cost: 0.23550327814234534\n",
      "gradient_weights:  [array([[ 7.83850915e-05, -1.96640273e-03,  1.84496069e-02],\n",
      "       [-8.53178528e-05, -6.11251060e-04,  9.80244385e-03]]), array([[ 0.00330059],\n",
      "       [-0.00314503],\n",
      "       [ 0.01806577]])]\n",
      "gradients_biases:  [array([ 1.68298080e-05, -1.90012303e-03, -9.50062573e-05]), array([-0.00303706])]\n",
      "Iteration 432, Cost: 0.2354632932545495\n",
      "gradient_weights:  [array([[ 7.69759889e-05, -1.96513764e-03,  1.84578726e-02],\n",
      "       [-8.36187346e-05, -6.09059991e-04,  9.81581546e-03]]), array([[ 0.0033055 ],\n",
      "       [-0.00315587],\n",
      "       [ 0.01808484]])]\n",
      "gradients_biases:  [array([ 1.64649078e-05, -1.90214368e-03, -1.12611130e-04]), array([-0.00303968])]\n",
      "Iteration 433, Cost: 0.2354232395342658\n",
      "gradient_weights:  [array([[ 7.55620946e-05, -1.96386996e-03,  1.84661112e-02],\n",
      "       [-8.19204860e-05, -6.06863346e-04,  9.82918193e-03]]), array([[ 0.00331042],\n",
      "       [-0.00316672],\n",
      "       [ 0.01810388]])]\n",
      "gradients_biases:  [array([ 1.61013216e-05, -1.90417131e-03, -1.30240929e-04]), array([-0.00304231])]\n",
      "Iteration 434, Cost: 0.23538311698111275\n",
      "gradient_weights:  [array([[ 7.41433909e-05, -1.96259965e-03,  1.84743227e-02],\n",
      "       [-8.02231190e-05, -6.04661088e-04,  9.84254319e-03]]), array([[ 0.00331535],\n",
      "       [-0.00317758],\n",
      "       [ 0.01812288]])]\n",
      "gradients_biases:  [array([ 1.57390545e-05, -1.90620594e-03, -1.47895414e-04]), array([-0.00304494])]\n",
      "Iteration 435, Cost: 0.23534292559499193\n",
      "gradient_weights:  [array([[ 7.27198599e-05, -1.96132669e-03,  1.84825070e-02],\n",
      "       [-7.85266454e-05, -6.02453183e-04,  9.85589919e-03]]), array([[ 0.00332028],\n",
      "       [-0.00318845],\n",
      "       [ 0.01814185]])]\n",
      "gradients_biases:  [array([ 1.53781118e-05, -1.90824758e-03, -1.65574346e-04]), array([-0.00304757])]\n",
      "Iteration 436, Cost: 0.23530266537608868\n",
      "gradient_weights:  [array([[ 7.12914839e-05, -1.96005105e-03,  1.84906643e-02],\n",
      "       [-7.68310772e-05, -6.00239597e-04,  9.86924987e-03]]), array([[ 0.00332522],\n",
      "       [-0.00319933],\n",
      "       [ 0.01816078]])]\n",
      "gradients_biases:  [array([ 1.50184984e-05, -1.91029623e-03, -1.83277484e-04]), array([-0.00305021])]\n",
      "Iteration 437, Cost: 0.23526233632487253\n",
      "gradient_weights:  [array([[ 6.98582451e-05, -1.95877268e-03,  1.84987945e-02],\n",
      "       [-7.51364264e-05, -5.98020294e-04,  9.88259518e-03]]), array([[ 0.00333017],\n",
      "       [-0.00321022],\n",
      "       [ 0.01817968]])]\n",
      "gradients_biases:  [array([ 1.46602197e-05, -1.91235190e-03, -2.01004587e-04]), array([-0.00305286])]\n",
      "Iteration 438, Cost: 0.2352219384420979\n",
      "gradient_weights:  [array([[ 6.84201256e-05, -1.95749155e-03,  1.85068977e-02],\n",
      "       [-7.34427051e-05, -5.95795238e-04,  9.89593505e-03]]), array([[ 0.00333513],\n",
      "       [-0.00322112],\n",
      "       [ 0.01819855]])]\n",
      "gradients_biases:  [array([ 1.43032807e-05, -1.91441461e-03, -2.18755413e-04]), array([-0.00305551])]\n",
      "Iteration 439, Cost: 0.23518147172880444\n",
      "gradient_weights:  [array([[ 6.69771074e-05, -1.95620764e-03,  1.85149738e-02],\n",
      "       [-7.17499253e-05, -5.93564395e-04,  9.90926943e-03]]), array([[ 0.0033401 ],\n",
      "       [-0.00323203],\n",
      "       [ 0.01821738]])]\n",
      "gradients_biases:  [array([ 1.39476867e-05, -1.91648437e-03, -2.36529720e-04]), array([-0.00305816])]\n",
      "Iteration 440, Cost: 0.23514093618631798\n",
      "gradient_weights:  [array([[ 6.55291726e-05, -1.95492090e-03,  1.85230230e-02],\n",
      "       [-7.00580992e-05, -5.91327729e-04,  9.92259826e-03]]), array([[ 0.00334508],\n",
      "       [-0.00324295],\n",
      "       [ 0.01823618]])]\n",
      "gradients_biases:  [array([ 1.35934428e-05, -1.91856117e-03, -2.54327264e-04]), array([-0.00306082])]\n",
      "Iteration 441, Cost: 0.2351003318162508\n",
      "gradient_weights:  [array([[ 6.40763031e-05, -1.95363131e-03,  1.85310451e-02],\n",
      "       [-6.83672390e-05, -5.89085204e-04,  9.93592149e-03]]), array([[ 0.00335006],\n",
      "       [-0.00325389],\n",
      "       [ 0.01825494]])]\n",
      "gradients_biases:  [array([ 1.32405543e-05, -1.92064505e-03, -2.72147800e-04]), array([-0.00306349])]\n",
      "Iteration 442, Cost: 0.2350596586205027\n",
      "gradient_weights:  [array([[ 6.26184810e-05, -1.95233883e-03,  1.85390404e-02],\n",
      "       [-6.66773570e-05, -5.86836786e-04,  9.94923906e-03]]), array([[ 0.00335506],\n",
      "       [-0.00326483],\n",
      "       [ 0.01827367]])]\n",
      "gradients_biases:  [array([ 1.28890265e-05, -1.92273600e-03, -2.89991084e-04]), array([-0.00306616])]\n",
      "Iteration 443, Cost: 0.23501891660126112\n",
      "gradient_weights:  [array([[ 6.11556881e-05, -1.95104342e-03,  1.85470087e-02],\n",
      "       [-6.49884654e-05, -5.84582438e-04,  9.96255091e-03]]), array([[ 0.00336006],\n",
      "       [-0.00327578],\n",
      "       [ 0.01829237]])]\n",
      "gradients_biases:  [array([ 1.25388646e-05, -1.92483403e-03, -3.07856871e-04]), array([-0.00306883])]\n",
      "Iteration 444, Cost: 0.23497810576100228\n",
      "gradient_weights:  [array([[ 5.96879063e-05, -1.94974506e-03,  1.85549500e-02],\n",
      "       [-6.33005767e-05, -5.82322126e-04,  9.97585698e-03]]), array([[ 0.00336507],\n",
      "       [-0.00328674],\n",
      "       [ 0.01831102]])]\n",
      "gradients_biases:  [array([ 1.21900738e-05, -1.92693916e-03, -3.25744914e-04]), array([-0.00307151])]\n",
      "Iteration 445, Cost: 0.23493722610249163\n",
      "gradient_weights:  [array([[ 5.82151174e-05, -1.94844371e-03,  1.85628645e-02],\n",
      "       [-6.16137031e-05, -5.80055812e-04,  9.98915722e-03]]), array([[ 0.00337008],\n",
      "       [-0.00329771],\n",
      "       [ 0.01832965]])]\n",
      "gradients_biases:  [array([ 1.18426595e-05, -1.92905139e-03, -3.43654966e-04]), array([-0.00307419])]\n",
      "Iteration 446, Cost: 0.2348962776287845\n",
      "gradient_weights:  [array([[ 5.67373031e-05, -1.94713933e-03,  1.85707522e-02],\n",
      "       [-5.99278573e-05, -5.77783462e-04,  1.00024516e-02]]), array([[ 0.00337511],\n",
      "       [-0.00330869],\n",
      "       [ 0.01834824]])]\n",
      "gradients_biases:  [array([ 1.14966268e-05, -1.93117073e-03, -3.61586779e-04]), array([-0.00307688])]\n",
      "Iteration 447, Cost: 0.23485526034322723\n",
      "gradient_weights:  [array([[ 5.52544453e-05, -1.94583190e-03,  1.85786130e-02],\n",
      "       [-5.82430516e-05, -5.75505040e-04,  1.00157400e-02]]), array([[ 0.00338014],\n",
      "       [-0.00331968],\n",
      "       [ 0.01836679]])]\n",
      "gradients_biases:  [array([ 1.11519813e-05, -1.93329720e-03, -3.79540106e-04]), array([-0.00307957])]\n",
      "Iteration 448, Cost: 0.23481417424945716\n",
      "gradient_weights:  [array([[ 5.37665255e-05, -1.94452137e-03,  1.85864470e-02],\n",
      "       [-5.65592987e-05, -5.73220509e-04,  1.00290224e-02]]), array([[ 0.00338519],\n",
      "       [-0.00333069],\n",
      "       [ 0.01838531]])]\n",
      "gradients_biases:  [array([ 1.08087281e-05, -1.93543081e-03, -3.97514697e-04]), array([-0.00308227])]\n",
      "Iteration 449, Cost: 0.23477301935140427\n",
      "gradient_weights:  [array([[ 5.22735255e-05, -1.94320772e-03,  1.85942542e-02],\n",
      "       [-5.48766111e-05, -5.70929834e-04,  1.00422987e-02]]), array([[ 0.00339024],\n",
      "       [-0.0033417 ],\n",
      "       [ 0.01840379]])]\n",
      "gradients_biases:  [array([ 1.04668726e-05, -1.93757156e-03, -4.15510302e-04]), array([-0.00308497])]\n",
      "Iteration 450, Cost: 0.234731795653291\n",
      "gradient_weights:  [array([[ 5.07754267e-05, -1.94189090e-03,  1.86020346e-02],\n",
      "       [-5.31950015e-05, -5.68632979e-04,  1.00555690e-02]]), array([[ 0.0033953 ],\n",
      "       [-0.00335272],\n",
      "       [ 0.01842224]])]\n",
      "gradients_biases:  [array([ 1.01264201e-05, -1.93971946e-03, -4.33526672e-04]), array([-0.00308767])]\n",
      "Iteration 451, Cost: 0.2346905031596339\n",
      "gradient_weights:  [array([[ 4.92722109e-05, -1.94057089e-03,  1.86097883e-02],\n",
      "       [-5.15144826e-05, -5.66329908e-04,  1.00688330e-02]]), array([[ 0.00340036],\n",
      "       [-0.00336375],\n",
      "       [ 0.01844065]])]\n",
      "gradients_biases:  [array([ 9.78737611e-06, -1.94187453e-03, -4.51563556e-04]), array([-0.00309038])]\n",
      "Iteration 452, Cost: 0.234649141875244\n",
      "gradient_weights:  [array([[ 4.77638595e-05, -1.93924765e-03,  1.86175152e-02],\n",
      "       [-4.98350672e-05, -5.64020583e-04,  1.00820909e-02]]), array([[ 0.00340544],\n",
      "       [-0.0033748 ],\n",
      "       [ 0.01845903]])]\n",
      "gradients_biases:  [array([ 9.44974591e-06, -1.94403677e-03, -4.69620701e-04]), array([-0.0030931])]\n",
      "Iteration 453, Cost: 0.23460771180522744\n",
      "gradient_weights:  [array([[ 4.62503539e-05, -1.93792115e-03,  1.86252155e-02],\n",
      "       [-4.81567681e-05, -5.61704970e-04,  1.00953424e-02]]), array([[ 0.00341052],\n",
      "       [-0.00338585],\n",
      "       [ 0.01847737]])]\n",
      "gradients_biases:  [array([ 9.11353491e-06, -1.94620620e-03, -4.87697856e-04]), array([-0.00309582])]\n",
      "Iteration 454, Cost: 0.2345662129549867\n",
      "gradient_weights:  [array([[ 4.47316756e-05, -1.93659135e-03,  1.86328890e-02],\n",
      "       [-4.64795982e-05, -5.59383032e-04,  1.01085877e-02]]), array([[ 0.00341562],\n",
      "       [-0.00339692],\n",
      "       [ 0.01849568]])]\n",
      "gradients_biases:  [array([ 8.77874854e-06, -1.94838282e-03, -5.05794768e-04]), array([-0.00309854])]\n",
      "Iteration 455, Cost: 0.23452464533022133\n",
      "gradient_weights:  [array([[ 4.32078059e-05, -1.93525821e-03,  1.86405360e-02],\n",
      "       [-4.48035702e-05, -5.57054732e-04,  1.01218265e-02]]), array([[ 0.00342072],\n",
      "       [-0.00340799],\n",
      "       [ 0.01851395]])]\n",
      "gradients_biases:  [array([ 8.44539221e-06, -1.95056665e-03, -5.23911184e-04]), array([-0.00310127])]\n",
      "Iteration 456, Cost: 0.23448300893692853\n",
      "gradient_weights:  [array([[ 4.16787263e-05, -1.93392171e-03,  1.86481563e-02],\n",
      "       [-4.31286974e-05, -5.54720034e-04,  1.01350588e-02]]), array([[ 0.00342583],\n",
      "       [-0.00341907],\n",
      "       [ 0.01853218]])]\n",
      "gradients_biases:  [array([ 8.11347136e-06, -1.95275769e-03, -5.42046849e-04]), array([-0.003104])]\n",
      "Iteration 457, Cost: 0.23444130378140443\n",
      "gradient_weights:  [array([[ 4.01444179e-05, -1.93258180e-03,  1.86557500e-02],\n",
      "       [-4.14549925e-05, -5.52378901e-04,  1.01482847e-02]]), array([[ 0.00343095],\n",
      "       [-0.00343017],\n",
      "       [ 0.01855038]])]\n",
      "gradients_biases:  [array([ 7.78299145e-06, -1.95495595e-03, -5.60201509e-04]), array([-0.00310674])]\n",
      "Iteration 458, Cost: 0.2343995298702447\n",
      "gradient_weights:  [array([[ 3.86048621e-05, -1.93123846e-03,  1.86633172e-02],\n",
      "       [-3.97824687e-05, -5.50031297e-04,  1.01615040e-02]]), array([[ 0.00343607],\n",
      "       [-0.00344127],\n",
      "       [ 0.01856854]])]\n",
      "gradients_biases:  [array([ 7.45395794e-06, -1.95716145e-03, -5.78374909e-04]), array([-0.00310948])]\n",
      "Iteration 459, Cost: 0.23435768721034567\n",
      "gradient_weights:  [array([[ 3.70600400e-05, -1.92989165e-03,  1.86708577e-02],\n",
      "       [-3.81111392e-05, -5.47677186e-04,  1.01747167e-02]]), array([[ 0.00344121],\n",
      "       [-0.00345239],\n",
      "       [ 0.01858667]])]\n",
      "gradients_biases:  [array([ 7.12637632e-06, -1.95937420e-03, -5.96566793e-04]), array([-0.00311222])]\n",
      "Iteration 460, Cost: 0.23431577580890475\n",
      "gradient_weights:  [array([[ 3.55099328e-05, -1.92854133e-03,  1.86783718e-02],\n",
      "       [-3.64410170e-05, -5.45316529e-04,  1.01879227e-02]]), array([[ 0.00344635],\n",
      "       [-0.00346352],\n",
      "       [ 0.01860476]])]\n",
      "gradients_biases:  [array([ 6.80025207e-06, -1.96159420e-03, -6.14776904e-04]), array([-0.00311497])]\n",
      "Iteration 461, Cost: 0.2342737956734221\n",
      "gradient_weights:  [array([[ 3.39545216e-05, -1.92718747e-03,  1.86858594e-02],\n",
      "       [-3.47721154e-05, -5.42949292e-04,  1.02011220e-02]]), array([[ 0.00345151],\n",
      "       [-0.00347465],\n",
      "       [ 0.01862281]])]\n",
      "gradients_biases:  [array([ 6.47559072e-06, -1.96382146e-03, -6.33004986e-04]), array([-0.00311773])]\n",
      "Iteration 462, Cost: 0.23423174681170117\n",
      "gradient_weights:  [array([[ 3.23937874e-05, -1.92583003e-03,  1.86933205e-02],\n",
      "       [-3.31044476e-05, -5.40575436e-04,  1.02143145e-02]]), array([[ 0.00345667],\n",
      "       [-0.0034858 ],\n",
      "       [ 0.01864083]])]\n",
      "gradients_biases:  [array([ 6.15239778e-06, -1.96605600e-03, -6.51250780e-04]), array([-0.00312048])]\n",
      "Iteration 463, Cost: 0.2341896292318495\n",
      "gradient_weights:  [array([[ 3.08277113e-05, -1.92446898e-03,  1.87007552e-02],\n",
      "       [-3.14380270e-05, -5.38194924e-04,  1.02275001e-02]]), array([[ 0.00346184],\n",
      "       [-0.00349695],\n",
      "       [ 0.01865881]])]\n",
      "gradients_biases:  [array([ 5.83067878e-06, -1.96829782e-03, -6.69514030e-04]), array([-0.00312325])]\n",
      "Iteration 464, Cost: 0.23414744294228007\n",
      "gradient_weights:  [array([[ 2.92562743e-05, -1.92310428e-03,  1.87081634e-02],\n",
      "       [-2.97728670e-05, -5.35807721e-04,  1.02406789e-02]]), array([[ 0.00346701],\n",
      "       [-0.00350812],\n",
      "       [ 0.01867675]])]\n",
      "gradients_biases:  [array([ 5.51043929e-06, -1.97054694e-03, -6.87794476e-04]), array([-0.00312601])]\n",
      "Iteration 465, Cost: 0.23410518795171195\n",
      "gradient_weights:  [array([[ 2.76794573e-05, -1.92173590e-03,  1.87155453e-02],\n",
      "       [-2.81089809e-05, -5.33413788e-04,  1.02538507e-02]]), array([[ 0.0034722 ],\n",
      "       [-0.0035193 ],\n",
      "       [ 0.01869466]])]\n",
      "gradients_biases:  [array([ 5.19168486e-06, -1.97280337e-03, -7.06091860e-04]), array([-0.00312879])]\n",
      "Iteration 466, Cost: 0.23406286426917172\n",
      "gradient_weights:  [array([[ 2.60972412e-05, -1.92036380e-03,  1.87229008e-02],\n",
      "       [-2.64463822e-05, -5.31013089e-04,  1.02670155e-02]]), array([[ 0.00347739],\n",
      "       [-0.00353049],\n",
      "       [ 0.01871253]])]\n",
      "gradients_biases:  [array([ 4.87442107e-06, -1.97506711e-03, -7.24405920e-04]), array([-0.00313156])]\n",
      "Iteration 467, Cost: 0.234020471903994\n",
      "gradient_weights:  [array([[ 2.45096069e-05, -1.91898795e-03,  1.87302299e-02],\n",
      "       [-2.47850844e-05, -5.28605586e-04,  1.02801732e-02]]), array([[ 0.0034826 ],\n",
      "       [-0.00354169],\n",
      "       [ 0.01873036]])]\n",
      "gradients_biases:  [array([ 4.55865352e-06, -1.97733817e-03, -7.42736399e-04]), array([-0.00313434])]\n",
      "Iteration 468, Cost: 0.23397801086582287\n",
      "gradient_weights:  [array([[ 2.29165350e-05, -1.91760831e-03,  1.87375328e-02],\n",
      "       [-2.31251011e-05, -5.26191243e-04,  1.02933238e-02]]), array([[ 0.00348781],\n",
      "       [-0.00355289],\n",
      "       [ 0.01874816]])]\n",
      "gradients_biases:  [array([ 4.24438780e-06, -1.97961657e-03, -7.61083034e-04]), array([-0.00313712])]\n",
      "Iteration 469, Cost: 0.2339354811646127\n",
      "gradient_weights:  [array([[ 2.13180065e-05, -1.91622485e-03,  1.87448093e-02],\n",
      "       [-2.14664459e-05, -5.23770021e-04,  1.03064673e-02]]), array([[ 0.00349303],\n",
      "       [-0.00356411],\n",
      "       [ 0.01876592]])]\n",
      "gradients_biases:  [array([ 3.93162955e-06, -1.98190232e-03, -7.79445564e-04]), array([-0.00313991])]\n",
      "Iteration 470, Cost: 0.2338928828106294\n",
      "gradient_weights:  [array([[ 1.97140019e-05, -1.91483752e-03,  1.87520596e-02],\n",
      "       [-1.98091326e-05, -5.21341883e-04,  1.03196035e-02]]), array([[ 0.00349826],\n",
      "       [-0.00357534],\n",
      "       [ 0.01878364]])]\n",
      "gradients_biases:  [array([ 3.62038438e-06, -1.98419542e-03, -7.97823727e-04]), array([-0.0031427])]\n",
      "Iteration 471, Cost: 0.23385021581445126\n",
      "gradient_weights:  [array([[ 1.81045020e-05, -1.91344630e-03,  1.87592837e-02],\n",
      "       [-1.81531747e-05, -5.18906793e-04,  1.03327324e-02]]), array([[ 0.0035035 ],\n",
      "       [-0.00358658],\n",
      "       [ 0.01880133]])]\n",
      "gradients_biases:  [array([ 3.31065795e-06, -1.98649589e-03, -8.16217263e-04]), array([-0.0031455])]\n",
      "Iteration 472, Cost: 0.23380748018697023\n",
      "gradient_weights:  [array([[ 1.64894873e-05, -1.91205116e-03,  1.87664816e-02],\n",
      "       [-1.64985861e-05, -5.16464712e-04,  1.03458540e-02]]), array([[ 0.00350874],\n",
      "       [-0.00359783],\n",
      "       [ 0.01881898]])]\n",
      "gradients_biases:  [array([ 3.00245591e-06, -1.98880373e-03, -8.34625906e-04]), array([-0.0031483])]\n",
      "Iteration 473, Cost: 0.23376467593939293\n",
      "gradient_weights:  [array([[ 1.48689385e-05, -1.91065204e-03,  1.87736533e-02],\n",
      "       [-1.48453805e-05, -5.14015603e-04,  1.03589682e-02]]), array([[ 0.003514  ],\n",
      "       [-0.0036091 ],\n",
      "       [ 0.01883659]])]\n",
      "gradients_biases:  [array([ 2.69578395e-06, -1.99111896e-03, -8.53049396e-04]), array([-0.00315111])]\n",
      "Iteration 474, Cost: 0.2337218030832418\n",
      "gradient_weights:  [array([[ 1.32428361e-05, -1.90924892e-03,  1.87807988e-02],\n",
      "       [-1.31935720e-05, -5.11559428e-04,  1.03720749e-02]]), array([[ 0.00351926],\n",
      "       [-0.00362037],\n",
      "       [ 0.01885417]])]\n",
      "gradients_biases:  [array([ 2.39064773e-06, -1.99344158e-03, -8.71487466e-04]), array([-0.00315391])]\n",
      "Iteration 475, Cost: 0.23367886163035628\n",
      "gradient_weights:  [array([[ 1.16111605e-05, -1.90784176e-03,  1.87879182e-02],\n",
      "       [-1.15431743e-05, -5.09096149e-04,  1.03851741e-02]]), array([[ 0.00352454],\n",
      "       [-0.00363165],\n",
      "       [ 0.0188717 ]])]\n",
      "gradients_biases:  [array([ 2.08705297e-06, -1.99577162e-03, -8.89939855e-04]), array([-0.00315673])]\n",
      "Iteration 476, Cost: 0.23363585159289382\n",
      "gradient_weights:  [array([[ 9.97389220e-06, -1.90643053e-03,  1.87950116e-02],\n",
      "       [-9.89420141e-06, -5.06625729e-04,  1.03982658e-02]]), array([[ 0.00352982],\n",
      "       [-0.00364294],\n",
      "       [ 0.0188892 ]])]\n",
      "gradients_biases:  [array([ 1.78500538e-06, -1.99810906e-03, -9.08406296e-04]), array([-0.00315954])]\n",
      "Iteration 477, Cost: 0.2335927729833312\n",
      "gradient_weights:  [array([[ 8.33101158e-06, -1.90501519e-03,  1.88020788e-02],\n",
      "       [-8.24666741e-06, -5.04148130e-04,  1.04113499e-02]]), array([[ 0.00353511],\n",
      "       [-0.00365424],\n",
      "       [ 0.01890666]])]\n",
      "gradients_biases:  [array([ 1.48451068e-06, -2.00045394e-03, -9.26886526e-04]), array([-0.00316236])]\n",
      "Iteration 478, Cost: 0.23354962581446553\n",
      "gradient_weights:  [array([[ 6.68249898e-06, -1.90359570e-03,  1.88091201e-02],\n",
      "       [-6.60058634e-06, -5.01663314e-04,  1.04244263e-02]]), array([[ 0.00354041],\n",
      "       [-0.00366556],\n",
      "       [ 0.01892409]])]\n",
      "gradients_biases:  [array([ 1.18557460e-06, -2.00280625e-03, -9.45380278e-04]), array([-0.00316519])]\n",
      "Iteration 479, Cost: 0.2335064100994157\n",
      "gradient_weights:  [array([[ 5.02833471e-06, -1.90217203e-03,  1.88161353e-02],\n",
      "       [-4.95597232e-06, -4.99171242e-04,  1.04374949e-02]]), array([[ 0.00354571],\n",
      "       [-0.00367688],\n",
      "       [ 0.01894148]])]\n",
      "gradients_biases:  [array([ 8.88202912e-07, -2.00516600e-03, -9.63887286e-04]), array([-0.00316802])]\n",
      "Iteration 480, Cost: 0.2334631258516232\n",
      "gradient_weights:  [array([[ 3.36849901e-06, -1.90074414e-03,  1.88231246e-02],\n",
      "       [-3.31283951e-06, -4.96671878e-04,  1.04505558e-02]]), array([[ 0.00355103],\n",
      "       [-0.00368821],\n",
      "       [ 0.01895882]])]\n",
      "gradients_biases:  [array([ 5.92401368e-07, -2.00753322e-03, -9.82407285e-04]), array([-0.00317085])]\n",
      "Iteration 481, Cost: 0.23341977308485357\n",
      "gradient_weights:  [array([[ 1.70297210e-06, -1.89931199e-03,  1.88300878e-02],\n",
      "       [-1.67120214e-06, -4.94165182e-04,  1.04636089e-02]]), array([[ 0.00355635],\n",
      "       [-0.00369956],\n",
      "       [ 0.01897614]])]\n",
      "gradients_biases:  [array([ 2.98175749e-07, -2.00990790e-03, -1.00094001e-03]), array([-0.00317369])]\n",
      "Iteration 482, Cost: 0.2333763518131977\n",
      "gradient_weights:  [array([[ 3.17341315e-08, -1.89787556e-03,  1.88370252e-02],\n",
      "       [-3.10744706e-08, -4.91651117e-04,  1.04766541e-02]]), array([[ 0.00356169],\n",
      "       [-0.00371091],\n",
      "       [ 0.01899341]])]\n",
      "gradients_biases:  [array([ 5.53184574e-09, -2.01229006e-03, -1.01948518e-03]), array([-0.00317653])]\n",
      "Iteration 483, Cost: 0.23333286205107284\n",
      "gradient_weights:  [array([[-1.64523477e-06, -1.89643479e-03,  1.88439367e-02],\n",
      "       [ 1.60752918e-06, -4.89129645e-04,  1.04896913e-02]]), array([[ 0.00356703],\n",
      "       [-0.00372228],\n",
      "       [ 0.01901064]])]\n",
      "gradients_biases:  [array([-2.85524538e-07, -2.01467970e-03, -1.03804255e-03]), array([-0.00317937])]\n",
      "Iteration 484, Cost: 0.23328930381322396\n",
      "gradient_weights:  [array([[-3.32795455e-06, -1.89498966e-03,  1.88508223e-02],\n",
      "       [ 3.24459444e-06, -4.86600726e-04,  1.05027205e-02]]), array([[ 0.00357238],\n",
      "       [-0.00373366],\n",
      "       [ 0.01902784]])]\n",
      "gradients_biases:  [array([-5.74987586e-07, -2.01707684e-03, -1.05661184e-03]), array([-0.00318222])]\n",
      "Iteration 485, Cost: 0.23324567711472505\n",
      "gradient_weights:  [array([[-5.01644516e-06, -1.89354013e-03,  1.88576821e-02],\n",
      "       [ 4.88010689e-06, -4.84064324e-04,  1.05157417e-02]]), array([[ 0.00357774],\n",
      "       [-0.00374504],\n",
      "       [ 0.019045  ]])]\n",
      "gradients_biases:  [array([-8.62851468e-07, -2.01948149e-03, -1.07519278e-03]), array([-0.00318507])]\n",
      "Iteration 486, Cost: 0.2332019819709803\n",
      "gradient_weights:  [array([[-6.71072664e-06, -1.89208616e-03,  1.88645160e-02],\n",
      "       [ 6.51405208e-06, -4.81520398e-04,  1.05287547e-02]]), array([[ 0.00358311],\n",
      "       [-0.00375644],\n",
      "       [ 0.01906212]])]\n",
      "gradients_biases:  [array([-1.14911034e-06, -2.02189366e-03, -1.09378510e-03]), array([-0.00318792])]\n",
      "Iteration 487, Cost: 0.23315821839772546\n",
      "gradient_weights:  [array([[-8.41081905e-06, -1.89062771e-03,  1.88713242e-02],\n",
      "       [ 8.14641547e-06, -4.78968912e-04,  1.05417596e-02]]), array([[ 0.00358849],\n",
      "       [-0.00376785],\n",
      "       [ 0.0190792 ]])]\n",
      "gradients_biases:  [array([-1.43375835e-06, -2.02431336e-03, -1.11238854e-03]), array([-0.00319078])]\n",
      "Iteration 488, Cost: 0.23311438641102908\n",
      "gradient_weights:  [array([[-1.01167425e-05, -1.88916475e-03,  1.88781067e-02],\n",
      "       [ 9.77718252e-06, -4.76409826e-04,  1.05547563e-02]]), array([[ 0.00359388],\n",
      "       [-0.00377926],\n",
      "       [ 0.01909625]])]\n",
      "gradients_biases:  [array([-1.71678964e-06, -2.02674059e-03, -1.13100282e-03]), array([-0.00319365])]\n",
      "Iteration 489, Cost: 0.2330704860272938\n",
      "gradient_weights:  [array([[-1.18285172e-05, -1.88769725e-03,  1.88848634e-02],\n",
      "       [ 1.14063386e-05, -4.73843102e-04,  1.05677447e-02]]), array([[ 0.00359927],\n",
      "       [-0.00379069],\n",
      "       [ 0.01911325]])]\n",
      "gradients_biases:  [array([-1.99819831e-06, -2.02917537e-03, -1.14962768e-03]), array([-0.00319651])]\n",
      "Iteration 490, Cost: 0.23302651726325768\n",
      "gradient_weights:  [array([[-1.35461632e-05, -1.88622516e-03,  1.88915944e-02],\n",
      "       [ 1.30338690e-05, -4.71268701e-04,  1.05807248e-02]]), array([[ 0.00360468],\n",
      "       [-0.00380213],\n",
      "       [ 0.01913022]])]\n",
      "gradients_biases:  [array([-2.27797848e-06, -2.03161771e-03, -1.16826284e-03]), array([-0.00319938])]\n",
      "Iteration 491, Cost: 0.23298248013599565\n",
      "gradient_weights:  [array([[-1.52697010e-05, -1.88474845e-03,  1.88982997e-02],\n",
      "       [ 1.46597591e-05, -4.68686584e-04,  1.05936966e-02]]), array([[ 0.00361009],\n",
      "       [-0.00381358],\n",
      "       [ 0.01914715]])]\n",
      "gradients_biases:  [array([-2.55612424e-06, -2.03406761e-03, -1.18690804e-03]), array([-0.00320226])]\n",
      "Iteration 492, Cost: 0.23293837466292072\n",
      "gradient_weights:  [array([[-1.69991506e-05, -1.88326708e-03,  1.89049794e-02],\n",
      "       [ 1.62839941e-05, -4.66096714e-04,  1.06066598e-02]]), array([[ 0.00361551],\n",
      "       [-0.00382504],\n",
      "       [ 0.01916404]])]\n",
      "gradients_biases:  [array([-2.83262968e-06, -2.03652510e-03, -1.20556299e-03]), array([-0.00320513])]\n",
      "Iteration 493, Cost: 0.23289420086178525\n",
      "gradient_weights:  [array([[-1.87345326e-05, -1.88178101e-03,  1.89116335e-02],\n",
      "       [ 1.79065592e-05, -4.63499050e-04,  1.06196146e-02]]), array([[ 0.00362094],\n",
      "       [-0.00383651],\n",
      "       [ 0.01918089]])]\n",
      "gradients_biases:  [array([-3.10748886e-06, -2.03899017e-03, -1.22422745e-03]), array([-0.00320802])]\n",
      "Iteration 494, Cost: 0.23284995875068248\n",
      "gradient_weights:  [array([[-2.04758673e-05, -1.88029021e-03,  1.89182621e-02],\n",
      "       [ 1.95274394e-05, -4.60893553e-04,  1.06325609e-02]]), array([[ 0.00362638],\n",
      "       [-0.00384799],\n",
      "       [ 0.0191977 ]])]\n",
      "gradients_biases:  [array([-3.38069585e-06, -2.04146285e-03, -1.24290112e-03]), array([-0.0032109])]\n",
      "Iteration 495, Cost: 0.232805648348048\n",
      "gradient_weights:  [array([[-2.22231751e-05, -1.87879464e-03,  1.89248650e-02],\n",
      "       [ 2.11466200e-05, -4.58280186e-04,  1.06454985e-02]]), array([[ 0.00363183],\n",
      "       [-0.00385949],\n",
      "       [ 0.01921447]])]\n",
      "gradients_biases:  [array([-3.65224469e-06, -2.04394313e-03, -1.26158374e-03]), array([-0.00321379])]\n",
      "Iteration 496, Cost: 0.23276126967266064\n",
      "gradient_weights:  [array([[-2.39764764e-05, -1.87729426e-03,  1.89314425e-02],\n",
      "       [ 2.27640859e-05, -4.55658909e-04,  1.06584275e-02]]), array([[ 0.00363729],\n",
      "       [-0.00387099],\n",
      "       [ 0.0192312 ]])]\n",
      "gradients_biases:  [array([-3.92212940e-06, -2.04643103e-03, -1.28027504e-03]), array([-0.00321668])]\n",
      "Iteration 497, Cost: 0.23271682274364439\n",
      "gradient_weights:  [array([[-2.57357920e-05, -1.87578903e-03,  1.89379945e-02],\n",
      "       [ 2.43798222e-05, -4.53029683e-04,  1.06713478e-02]]), array([[ 0.00364276],\n",
      "       [-0.0038825 ],\n",
      "       [ 0.0192479 ]])]\n",
      "gradients_biases:  [array([-4.19034402e-06, -2.04892656e-03, -1.29897474e-03]), array([-0.00321958])]\n",
      "Iteration 498, Cost: 0.23267230758046964\n",
      "gradient_weights:  [array([[-2.75011422e-05, -1.87427892e-03,  1.89445210e-02],\n",
      "       [ 2.59938137e-05, -4.50392469e-04,  1.06842593e-02]]), array([[ 0.00364823],\n",
      "       [-0.00389402],\n",
      "       [ 0.01926456]])]\n",
      "gradients_biases:  [array([-4.45688256e-06, -2.05142973e-03, -1.31768257e-03]), array([-0.00322247])]\n",
      "Iteration 499, Cost: 0.23262772420295447\n",
      "gradient_weights:  [array([[-2.92725478e-05, -1.87276390e-03,  1.89510220e-02],\n",
      "       [ 2.76060455e-05, -4.47747228e-04,  1.06971620e-02]]), array([[ 0.00365372],\n",
      "       [-0.00390556],\n",
      "       [ 0.01928117]])]\n",
      "gradients_biases:  [array([-4.72173900e-06, -2.05394055e-03, -1.33639826e-03]), array([-0.00322538])]\n",
      "Iteration 500, Cost: 0.23258307263126615\n",
      "gradient_weights:  [array([[-3.10500293e-05, -1.87124391e-03,  1.89574977e-02],\n",
      "       [ 2.92165022e-05, -4.45093920e-04,  1.07100559e-02]]), array([[ 0.00365921],\n",
      "       [-0.0039171 ],\n",
      "       [ 0.01929775]])]\n",
      "gradients_biases:  [array([-4.98490734e-06, -2.05645902e-03, -1.35512154e-03]), array([-0.00322828])]\n",
      "Iteration 501, Cost: 0.2325383528859226\n",
      "gradient_weights:  [array([[-3.28336076e-05, -1.86971893e-03,  1.89639480e-02],\n",
      "       [ 3.08251687e-05, -4.42432506e-04,  1.07229408e-02]]), array([[ 0.00366472],\n",
      "       [-0.00392866],\n",
      "       [ 0.01931429]])]\n",
      "gradients_biases:  [array([-5.24638154e-06, -2.05898517e-03, -1.37385213e-03]), array([-0.00323119])]\n",
      "Iteration 502, Cost: 0.23249356498779383\n",
      "gradient_weights:  [array([[-3.46233033e-05, -1.86818892e-03,  1.89703729e-02],\n",
      "       [ 3.24320299e-05, -4.39762946e-04,  1.07358167e-02]]), array([[ 0.00367023],\n",
      "       [-0.00394022],\n",
      "       [ 0.01933079]])]\n",
      "gradients_biases:  [array([-5.50615558e-06, -2.06151900e-03, -1.39258975e-03]), array([-0.0032341])]\n",
      "Iteration 503, Cost: 0.23244870895810332\n",
      "gradient_weights:  [array([[-3.64191372e-05, -1.86665384e-03,  1.89767725e-02],\n",
      "       [ 3.40370703e-05, -4.37085203e-04,  1.07486837e-02]]), array([[ 0.00367575],\n",
      "       [-0.0039518 ],\n",
      "       [ 0.01934724]])]\n",
      "gradients_biases:  [array([-5.76422339e-06, -2.06406052e-03, -1.41133414e-03]), array([-0.00323702])]\n",
      "Iteration 504, Cost: 0.2324037848184295\n",
      "gradient_weights:  [array([[-3.82211303e-05, -1.86511364e-03,  1.89831469e-02],\n",
      "       [ 3.56402746e-05, -4.34399235e-04,  1.07615415e-02]]), array([[ 0.00368128],\n",
      "       [-0.00396339],\n",
      "       [ 0.01936366]])]\n",
      "gradients_biases:  [array([-6.02057892e-06, -2.06660974e-03, -1.43008502e-03]), array([-0.00323994])]\n",
      "Iteration 505, Cost: 0.23235879259070735\n",
      "gradient_weights:  [array([[-4.00293035e-05, -1.86356831e-03,  1.89894960e-02],\n",
      "       [ 3.72416274e-05, -4.31705004e-04,  1.07743902e-02]]), array([[ 0.00368682],\n",
      "       [-0.00397498],\n",
      "       [ 0.01938004]])]\n",
      "gradients_biases:  [array([-6.27521609e-06, -2.06916666e-03, -1.44884212e-03]), array([-0.00324286])]\n",
      "Iteration 506, Cost: 0.23231373229722957\n",
      "gradient_weights:  [array([[-4.18436775e-05, -1.86201779e-03,  1.89958198e-02],\n",
      "       [ 3.88411133e-05, -4.29002469e-04,  1.07872297e-02]]), array([[ 0.00369237],\n",
      "       [-0.00398659],\n",
      "       [ 0.01939638]])]\n",
      "gradients_biases:  [array([-6.52812882e-06, -2.07173131e-03, -1.46760515e-03]), array([-0.00324578])]\n",
      "Iteration 507, Cost: 0.2322686039606485\n",
      "gradient_weights:  [array([[-4.36642735e-05, -1.86046204e-03,  1.90021185e-02],\n",
      "       [ 4.04387168e-05, -4.26291592e-04,  1.08000599e-02]]), array([[ 0.00369793],\n",
      "       [-0.00399821],\n",
      "       [ 0.01941268]])]\n",
      "gradients_biases:  [array([-6.77931101e-06, -2.07430369e-03, -1.48637385e-03]), array([-0.00324871])]\n",
      "Iteration 508, Cost: 0.23222340760397717\n",
      "gradient_weights:  [array([[-4.54911125e-05, -1.85890104e-03,  1.90083919e-02],\n",
      "       [ 4.20344223e-05, -4.23572333e-04,  1.08128808e-02]]), array([[ 0.0037035 ],\n",
      "       [-0.00400984],\n",
      "       [ 0.01942894]])]\n",
      "gradients_biases:  [array([-7.02875655e-06, -2.07688381e-03, -1.50514794e-03]), array([-0.00325164])]\n",
      "Iteration 509, Cost: 0.23217814325059105\n",
      "gradient_weights:  [array([[-4.73242156e-05, -1.85733474e-03,  1.90146403e-02],\n",
      "       [ 4.36282143e-05, -4.20844651e-04,  1.08256924e-02]]), array([[ 0.00370907],\n",
      "       [-0.00402148],\n",
      "       [ 0.01944517]])]\n",
      "gradients_biases:  [array([-7.27645932e-06, -2.07947168e-03, -1.52392715e-03]), array([-0.00325458])]\n",
      "Iteration 510, Cost: 0.2321328109242295\n",
      "gradient_weights:  [array([[-4.91636039e-05, -1.85576310e-03,  1.90208635e-02],\n",
      "       [ 4.52200771e-05, -4.18108508e-04,  1.08384946e-02]]), array([[ 0.00371466],\n",
      "       [-0.00403313],\n",
      "       [ 0.01946135]])]\n",
      "gradients_biases:  [array([-7.52241318e-06, -2.08206731e-03, -1.54271119e-03]), array([-0.00325751])]\n",
      "Iteration 511, Cost: 0.23208741064899735\n",
      "gradient_weights:  [array([[-5.10092985e-05, -1.85418610e-03,  1.90270616e-02],\n",
      "       [ 4.68099951e-05, -4.15363863e-04,  1.08512873e-02]]), array([[ 0.00372025],\n",
      "       [-0.00404479],\n",
      "       [ 0.01947749]])]\n",
      "gradients_biases:  [array([-7.76661201e-06, -2.08467072e-03, -1.56149980e-03]), array([-0.00326046])]\n",
      "Iteration 512, Cost: 0.23204194244936635\n",
      "gradient_weights:  [array([[-5.28613207e-05, -1.85260367e-03,  1.90332347e-02],\n",
      "       [ 4.83979524e-05, -4.12610676e-04,  1.08640705e-02]]), array([[ 0.00372586],\n",
      "       [-0.00405646],\n",
      "       [ 0.01949359]])]\n",
      "gradients_biases:  [array([-8.00904963e-06, -2.08728190e-03, -1.58029269e-03]), array([-0.0032634])]\n",
      "Iteration 513, Cost: 0.23199640635017654\n",
      "gradient_weights:  [array([[-5.47196918e-05, -1.85101580e-03,  1.90393828e-02],\n",
      "       [ 4.99839334e-05, -4.09848908e-04,  1.08768441e-02]]), array([[ 0.00373147],\n",
      "       [-0.00406814],\n",
      "       [ 0.01950965]])]\n",
      "gradients_biases:  [array([-8.24971989e-06, -2.08990088e-03, -1.59908960e-03]), array([-0.00326635])]\n",
      "Iteration 514, Cost: 0.2319508023766382\n",
      "gradient_weights:  [array([[-5.65844330e-05, -1.84942244e-03,  1.90455058e-02],\n",
      "       [ 5.15679222e-05, -4.07078518e-04,  1.08896081e-02]]), array([[ 0.00373709],\n",
      "       [-0.00407983],\n",
      "       [ 0.01952567]])]\n",
      "gradients_biases:  [array([-8.48861661e-06, -2.09252765e-03, -1.61789024e-03]), array([-0.0032693])]\n",
      "Iteration 515, Cost: 0.23190513055433293\n",
      "gradient_weights:  [array([[-5.84555658e-05, -1.84782355e-03,  1.90516039e-02],\n",
      "       [ 5.31499029e-05, -4.04299467e-04,  1.09023624e-02]]), array([[ 0.00374272],\n",
      "       [-0.00409153],\n",
      "       [ 0.01954165]])]\n",
      "gradients_biases:  [array([-8.72573361e-06, -2.09516224e-03, -1.63669434e-03]), array([-0.00327225])]\n",
      "Iteration 516, Cost: 0.23185939090921565\n",
      "gradient_weights:  [array([[-6.03331115e-05, -1.84621910e-03,  1.90576771e-02],\n",
      "       [ 5.47298597e-05, -4.01511714e-04,  1.09151070e-02]]), array([[ 0.00374837],\n",
      "       [-0.00410325],\n",
      "       [ 0.01955759]])]\n",
      "gradients_biases:  [array([-8.96106467e-06, -2.09780465e-03, -1.65550163e-03]), array([-0.0032752])]\n",
      "Iteration 517, Cost: 0.2318135834676156\n",
      "gradient_weights:  [array([[-6.22170915e-05, -1.84460905e-03,  1.90637253e-02],\n",
      "       [ 5.63077764e-05, -3.98715219e-04,  1.09278418e-02]]), array([[ 0.00375402],\n",
      "       [-0.00411497],\n",
      "       [ 0.01957348]])]\n",
      "gradients_biases:  [array([-9.19460360e-06, -2.10045489e-03, -1.67431182e-03]), array([-0.00327816])]\n",
      "Iteration 518, Cost: 0.2317677082562386\n",
      "gradient_weights:  [array([[-6.41075275e-05, -1.84299335e-03,  1.90697487e-02],\n",
      "       [ 5.78836372e-05, -3.95909943e-04,  1.09405667e-02]]), array([[ 0.00375968],\n",
      "       [-0.00412671],\n",
      "       [ 0.01958934]])]\n",
      "gradients_biases:  [array([-9.42634417e-06, -2.10311298e-03, -1.69312463e-03]), array([-0.00328112])]\n",
      "Iteration 519, Cost: 0.23172176530216787\n",
      "gradient_weights:  [array([[-6.60044408e-05, -1.84137197e-03,  1.90757472e-02],\n",
      "       [ 5.94574260e-05, -3.93095844e-04,  1.09532818e-02]]), array([[ 0.00376535],\n",
      "       [-0.00413845],\n",
      "       [ 0.01960516]])]\n",
      "gradients_biases:  [array([-9.65628015e-06, -2.10577891e-03, -1.71193980e-03]), array([-0.00328409])]\n",
      "Iteration 520, Cost: 0.23167575463286624\n",
      "gradient_weights:  [array([[-6.79078532e-05, -1.83974487e-03,  1.90817208e-02],\n",
      "       [ 6.10291266e-05, -3.90272883e-04,  1.09659869e-02]]), array([[ 0.00377102],\n",
      "       [-0.00415021],\n",
      "       [ 0.01962094]])]\n",
      "gradients_biases:  [array([-9.88440530e-06, -2.10845271e-03, -1.73075705e-03]), array([-0.00328705])]\n",
      "Iteration 521, Cost: 0.23162967627617737\n",
      "gradient_weights:  [array([[-6.98177862e-05, -1.83811201e-03,  1.90876697e-02],\n",
      "       [ 6.25987229e-05, -3.87441019e-04,  1.09786820e-02]]), array([[ 0.00377671],\n",
      "       [-0.00416197],\n",
      "       [ 0.01963667]])]\n",
      "gradients_biases:  [array([-1.01107134e-05, -2.11113438e-03, -1.74957609e-03]), array([-0.00329002])]\n",
      "Iteration 522, Cost: 0.23158353026032735\n",
      "gradient_weights:  [array([[-7.17342616e-05, -1.83647336e-03,  1.90935938e-02],\n",
      "       [ 6.41661987e-05, -3.84600212e-04,  1.09913670e-02]]), array([[ 0.00378241],\n",
      "       [-0.00417375],\n",
      "       [ 0.01965237]])]\n",
      "gradients_biases:  [array([-1.03351981e-05, -2.11382393e-03, -1.76839665e-03]), array([-0.003293])]\n",
      "Iteration 523, Cost: 0.23153731661392646\n",
      "gradient_weights:  [array([[-7.36573010e-05, -1.83482887e-03,  1.90994932e-02],\n",
      "       [ 6.57315378e-05, -3.81750421e-04,  1.10040419e-02]]), array([[ 0.00378812],\n",
      "       [-0.00418554],\n",
      "       [ 0.01966802]])]\n",
      "gradients_biases:  [array([-1.05578532e-05, -2.11652138e-03, -1.78721846e-03]), array([-0.00329597])]\n",
      "Iteration 524, Cost: 0.23149103536597063\n",
      "gradient_weights:  [array([[-7.55869262e-05, -1.83317851e-03,  1.91053679e-02],\n",
      "       [ 6.72947238e-05, -3.78891607e-04,  1.10167067e-02]]), array([[ 0.00379383],\n",
      "       [-0.00419733],\n",
      "       [ 0.01968364]])]\n",
      "gradients_biases:  [array([-1.07786724e-05, -2.11922673e-03, -1.80604123e-03]), array([-0.00329895])]\n",
      "Iteration 525, Cost: 0.2314446865458431\n",
      "gradient_weights:  [array([[-7.75231590e-05, -1.83152223e-03,  1.91112179e-02],\n",
      "       [ 6.88557405e-05, -3.76023728e-04,  1.10293612e-02]]), array([[ 0.00379956],\n",
      "       [-0.00420914],\n",
      "       [ 0.01969921]])]\n",
      "gradients_biases:  [array([-1.09976493e-05, -2.12193999e-03, -1.82486469e-03]), array([-0.00330193])]\n",
      "Iteration 526, Cost: 0.23139827018331588\n",
      "gradient_weights:  [array([[-7.94660214e-05, -1.82986000e-03,  1.91170432e-02],\n",
      "       [ 7.04145714e-05, -3.73146744e-04,  1.10420055e-02]]), array([[ 0.00380529],\n",
      "       [-0.00422096],\n",
      "       [ 0.01971474]])]\n",
      "gradients_biases:  [array([-1.12147778e-05, -2.12466117e-03, -1.84368856e-03]), array([-0.00330491])]\n",
      "Iteration 527, Cost: 0.23135178630855158\n",
      "gradient_weights:  [array([[-8.14155351e-05, -1.82819178e-03,  1.91228439e-02],\n",
      "       [ 7.19712001e-05, -3.70260614e-04,  1.10546394e-02]]), array([[ 0.00381104],\n",
      "       [-0.00423279],\n",
      "       [ 0.01973023]])]\n",
      "gradients_biases:  [array([-1.14300515e-05, -2.12739029e-03, -1.86251256e-03]), array([-0.0033079])]\n",
      "Iteration 528, Cost: 0.2313052349521048\n",
      "gradient_weights:  [array([[-8.33717222e-05, -1.82651753e-03,  1.91286200e-02],\n",
      "       [ 7.35256102e-05, -3.67365298e-04,  1.10672629e-02]]), array([[ 0.00381679],\n",
      "       [-0.00424463],\n",
      "       [ 0.01974568]])]\n",
      "gradients_biases:  [array([-1.16434639e-05, -2.13012735e-03, -1.88133642e-03]), array([-0.00331088])]\n",
      "Iteration 529, Cost: 0.231258616144924\n",
      "gradient_weights:  [array([[-8.53346047e-05, -1.82483721e-03,  1.91343715e-02],\n",
      "       [ 7.50777850e-05, -3.64460755e-04,  1.10798761e-02]]), array([[ 0.00382256],\n",
      "       [-0.00425648],\n",
      "       [ 0.01976109]])]\n",
      "gradients_biases:  [array([-1.18550090e-05, -2.13287236e-03, -1.90015986e-03]), array([-0.00331387])]\n",
      "Iteration 530, Cost: 0.23121192991835282\n",
      "gradient_weights:  [array([[-8.73042046e-05, -1.82315079e-03,  1.91400985e-02],\n",
      "       [ 7.66277081e-05, -3.61546944e-04,  1.10924787e-02]]), array([[ 0.00382833],\n",
      "       [-0.00426834],\n",
      "       [ 0.01977646]])]\n",
      "gradients_biases:  [array([-1.20646801e-05, -2.13562533e-03, -1.91898259e-03]), array([-0.00331687])]\n",
      "Iteration 531, Cost: 0.23116517630413191\n",
      "gradient_weights:  [array([[-8.92805441e-05, -1.82145821e-03,  1.91458010e-02],\n",
      "       [ 7.81753629e-05, -3.58623825e-04,  1.11050707e-02]]), array([[ 0.00383411],\n",
      "       [-0.00428021],\n",
      "       [ 0.01979178]])]\n",
      "gradients_biases:  [array([-1.22724711e-05, -2.13838628e-03, -1.93780435e-03]), array([-0.00331986])]\n",
      "Iteration 532, Cost: 0.23111835533440045\n",
      "gradient_weights:  [array([[-9.12636451e-05, -1.81975945e-03,  1.91514790e-02],\n",
      "       [ 7.97207326e-05, -3.55691356e-04,  1.11176522e-02]]), array([[ 0.00383991],\n",
      "       [-0.00429209],\n",
      "       [ 0.01980707]])]\n",
      "gradients_biases:  [array([-1.24783755e-05, -2.14115521e-03, -1.95662485e-03]), array([-0.00332286])]\n",
      "Iteration 533, Cost: 0.23107146704169784\n",
      "gradient_weights:  [array([[-9.32535300e-05, -1.81805447e-03,  1.91571325e-02],\n",
      "       [ 8.12638005e-05, -3.52749497e-04,  1.11302230e-02]]), array([[ 0.00384571],\n",
      "       [-0.00430399],\n",
      "       [ 0.01982231]])]\n",
      "gradients_biases:  [array([-1.26823870e-05, -2.14393214e-03, -1.97544382e-03]), array([-0.00332586])]\n",
      "Iteration 534, Cost: 0.2310245114589652\n",
      "gradient_weights:  [array([[-9.52502209e-05, -1.81634322e-03,  1.91627617e-02],\n",
      "       [ 8.28045501e-05, -3.49798207e-04,  1.11427831e-02]]), array([[ 0.00385152],\n",
      "       [-0.00431589],\n",
      "       [ 0.01983751]])]\n",
      "gradients_biases:  [array([-1.28844992e-05, -2.14671707e-03, -1.99426098e-03]), array([-0.00332886])]\n",
      "Iteration 535, Cost: 0.23097748861954726\n",
      "gradient_weights:  [array([[-9.72537401e-05, -1.81462566e-03,  1.91683664e-02],\n",
      "       [ 8.43429643e-05, -3.46837445e-04,  1.11553325e-02]]), array([[ 0.00385734],\n",
      "       [-0.0043278 ],\n",
      "       [ 0.01985267]])]\n",
      "gradients_biases:  [array([-1.30847056e-05, -2.14951001e-03, -2.01307605e-03]), array([-0.00333186])]\n",
      "Iteration 536, Cost: 0.23093039855719363\n",
      "gradient_weights:  [array([[-9.92641100e-05, -1.81290177e-03,  1.91739467e-02],\n",
      "       [ 8.58790265e-05, -3.43867170e-04,  1.11678710e-02]]), array([[ 0.00386317],\n",
      "       [-0.00433973],\n",
      "       [ 0.01986779]])]\n",
      "gradients_biases:  [array([-1.32829999e-05, -2.15231097e-03, -2.03188876e-03]), array([-0.00333487])]\n",
      "Iteration 537, Cost: 0.23088324130606086\n",
      "gradient_weights:  [array([[-1.01281353e-04, -1.81117148e-03,  1.91795027e-02],\n",
      "       [ 8.74127196e-05, -3.40887341e-04,  1.11803987e-02]]), array([[ 0.00386901],\n",
      "       [-0.00435166],\n",
      "       [ 0.01988286]])]\n",
      "gradients_biases:  [array([-1.34793757e-05, -2.15511997e-03, -2.05069882e-03]), array([-0.00333788])]\n",
      "Iteration 538, Cost: 0.23083601690071354\n",
      "gradient_weights:  [array([[-1.03305491e-04, -1.80943478e-03,  1.91850343e-02],\n",
      "       [ 8.89440269e-05, -3.37897917e-04,  1.11929154e-02]]), array([[ 0.00387486],\n",
      "       [-0.00436361],\n",
      "       [ 0.0198979 ]])]\n",
      "gradients_biases:  [array([-1.36738265e-05, -2.15793701e-03, -2.06950595e-03]), array([-0.00334089])]\n",
      "Iteration 539, Cost: 0.23078872537612638\n",
      "gradient_weights:  [array([[-1.05336547e-04, -1.80769162e-03,  1.91905417e-02],\n",
      "       [ 9.04729313e-05, -3.34898857e-04,  1.12054211e-02]]), array([[ 0.00388072],\n",
      "       [-0.00437556],\n",
      "       [ 0.01991289]])]\n",
      "gradients_biases:  [array([-1.38663459e-05, -2.16076211e-03, -2.08830989e-03]), array([-0.0033439])]\n",
      "Iteration 540, Cost: 0.2307413667676858\n",
      "gradient_weights:  [array([[-1.07374544e-04, -1.80594195e-03,  1.91960248e-02],\n",
      "       [ 9.19994158e-05, -3.31890119e-04,  1.12179159e-02]]), array([[ 0.00388659],\n",
      "       [-0.00438753],\n",
      "       [ 0.01992784]])]\n",
      "gradients_biases:  [array([-1.40569275e-05, -2.16359527e-03, -2.10711035e-03]), array([-0.00334692])]\n",
      "Iteration 541, Cost: 0.2306939411111913\n",
      "gradient_weights:  [array([[-1.09419503e-04, -1.80418575e-03,  1.92014836e-02],\n",
      "       [ 9.35234633e-05, -3.28871663e-04,  1.12303995e-02]]), array([[ 0.00389247],\n",
      "       [-0.0043995 ],\n",
      "       [ 0.01994275]])]\n",
      "gradients_biases:  [array([-1.42455647e-05, -2.16643650e-03, -2.12590706e-03]), array([-0.00334993])]\n",
      "Iteration 542, Cost: 0.23064644844285728\n",
      "gradient_weights:  [array([[-1.11471448e-04, -1.80242296e-03,  1.92069182e-02],\n",
      "       [ 9.50450569e-05, -3.25843447e-04,  1.12428720e-02]]), array([[ 0.00389836],\n",
      "       [-0.00441149],\n",
      "       [ 0.01995761]])]\n",
      "gradients_biases:  [array([-1.44322512e-05, -2.16928582e-03, -2.14469973e-03]), array([-0.00335295])]\n",
      "Iteration 543, Cost: 0.23059888879931473\n",
      "gradient_weights:  [array([[-1.13530401e-04, -1.80065355e-03,  1.92123287e-02],\n",
      "       [ 9.65641792e-05, -3.22805430e-04,  1.12553333e-02]]), array([[ 0.00390426],\n",
      "       [-0.00442349],\n",
      "       [ 0.01997244]])]\n",
      "gradients_biases:  [array([-1.46169805e-05, -2.17214323e-03, -2.16348809e-03]), array([-0.00335597])]\n",
      "Iteration 544, Cost: 0.23055126221761266\n",
      "gradient_weights:  [array([[-1.15596385e-04, -1.79887749e-03,  1.92177150e-02],\n",
      "       [ 9.80808132e-05, -3.19757570e-04,  1.12677833e-02]]), array([[ 0.00391017],\n",
      "       [-0.0044355 ],\n",
      "       [ 0.01998722]])]\n",
      "gradients_biases:  [array([-1.47997460e-05, -2.17500874e-03, -2.18227187e-03]), array([-0.00335899])]\n",
      "Iteration 545, Cost: 0.23050356873521996\n",
      "gradient_weights:  [array([[-1.17669422e-04, -1.79709472e-03,  1.92230771e-02],\n",
      "       [ 9.95949416e-05, -3.16699828e-04,  1.12802221e-02]]), array([[ 0.00391609],\n",
      "       [-0.00444751],\n",
      "       [ 0.02000196]])]\n",
      "gradients_biases:  [array([-1.49805413e-05, -2.17788237e-03, -2.20105078e-03]), array([-0.00336202])]\n",
      "Iteration 546, Cost: 0.230455808390027\n",
      "gradient_weights:  [array([[-0.00011975, -0.00179531,  0.01922842],\n",
      "       [ 0.00010111, -0.00031363,  0.01129265]]), array([[ 0.00392202],\n",
      "       [-0.00445954],\n",
      "       [ 0.02001666]])]\n",
      "gradients_biases:  [array([-1.51593599e-05, -2.18076412e-03, -2.21982455e-03]), array([-0.00336505])]\n",
      "Iteration 547, Cost: 0.23040798122034692\n",
      "gradient_weights:  [array([[-0.00012184, -0.00179351,  0.01923373],\n",
      "       [ 0.00010262, -0.00031055,  0.01130507]]), array([[ 0.00392796],\n",
      "       [-0.00447158],\n",
      "       [ 0.02003131]])]\n",
      "gradients_biases:  [array([-1.53361953e-05, -2.18365400e-03, -2.23859291e-03]), array([-0.00336807])]\n",
      "Iteration 548, Cost: 0.23036008726491788\n",
      "gradient_weights:  [array([[-0.00012393, -0.00179171,  0.01923902],\n",
      "       [ 0.00010412, -0.00030747,  0.01131747]]), array([[ 0.00393391],\n",
      "       [-0.00448363],\n",
      "       [ 0.02004592]])]\n",
      "gradients_biases:  [array([-1.55110410e-05, -2.18655203e-03, -2.25735556e-03]), array([-0.0033711])]\n",
      "Iteration 549, Cost: 0.2303121265629041\n",
      "gradient_weights:  [array([[-0.00012603, -0.0017899 ,  0.01924428],\n",
      "       [ 0.00010563, -0.00030437,  0.01132986]]), array([[ 0.00393987],\n",
      "       [-0.00449569],\n",
      "       [ 0.02006049]])]\n",
      "gradients_biases:  [array([-1.56838904e-05, -2.18945821e-03, -2.27611225e-03]), array([-0.00337413])]\n",
      "Iteration 550, Cost: 0.23026409915389798\n",
      "gradient_weights:  [array([[-0.00012814, -0.00178808,  0.01924953],\n",
      "       [ 0.00010713, -0.00030126,  0.01134224]]), array([[ 0.00394584],\n",
      "       [-0.00450776],\n",
      "       [ 0.02007502]])]\n",
      "gradients_biases:  [array([-1.58547370e-05, -2.19237255e-03, -2.29486268e-03]), array([-0.00337717])]\n",
      "Iteration 551, Cost: 0.23021600507792106\n",
      "gradient_weights:  [array([[-0.00013026, -0.00178626,  0.01925474],\n",
      "       [ 0.00010863, -0.00029814,  0.01135461]]), array([[ 0.00395181],\n",
      "       [-0.00451984],\n",
      "       [ 0.02008951]])]\n",
      "gradients_biases:  [array([-1.60235742e-05, -2.19529507e-03, -2.31360660e-03]), array([-0.0033802])]\n",
      "Iteration 552, Cost: 0.23016784437542656\n",
      "gradient_weights:  [array([[-0.00013238, -0.00178442,  0.01925994],\n",
      "       [ 0.00011012, -0.00029502,  0.01136697]]), array([[ 0.0039578 ],\n",
      "       [-0.00453193],\n",
      "       [ 0.02010395]])]\n",
      "gradients_biases:  [array([-1.61903956e-05, -2.19822577e-03, -2.33234370e-03]), array([-0.00338324])]\n",
      "Iteration 553, Cost: 0.23011961708730028\n",
      "gradient_weights:  [array([[-0.00013451, -0.00178259,  0.01926511],\n",
      "       [ 0.00011162, -0.00029188,  0.01137932]]), array([[ 0.0039638 ],\n",
      "       [-0.00454404],\n",
      "       [ 0.02011835]])]\n",
      "gradients_biases:  [array([-1.63551945e-05, -2.20116467e-03, -2.35107374e-03]), array([-0.00338628])]\n",
      "Iteration 554, Cost: 0.2300713232548623\n",
      "gradient_weights:  [array([[-0.00013665, -0.00178074,  0.01927025],\n",
      "       [ 0.00011311, -0.00028873,  0.01139165]]), array([[ 0.00396981],\n",
      "       [-0.00455615],\n",
      "       [ 0.02013271]])]\n",
      "gradients_biases:  [array([-1.65179644e-05, -2.20411177e-03, -2.36979641e-03]), array([-0.00338931])]\n",
      "Iteration 555, Cost: 0.230022962919869\n",
      "gradient_weights:  [array([[-0.00013879, -0.00177889,  0.01927538],\n",
      "       [ 0.00011459, -0.00028557,  0.01140398]]), array([[ 0.00397583],\n",
      "       [-0.00456827],\n",
      "       [ 0.02014702]])]\n",
      "gradients_biases:  [array([-1.66786987e-05, -2.20706708e-03, -2.38851146e-03]), array([-0.00339236])]\n",
      "Iteration 556, Cost: 0.2299745361245144\n",
      "gradient_weights:  [array([[-0.00014094, -0.00177703,  0.01928048],\n",
      "       [ 0.00011608, -0.0002824 ,  0.01141629]]), array([[ 0.00398186],\n",
      "       [-0.0045804 ],\n",
      "       [ 0.02016129]])]\n",
      "gradients_biases:  [array([-1.68373907e-05, -2.21003061e-03, -2.40721860e-03]), array([-0.0033954])]\n",
      "Iteration 557, Cost: 0.2299260429114316\n",
      "gradient_weights:  [array([[-0.0001431 , -0.00177516,  0.01928555],\n",
      "       [ 0.00011756, -0.00027922,  0.01142859]]), array([[ 0.0039879 ],\n",
      "       [-0.00459255],\n",
      "       [ 0.02017552]])]\n",
      "gradients_biases:  [array([-1.69940340e-05, -2.21300238e-03, -2.42591756e-03]), array([-0.00339844])]\n",
      "Iteration 558, Cost: 0.2298774833236946\n",
      "gradient_weights:  [array([[-0.00014527, -0.00177329,  0.0192906 ],\n",
      "       [ 0.00011904, -0.00027603,  0.01144087]]), array([[ 0.00399394],\n",
      "       [-0.0046047 ],\n",
      "       [ 0.0201897 ]])]\n",
      "gradients_biases:  [array([-1.71486219e-05, -2.21598239e-03, -2.44460806e-03]), array([-0.00340149])]\n",
      "Iteration 559, Cost: 0.22982885740482006\n",
      "gradient_weights:  [array([[-0.00014745, -0.00177141,  0.01929563],\n",
      "       [ 0.00012052, -0.00027283,  0.01145315]]), array([[ 0.004     ],\n",
      "       [-0.00461686],\n",
      "       [ 0.02020384]])]\n",
      "gradients_biases:  [array([-1.73011478e-05, -2.21897065e-03, -2.46328984e-03]), array([-0.00340453])]\n",
      "Iteration 560, Cost: 0.22978016519876845\n",
      "gradient_weights:  [array([[-0.00014963, -0.00176952,  0.01930063],\n",
      "       [ 0.00012199, -0.00026962,  0.01146541]]), array([[ 0.00400607],\n",
      "       [-0.00462904],\n",
      "       [ 0.02021794]])]\n",
      "gradients_biases:  [array([-1.74516051e-05, -2.22196718e-03, -2.48196260e-03]), array([-0.00340758])]\n",
      "Iteration 561, Cost: 0.22973140674994613\n",
      "gradient_weights:  [array([[-0.00015182, -0.00176763,  0.01930561],\n",
      "       [ 0.00012347, -0.0002664 ,  0.01147766]]), array([[ 0.00401215],\n",
      "       [-0.00464122],\n",
      "       [ 0.020232  ]])]\n",
      "gradients_biases:  [array([-1.75999871e-05, -2.22497197e-03, -2.50062608e-03]), array([-0.00341063])]\n",
      "Iteration 562, Cost: 0.22968258210320647\n",
      "gradient_weights:  [array([[-0.00015401, -0.00176572,  0.01931057],\n",
      "       [ 0.00012493, -0.00026317,  0.0114899 ]]), array([[ 0.00401824],\n",
      "       [-0.00465342],\n",
      "       [ 0.02024601]])]\n",
      "gradients_biases:  [array([-1.77462873e-05, -2.22798506e-03, -2.51928001e-03]), array([-0.00341368])]\n",
      "Iteration 563, Cost: 0.22963369130385183\n",
      "gradient_weights:  [array([[-0.00015622, -0.00176381,  0.0193155 ],\n",
      "       [ 0.0001264 , -0.00025992,  0.01150212]]), array([[ 0.00402434],\n",
      "       [-0.00466562],\n",
      "       [ 0.02025998]])]\n",
      "gradients_biases:  [array([-1.78904989e-05, -2.23100643e-03, -2.53792411e-03]), array([-0.00341673])]\n",
      "Iteration 564, Cost: 0.22958473439763488\n",
      "gradient_weights:  [array([[-0.00015843, -0.0017619 ,  0.01932041],\n",
      "       [ 0.00012786, -0.00025667,  0.01151434]]), array([[ 0.00403045],\n",
      "       [-0.00467784],\n",
      "       [ 0.02027391]])]\n",
      "gradients_biases:  [array([-1.80326154e-05, -2.23403610e-03, -2.55655810e-03]), array([-0.00341978])]\n",
      "Iteration 565, Cost: 0.22953571143076035\n",
      "gradient_weights:  [array([[-0.00016065, -0.00175997,  0.01932529],\n",
      "       [ 0.00012932, -0.00025341,  0.01152654]]), array([[ 0.00403657],\n",
      "       [-0.00469007],\n",
      "       [ 0.02028779]])]\n",
      "gradients_biases:  [array([-1.81726300e-05, -2.23707409e-03, -2.57518172e-03]), array([-0.00342283])]\n",
      "Iteration 566, Cost: 0.22948662244988627\n",
      "gradient_weights:  [array([[-0.00016287, -0.00175804,  0.01933015],\n",
      "       [ 0.00013078, -0.00025013,  0.01153873]]), array([[ 0.0040427 ],\n",
      "       [-0.0047023 ],\n",
      "       [ 0.02030163]])]\n",
      "gradients_biases:  [array([-1.83105361e-05, -2.24012040e-03, -2.59379468e-03]), array([-0.00342589])]\n",
      "Iteration 567, Cost: 0.229437467502126\n",
      "gradient_weights:  [array([[-0.00016511, -0.0017561 ,  0.01933499],\n",
      "       [ 0.00013224, -0.00024684,  0.0115509 ]]), array([[ 0.00404884],\n",
      "       [-0.00471455],\n",
      "       [ 0.02031542]])]\n",
      "gradients_biases:  [array([-1.84463271e-05, -2.24317504e-03, -2.61239672e-03]), array([-0.00342894])]\n",
      "Iteration 568, Cost: 0.22938824663504936\n",
      "gradient_weights:  [array([[-0.00016735, -0.00175415,  0.0193398 ],\n",
      "       [ 0.00013369, -0.00024355,  0.01156307]]), array([[ 0.00405499],\n",
      "       [-0.00472681],\n",
      "       [ 0.02032917]])]\n",
      "gradients_biases:  [array([-1.85799962e-05, -2.24623803e-03, -2.63098756e-03]), array([-0.003432])]\n",
      "Iteration 569, Cost: 0.2293389598966845\n",
      "gradient_weights:  [array([[-0.0001696 , -0.00175219,  0.01934459],\n",
      "       [ 0.00013514, -0.00024024,  0.01157522]]), array([[ 0.00406115],\n",
      "       [-0.00473907],\n",
      "       [ 0.02034288]])]\n",
      "gradients_biases:  [array([-1.87115367e-05, -2.24930936e-03, -2.64956693e-03]), array([-0.00343505])]\n",
      "Iteration 570, Cost: 0.2292896073355191\n",
      "gradient_weights:  [array([[-0.00017186, -0.00175023,  0.01934936],\n",
      "       [ 0.00013658, -0.00023692,  0.01158736]]), array([[ 0.00406732],\n",
      "       [-0.00475135],\n",
      "       [ 0.02035654]])]\n",
      "gradients_biases:  [array([-1.88409420e-05, -2.25238906e-03, -2.66813456e-03]), array([-0.00343811])]\n",
      "Iteration 571, Cost: 0.22924018900050214\n",
      "gradient_weights:  [array([[-0.00017412, -0.00174826,  0.0193541 ],\n",
      "       [ 0.00013803, -0.00023359,  0.01159948]]), array([[ 0.0040735 ],\n",
      "       [-0.00476364],\n",
      "       [ 0.02037017]])]\n",
      "gradients_biases:  [array([-1.89682054e-05, -2.25547713e-03, -2.68669018e-03]), array([-0.00344117])]\n",
      "Iteration 572, Cost: 0.2291907049410455\n",
      "gradient_weights:  [array([[-0.00017639, -0.00174628,  0.01935882],\n",
      "       [ 0.00013947, -0.00023025,  0.0116116 ]]), array([[ 0.0040797 ],\n",
      "       [-0.00477594],\n",
      "       [ 0.02038374]])]\n",
      "gradients_biases:  [array([-1.90933202e-05, -2.25857357e-03, -2.70523350e-03]), array([-0.00344423])]\n",
      "Iteration 573, Cost: 0.229141155207025\n",
      "gradient_weights:  [array([[-0.00017867, -0.0017443 ,  0.01936352],\n",
      "       [ 0.0001409 , -0.0002269 ,  0.0116237 ]]), array([[ 0.0040859 ],\n",
      "       [-0.00478825],\n",
      "       [ 0.02039727]])]\n",
      "gradients_biases:  [array([-1.92162795e-05, -2.26167841e-03, -2.72376427e-03]), array([-0.00344728])]\n",
      "Iteration 574, Cost: 0.2290915398487826\n",
      "gradient_weights:  [array([[-0.00018096, -0.0017423 ,  0.01936819],\n",
      "       [ 0.00014234, -0.00022354,  0.01163578]]), array([[ 0.00409211],\n",
      "       [-0.00480056],\n",
      "       [ 0.02041076]])]\n",
      "gradients_biases:  [array([-1.93370768e-05, -2.26479165e-03, -2.74228221e-03]), array([-0.00345034])]\n",
      "Iteration 575, Cost: 0.2290418589171272\n",
      "gradient_weights:  [array([[-0.00018325, -0.0017403 ,  0.01937284],\n",
      "       [ 0.00014377, -0.00022017,  0.01164786]]), array([[ 0.00409833],\n",
      "       [-0.00481289],\n",
      "       [ 0.02042421]])]\n",
      "gradients_biases:  [array([-1.94557053e-05, -2.26791330e-03, -2.76078705e-03]), array([-0.00345341])]\n",
      "Iteration 576, Cost: 0.22899211246333667\n",
      "gradient_weights:  [array([[-0.00018556, -0.00173829,  0.01937747],\n",
      "       [ 0.00014519, -0.00021678,  0.01165992]]), array([[ 0.00410457],\n",
      "       [-0.00482523],\n",
      "       [ 0.02043761]])]\n",
      "gradients_biases:  [array([-1.95721582e-05, -2.27104337e-03, -2.77927852e-03]), array([-0.00345647])]\n",
      "Iteration 577, Cost: 0.22894230053915904\n",
      "gradient_weights:  [array([[-0.00018787, -0.00173627,  0.01938207],\n",
      "       [ 0.00014662, -0.00021338,  0.01167197]]), array([[ 0.00411081],\n",
      "       [-0.00483758],\n",
      "       [ 0.02045097]])]\n",
      "gradients_biases:  [array([-1.96864288e-05, -2.27418187e-03, -2.79775635e-03]), array([-0.00345953])]\n",
      "Iteration 578, Cost: 0.2288924231968139\n",
      "gradient_weights:  [array([[-0.00019019, -0.00173424,  0.01938665],\n",
      "       [ 0.00014804, -0.00020998,  0.011684  ]]), array([[ 0.00411707],\n",
      "       [-0.00484994],\n",
      "       [ 0.02046428]])]\n",
      "gradients_biases:  [array([-1.97985103e-05, -2.27732881e-03, -2.81622027e-03]), array([-0.00346259])]\n",
      "Iteration 579, Cost: 0.22884248048899425\n",
      "gradient_weights:  [array([[-0.00019251, -0.00173221,  0.01939121],\n",
      "       [ 0.00014946, -0.00020656,  0.01169602]]), array([[ 0.00412333],\n",
      "       [-0.00486231],\n",
      "       [ 0.02047755]])]\n",
      "gradients_biases:  [array([-1.99083961e-05, -2.28048420e-03, -2.83467001e-03]), array([-0.00346565])]\n",
      "Iteration 580, Cost: 0.2287924724688674\n",
      "gradient_weights:  [array([[-0.00019485, -0.00173017,  0.01939574],\n",
      "       [ 0.00015087, -0.00020313,  0.01170803]]), array([[ 0.00412961],\n",
      "       [-0.00487469],\n",
      "       [ 0.02049077]])]\n",
      "gradients_biases:  [array([-2.00160793e-05, -2.28364805e-03, -2.85310530e-03]), array([-0.00346871])]\n",
      "Iteration 581, Cost: 0.22874239919007686\n",
      "gradient_weights:  [array([[-0.00019719, -0.00172812,  0.01940025],\n",
      "       [ 0.00015229, -0.00019969,  0.01172003]]), array([[ 0.00413589],\n",
      "       [-0.00488708],\n",
      "       [ 0.02050395]])]\n",
      "gradients_biases:  [array([-2.01215531e-05, -2.28682036e-03, -2.87152587e-03]), array([-0.00347178])]\n",
      "Iteration 582, Cost: 0.2286922607067436\n",
      "gradient_weights:  [array([[-0.00019954, -0.00172606,  0.01940474],\n",
      "       [ 0.0001537 , -0.00019623,  0.01173201]]), array([[ 0.00414219],\n",
      "       [-0.00489948],\n",
      "       [ 0.02051709]])]\n",
      "gradients_biases:  [array([-2.02248108e-05, -2.29000116e-03, -2.88993146e-03]), array([-0.00347484])]\n",
      "Iteration 583, Cost: 0.22864205707346735\n",
      "gradient_weights:  [array([[-0.0002019 , -0.00172399,  0.0194092 ],\n",
      "       [ 0.0001551 , -0.00019277,  0.01174398]]), array([[ 0.0041485 ],\n",
      "       [-0.00491189],\n",
      "       [ 0.02053018]])]\n",
      "gradients_biases:  [array([-2.03258456e-05, -2.29319044e-03, -2.90832180e-03]), array([-0.0034779])]\n",
      "Iteration 584, Cost: 0.22859178834532823\n",
      "gradient_weights:  [array([[-0.00020427, -0.00172192,  0.01941364],\n",
      "       [ 0.0001565 , -0.00018929,  0.01175593]]), array([[ 0.00415482],\n",
      "       [-0.00492431],\n",
      "       [ 0.02054322]])]\n",
      "gradients_biases:  [array([-2.04246507e-05, -2.29638822e-03, -2.92669661e-03]), array([-0.00348097])]\n",
      "Iteration 585, Cost: 0.22854145457788788\n",
      "gradient_weights:  [array([[-0.00020664, -0.00171983,  0.01941806],\n",
      "       [ 0.0001579 , -0.00018581,  0.01176788]]), array([[ 0.00416114],\n",
      "       [-0.00493674],\n",
      "       [ 0.02055623]])]\n",
      "gradients_biases:  [array([-2.05212194e-05, -2.29959451e-03, -2.94505563e-03]), array([-0.00348403])]\n",
      "Iteration 586, Cost: 0.2284910558271912\n",
      "gradient_weights:  [array([[-0.00020902, -0.00171774,  0.01942245],\n",
      "       [ 0.0001593 , -0.00018231,  0.0117798 ]]), array([[ 0.00416748],\n",
      "       [-0.00494918],\n",
      "       [ 0.02056918]])]\n",
      "gradients_biases:  [array([-2.06155447e-05, -2.30280932e-03, -2.96339860e-03]), array([-0.00348709])]\n",
      "Iteration 587, Cost: 0.2284405921497672\n",
      "gradient_weights:  [array([[-0.00021141, -0.00171564,  0.01942682],\n",
      "       [ 0.00016069, -0.0001788 ,  0.01179172]]), array([[ 0.00417383],\n",
      "       [-0.00496163],\n",
      "       [ 0.0205821 ]])]\n",
      "gradients_biases:  [array([-2.07076200e-05, -2.30603265e-03, -2.98172525e-03]), array([-0.00349016])]\n",
      "Iteration 588, Cost: 0.2283900636026311\n",
      "gradient_weights:  [array([[-0.00021381, -0.00171353,  0.01943117],\n",
      "       [ 0.00016208, -0.00017528,  0.01180362]]), array([[ 0.00418019],\n",
      "       [-0.00497409],\n",
      "       [ 0.02059496]])]\n",
      "gradients_biases:  [array([-2.07974383e-05, -2.30926452e-03, -3.00003531e-03]), array([-0.00349322])]\n",
      "Iteration 589, Cost: 0.22833947024328471\n",
      "gradient_weights:  [array([[-0.00021622, -0.00171141,  0.01943549],\n",
      "       [ 0.00016347, -0.00017174,  0.01181551]]), array([[ 0.00418657],\n",
      "       [-0.00498656],\n",
      "       [ 0.02060779]])]\n",
      "gradients_biases:  [array([-2.08849929e-05, -2.31250494e-03, -3.01832852e-03]), array([-0.00349629])]\n",
      "Iteration 590, Cost: 0.22828881212971863\n",
      "gradient_weights:  [array([[-0.00021863, -0.00170929,  0.01943979],\n",
      "       [ 0.00016485, -0.0001682 ,  0.01182738]]), array([[ 0.00419295],\n",
      "       [-0.00499904],\n",
      "       [ 0.02062057]])]\n",
      "gradients_biases:  [array([-2.09702770e-05, -2.31575391e-03, -3.03660462e-03]), array([-0.00349935])]\n",
      "Iteration 591, Cost: 0.22823808932041312\n",
      "gradient_weights:  [array([[-0.00022106, -0.00170715,  0.01944407],\n",
      "       [ 0.00016623, -0.00016464,  0.01183924]]), array([[ 0.00419934],\n",
      "       [-0.00501153],\n",
      "       [ 0.0206333 ]])]\n",
      "gradients_biases:  [array([-2.10532837e-05, -2.31901145e-03, -3.05486333e-03]), array([-0.00350241])]\n",
      "Iteration 592, Cost: 0.22818730187433958\n",
      "gradient_weights:  [array([[-0.00022349, -0.00170501,  0.01944833],\n",
      "       [ 0.00016761, -0.00016107,  0.01185108]]), array([[ 0.00420574],\n",
      "       [-0.00502403],\n",
      "       [ 0.02064599]])]\n",
      "gradients_biases:  [array([-2.11340062e-05, -2.32227756e-03, -3.07310440e-03]), array([-0.00350547])]\n",
      "Iteration 593, Cost: 0.22813644985096157\n",
      "gradient_weights:  [array([[-0.00022593, -0.00170286,  0.01945256],\n",
      "       [ 0.00016898, -0.00015749,  0.01186292]]), array([[ 0.00421216],\n",
      "       [-0.00503654],\n",
      "       [ 0.02065863]])]\n",
      "gradients_biases:  [array([-2.12124376e-05, -2.32555226e-03, -3.09132755e-03]), array([-0.00350854])]\n",
      "Iteration 594, Cost: 0.22808553331023637\n",
      "gradient_weights:  [array([[-0.00022837, -0.0017007 ,  0.01945677],\n",
      "       [ 0.00017035, -0.0001539 ,  0.01187473]]), array([[ 0.00421858],\n",
      "       [-0.00504907],\n",
      "       [ 0.02067123]])]\n",
      "gradients_biases:  [array([-2.12885711e-05, -2.32883555e-03, -3.10953254e-03]), array([-0.0035116])]\n",
      "Iteration 595, Cost: 0.22803455231261632\n",
      "gradient_weights:  [array([[-0.00023083, -0.00169853,  0.01946095],\n",
      "       [ 0.00017171, -0.00015029,  0.01188654]]), array([[ 0.00422502],\n",
      "       [-0.0050616 ],\n",
      "       [ 0.02068379]])]\n",
      "gradients_biases:  [array([-2.13623998e-05, -2.33212745e-03, -3.12771909e-03]), array([-0.00351466])]\n",
      "Iteration 596, Cost: 0.22798350691904967\n",
      "gradient_weights:  [array([[-0.00023329, -0.00169635,  0.01946511],\n",
      "       [ 0.00017308, -0.00014667,  0.01189833]]), array([[ 0.00423147],\n",
      "       [-0.00507414],\n",
      "       [ 0.0206963 ]])]\n",
      "gradients_biases:  [array([-2.14339170e-05, -2.33542796e-03, -3.14588695e-03]), array([-0.00351772])]\n",
      "Iteration 597, Cost: 0.22793239719098207\n",
      "gradient_weights:  [array([[-0.00023577, -0.00169417,  0.01946925],\n",
      "       [ 0.00017444, -0.00014304,  0.0119101 ]]), array([[ 0.00423792],\n",
      "       [-0.00508669],\n",
      "       [ 0.02070876]])]\n",
      "gradients_biases:  [array([-2.15031157e-05, -2.33873710e-03, -3.16403585e-03]), array([-0.00352078])]\n",
      "Iteration 598, Cost: 0.22788122319035775\n",
      "gradient_weights:  [array([[-0.00023825, -0.00169197,  0.01947337],\n",
      "       [ 0.00017579, -0.0001394 ,  0.01192186]]), array([[ 0.00424439],\n",
      "       [-0.00509925],\n",
      "       [ 0.02072118]])]\n",
      "gradients_biases:  [array([-2.15699890e-05, -2.34205486e-03, -3.18216553e-03]), array([-0.00352384])]\n",
      "Iteration 599, Cost: 0.22782998497962098\n",
      "gradient_weights:  [array([[-0.00024073, -0.00168977,  0.01947746],\n",
      "       [ 0.00017714, -0.00013575,  0.01193361]]), array([[ 0.00425087],\n",
      "       [-0.00511182],\n",
      "       [ 0.02073355]])]\n",
      "gradients_biases:  [array([-2.16345301e-05, -2.34538127e-03, -3.20027573e-03]), array([-0.0035269])]\n",
      "Iteration 600, Cost: 0.22777868262171655\n",
      "gradient_weights:  [array([[-0.00024323, -0.00168755,  0.01948154],\n",
      "       [ 0.00017849, -0.00013208,  0.01194534]]), array([[ 0.00425736],\n",
      "       [-0.0051244 ],\n",
      "       [ 0.02074588]])]\n",
      "gradients_biases:  [array([-2.16967321e-05, -2.34871633e-03, -3.21836619e-03]), array([-0.00352996])]\n",
      "Iteration 601, Cost: 0.2277273161800917\n",
      "gradient_weights:  [array([[-0.00024574, -0.00168533,  0.01948558],\n",
      "       [ 0.00017984, -0.0001284 ,  0.01195706]]), array([[ 0.00426386],\n",
      "       [-0.00513699],\n",
      "       [ 0.02075816]])]\n",
      "gradients_biases:  [array([-2.17565881e-05, -2.35206006e-03, -3.23643665e-03]), array([-0.00353302])]\n",
      "Iteration 602, Cost: 0.2276758857186969\n",
      "gradient_weights:  [array([[-0.00024825, -0.0016831 ,  0.01948961],\n",
      "       [ 0.00018118, -0.00012471,  0.01196877]]), array([[ 0.00427038],\n",
      "       [-0.00514959],\n",
      "       [ 0.0207704 ]])]\n",
      "gradients_biases:  [array([-2.18140913e-05, -2.35541245e-03, -3.25448685e-03]), array([-0.00353608])]\n",
      "Iteration 603, Cost: 0.22762439130198697\n",
      "gradient_weights:  [array([[-0.00025077, -0.00168086,  0.01949361],\n",
      "       [ 0.00018252, -0.00012101,  0.01198045]]), array([[ 0.0042769 ],\n",
      "       [-0.0051622 ],\n",
      "       [ 0.02078259]])]\n",
      "gradients_biases:  [array([-2.18692347e-05, -2.35877352e-03, -3.27251654e-03]), array([-0.00353914])]\n",
      "Iteration 604, Cost: 0.2275728329949226\n",
      "gradient_weights:  [array([[-0.0002533 , -0.00167861,  0.01949759],\n",
      "       [ 0.00018385, -0.0001173 ,  0.01199213]]), array([[ 0.00428343],\n",
      "       [-0.00517482],\n",
      "       [ 0.02079474]])]\n",
      "gradients_biases:  [array([-2.19220115e-05, -2.36214329e-03, -3.29052546e-03]), array([-0.00354219])]\n",
      "Iteration 605, Cost: 0.22752121086297053\n",
      "gradient_weights:  [array([[-0.00025584, -0.00167635,  0.01950155],\n",
      "       [ 0.00018518, -0.00011357,  0.01200379]]), array([[ 0.00428998],\n",
      "       [-0.00518745],\n",
      "       [ 0.02080684]])]\n",
      "gradients_biases:  [array([-2.19724147e-05, -2.36552175e-03, -3.30851335e-03]), array([-0.00354525])]\n",
      "Iteration 606, Cost: 0.22746952497210576\n",
      "gradient_weights:  [array([[-0.00025839, -0.00167409,  0.01950548],\n",
      "       [ 0.00018651, -0.00010983,  0.01201543]]), array([[ 0.00429653],\n",
      "       [-0.00520008],\n",
      "       [ 0.0208189 ]])]\n",
      "gradients_biases:  [array([-2.20204375e-05, -2.36890893e-03, -3.32647994e-03]), array([-0.0035483])]\n",
      "Iteration 607, Cost: 0.22741777538881194\n",
      "gradient_weights:  [array([[-0.00026095, -0.00167181,  0.01950939],\n",
      "       [ 0.00018783, -0.00010608,  0.01202706]]), array([[ 0.0043031 ],\n",
      "       [-0.00521273],\n",
      "       [ 0.02083091]])]\n",
      "gradients_biases:  [array([-2.20660729e-05, -2.37230482e-03, -3.34442500e-03]), array([-0.00355136])]\n",
      "Iteration 608, Cost: 0.22736596218008248\n",
      "gradient_weights:  [array([[-0.00026351, -0.00166952,  0.01951328],\n",
      "       [ 0.00018915, -0.00010231,  0.01203868]]), array([[ 0.00430968],\n",
      "       [-0.00522539],\n",
      "       [ 0.02084287]])]\n",
      "gradients_biases:  [array([-2.21093140e-05, -2.37570944e-03, -3.36234826e-03]), array([-0.00355441])]\n",
      "Iteration 609, Cost: 0.2273140854134218\n",
      "gradient_weights:  [array([[-2.66087301e-04, -1.66722824e-03,  1.95171427e-02],\n",
      "       [ 1.90468415e-04, -9.85360563e-05,  1.20502812e-02]]), array([[ 0.00431627],\n",
      "       [-0.00523806],\n",
      "       [ 0.02085479]])]\n",
      "gradients_biases:  [array([-2.21501539e-05, -2.37912280e-03, -3.38024946e-03]), array([-0.00355746])]\n",
      "Iteration 610, Cost: 0.22726214515684615\n",
      "gradient_weights:  [array([[-2.68669557e-04, -1.66492383e-03,  1.95209850e-02],\n",
      "       [ 1.91780843e-04, -9.47461427e-05,  1.20618672e-02]]), array([[ 0.00432287],\n",
      "       [-0.00525074],\n",
      "       [ 0.02086667]])]\n",
      "gradients_biases:  [array([-2.21885857e-05, -2.38254490e-03, -3.39812836e-03]), array([-0.00356051])]\n",
      "Iteration 611, Cost: 0.22721014147888463\n",
      "gradient_weights:  [array([[-2.71260456e-04, -1.66261007e-03,  1.95248045e-02],\n",
      "       [ 1.93089545e-04, -9.09435768e-05,  1.20734382e-02]]), array([[ 0.00432948],\n",
      "       [-0.00526343],\n",
      "       [ 0.02087849]])]\n",
      "gradients_biases:  [array([-2.22246025e-05, -2.38597576e-03, -3.41598469e-03]), array([-0.00356356])]\n",
      "Iteration 612, Cost: 0.22715807444858044\n",
      "gradient_weights:  [array([[-2.73860023e-04, -1.66028690e-03,  1.95286013e-02],\n",
      "       [ 1.94394500e-04, -8.71283166e-05,  1.20849942e-02]]), array([[ 0.00433611],\n",
      "       [-0.00527612],\n",
      "       [ 0.02089027]])]\n",
      "gradients_biases:  [array([-2.22581973e-05, -2.38941539e-03, -3.43381822e-03]), array([-0.00356661])]\n",
      "Iteration 613, Cost: 0.22710594413549126\n",
      "gradient_weights:  [array([[-2.76468285e-04, -1.65795430e-03,  1.95323754e-02],\n",
      "       [ 1.95695690e-04, -8.33003201e-05,  1.20965351e-02]]), array([[ 0.00434274],\n",
      "       [-0.00528883],\n",
      "       [ 0.02090201]])]\n",
      "gradients_biases:  [array([-2.22893632e-05, -2.39286380e-03, -3.45162867e-03]), array([-0.00356965])]\n",
      "Iteration 614, Cost: 0.22705375060969085\n",
      "gradient_weights:  [array([[-2.79085266e-04, -1.65561222e-03,  1.95361267e-02],\n",
      "       [ 1.96993095e-04, -7.94595455e-05,  1.21080609e-02]]), array([[ 0.00434939],\n",
      "       [-0.00530155],\n",
      "       [ 0.0209137 ]])]\n",
      "gradients_biases:  [array([-2.23180932e-05, -2.39632098e-03, -3.46941581e-03]), array([-0.0035727])]\n",
      "Iteration 615, Cost: 0.22700149394176947\n",
      "gradient_weights:  [array([[-2.81710992e-04, -1.65326063e-03,  1.95398553e-02],\n",
      "       [ 1.98286696e-04, -7.56059508e-05,  1.21195715e-02]]), array([[ 0.00435604],\n",
      "       [-0.00531427],\n",
      "       [ 0.02092534]])]\n",
      "gradients_biases:  [array([-2.23443804e-05, -2.39978697e-03, -3.48717938e-03]), array([-0.00357574])]\n",
      "Iteration 616, Cost: 0.226949174202835\n",
      "gradient_weights:  [array([[-2.84345487e-04, -1.65089947e-03,  1.95435612e-02],\n",
      "       [ 1.99576472e-04, -7.17394941e-05,  1.21310669e-02]]), array([[ 0.00436271],\n",
      "       [-0.00532701],\n",
      "       [ 0.02093694]])]\n",
      "gradients_biases:  [array([-2.23682179e-05, -2.40326175e-03, -3.50491912e-03]), array([-0.00357878])]\n",
      "Iteration 617, Cost: 0.22689679146451386\n",
      "gradient_weights:  [array([[-2.86988779e-04, -1.64852872e-03,  1.95472445e-02],\n",
      "       [ 2.00862404e-04, -6.78601336e-05,  1.21425469e-02]]), array([[ 0.00436939],\n",
      "       [-0.00533976],\n",
      "       [ 0.02094849]])]\n",
      "gradients_biases:  [array([-2.23895987e-05, -2.40674535e-03, -3.52263480e-03]), array([-0.00358182])]\n",
      "Iteration 618, Cost: 0.22684434579895157\n",
      "gradient_weights:  [array([[-2.89640891e-04, -1.64614833e-03,  1.95509050e-02],\n",
      "       [ 2.02144472e-04, -6.39678275e-05,  1.21540116e-02]]), array([[ 0.00437608],\n",
      "       [-0.00535251],\n",
      "       [ 0.02096   ]])]\n",
      "gradients_biases:  [array([-2.24085159e-05, -2.41023777e-03, -3.54032617e-03]), array([-0.00358486])]\n",
      "Iteration 619, Cost: 0.22679183727881402\n",
      "gradient_weights:  [array([[-2.92301850e-04, -1.64375826e-03,  1.95545430e-02],\n",
      "       [ 2.03422656e-04, -6.00625339e-05,  1.21654609e-02]]), array([[ 0.00438278],\n",
      "       [-0.00536528],\n",
      "       [ 0.02097146]])]\n",
      "gradients_biases:  [array([-2.24249624e-05, -2.41373903e-03, -3.55799296e-03]), array([-0.0035879])]\n",
      "Iteration 620, Cost: 0.226739265977288\n",
      "gradient_weights:  [array([[-2.94971681e-04, -1.64135847e-03,  1.95581582e-02],\n",
      "       [ 2.04696938e-04, -5.61442110e-05,  1.21768947e-02]]), array([[ 0.00438949],\n",
      "       [-0.00537805],\n",
      "       [ 0.02098287]])]\n",
      "gradients_biases:  [array([-2.24389315e-05, -2.41724912e-03, -3.57563494e-03]), array([-0.00359093])]\n",
      "Iteration 621, Cost: 0.22668663196808203\n",
      "gradient_weights:  [array([[-2.97650409e-04, -1.63894893e-03,  1.95617508e-02],\n",
      "       [ 2.05967296e-04, -5.22128171e-05,  1.21883129e-02]]), array([[ 0.00439622],\n",
      "       [-0.00539083],\n",
      "       [ 0.02099424]])]\n",
      "gradients_biases:  [array([-2.24504160e-05, -2.42076807e-03, -3.59325186e-03]), array([-0.00359397])]\n",
      "Iteration 622, Cost: 0.22663393532542714\n",
      "gradient_weights:  [array([[-3.00338061e-04, -1.63652959e-03,  1.95653209e-02],\n",
      "       [ 2.07233712e-04, -4.82683104e-05,  1.21997156e-02]]), array([[ 0.00440295],\n",
      "       [-0.00540363],\n",
      "       [ 0.02100556]])]\n",
      "gradients_biases:  [array([-2.24594090e-05, -2.42429587e-03, -3.61084348e-03]), array([-0.003597])]\n",
      "Iteration 623, Cost: 0.22658117612407758\n",
      "gradient_weights:  [array([[-3.03034661e-04, -1.63410041e-03,  1.95688683e-02],\n",
      "       [ 2.08496165e-04, -4.43106493e-05,  1.22111026e-02]]), array([[ 0.0044097 ],\n",
      "       [-0.00541643],\n",
      "       [ 0.02101683]])]\n",
      "gradients_biases:  [array([-2.24659036e-05, -2.42783255e-03, -3.62840953e-03]), array([-0.00360003])]\n",
      "Iteration 624, Cost: 0.22652835443931177\n",
      "gradient_weights:  [array([[-3.05740236e-04, -1.63166135e-03,  1.95723931e-02],\n",
      "       [ 2.09754636e-04, -4.03397922e-05,  1.22224739e-02]]), array([[ 0.00441646],\n",
      "       [-0.00542924],\n",
      "       [ 0.02102806]])]\n",
      "gradients_biases:  [array([-2.24698927e-05, -2.43137810e-03, -3.64594980e-03]), array([-0.00360306])]\n",
      "Iteration 625, Cost: 0.22647547034693283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient_weights:  [array([[-3.08454811e-04, -1.62921238e-03,  1.95758953e-02],\n",
      "       [ 2.11009104e-04, -3.63556972e-05,  1.22338294e-02]]), array([[ 0.00442323],\n",
      "       [-0.00544206],\n",
      "       [ 0.02103924]])]\n",
      "gradients_biases:  [array([-2.24713695e-05, -2.43493255e-03, -3.66346402e-03]), array([-0.00360608])]\n",
      "Iteration 626, Cost: 0.22642252392326906\n",
      "gradient_weights:  [array([[-3.11178412e-04, -1.62675345e-03,  1.95793749e-02],\n",
      "       [ 2.12259551e-04, -3.23583230e-05,  1.22451691e-02]]), array([[ 0.00443001],\n",
      "       [-0.00545489],\n",
      "       [ 0.02105038]])]\n",
      "gradients_biases:  [array([-2.24703268e-05, -2.43849589e-03, -3.68095195e-03]), array([-0.00360911])]\n",
      "Iteration 627, Cost: 0.22636951524517507\n",
      "gradient_weights:  [array([[-3.13911064e-04, -1.62428452e-03,  1.95828320e-02],\n",
      "       [ 2.13505955e-04, -2.83476278e-05,  1.22564930e-02]]), array([[ 0.0044368 ],\n",
      "       [-0.00546773],\n",
      "       [ 0.02106147]])]\n",
      "gradients_biases:  [array([-2.24667578e-05, -2.44206813e-03, -3.69841336e-03]), array([-0.00361213])]\n",
      "Iteration 628, Cost: 0.22631644439003198\n",
      "gradient_weights:  [array([[-3.16652794e-04, -1.62180556e-03,  1.95862665e-02],\n",
      "       [ 2.14748298e-04, -2.43235702e-05,  1.22678009e-02]]), array([[ 0.00444361],\n",
      "       [-0.00548058],\n",
      "       [ 0.02107251]])]\n",
      "gradients_biases:  [array([-2.24606555e-05, -2.44564930e-03, -3.71584799e-03]), array([-0.00361515])]\n",
      "Iteration 629, Cost: 0.22626331143574832\n",
      "gradient_weights:  [array([[-3.19403626e-04, -1.61931652e-03,  1.95896785e-02],\n",
      "       [ 2.15986559e-04, -2.02861087e-05,  1.22790928e-02]]), array([[ 0.00445042],\n",
      "       [-0.00549344],\n",
      "       [ 0.02108351]])]\n",
      "gradients_biases:  [array([-2.24520129e-05, -2.44923938e-03, -3.73325562e-03]), array([-0.00361817])]\n",
      "Iteration 630, Cost: 0.22621011646076045\n",
      "gradient_weights:  [array([[-3.22163587e-04, -1.61681737e-03,  1.95930679e-02],\n",
      "       [ 2.17220718e-04, -1.62352018e-05,  1.22903686e-02]]), array([[ 0.00445725],\n",
      "       [-0.00550631],\n",
      "       [ 0.02109445]])]\n",
      "gradients_biases:  [array([-2.24408229e-05, -2.45283841e-03, -3.75063599e-03]), array([-0.00362118])]\n",
      "Iteration 631, Cost: 0.2261568595440332\n",
      "gradient_weights:  [array([[-3.24932703e-04, -1.61430806e-03,  1.95964348e-02],\n",
      "       [ 2.18450756e-04, -1.21708081e-05,  1.23016283e-02]]), array([[ 0.00446409],\n",
      "       [-0.00551919],\n",
      "       [ 0.02110536]])]\n",
      "gradients_biases:  [array([-2.24270786e-05, -2.45644637e-03, -3.76798887e-03]), array([-0.0036242])]\n",
      "Iteration 632, Cost: 0.22610354076506042\n",
      "gradient_weights:  [array([[-3.27710999e-04, -1.61178856e-03,  1.95997792e-02],\n",
      "       [ 2.19676652e-04, -8.09288623e-06,  1.23128719e-02]]), array([[ 0.00447094],\n",
      "       [-0.00553208],\n",
      "       [ 0.02111621]])]\n",
      "gradients_biases:  [array([-2.24107730e-05, -2.46006329e-03, -3.78531403e-03]), array([-0.00362721])]\n",
      "Iteration 633, Cost: 0.2260501602038656\n",
      "gradient_weights:  [array([[-3.30498502e-04, -1.60925883e-03,  1.96031011e-02],\n",
      "       [ 2.20898386e-04, -4.00139486e-06,  1.23240992e-02]]), array([[ 0.0044778 ],\n",
      "       [-0.00554497],\n",
      "       [ 0.02112702]])]\n",
      "gradients_biases:  [array([-2.23918991e-05, -2.46368918e-03, -3.80261121e-03]), array([-0.00363022])]\n",
      "Iteration 634, Cost: 0.225996717941002\n",
      "gradient_weights:  [array([[-3.33295236e-04, -1.60671882e-03,  1.96064006e-02],\n",
      "       [ 2.22115939e-04,  1.03707306e-07,  1.23353103e-02]]), array([[ 0.00448468],\n",
      "       [-0.00555788],\n",
      "       [ 0.02113778]])]\n",
      "gradients_biases:  [array([-2.23704500e-05, -2.46732403e-03, -3.81988019e-03]), array([-0.00363322])]\n",
      "Iteration 635, Cost: 0.22594321405755358\n",
      "gradient_weights:  [array([[-3.36101229e-04, -1.60416850e-03,  1.96096775e-02],\n",
      "       [ 2.23329290e-04,  4.22246152e-06,  1.23465050e-02]]), array([[ 0.00449156],\n",
      "       [-0.00557079],\n",
      "       [ 0.0211485 ]])]\n",
      "gradients_biases:  [array([-2.23464185e-05, -2.47096787e-03, -3.83712073e-03]), array([-0.00363623])]\n",
      "Iteration 636, Cost: 0.2258896486351351\n",
      "gradient_weights:  [array([[-3.38916505e-04, -1.60160783e-03,  1.96129319e-02],\n",
      "       [ 2.24538419e-04,  8.35490899e-06,  1.23576833e-02]]), array([[ 0.00449846],\n",
      "       [-0.00558372],\n",
      "       [ 0.02115917]])]\n",
      "gradients_biases:  [array([-2.23197977e-05, -2.47462069e-03, -3.85433259e-03]), array([-0.00363923])]\n",
      "Iteration 637, Cost: 0.22583602175589276\n",
      "gradient_weights:  [array([[-3.41741091e-04, -1.59903677e-03,  1.96161639e-02],\n",
      "       [ 2.25743307e-04,  1.25010909e-05,  1.23688452e-02]]), array([[ 0.00450537],\n",
      "       [-0.00559665],\n",
      "       [ 0.02116979]])]\n",
      "gradients_biases:  [array([-2.22905807e-05, -2.47828252e-03, -3.87151553e-03]), array([-0.00364223])]\n",
      "Iteration 638, Cost: 0.22578233350250437\n",
      "gradient_weights:  [array([[-3.44575013e-04, -1.59645528e-03,  1.96193734e-02],\n",
      "       [ 2.26943933e-04,  1.66610483e-05,  1.23799905e-02]]), array([[ 0.00451229],\n",
      "       [-0.0056096 ],\n",
      "       [ 0.02118037]])]\n",
      "gradients_biases:  [array([-2.22587603e-05, -2.48195335e-03, -3.88866933e-03]), array([-0.00364522])]\n",
      "Iteration 639, Cost: 0.22572858395818005\n",
      "gradient_weights:  [array([[-3.47418297e-04, -1.59386333e-03,  1.96225605e-02],\n",
      "       [ 2.28140277e-04,  2.08348223e-05,  1.23911193e-02]]), array([[ 0.00451922],\n",
      "       [-0.00562255],\n",
      "       [ 0.02119089]])]\n",
      "gradients_biases:  [array([-2.22243296e-05, -2.48563320e-03, -3.90579375e-03]), array([-0.00364822])]\n",
      "Iteration 640, Cost: 0.22567477320666218\n",
      "gradient_weights:  [array([[-3.50270969e-04, -1.59126086e-03,  1.96257251e-02],\n",
      "       [ 2.29332320e-04,  2.50224538e-05,  1.24022314e-02]]), array([[ 0.00452616],\n",
      "       [-0.00563551],\n",
      "       [ 0.02120137]])]\n",
      "gradients_biases:  [array([-2.21872815e-05, -2.48932208e-03, -3.92288856e-03]), array([-0.00365121])]\n",
      "Iteration 641, Cost: 0.22562090133222606\n",
      "gradient_weights:  [array([[-3.53133055e-04, -1.58864785e-03,  1.96288673e-02],\n",
      "       [ 2.30520040e-04,  2.92239840e-05,  1.24133269e-02]]), array([[ 0.00453312],\n",
      "       [-0.00564848],\n",
      "       [ 0.02121181]])]\n",
      "gradients_biases:  [array([-2.21476091e-05, -2.49302000e-03, -3.93995353e-03]), array([-0.00365419])]\n",
      "Iteration 642, Cost: 0.22556696841967985\n",
      "gradient_weights:  [array([[-3.56004580e-04, -1.58602426e-03,  1.96319871e-02],\n",
      "       [ 2.31703419e-04,  3.34394537e-05,  1.24244055e-02]]), array([[ 0.00454008],\n",
      "       [-0.00566146],\n",
      "       [ 0.02122219]])]\n",
      "gradients_biases:  [array([-2.21053054e-05, -2.49672696e-03, -3.95698842e-03]), array([-0.00365718])]\n",
      "Iteration 643, Cost: 0.22551297455436528\n",
      "gradient_weights:  [array([[-3.58885571e-04, -1.58339004e-03,  1.96350844e-02],\n",
      "       [ 2.32882435e-04,  3.76689037e-05,  1.24354674e-02]]), array([[ 0.00454706],\n",
      "       [-0.00567444],\n",
      "       [ 0.02123253]])]\n",
      "gradients_biases:  [array([-2.20603633e-05, -2.50044298e-03, -3.97399300e-03]), array([-0.00366016])]\n",
      "Iteration 644, Cost: 0.22545891982215738\n",
      "gradient_weights:  [array([[-3.61776055e-04, -1.58074516e-03,  1.96381593e-02],\n",
      "       [ 2.34057069e-04,  4.19123748e-05,  1.24465124e-02]]), array([[ 0.00455405],\n",
      "       [-0.00568744],\n",
      "       [ 0.02124283]])]\n",
      "gradients_biases:  [array([-2.20127757e-05, -2.50416806e-03, -3.99096705e-03]), array([-0.00366314])]\n",
      "Iteration 645, Cost: 0.22540480430946516\n",
      "gradient_weights:  [array([[-3.64676056e-04, -1.57808958e-03,  1.96412118e-02],\n",
      "       [ 2.35227300e-04,  4.61699079e-05,  1.24575405e-02]]), array([[ 0.00456105],\n",
      "       [-0.00570045],\n",
      "       [ 0.02125307]])]\n",
      "gradients_biases:  [array([-2.19625358e-05, -2.50790221e-03, -4.00791035e-03]), array([-0.00366611])]\n",
      "Iteration 646, Cost: 0.22535062810323142\n",
      "gradient_weights:  [array([[-3.67585601e-04, -1.57542326e-03,  1.96442420e-02],\n",
      "       [ 2.36393109e-04,  5.04415435e-05,  1.24685516e-02]]), array([[ 0.00456807],\n",
      "       [-0.00571346],\n",
      "       [ 0.02126327]])]\n",
      "gradients_biases:  [array([-2.19096364e-05, -2.51164545e-03, -4.02482265e-03]), array([-0.00366908])]\n",
      "Iteration 647, Cost: 0.2252963912909332\n",
      "gradient_weights:  [array([[-3.70504717e-04, -1.57274616e-03,  1.96472497e-02],\n",
      "       [ 2.37554475e-04,  5.47273224e-05,  1.24795457e-02]]), array([[ 0.00457509],\n",
      "       [-0.00572649],\n",
      "       [ 0.02127342]])]\n",
      "gradients_biases:  [array([-2.18540706e-05, -2.51539778e-03, -4.04170374e-03]), array([-0.00367205])]\n",
      "Iteration 648, Cost: 0.22524209396058165\n",
      "gradient_weights:  [array([[-3.73433429e-04, -1.57005824e-03,  1.96502351e-02],\n",
      "       [ 2.38711378e-04,  5.90272852e-05,  1.24905227e-02]]), array([[ 0.00458213],\n",
      "       [-0.00573952],\n",
      "       [ 0.02128353]])]\n",
      "gradients_biases:  [array([-2.17958313e-05, -2.51915921e-03, -4.05855338e-03]), array([-0.00367502])]\n",
      "Iteration 649, Cost: 0.22518773620072238\n",
      "gradient_weights:  [array([[-3.76371764e-04, -1.56735947e-03,  1.96531980e-02],\n",
      "       [ 2.39863798e-04,  6.33414722e-05,  1.25014825e-02]]), array([[ 0.00458918],\n",
      "       [-0.00575256],\n",
      "       [ 0.02129358]])]\n",
      "gradients_biases:  [array([-2.17349115e-05, -2.52292975e-03, -4.07537137e-03]), array([-0.00367798])]\n",
      "Iteration 650, Cost: 0.22513331810043513\n",
      "gradient_weights:  [array([[-3.79319747e-04, -1.56464981e-03,  1.96561386e-02],\n",
      "       [ 2.41011715e-04,  6.76699241e-05,  1.25124251e-02]]), array([[ 0.00459624],\n",
      "       [-0.00576561],\n",
      "       [ 0.02130359]])]\n",
      "gradients_biases:  [array([-2.16713042e-05, -2.52670941e-03, -4.09215746e-03]), array([-0.00368094])]\n",
      "Iteration 651, Cost: 0.22507883974933438\n",
      "gradient_weights:  [array([[-3.82277405e-04, -1.56192921e-03,  1.96590568e-02],\n",
      "       [ 2.42155108e-04,  7.20126812e-05,  1.25233504e-02]]), array([[ 0.00460332],\n",
      "       [-0.00577867],\n",
      "       [ 0.02131355]])]\n",
      "gradients_biases:  [array([-2.16050023e-05, -2.53049819e-03, -4.10891144e-03]), array([-0.0036839])]\n",
      "Iteration 652, Cost: 0.22502430123756897\n",
      "gradient_weights:  [array([[-3.85244765e-04, -1.55919764e-03,  1.96619527e-02],\n",
      "       [ 2.43293958e-04,  7.63697839e-05,  1.25342584e-02]]), array([[ 0.0046104 ],\n",
      "       [-0.00579174],\n",
      "       [ 0.02132347]])]\n",
      "gradients_biases:  [array([-2.15359988e-05, -2.53429612e-03, -4.12563309e-03]), array([-0.00368685])]\n",
      "Iteration 653, Cost: 0.22496970265582206\n",
      "gradient_weights:  [array([[-3.88221851e-04, -1.55645507e-03,  1.96648262e-02],\n",
      "       [ 2.44428243e-04,  8.07412723e-05,  1.25451490e-02]]), array([[ 0.0046175 ],\n",
      "       [-0.00580482],\n",
      "       [ 0.02133333]])]\n",
      "gradients_biases:  [array([-2.14642867e-05, -2.53810319e-03, -4.14232219e-03]), array([-0.0036898])]\n",
      "Iteration 654, Cost: 0.22491504409531138\n",
      "gradient_weights:  [array([[-3.91208692e-04, -1.55370145e-03,  1.96676774e-02],\n",
      "       [ 2.45557945e-04,  8.51271868e-05,  1.25560221e-02]]), array([[ 0.00462461],\n",
      "       [-0.00581791],\n",
      "       [ 0.02134315]])]\n",
      "gradients_biases:  [array([-2.13898590e-05, -2.54191941e-03, -4.15897851e-03]), array([-0.00369274])]\n",
      "Iteration 655, Cost: 0.2248603256477889\n",
      "gradient_weights:  [array([[-3.94205311e-04, -1.55093675e-03,  1.96705062e-02],\n",
      "       [ 2.46683043e-04,  8.95275675e-05,  1.25668777e-02]]), array([[ 0.00463173],\n",
      "       [-0.005831  ],\n",
      "       [ 0.02135292]])]\n",
      "gradients_biases:  [array([-2.13127086e-05, -2.54574480e-03, -4.17560184e-03]), array([-0.00369568])]\n",
      "Iteration 656, Cost: 0.22480554740554087\n",
      "gradient_weights:  [array([[-3.97211737e-04, -1.54816093e-03,  1.96733127e-02],\n",
      "       [ 2.47803516e-04,  9.39424544e-05,  1.25777158e-02]]), array([[ 0.00463886],\n",
      "       [-0.0058441 ],\n",
      "       [ 0.02136265]])]\n",
      "gradients_biases:  [array([-2.12328285e-05, -2.54957937e-03, -4.19219195e-03]), array([-0.00369862])]\n",
      "Iteration 657, Cost: 0.2247507094613876\n",
      "gradient_weights:  [array([[-4.00227996e-04, -1.54537394e-03,  1.96760968e-02],\n",
      "       [ 2.48919344e-04,  9.83718875e-05,  1.25885362e-02]]), array([[ 0.00464601],\n",
      "       [-0.00585722],\n",
      "       [ 0.02137232]])]\n",
      "gradients_biases:  [array([-2.11502116e-05, -2.55342312e-03, -4.20874864e-03]), array([-0.00370156])]\n",
      "Iteration 658, Cost: 0.22469581190868326\n",
      "gradient_weights:  [array([[-0.00040325, -0.00154258,  0.01967886],\n",
      "       [ 0.00025003,  0.00010282,  0.01259934]]), array([[ 0.00465316],\n",
      "       [-0.00587034],\n",
      "       [ 0.02138195]])]\n",
      "gradients_biases:  [array([-2.10648510e-05, -2.55727606e-03, -4.22527169e-03]), array([-0.00370449])]\n",
      "Iteration 659, Cost: 0.2246408548413159\n",
      "gradient_weights:  [array([[-0.00040629, -0.00153977,  0.0196816 ],\n",
      "       [ 0.00025114,  0.00010727,  0.01261012]]), array([[ 0.00466033],\n",
      "       [-0.00588347],\n",
      "       [ 0.02139153]])]\n",
      "gradients_biases:  [array([-2.09767396e-05, -2.56113820e-03, -4.24176087e-03]), array([-0.00370741])]\n",
      "Iteration 660, Cost: 0.22458583835370713\n",
      "gradient_weights:  [array([[-0.00040934, -0.00153695,  0.01968432],\n",
      "       [ 0.00025224,  0.00011175,  0.01262089]]), array([[ 0.00466751],\n",
      "       [-0.00589661],\n",
      "       [ 0.02140107]])]\n",
      "gradients_biases:  [array([-2.08858703e-05, -2.56500955e-03, -4.25821598e-03]), array([-0.00371034])]\n",
      "Iteration 661, Cost: 0.2245307625408116\n",
      "gradient_weights:  [array([[-0.00041239, -0.00153411,  0.01968701],\n",
      "       [ 0.00025334,  0.00011624,  0.01263164]]), array([[ 0.00467471],\n",
      "       [-0.00590975],\n",
      "       [ 0.02141055]])]\n",
      "gradients_biases:  [array([-2.07922361e-05, -2.56889013e-03, -4.27463680e-03]), array([-0.00371325])]\n",
      "Iteration 662, Cost: 0.22447562749811711\n",
      "gradient_weights:  [array([[-0.00041546, -0.00153127,  0.01968968],\n",
      "       [ 0.00025443,  0.00012074,  0.01264237]]), array([[ 0.00468191],\n",
      "       [-0.00592291],\n",
      "       [ 0.02141999]])]\n",
      "gradients_biases:  [array([-2.06958301e-05, -2.57277993e-03, -4.29102313e-03]), array([-0.00371617])]\n",
      "Iteration 663, Cost: 0.2244204333216443\n",
      "gradient_weights:  [array([[-0.00041853, -0.00152842,  0.01969233],\n",
      "       [ 0.00025552,  0.00012526,  0.01265309]]), array([[ 0.00468913],\n",
      "       [-0.00593607],\n",
      "       [ 0.02142938]])]\n",
      "gradients_biases:  [array([-2.05966451e-05, -2.57667897e-03, -4.30737474e-03]), array([-0.00371908])]\n",
      "Iteration 664, Cost: 0.22436518010794587\n",
      "gradient_weights:  [array([[-0.00042162, -0.00152555,  0.01969496],\n",
      "       [ 0.0002566 ,  0.00012979,  0.01266378]]), array([[ 0.00469636],\n",
      "       [-0.00594925],\n",
      "       [ 0.02143872]])]\n",
      "gradients_biases:  [array([-2.04946741e-05, -2.58058725e-03, -4.32369143e-03]), array([-0.00372198])]\n",
      "Iteration 665, Cost: 0.22430986795410673\n",
      "gradient_weights:  [array([[-0.00042472, -0.00152267,  0.01969757],\n",
      "       [ 0.00025768,  0.00013434,  0.01267446]]), array([[ 0.0047036 ],\n",
      "       [-0.00596243],\n",
      "       [ 0.02144802]])]\n",
      "gradients_biases:  [array([-2.03899101e-05, -2.58450480e-03, -4.33997300e-03]), array([-0.00372489])]\n",
      "Iteration 666, Cost: 0.22425449695774313\n",
      "gradient_weights:  [array([[-0.00042782, -0.00151978,  0.01970015],\n",
      "       [ 0.00025875,  0.0001389 ,  0.01268512]]), array([[ 0.00471085],\n",
      "       [-0.00597562],\n",
      "       [ 0.02145726]])]\n",
      "gradients_biases:  [array([-2.02823460e-05, -2.58843160e-03, -4.35621922e-03]), array([-0.00372778])]\n",
      "Iteration 667, Cost: 0.22419906721700272\n",
      "gradient_weights:  [array([[-0.00043094, -0.00151688,  0.01970271],\n",
      "       [ 0.00025982,  0.00014348,  0.01269576]]), array([[ 0.00471812],\n",
      "       [-0.00598882],\n",
      "       [ 0.02146646]])]\n",
      "gradients_biases:  [array([-2.01719749e-05, -2.59236768e-03, -4.37242990e-03]), array([-0.00373068])]\n",
      "Iteration 668, Cost: 0.22414357883056368\n",
      "gradient_weights:  [array([[-0.00043406, -0.00151397,  0.01970525],\n",
      "       [ 0.00026088,  0.00014807,  0.01270638]]), array([[ 0.0047254 ],\n",
      "       [-0.00600202],\n",
      "       [ 0.02147561]])]\n",
      "gradients_biases:  [array([-2.00587896e-05, -2.59631305e-03, -4.38860482e-03]), array([-0.00373357])]\n",
      "Iteration 669, Cost: 0.22408803189763432\n",
      "gradient_weights:  [array([[-0.0004372 , -0.00151105,  0.01970777],\n",
      "       [ 0.00026194,  0.00015267,  0.01271698]]), array([[ 0.00473269],\n",
      "       [-0.00601524],\n",
      "       [ 0.02148472]])]\n",
      "gradients_biases:  [array([-1.99427831e-05, -2.60026771e-03, -4.40474379e-03]), array([-0.00373645])]\n",
      "Iteration 670, Cost: 0.22403242651795272\n",
      "gradient_weights:  [array([[-0.00044035, -0.00150811,  0.01971026],\n",
      "       [ 0.00026299,  0.0001573 ,  0.01272757]]), array([[ 0.00473999],\n",
      "       [-0.00602846],\n",
      "       [ 0.02149377]])]\n",
      "gradients_biases:  [array([-1.98239485e-05, -2.60423166e-03, -4.42084660e-03]), array([-0.00373933])]\n",
      "Iteration 671, Cost: 0.22397676279178602\n",
      "gradient_weights:  [array([[-0.0004435 , -0.00150516,  0.01971273],\n",
      "       [ 0.00026404,  0.00016193,  0.01273814]]), array([[ 0.0047473 ],\n",
      "       [-0.00604169],\n",
      "       [ 0.02150278]])]\n",
      "gradients_biases:  [array([-1.97022786e-05, -2.60820493e-03, -4.43691304e-03]), array([-0.0037422])]\n",
      "Iteration 672, Cost: 0.22392104081992975\n",
      "gradient_weights:  [array([[-0.00044667, -0.0015022 ,  0.01971518],\n",
      "       [ 0.00026509,  0.00016659,  0.01274869]]), array([[ 0.00475463],\n",
      "       [-0.00605493],\n",
      "       [ 0.02151174]])]\n",
      "gradients_biases:  [array([-1.95777664e-05, -2.61218752e-03, -4.45294292e-03]), array([-0.00374507])]\n",
      "Iteration 673, Cost: 0.22386526070370755\n",
      "gradient_weights:  [array([[-0.00044985, -0.00149923,  0.01971761],\n",
      "       [ 0.00026612,  0.00017125,  0.01275922]]), array([[ 0.00476197],\n",
      "       [-0.00606818],\n",
      "       [ 0.02152065]])]\n",
      "gradients_biases:  [array([-1.94504050e-05, -2.61617944e-03, -4.46893603e-03]), array([-0.00374794])]\n",
      "Iteration 674, Cost: 0.22380942254497005\n",
      "gradient_weights:  [array([[-0.00045303, -0.00149625,  0.01972001],\n",
      "       [ 0.00026716,  0.00017594,  0.01276973]]), array([[ 0.00476932],\n",
      "       [-0.00608144],\n",
      "       [ 0.02152951]])]\n",
      "gradients_biases:  [array([-1.93201872e-05, -2.62018070e-03, -4.48489217e-03]), array([-0.0037508])]\n",
      "Iteration 675, Cost: 0.2237535264460945\n",
      "gradient_weights:  [array([[-0.00045623, -0.00149326,  0.0197224 ],\n",
      "       [ 0.00026819,  0.00018063,  0.01278022]]), array([[ 0.00477669],\n",
      "       [-0.00609471],\n",
      "       [ 0.02153832]])]\n",
      "gradients_biases:  [array([-1.91871060e-05, -2.62419130e-03, -4.50081115e-03]), array([-0.00375366])]\n",
      "Iteration 676, Cost: 0.22369757250998396\n",
      "gradient_weights:  [array([[-0.00045944, -0.00149025,  0.01972476],\n",
      "       [ 0.00026921,  0.00018535,  0.0127907 ]]), array([[ 0.00478406],\n",
      "       [-0.00610798],\n",
      "       [ 0.02154709]])]\n",
      "gradients_biases:  [array([-1.90511543e-05, -2.62821126e-03, -4.51669276e-03]), array([-0.00375651])]\n",
      "Iteration 677, Cost: 0.22364156084006664\n",
      "gradient_weights:  [array([[-0.00046266, -0.00148723,  0.01972709],\n",
      "       [ 0.00027023,  0.00019008,  0.01280115]]), array([[ 0.00479145],\n",
      "       [-0.00612126],\n",
      "       [ 0.02155581]])]\n",
      "gradients_biases:  [array([-1.89123253e-05, -2.63224058e-03, -4.53253682e-03]), array([-0.00375935])]\n",
      "Iteration 678, Cost: 0.22358549154029483\n",
      "gradient_weights:  [array([[-0.00046589, -0.0014842 ,  0.01972941],\n",
      "       [ 0.00027124,  0.00019482,  0.01281159]]), array([[ 0.00479885],\n",
      "       [-0.00613455],\n",
      "       [ 0.02156448]])]\n",
      "gradients_biases:  [array([-1.87706117e-05, -2.63627928e-03, -4.54834311e-03]), array([-0.0037622])]\n",
      "Iteration 679, Cost: 0.22352936471514448\n",
      "gradient_weights:  [array([[-0.00046913, -0.00148116,  0.0197317 ],\n",
      "       [ 0.00027225,  0.00019958,  0.01282201]]), array([[ 0.00480626],\n",
      "       [-0.00614785],\n",
      "       [ 0.0215731 ]])]\n",
      "gradients_biases:  [array([-1.86260066e-05, -2.64032736e-03, -4.56411146e-03]), array([-0.00376503])]\n",
      "Iteration 680, Cost: 0.22347318046961406\n",
      "gradient_weights:  [array([[-0.00047238, -0.00147811,  0.01973397],\n",
      "       [ 0.00027325,  0.00020435,  0.01283241]]), array([[ 0.00481369],\n",
      "       [-0.00616116],\n",
      "       [ 0.02158167]])]\n",
      "gradients_biases:  [array([-1.84785030e-05, -2.64438483e-03, -4.57984166e-03]), array([-0.00376786])]\n",
      "Iteration 681, Cost: 0.22341693890922365\n",
      "gradient_weights:  [array([[-0.00047564, -0.00147504,  0.01973622],\n",
      "       [ 0.00027425,  0.00020914,  0.01284278]]), array([[ 0.00482113],\n",
      "       [-0.00617447],\n",
      "       [ 0.0215902 ]])]\n",
      "gradients_biases:  [array([-1.83280937e-05, -2.64845171e-03, -4.59553353e-03]), array([-0.00377069])]\n",
      "Iteration 682, Cost: 0.22336064014001447\n",
      "gradient_weights:  [array([[-0.00047891, -0.00147196,  0.01973845],\n",
      "       [ 0.00027525,  0.00021395,  0.01285315]]), array([[ 0.00482858],\n",
      "       [-0.00618779],\n",
      "       [ 0.02159868]])]\n",
      "gradients_biases:  [array([-1.81747719e-05, -2.65252799e-03, -4.61118686e-03]), array([-0.00377351])]\n",
      "Iteration 683, Cost: 0.22330428426854715\n",
      "gradient_weights:  [array([[-0.00048219, -0.00146887,  0.01974065],\n",
      "       [ 0.00027623,  0.00021877,  0.01286349]]), array([[ 0.00483604],\n",
      "       [-0.00620112],\n",
      "       [ 0.0216071 ]])]\n",
      "gradients_biases:  [array([-1.80185304e-05, -2.65661370e-03, -4.62680148e-03]), array([-0.00377633])]\n",
      "Iteration 684, Cost: 0.22324787140190153\n",
      "gradient_weights:  [array([[-0.00048548, -0.00146577,  0.01974284],\n",
      "       [ 0.00027722,  0.00022361,  0.01287381]]), array([[ 0.00484351],\n",
      "       [-0.00621446],\n",
      "       [ 0.02161549]])]\n",
      "gradients_biases:  [array([-1.78593622e-05, -2.66070883e-03, -4.64237720e-03]), array([-0.00377914])]\n",
      "Iteration 685, Cost: 0.22319140164767517\n",
      "gradient_weights:  [array([[-0.00048878, -0.00146266,  0.019745  ],\n",
      "       [ 0.00027819,  0.00022846,  0.01288411]]), array([[ 0.004851  ],\n",
      "       [-0.00622781],\n",
      "       [ 0.02162382]])]\n",
      "gradients_biases:  [array([-1.76972603e-05, -2.66481340e-03, -4.65791382e-03]), array([-0.00378194])]\n",
      "Iteration 686, Cost: 0.22313487511398244\n",
      "gradient_weights:  [array([[-0.00049209, -0.00145953,  0.01974713],\n",
      "       [ 0.00027917,  0.00023333,  0.0128944 ]]), array([[ 0.0048585 ],\n",
      "       [-0.00624116],\n",
      "       [ 0.0216321 ]])]\n",
      "gradients_biases:  [array([-1.75322176e-05, -2.66892742e-03, -4.67341116e-03]), array([-0.00378474])]\n",
      "Iteration 687, Cost: 0.2230782919094535\n",
      "gradient_weights:  [array([[-0.00049542, -0.00145639,  0.01974925],\n",
      "       [ 0.00028013,  0.00023821,  0.01290466]]), array([[ 0.00486601],\n",
      "       [-0.00625452],\n",
      "       [ 0.02164034]])]\n",
      "gradients_biases:  [array([-1.73642272e-05, -2.67305090e-03, -4.68886903e-03]), array([-0.00378753])]\n",
      "Iteration 688, Cost: 0.22302165214323294\n",
      "gradient_weights:  [array([[-0.00049875, -0.00145324,  0.01975134],\n",
      "       [ 0.0002811 ,  0.00024311,  0.0129149 ]]), array([[ 0.00487354],\n",
      "       [-0.00626789],\n",
      "       [ 0.02164852]])]\n",
      "gradients_biases:  [array([-1.71932819e-05, -2.67718384e-03, -4.70428726e-03]), array([-0.00379032])]\n",
      "Iteration 689, Cost: 0.22296495592497906\n",
      "gradient_weights:  [array([[-0.0005021 , -0.00145008,  0.01975341],\n",
      "       [ 0.00028205,  0.00024803,  0.01292513]]), array([[ 0.00488107],\n",
      "       [-0.00628127],\n",
      "       [ 0.02165666]])]\n",
      "gradients_biases:  [array([-1.70193749e-05, -2.68132625e-03, -4.71966564e-03]), array([-0.0037931])]\n",
      "Iteration 690, Cost: 0.22290820336486222\n",
      "gradient_weights:  [array([[-0.00050545, -0.0014469 ,  0.01975546],\n",
      "       [ 0.000283  ,  0.00025296,  0.01293533]]), array([[ 0.00488862],\n",
      "       [-0.00629466],\n",
      "       [ 0.02166475]])]\n",
      "gradients_biases:  [array([-1.68424989e-05, -2.68547815e-03, -4.73500402e-03]), array([-0.00379588])]\n",
      "Iteration 691, Cost: 0.2228513945735639\n",
      "gradient_weights:  [array([[-0.00050882, -0.00144371,  0.01975749],\n",
      "       [ 0.00028395,  0.00025791,  0.01294552]]), array([[ 0.00489618],\n",
      "       [-0.00630805],\n",
      "       [ 0.02167279]])]\n",
      "gradients_biases:  [array([-1.66626471e-05, -2.68963954e-03, -4.75030219e-03]), array([-0.00379865])]\n",
      "Iteration 692, Cost: 0.22279452966227525\n",
      "gradient_weights:  [array([[-0.0005122 , -0.00144051,  0.01975949],\n",
      "       [ 0.00028489,  0.00026287,  0.01295569]]), array([[ 0.00490376],\n",
      "       [-0.00632145],\n",
      "       [ 0.02168078]])]\n",
      "gradients_biases:  [array([-1.64798124e-05, -2.69381043e-03, -4.76555999e-03]), array([-0.00380142])]\n",
      "Iteration 693, Cost: 0.22273760874269616\n",
      "gradient_weights:  [array([[-0.00051558, -0.0014373 ,  0.01976147],\n",
      "       [ 0.00028583,  0.00026785,  0.01296583]]), array([[ 0.00491135],\n",
      "       [-0.00633486],\n",
      "       [ 0.02168873]])]\n",
      "gradients_biases:  [array([-1.62939878e-05, -2.69799083e-03, -4.78077724e-03]), array([-0.00380417])]\n",
      "Iteration 694, Cost: 0.22268063192703352\n",
      "gradient_weights:  [array([[-0.00051898, -0.00143407,  0.01976343],\n",
      "       [ 0.00028676,  0.00027285,  0.01297596]]), array([[ 0.00491895],\n",
      "       [-0.00634828],\n",
      "       [ 0.02169662]])]\n",
      "gradients_biases:  [array([-1.61051662e-05, -2.70218075e-03, -4.79595375e-03]), array([-0.00380693])]\n",
      "Iteration 695, Cost: 0.22262359932800005\n",
      "gradient_weights:  [array([[-0.00052239, -0.00143084,  0.01976537],\n",
      "       [ 0.00028768,  0.00027786,  0.01298607]]), array([[ 0.00492656],\n",
      "       [-0.0063617 ],\n",
      "       [ 0.02170447]])]\n",
      "gradients_biases:  [array([-1.59133406e-05, -2.70638020e-03, -4.81108935e-03]), array([-0.00380967])]\n",
      "Iteration 696, Cost: 0.22256651105881287\n",
      "gradient_weights:  [array([[-0.00052581, -0.00142759,  0.01976728],\n",
      "       [ 0.0002886 ,  0.00028288,  0.01299615]]), array([[ 0.00493418],\n",
      "       [-0.00637514],\n",
      "       [ 0.02171227]])]\n",
      "gradients_biases:  [array([-1.57185041e-05, -2.71058919e-03, -4.82618387e-03]), array([-0.00381242])]\n",
      "Iteration 697, Cost: 0.22250936723319228\n",
      "gradient_weights:  [array([[-0.00052924, -0.00142432,  0.01976918],\n",
      "       [ 0.00028951,  0.00028793,  0.01300622]]), array([[ 0.00494182],\n",
      "       [-0.00638858],\n",
      "       [ 0.02172002]])]\n",
      "gradients_biases:  [array([-1.55206495e-05, -2.71480772e-03, -4.84123713e-03]), array([-0.00381515])]\n",
      "Iteration 698, Cost: 0.22245216796536\n",
      "gradient_weights:  [array([[-0.00053269, -0.00142105,  0.01977105],\n",
      "       [ 0.00029042,  0.00029299,  0.01301627]]), array([[ 0.00494947],\n",
      "       [-0.00640202],\n",
      "       [ 0.02172772]])]\n",
      "gradients_biases:  [array([-1.53197699e-05, -2.71903581e-03, -4.85624896e-03]), array([-0.00381788])]\n",
      "Iteration 699, Cost: 0.22239491337003772\n",
      "gradient_weights:  [array([[-0.00053614, -0.00141776,  0.01977289],\n",
      "       [ 0.00029132,  0.00029806,  0.01302629]]), array([[ 0.00495713],\n",
      "       [-0.00641548],\n",
      "       [ 0.02173537]])]\n",
      "gradients_biases:  [array([-1.51158583e-05, -2.72327347e-03, -4.87121919e-03]), array([-0.0038206])]\n",
      "Iteration 700, Cost: 0.22233760356244575\n",
      "gradient_weights:  [array([[-0.0005396 , -0.00141446,  0.01977472],\n",
      "       [ 0.00029222,  0.00030315,  0.0130363 ]]), array([[ 0.00496481],\n",
      "       [-0.00642894],\n",
      "       [ 0.02174297]])]\n",
      "gradients_biases:  [array([-1.49089076e-05, -2.72752070e-03, -4.88614765e-03]), array([-0.00382332])]\n",
      "Iteration 701, Cost: 0.2222802386583014\n",
      "gradient_weights:  [array([[-0.00054308, -0.00141114,  0.01977652],\n",
      "       [ 0.00029311,  0.00030826,  0.01304629]]), array([[ 0.0049725 ],\n",
      "       [-0.00644241],\n",
      "       [ 0.02175053]])]\n",
      "gradients_biases:  [array([-1.46989108e-05, -2.73177752e-03, -4.90103417e-03]), array([-0.00382603])]\n",
      "Iteration 702, Cost: 0.22222281877381717\n",
      "gradient_weights:  [array([[-0.00054656, -0.00140782,  0.0197783 ],\n",
      "       [ 0.000294  ,  0.00031338,  0.01305625]]), array([[ 0.0049802 ],\n",
      "       [-0.00645589],\n",
      "       [ 0.02175804]])]\n",
      "gradients_biases:  [array([-1.44858609e-05, -2.73604392e-03, -4.91587858e-03]), array([-0.00382873])]\n",
      "Iteration 703, Cost: 0.22216534402569948\n",
      "gradient_weights:  [array([[-0.00055006, -0.00140448,  0.01978006],\n",
      "       [ 0.00029488,  0.00031852,  0.0130662 ]]), array([[ 0.00498791],\n",
      "       [-0.00646938],\n",
      "       [ 0.02176549]])]\n",
      "gradients_biases:  [array([-1.42697510e-05, -2.74031993e-03, -4.93068072e-03]), array([-0.00383143])]\n",
      "Iteration 704, Cost: 0.22210781453114659\n",
      "gradient_weights:  [array([[-0.00055357, -0.00140113,  0.0197818 ],\n",
      "       [ 0.00029575,  0.00032368,  0.01307612]]), array([[ 0.00499563],\n",
      "       [-0.00648287],\n",
      "       [ 0.0217729 ]])]\n",
      "gradients_biases:  [array([-1.40505739e-05, -2.74460556e-03, -4.94544041e-03]), array([-0.00383412])]\n",
      "Iteration 705, Cost: 0.22205023040784733\n",
      "gradient_weights:  [array([[-0.00055709, -0.00139776,  0.01978351],\n",
      "       [ 0.00029662,  0.00032885,  0.01308603]]), array([[ 0.00500337],\n",
      "       [-0.00649637],\n",
      "       [ 0.02178026]])]\n",
      "gradients_biases:  [array([-1.38283227e-05, -2.74890080e-03, -4.96015751e-03]), array([-0.0038368])]\n",
      "Iteration 706, Cost: 0.2219925917739789\n",
      "gradient_weights:  [array([[-0.00056062, -0.00139438,  0.0197852 ],\n",
      "       [ 0.00029748,  0.00033404,  0.01309591]]), array([[ 0.00501112],\n",
      "       [-0.00650988],\n",
      "       [ 0.02178757]])]\n",
      "gradients_biases:  [array([-1.36029904e-05, -2.75320567e-03, -4.97483183e-03]), array([-0.00383948])]\n",
      "Iteration 707, Cost: 0.2219348987482055\n",
      "gradient_weights:  [array([[-0.00056416, -0.00139099,  0.01978687],\n",
      "       [ 0.00029834,  0.00033925,  0.01310578]]), array([[ 0.00501889],\n",
      "       [-0.00652339],\n",
      "       [ 0.02179484]])]\n",
      "gradients_biases:  [array([-1.33745699e-05, -2.75752018e-03, -4.98946323e-03]), array([-0.00384215])]\n",
      "Iteration 708, Cost: 0.22187715144967637\n",
      "gradient_weights:  [array([[-0.00056771, -0.00138759,  0.01978852],\n",
      "       [ 0.00029919,  0.00034447,  0.01311562]]), array([[ 0.00502666],\n",
      "       [-0.00653691],\n",
      "       [ 0.02180205]])]\n",
      "gradients_biases:  [array([-1.31430544e-05, -2.76184434e-03, -5.00405154e-03]), array([-0.00384481])]\n",
      "Iteration 709, Cost: 0.22181934999802377\n",
      "gradient_weights:  [array([[-0.00057128, -0.00138417,  0.01979014],\n",
      "       [ 0.00030004,  0.00034971,  0.01312544]]), array([[ 0.00503445],\n",
      "       [-0.00655044],\n",
      "       [ 0.02180922]])]\n",
      "gradients_biases:  [array([-1.29084366e-05, -2.76617816e-03, -5.01859660e-03]), array([-0.00384747])]\n",
      "Iteration 710, Cost: 0.22176149451336138\n",
      "gradient_weights:  [array([[-0.00057485, -0.00138074,  0.01979175],\n",
      "       [ 0.00030088,  0.00035497,  0.01313524]]), array([[ 0.00504225],\n",
      "       [-0.00656398],\n",
      "       [ 0.02181633]])]\n",
      "gradients_biases:  [array([-1.26707098e-05, -2.77052164e-03, -5.03309826e-03]), array([-0.00385012])]\n",
      "Iteration 711, Cost: 0.2217035851162823\n",
      "gradient_weights:  [array([[-0.00057844, -0.0013773 ,  0.01979333],\n",
      "       [ 0.00030172,  0.00036024,  0.01314502]]), array([[ 0.00505007],\n",
      "       [-0.00657752],\n",
      "       [ 0.0218234 ]])]\n",
      "gradients_biases:  [array([-1.24298668e-05, -2.77487480e-03, -5.04755635e-03]), array([-0.00385276])]\n",
      "Iteration 712, Cost: 0.22164562192785697\n",
      "gradient_weights:  [array([[-0.00058203, -0.00137384,  0.01979488],\n",
      "       [ 0.00030255,  0.00036553,  0.01315478]]), array([[ 0.0050579 ],\n",
      "       [-0.00659107],\n",
      "       [ 0.02183042]])]\n",
      "gradients_biases:  [array([-1.21859007e-05, -2.77923764e-03, -5.06197073e-03]), array([-0.0038554])]\n",
      "Iteration 713, Cost: 0.22158760506963132\n",
      "gradient_weights:  [array([[-0.00058564, -0.00137038,  0.01979642],\n",
      "       [ 0.00030337,  0.00037083,  0.01316452]]), array([[ 0.00506574],\n",
      "       [-0.00660463],\n",
      "       [ 0.02183739]])]\n",
      "gradients_biases:  [array([-1.19388044e-05, -2.78361018e-03, -5.07634124e-03]), array([-0.00385803])]\n",
      "Iteration 714, Cost: 0.22152953466362468\n",
      "gradient_weights:  [array([[-0.00058926, -0.00136689,  0.01979793],\n",
      "       [ 0.00030419,  0.00037615,  0.01317424]]), array([[ 0.00507359],\n",
      "       [-0.0066182 ],\n",
      "       [ 0.02184431]])]\n",
      "gradients_biases:  [array([-1.16885710e-05, -2.78799242e-03, -5.09066772e-03]), array([-0.00386065])]\n",
      "Iteration 715, Cost: 0.2214714108323279\n",
      "gradient_weights:  [array([[-0.00059289, -0.0013634 ,  0.01979942],\n",
      "       [ 0.000305  ,  0.00038149,  0.01318394]]), array([[ 0.00508145],\n",
      "       [-0.00663177],\n",
      "       [ 0.02185118]])]\n",
      "gradients_biases:  [array([-1.14351935e-05, -2.79238438e-03, -5.10495003e-03]), array([-0.00386326])]\n",
      "Iteration 716, Cost: 0.22141323369870103\n",
      "gradient_weights:  [array([[-0.00059654, -0.00135989,  0.01980089],\n",
      "       [ 0.00030581,  0.00038685,  0.01319361]]), array([[ 0.00508933],\n",
      "       [-0.00664535],\n",
      "       [ 0.021858  ]])]\n",
      "gradients_biases:  [array([-1.11786648e-05, -2.79678606e-03, -5.11918801e-03]), array([-0.00386587])]\n",
      "Iteration 717, Cost: 0.22135500338617112\n",
      "gradient_weights:  [array([[-0.00060019, -0.00135637,  0.01980234],\n",
      "       [ 0.00030661,  0.00039222,  0.01320327]]), array([[ 0.00509722],\n",
      "       [-0.00665894],\n",
      "       [ 0.02186478]])]\n",
      "gradients_biases:  [array([-1.09189781e-05, -2.80119747e-03, -5.13338152e-03]), array([-0.00386847])]\n",
      "Iteration 718, Cost: 0.22129672001863052\n",
      "gradient_weights:  [array([[-0.00060386, -0.00135283,  0.01980376],\n",
      "       [ 0.0003074 ,  0.00039761,  0.0132129 ]]), array([[ 0.00510513],\n",
      "       [-0.00667253],\n",
      "       [ 0.0218715 ]])]\n",
      "gradients_biases:  [array([-1.06561263e-05, -2.80561862e-03, -5.14753041e-03]), array([-0.00387106])]\n",
      "Iteration 719, Cost: 0.2212383837204342\n",
      "gradient_weights:  [array([[-0.00060753, -0.00134929,  0.01980516],\n",
      "       [ 0.00030819,  0.00040301,  0.01322251]]), array([[ 0.00511304],\n",
      "       [-0.00668613],\n",
      "       [ 0.02187818]])]\n",
      "gradients_biases:  [array([-1.03901023e-05, -2.81004952e-03, -5.16163454e-03]), array([-0.00387365])]\n",
      "Iteration 720, Cost: 0.22117999461639776\n",
      "gradient_weights:  [array([[-0.00061122, -0.00134572,  0.01980654],\n",
      "       [ 0.00030897,  0.00040844,  0.0132321 ]]), array([[ 0.00512097],\n",
      "       [-0.00669974],\n",
      "       [ 0.02188481]])]\n",
      "gradients_biases:  [array([-1.01208993e-05, -2.81449018e-03, -5.17569375e-03]), array([-0.00387622])]\n",
      "Iteration 721, Cost: 0.22112155283179508\n",
      "gradient_weights:  [array([[-0.00061492, -0.00134215,  0.0198079 ],\n",
      "       [ 0.00030975,  0.00041388,  0.01324167]]), array([[ 0.00512891],\n",
      "       [-0.00671335],\n",
      "       [ 0.02189139]])]\n",
      "gradients_biases:  [array([-9.84851028e-06, -2.81894061e-03, -5.18970790e-03]), array([-0.00387879])]\n",
      "Iteration 722, Cost: 0.22106305849235613\n",
      "gradient_weights:  [array([[-0.00061864, -0.00133856,  0.01980923],\n",
      "       [ 0.00031052,  0.00041933,  0.01325122]]), array([[ 0.00513687],\n",
      "       [-0.00672697],\n",
      "       [ 0.02189792]])]\n",
      "gradients_biases:  [array([-9.57292819e-06, -2.82340082e-03, -5.20367686e-03]), array([-0.00388136])]\n",
      "Iteration 723, Cost: 0.22100451172426458\n",
      "gradient_weights:  [array([[-0.00062236, -0.00133496,  0.01981054],\n",
      "       [ 0.00031128,  0.00042481,  0.01326075]]), array([[ 0.00514484],\n",
      "       [-0.0067406 ],\n",
      "       [ 0.0219044 ]])]\n",
      "gradients_biases:  [array([-9.29414607e-06, -2.82787082e-03, -5.21760048e-03]), array([-0.00388391])]\n",
      "Iteration 724, Cost: 0.22094591265415525\n",
      "gradient_weights:  [array([[-0.00062609, -0.00133135,  0.01981183],\n",
      "       [ 0.00031204,  0.0004303 ,  0.01327025]]), array([[ 0.00515282],\n",
      "       [-0.00675424],\n",
      "       [ 0.02191083]])]\n",
      "gradients_biases:  [array([-9.01215695e-06, -2.83235061e-03, -5.23147862e-03]), array([-0.00388646])]\n",
      "Iteration 725, Cost: 0.22088726140911202\n",
      "gradient_weights:  [array([[-0.00062984, -0.00132772,  0.0198131 ],\n",
      "       [ 0.0003128 ,  0.0004358 ,  0.01327974]]), array([[ 0.00516081],\n",
      "       [-0.00676788],\n",
      "       [ 0.02191721]])]\n",
      "gradients_biases:  [array([-8.72695386e-06, -2.83684021e-03, -5.24531114e-03]), array([-0.003889])]\n",
      "Iteration 726, Cost: 0.22082855811666502\n",
      "gradient_weights:  [array([[-0.0006336 , -0.00132408,  0.01981434],\n",
      "       [ 0.00031354,  0.00044133,  0.0132892 ]]), array([[ 0.00516882],\n",
      "       [-0.00678153],\n",
      "       [ 0.02192355]])]\n",
      "gradients_biases:  [array([-8.43852981e-06, -2.84133963e-03, -5.25909791e-03]), array([-0.00389153])]\n",
      "Iteration 727, Cost: 0.2207698029047887\n",
      "gradient_weights:  [array([[-0.00063737, -0.00132042,  0.01981556],\n",
      "       [ 0.00031428,  0.00044687,  0.01329864]]), array([[ 0.00517684],\n",
      "       [-0.00679518],\n",
      "       [ 0.02192983]])]\n",
      "gradients_biases:  [array([-8.14687782e-06, -2.84584887e-03, -5.27283879e-03]), array([-0.00389405])]\n",
      "Iteration 728, Cost: 0.22071099590189874\n",
      "gradient_weights:  [array([[-0.00064115, -0.00131675,  0.01981676],\n",
      "       [ 0.00031502,  0.00045243,  0.01330806]]), array([[ 0.00518487],\n",
      "       [-0.00680884],\n",
      "       [ 0.02193607]])]\n",
      "gradients_biases:  [array([-7.85199093e-06, -2.85036795e-03, -5.28653365e-03]), array([-0.00389657])]\n",
      "Iteration 729, Cost: 0.2206521372368499\n",
      "gradient_weights:  [array([[-0.00064494, -0.00131307,  0.01981794],\n",
      "       [ 0.00031575,  0.000458  ,  0.01331745]]), array([[ 0.00519291],\n",
      "       [-0.00682251],\n",
      "       [ 0.02194225]])]\n",
      "gradients_biases:  [array([-7.55386216e-06, -2.85489688e-03, -5.30018236e-03]), array([-0.00389907])]\n",
      "Iteration 730, Cost: 0.22059322703893308\n",
      "gradient_weights:  [array([[-0.00064875, -0.00130937,  0.01981909],\n",
      "       [ 0.00031647,  0.0004636 ,  0.01332683]]), array([[ 0.00520097],\n",
      "       [-0.00683619],\n",
      "       [ 0.02194839]])]\n",
      "gradients_biases:  [array([-7.25248453e-06, -2.85943566e-03, -5.31378477e-03]), array([-0.00390157])]\n",
      "Iteration 731, Cost: 0.22053426543787324\n",
      "gradient_weights:  [array([[-0.00065256, -0.00130566,  0.01982022],\n",
      "       [ 0.00031719,  0.00046921,  0.01333618]]), array([[ 0.00520904],\n",
      "       [-0.00684987],\n",
      "       [ 0.02195448]])]\n",
      "gradients_biases:  [array([-6.94785109e-06, -2.86398430e-03, -5.32734077e-03]), array([-0.00390406])]\n",
      "Iteration 732, Cost: 0.22047525256382605\n",
      "gradient_weights:  [array([[-0.00065639, -0.00130194,  0.01982133],\n",
      "       [ 0.0003179 ,  0.00047484,  0.01334551]]), array([[ 0.00521713],\n",
      "       [-0.00686356],\n",
      "       [ 0.02196052]])]\n",
      "gradients_biases:  [array([-6.63995487e-06, -2.86854282e-03, -5.34085022e-03]), array([-0.00390655])]\n",
      "Iteration 733, Cost: 0.2204161885473759\n",
      "gradient_weights:  [array([[-0.00066023, -0.0012982 ,  0.01982242],\n",
      "       [ 0.0003186 ,  0.00048048,  0.01335482]]), array([[ 0.00522522],\n",
      "       [-0.00687725],\n",
      "       [ 0.02196651]])]\n",
      "gradients_biases:  [array([-6.32878889e-06, -2.87311122e-03, -5.35431300e-03]), array([-0.00390902])]\n",
      "Iteration 734, Cost: 0.22035707351953282\n",
      "gradient_weights:  [array([[-0.00066409, -0.00129445,  0.01982349],\n",
      "       [ 0.0003193 ,  0.00048614,  0.0133641 ]]), array([[ 0.00523333],\n",
      "       [-0.00689095],\n",
      "       [ 0.02197245]])]\n",
      "gradients_biases:  [array([-6.01434619e-06, -2.87768951e-03, -5.36772897e-03]), array([-0.00391149])]\n",
      "Iteration 735, Cost: 0.22029790761172963\n",
      "gradient_weights:  [array([[-0.00066795, -0.00129069,  0.01982453],\n",
      "       [ 0.00031999,  0.00049182,  0.01337336]]), array([[ 0.00524145],\n",
      "       [-0.00690466],\n",
      "       [ 0.02197835]])]\n",
      "gradients_biases:  [array([-5.69661980e-06, -2.88227771e-03, -5.38109803e-03]), array([-0.00391394])]\n",
      "Iteration 736, Cost: 0.22023869095581944\n",
      "gradient_weights:  [array([[-0.00067183, -0.00128691,  0.01982555],\n",
      "       [ 0.00032068,  0.00049752,  0.01338261]]), array([[ 0.00524959],\n",
      "       [-0.00691837],\n",
      "       [ 0.02198419]])]\n",
      "gradients_biases:  [array([-5.37560278e-06, -2.88687581e-03, -5.39442003e-03]), array([-0.00391639])]\n",
      "Iteration 737, Cost: 0.22017942368407256\n",
      "gradient_weights:  [array([[-0.00067571, -0.00128312,  0.01982655],\n",
      "       [ 0.00032136,  0.00050323,  0.01339183]]), array([[ 0.00525774],\n",
      "       [-0.0069321 ],\n",
      "       [ 0.02198999]])]\n",
      "gradients_biases:  [array([-5.05128814e-06, -2.89148384e-03, -5.40769486e-03]), array([-0.00391883])]\n",
      "Iteration 738, Cost: 0.22012010592917397\n",
      "gradient_weights:  [array([[-0.00067961, -0.00127931,  0.01982752],\n",
      "       [ 0.00032203,  0.00050896,  0.01340102]]), array([[ 0.0052659 ],\n",
      "       [-0.00694582],\n",
      "       [ 0.02199573]])]\n",
      "gradients_biases:  [array([-4.72366895e-06, -2.89610180e-03, -5.42092241e-03]), array([-0.00392127])]\n",
      "Iteration 739, Cost: 0.22006073782422014\n",
      "gradient_weights:  [array([[-0.00068352, -0.00127549,  0.01982847],\n",
      "       [ 0.0003227 ,  0.00051471,  0.0134102 ]]), array([[ 0.00527407],\n",
      "       [-0.00695956],\n",
      "       [ 0.02200143]])]\n",
      "gradients_biases:  [array([-4.39273823e-06, -2.90072971e-03, -5.43410254e-03]), array([-0.00392369])]\n",
      "Iteration 740, Cost: 0.22000131950271606\n",
      "gradient_weights:  [array([[-0.00068745, -0.00127166,  0.0198294 ],\n",
      "       [ 0.00032336,  0.00052048,  0.01341935]]), array([[ 0.00528226],\n",
      "       [-0.0069733 ],\n",
      "       [ 0.02200708]])]\n",
      "gradients_biases:  [array([-4.05848903e-06, -2.90536756e-03, -5.44723514e-03]), array([-0.00392611])]\n",
      "Iteration 741, Cost: 0.21994185109857262\n",
      "gradient_weights:  [array([[-0.00069138, -0.00126781,  0.01983031],\n",
      "       [ 0.00032401,  0.00052626,  0.01342848]]), array([[ 0.00529046],\n",
      "       [-0.00698704],\n",
      "       [ 0.02201268]])]\n",
      "gradients_biases:  [array([-3.72091440e-06, -2.91001538e-03, -5.46032010e-03]), array([-0.00392851])]\n",
      "Iteration 742, Cost: 0.21988233274610336\n",
      "gradient_weights:  [array([[-0.00069533, -0.00126395,  0.01983119],\n",
      "       [ 0.00032466,  0.00053207,  0.01343758]]), array([[ 0.00529867],\n",
      "       [-0.00700079],\n",
      "       [ 0.02201823]])]\n",
      "gradients_biases:  [array([-3.38000738e-06, -2.91467316e-03, -5.47335730e-03]), array([-0.00393091])]\n",
      "Iteration 743, Cost: 0.2198227645800216\n",
      "gradient_weights:  [array([[-0.00069929, -0.00126008,  0.01983206],\n",
      "       [ 0.0003253 ,  0.00053789,  0.01344667]]), array([[ 0.0053069 ],\n",
      "       [-0.00701455],\n",
      "       [ 0.02202373]])]\n",
      "gradients_biases:  [array([-3.03576102e-06, -2.91934093e-03, -5.48634663e-03]), array([-0.0039333])]\n",
      "Iteration 744, Cost: 0.21976314673543712\n",
      "gradient_weights:  [array([[-0.00070326, -0.00125619,  0.0198329 ],\n",
      "       [ 0.00032594,  0.00054372,  0.01345573]]), array([[ 0.00531514],\n",
      "       [-0.00702832],\n",
      "       [ 0.02202918]])]\n",
      "gradients_biases:  [array([-2.68816836e-06, -2.92401869e-03, -5.49928797e-03]), array([-0.00393568])]\n",
      "Iteration 745, Cost: 0.21970347934785334\n",
      "gradient_weights:  [array([[-0.00070725, -0.00125229,  0.01983371],\n",
      "       [ 0.00032657,  0.00054958,  0.01346477]]), array([[ 0.00532339],\n",
      "       [-0.00704209],\n",
      "       [ 0.02203459]])]\n",
      "gradients_biases:  [array([-2.33722247e-06, -2.92870645e-03, -5.51218122e-03]), array([-0.00393805])]\n",
      "Iteration 746, Cost: 0.21964376255316403\n",
      "gradient_weights:  [array([[-0.00071124, -0.00124837,  0.01983451],\n",
      "       [ 0.00032719,  0.00055545,  0.01347378]]), array([[ 0.00533165],\n",
      "       [-0.00705586],\n",
      "       [ 0.02203994]])]\n",
      "gradients_biases:  [array([-1.98291638e-06, -2.93340422e-03, -5.52502626e-03]), array([-0.00394041])]\n",
      "Iteration 747, Cost: 0.21958399648765037\n",
      "gradient_weights:  [array([[-0.00071525, -0.00124444,  0.01983528],\n",
      "       [ 0.00032781,  0.00056134,  0.01348277]]), array([[ 0.00533993],\n",
      "       [-0.00706965],\n",
      "       [ 0.02204525]])]\n",
      "gradients_biases:  [array([-1.62524315e-06, -2.93811201e-03, -5.53782299e-03]), array([-0.00394277])]\n",
      "Iteration 748, Cost: 0.21952418128797738\n",
      "gradient_weights:  [array([[-0.00071927, -0.00124049,  0.01983603],\n",
      "       [ 0.00032841,  0.00056725,  0.01349174]]), array([[ 0.00534822],\n",
      "       [-0.00708344],\n",
      "       [ 0.02205051]])]\n",
      "gradients_biases:  [array([-1.26419583e-06, -2.94282983e-03, -5.55057129e-03]), array([-0.00394511])]\n",
      "Iteration 749, Cost: 0.21946431709119119\n",
      "gradient_weights:  [array([[-0.00072331, -0.00123653,  0.01983676],\n",
      "       [ 0.00032902,  0.00057317,  0.01350069]]), array([[ 0.00535652],\n",
      "       [-0.00709723],\n",
      "       [ 0.02205571]])]\n",
      "gradients_biases:  [array([-8.99767487e-07, -2.94755769e-03, -5.56327107e-03]), array([-0.00394744])]\n",
      "Iteration 750, Cost: 0.21940440403471523\n",
      "gradient_weights:  [array([[-0.00072735, -0.00123256,  0.01983746],\n",
      "       [ 0.00032962,  0.00057912,  0.01350961]]), array([[ 0.00536484],\n",
      "       [-0.00711103],\n",
      "       [ 0.02206087]])]\n",
      "gradients_biases:  [array([-5.31951163e-07, -2.95229560e-03, -5.57592222e-03]), array([-0.00394977])]\n",
      "Iteration 751, Cost: 0.21934444225634747\n",
      "gradient_weights:  [array([[-0.00073141, -0.00122857,  0.01983814],\n",
      "       [ 0.00033021,  0.00058508,  0.01351851]]), array([[ 0.00537317],\n",
      "       [-0.00712484],\n",
      "       [ 0.02206598]])]\n",
      "gradients_biases:  [array([-1.60739918e-07, -2.95704357e-03, -5.58852464e-03]), array([-0.00395209])]\n",
      "Iteration 752, Cost: 0.21928443189425656\n",
      "gradient_weights:  [array([[-0.00073548, -0.00122457,  0.0198388 ],\n",
      "       [ 0.00033079,  0.00059106,  0.01352739]]), array([[ 0.00538151],\n",
      "       [-0.00713865],\n",
      "       [ 0.02207104]])]\n",
      "gradients_biases:  [array([ 2.13873190e-07, -2.96180161e-03, -5.60107823e-03]), array([-0.00395439])]\n",
      "Iteration 753, Cost: 0.21922437308697912\n",
      "gradient_weights:  [array([[-0.00073956, -0.00122056,  0.01983944],\n",
      "       [ 0.00033137,  0.00059705,  0.01353625]]), array([[ 0.00538987],\n",
      "       [-0.00715247],\n",
      "       [ 0.02207606]])]\n",
      "gradients_biases:  [array([ 5.91895103e-07, -2.96656973e-03, -5.61358288e-03]), array([-0.00395669])]\n",
      "Iteration 754, Cost: 0.21916426597341562\n",
      "gradient_weights:  [array([[-0.00074365, -0.00121653,  0.01984005],\n",
      "       [ 0.00033194,  0.00060307,  0.01354508]]), array([[ 0.00539823],\n",
      "       [-0.0071663 ],\n",
      "       [ 0.02208102]])]\n",
      "gradients_biases:  [array([ 9.73332763e-07, -2.97134794e-03, -5.62603850e-03]), array([-0.00395898])]\n",
      "Iteration 755, Cost: 0.21910411069282762\n",
      "gradient_weights:  [array([[-0.00074776, -0.00121248,  0.01984064],\n",
      "       [ 0.0003325 ,  0.0006091 ,  0.01355388]]), array([[ 0.00540662],\n",
      "       [-0.00718013],\n",
      "       [ 0.02208593]])]\n",
      "gradients_biases:  [array([ 1.35819311e-06, -2.97613626e-03, -5.63844499e-03]), array([-0.00396126])]\n",
      "Iteration 756, Cost: 0.21904390738483392\n",
      "gradient_weights:  [array([[-0.00075188, -0.00120843,  0.01984121],\n",
      "       [ 0.00033306,  0.00061515,  0.01356267]]), array([[ 0.00541501],\n",
      "       [-0.00719396],\n",
      "       [ 0.0220908 ]])]\n",
      "gradients_biases:  [array([ 1.74648308e-06, -2.98093468e-03, -5.65080226e-03]), array([-0.00396353])]\n",
      "Iteration 757, Cost: 0.21898365618940713\n",
      "gradient_weights:  [array([[-0.00075601, -0.00120435,  0.01984176],\n",
      "       [ 0.00033361,  0.00062122,  0.01357143]]), array([[ 0.00542342],\n",
      "       [-0.0072078 ],\n",
      "       [ 0.02209562]])]\n",
      "gradients_biases:  [array([ 2.13820961e-06, -2.98574323e-03, -5.66311021e-03]), array([-0.00396579])]\n",
      "Iteration 758, Cost: 0.2189233572468703\n",
      "gradient_weights:  [array([[-0.00076015, -0.00120027,  0.01984228],\n",
      "       [ 0.00033416,  0.0006273 ,  0.01358017]]), array([[ 0.00543184],\n",
      "       [-0.00722165],\n",
      "       [ 0.02210038]])]\n",
      "gradients_biases:  [array([ 2.53337965e-06, -2.99056191e-03, -5.67536876e-03]), array([-0.00396803])]\n",
      "Iteration 759, Cost: 0.21886301069789316\n",
      "gradient_weights:  [array([[-0.00076431, -0.00119617,  0.01984278],\n",
      "       [ 0.00033469,  0.00063341,  0.01358888]]), array([[ 0.00544027],\n",
      "       [-0.00723551],\n",
      "       [ 0.0221051 ]])]\n",
      "gradients_biases:  [array([ 2.93200013e-06, -2.99539073e-03, -5.68757780e-03]), array([-0.00397027])]\n",
      "Iteration 760, Cost: 0.21880261668348888\n",
      "gradient_weights:  [array([[-0.00076848, -0.00119205,  0.01984326],\n",
      "       [ 0.00033523,  0.00063953,  0.01359757]]), array([[ 0.00544872],\n",
      "       [-0.00724937],\n",
      "       [ 0.02210977]])]\n",
      "gradients_biases:  [array([ 3.33407798e-06, -3.00022971e-03, -5.69973725e-03]), array([-0.00397251])]\n",
      "Iteration 761, Cost: 0.2187421753450099\n",
      "gradient_weights:  [array([[-0.00077266, -0.00118792,  0.01984372],\n",
      "       [ 0.00033575,  0.00064567,  0.01360623]]), array([[ 0.00545717],\n",
      "       [-0.00726323],\n",
      "       [ 0.0221144 ]])]\n",
      "gradients_biases:  [array([ 3.73962014e-06, -3.00507885e-03, -5.71184703e-03]), array([-0.00397473])]\n",
      "Iteration 762, Cost: 0.2186816868241449\n",
      "gradient_weights:  [array([[-0.00077685, -0.00118378,  0.01984415],\n",
      "       [ 0.00033627,  0.00065183,  0.01361488]]), array([[ 0.00546565],\n",
      "       [-0.0072771 ],\n",
      "       [ 0.02211897]])]\n",
      "gradients_biases:  [array([ 4.14863355e-06, -3.00993816e-03, -5.72390704e-03]), array([-0.00397694])]\n",
      "Iteration 763, Cost: 0.21862115126291481\n",
      "gradient_weights:  [array([[-0.00078106, -0.00117962,  0.01984456],\n",
      "       [ 0.00033678,  0.00065801,  0.0136235 ]]), array([[ 0.00547413],\n",
      "       [-0.00729098],\n",
      "       [ 0.02212349]])]\n",
      "gradients_biases:  [array([ 4.56112514e-06, -3.01480765e-03, -5.73591719e-03]), array([-0.00397914])]\n",
      "Iteration 764, Cost: 0.21856056880366892\n",
      "gradient_weights:  [array([[-0.00078528, -0.00117545,  0.01984495],\n",
      "       [ 0.00033728,  0.0006642 ,  0.01363209]]), array([[ 0.00548263],\n",
      "       [-0.00730486],\n",
      "       [ 0.02212797]])]\n",
      "gradients_biases:  [array([ 4.97710185e-06, -3.01968734e-03, -5.74787742e-03]), array([-0.00398133])]\n",
      "Iteration 765, Cost: 0.2184999395890817\n",
      "gradient_weights:  [array([[-0.00078951, -0.00117126,  0.01984532],\n",
      "       [ 0.00033778,  0.00067042,  0.01364066]]), array([[ 0.00549114],\n",
      "       [-0.00731874],\n",
      "       [ 0.02213239]])]\n",
      "gradients_biases:  [array([ 5.39657061e-06, -3.02457724e-03, -5.75978763e-03]), array([-0.00398351])]\n",
      "Iteration 766, Cost: 0.2184392637621485\n",
      "gradient_weights:  [array([[-0.00079375, -0.00116706,  0.01984566],\n",
      "       [ 0.00033827,  0.00067665,  0.01364921]]), array([[ 0.00549966],\n",
      "       [-0.00733264],\n",
      "       [ 0.02213677]])]\n",
      "gradients_biases:  [array([ 5.81953836e-06, -3.02947735e-03, -5.77164774e-03]), array([-0.00398568])]\n",
      "Iteration 767, Cost: 0.21837854146618202\n",
      "gradient_weights:  [array([[-0.00079801, -0.00116284,  0.01984598],\n",
      "       [ 0.00033876,  0.0006829 ,  0.01365773]]), array([[ 0.0055082 ],\n",
      "       [-0.00734654],\n",
      "       [ 0.0221411 ]])]\n",
      "gradients_biases:  [array([ 6.24601203e-06, -3.03438769e-03, -5.78345767e-03]), array([-0.00398784])]\n",
      "Iteration 768, Cost: 0.2183177728448083\n",
      "gradient_weights:  [array([[-0.00080228, -0.00115861,  0.01984627],\n",
      "       [ 0.00033924,  0.00068917,  0.01366623]]), array([[ 0.00551675],\n",
      "       [-0.00736044],\n",
      "       [ 0.02214538]])]\n",
      "gradients_biases:  [array([ 6.67599855e-06, -3.03930826e-03, -5.79521735e-03]), array([-0.00398999])]\n",
      "Iteration 769, Cost: 0.2182569580419632\n",
      "gradient_weights:  [array([[-0.00080656, -0.00115437,  0.01984655],\n",
      "       [ 0.00033971,  0.00069545,  0.01367471]]), array([[ 0.00552531],\n",
      "       [-0.00737435],\n",
      "       [ 0.02214961]])]\n",
      "gradients_biases:  [array([ 7.10950486e-06, -3.04423908e-03, -5.80692669e-03]), array([-0.00399213])]\n",
      "Iteration 770, Cost: 0.21819609720188815\n",
      "gradient_weights:  [array([[-0.00081086, -0.00115011,  0.0198468 ],\n",
      "       [ 0.00034017,  0.00070176,  0.01368316]]), array([[ 0.00553389],\n",
      "       [-0.00738826],\n",
      "       [ 0.0221538 ]])]\n",
      "gradients_biases:  [array([ 7.54653789e-06, -3.04918016e-03, -5.81858563e-03]), array([-0.00399426])]\n",
      "Iteration 771, Cost: 0.21813519046912633\n",
      "gradient_weights:  [array([[-0.00081517, -0.00114583,  0.01984703],\n",
      "       [ 0.00034063,  0.00070808,  0.01369158]]), array([[ 0.00554248],\n",
      "       [-0.00740218],\n",
      "       [ 0.02215793]])]\n",
      "gradients_biases:  [array([ 7.98710457e-06, -3.05413151e-03, -5.83019409e-03]), array([-0.00399638])]\n",
      "Iteration 772, Cost: 0.218074237988519\n",
      "gradient_weights:  [array([[-0.00081949, -0.00114155,  0.01984724],\n",
      "       [ 0.00034108,  0.00071442,  0.01369998]]), array([[ 0.00555108],\n",
      "       [-0.00741611],\n",
      "       [ 0.02216202]])]\n",
      "gradients_biases:  [array([ 8.43121184e-06, -3.05909314e-03, -5.84175199e-03]), array([-0.00399849])]\n",
      "Iteration 773, Cost: 0.21801323990520105\n",
      "gradient_weights:  [array([[-0.00082382, -0.00113724,  0.01984742],\n",
      "       [ 0.00034152,  0.00072078,  0.01370836]]), array([[ 0.00555969],\n",
      "       [-0.00743004],\n",
      "       [ 0.02216605]])]\n",
      "gradients_biases:  [array([ 8.87886664e-06, -3.06406507e-03, -5.85325927e-03]), array([-0.00400059])]\n",
      "Iteration 774, Cost: 0.2179521963645974\n",
      "gradient_weights:  [array([[-0.00082816, -0.00113292,  0.01984758],\n",
      "       [ 0.00034196,  0.00072716,  0.01371672]]), array([[ 0.00556832],\n",
      "       [-0.00744397],\n",
      "       [ 0.02217004]])]\n",
      "gradients_biases:  [array([ 9.33007589e-06, -3.06904729e-03, -5.86471586e-03]), array([-0.00400267])]\n",
      "Iteration 775, Cost: 0.21789110751241875\n",
      "gradient_weights:  [array([[-0.00083252, -0.00112859,  0.01984772],\n",
      "       [ 0.00034239,  0.00073355,  0.01372504]]), array([[ 0.00557696],\n",
      "       [-0.00745791],\n",
      "       [ 0.02217398]])]\n",
      "gradients_biases:  [array([ 9.78484654e-06, -3.07403983e-03, -5.87612168e-03]), array([-0.00400475])]\n",
      "Iteration 776, Cost: 0.2178299734946577\n",
      "gradient_weights:  [array([[-0.0008369 , -0.00112425,  0.01984784],\n",
      "       [ 0.00034281,  0.00073997,  0.01373335]]), array([[ 0.00558561],\n",
      "       [-0.00747186],\n",
      "       [ 0.02217788]])]\n",
      "gradients_biases:  [array([ 1.02431855e-05, -3.07904269e-03, -5.88747668e-03]), array([-0.00400682])]\n",
      "Iteration 777, Cost: 0.2177687944575844\n",
      "gradient_weights:  [array([[-0.00084128, -0.00111988,  0.01984793],\n",
      "       [ 0.00034323,  0.0007464 ,  0.01374163]]), array([[ 0.00559428],\n",
      "       [-0.00748581],\n",
      "       [ 0.02218172]])]\n",
      "gradients_biases:  [array([ 1.07050998e-05, -3.08405588e-03, -5.89878078e-03]), array([-0.00400887])]\n",
      "Iteration 778, Cost: 0.21770757054774276\n",
      "gradient_weights:  [array([[-0.00084568, -0.00111551,  0.019848  ],\n",
      "       [ 0.00034364,  0.00075285,  0.01374988]]), array([[ 0.00560296],\n",
      "       [-0.00749976],\n",
      "       [ 0.02218551]])]\n",
      "gradients_biases:  [array([ 1.11705962e-05, -3.08907942e-03, -5.91003393e-03]), array([-0.00401092])]\n",
      "Iteration 779, Cost: 0.2176463019119459\n",
      "gradient_weights:  [array([[-0.00085009, -0.00111112,  0.01984805],\n",
      "       [ 0.00034404,  0.00075932,  0.01375812]]), array([[ 0.00561165],\n",
      "       [-0.00751372],\n",
      "       [ 0.02218926]])]\n",
      "gradients_biases:  [array([ 1.16396818e-05, -3.09411332e-03, -5.92123606e-03]), array([-0.00401295])]\n",
      "Iteration 780, Cost: 0.2175849886972723\n",
      "gradient_weights:  [array([[-0.00085451, -0.00110671,  0.01984807],\n",
      "       [ 0.00034444,  0.00076581,  0.01376632]]), array([[ 0.00562036],\n",
      "       [-0.00752769],\n",
      "       [ 0.02219296]])]\n",
      "gradients_biases:  [array([ 1.21123634e-05, -3.09915759e-03, -5.93238711e-03]), array([-0.00401498])]\n",
      "Iteration 781, Cost: 0.21752363105106157\n",
      "gradient_weights:  [array([[-0.00085895, -0.00110229,  0.01984807],\n",
      "       [ 0.00034482,  0.00077232,  0.0137745 ]]), array([[ 0.00562907],\n",
      "       [-0.00754166],\n",
      "       [ 0.02219661]])]\n",
      "gradients_biases:  [array([ 1.25886481e-05, -3.10421224e-03, -5.94348702e-03]), array([-0.00401699])]\n",
      "Iteration 782, Cost: 0.21746222912091018\n",
      "gradient_weights:  [array([[-0.0008634 , -0.00109786,  0.01984805],\n",
      "       [ 0.00034521,  0.00077884,  0.01378266]]), array([[ 0.0056378 ],\n",
      "       [-0.00755564],\n",
      "       [ 0.02220021]])]\n",
      "gradients_biases:  [array([ 1.30685427e-05, -3.10927728e-03, -5.95453574e-03]), array([-0.00401899])]\n",
      "Iteration 783, Cost: 0.21740078305466698\n",
      "gradient_weights:  [array([[-0.00086786, -0.00109341,  0.01984801],\n",
      "       [ 0.00034558,  0.00078539,  0.01379079]]), array([[ 0.00564655],\n",
      "       [-0.00756962],\n",
      "       [ 0.02220376]])]\n",
      "gradients_biases:  [array([ 1.35520542e-05, -3.11435272e-03, -5.96553320e-03]), array([-0.00402098])]\n",
      "Iteration 784, Cost: 0.21733929300042942\n",
      "gradient_weights:  [array([[-0.00087233, -0.00108894,  0.01984794],\n",
      "       [ 0.00034595,  0.00079195,  0.0137989 ]]), array([[ 0.0056553 ],\n",
      "       [-0.0075836 ],\n",
      "       [ 0.02220726]])]\n",
      "gradients_biases:  [array([ 1.40391896e-05, -3.11943857e-03, -5.97647936e-03]), array([-0.00402297])]\n",
      "Iteration 785, Cost: 0.2172777591065386\n",
      "gradient_weights:  [array([[-0.00087682, -0.00108447,  0.01984785],\n",
      "       [ 0.00034631,  0.00079853,  0.01380698]]), array([[ 0.00566407],\n",
      "       [-0.00759759],\n",
      "       [ 0.02221072]])]\n",
      "gradients_biases:  [array([ 1.45299557e-05, -3.12453485e-03, -5.98737415e-03]), array([-0.00402493])]\n",
      "Iteration 786, Cost: 0.21721618152157562\n",
      "gradient_weights:  [array([[-0.00088132, -0.00107997,  0.01984774],\n",
      "       [ 0.00034666,  0.00080513,  0.01381504]]), array([[ 0.00567286],\n",
      "       [-0.00761159],\n",
      "       [ 0.02221413]])]\n",
      "gradients_biases:  [array([ 1.50243595e-05, -3.12964157e-03, -5.99821754e-03]), array([-0.00402689])]\n",
      "Iteration 787, Cost: 0.21715456039435665\n",
      "gradient_weights:  [array([[-0.00088584, -0.00107546,  0.0198476 ],\n",
      "       [ 0.00034701,  0.00081175,  0.01382307]]), array([[ 0.00568165],\n",
      "       [-0.00762559],\n",
      "       [ 0.02221749]])]\n",
      "gradients_biases:  [array([ 1.55224080e-05, -3.13475874e-03, -6.00900946e-03]), array([-0.00402884])]\n",
      "Iteration 788, Cost: 0.21709289587392888\n",
      "gradient_weights:  [array([[-0.00089036, -0.00107094,  0.01984745],\n",
      "       [ 0.00034735,  0.00081839,  0.01383108]]), array([[ 0.00569046],\n",
      "       [-0.00763959],\n",
      "       [ 0.0222208 ]])]\n",
      "gradients_biases:  [array([ 1.60241082e-05, -3.13988636e-03, -6.01974987e-03]), array([-0.00403077])]\n",
      "Iteration 789, Cost: 0.21703118810956595\n",
      "gradient_weights:  [array([[-0.00089491, -0.0010664 ,  0.01984727],\n",
      "       [ 0.00034768,  0.00082505,  0.01383906]]), array([[ 0.00569928],\n",
      "       [-0.0076536 ],\n",
      "       [ 0.02222406]])]\n",
      "gradients_biases:  [array([ 1.65294668e-05, -3.14502446e-03, -6.03043872e-03]), array([-0.0040327])]\n",
      "Iteration 790, Cost: 0.21696943725076362\n",
      "gradient_weights:  [array([[-0.00089946, -0.00106185,  0.01984706],\n",
      "       [ 0.000348  ,  0.00083172,  0.01384701]]), array([[ 0.00570812],\n",
      "       [-0.00766762],\n",
      "       [ 0.02222728]])]\n",
      "gradients_biases:  [array([ 1.70384910e-05, -3.15017304e-03, -6.04107596e-03]), array([-0.00403461])]\n",
      "Iteration 791, Cost: 0.21690764344723534\n",
      "gradient_weights:  [array([[-0.00090403, -0.00105728,  0.01984684],\n",
      "       [ 0.00034832,  0.00083842,  0.01385495]]), array([[ 0.00571696],\n",
      "       [-0.00768164],\n",
      "       [ 0.02223044]])]\n",
      "gradients_biases:  [array([ 1.75511877e-05, -3.15533212e-03, -6.05166156e-03]), array([-0.00403651])]\n",
      "Iteration 792, Cost: 0.2168458068489074\n",
      "gradient_weights:  [array([[-0.00090861, -0.0010527 ,  0.01984659],\n",
      "       [ 0.00034863,  0.00084513,  0.01386285]]), array([[ 0.00572582],\n",
      "       [-0.00769566],\n",
      "       [ 0.02223356]])]\n",
      "gradients_biases:  [array([ 1.80675638e-05, -3.16050170e-03, -6.06219546e-03]), array([-0.0040384])]\n",
      "Iteration 793, Cost: 0.21678392760591494\n",
      "gradient_weights:  [array([[-0.0009132 , -0.0010481 ,  0.01984632],\n",
      "       [ 0.00034893,  0.00085186,  0.01387073]]), array([[ 0.00573469],\n",
      "       [-0.00770969],\n",
      "       [ 0.02223663]])]\n",
      "gradients_biases:  [array([ 1.85876262e-05, -3.16568181e-03, -6.07267763e-03]), array([-0.00404028])]\n",
      "Iteration 794, Cost: 0.21672200586859713\n",
      "gradient_weights:  [array([[-0.00091781, -0.00104349,  0.01984602],\n",
      "       [ 0.00034923,  0.00085861,  0.01387859]]), array([[ 0.00574358],\n",
      "       [-0.00772372],\n",
      "       [ 0.02223965]])]\n",
      "gradients_biases:  [array([ 1.91113820e-05, -3.17087244e-03, -6.08310802e-03]), array([-0.00404215])]\n",
      "Iteration 795, Cost: 0.21666004178749265\n",
      "gradient_weights:  [array([[-0.00092243, -0.00103886,  0.0198457 ],\n",
      "       [ 0.00034952,  0.00086538,  0.01388642]]), array([[ 0.00575248],\n",
      "       [-0.00773776],\n",
      "       [ 0.02224263]])]\n",
      "gradients_biases:  [array([ 1.96388381e-05, -3.17607362e-03, -6.09348660e-03]), array([-0.00404401])]\n",
      "Iteration 796, Cost: 0.2165980355133352\n",
      "gradient_weights:  [array([[-0.00092706, -0.00103422,  0.01984536],\n",
      "       [ 0.0003498 ,  0.00087217,  0.01389422]]), array([[ 0.00576139],\n",
      "       [-0.0077518 ],\n",
      "       [ 0.02224556]])]\n",
      "gradients_biases:  [array([ 2.01700014e-05, -3.18128535e-03, -6.10381333e-03]), array([-0.00404585])]\n",
      "Iteration 797, Cost: 0.21653598719704858\n",
      "gradient_weights:  [array([[-0.00093171, -0.00102956,  0.019845  ],\n",
      "       [ 0.00035008,  0.00087897,  0.013902  ]]), array([[ 0.00577031],\n",
      "       [-0.00776585],\n",
      "       [ 0.02224843]])]\n",
      "gradients_biases:  [array([ 2.07048789e-05, -3.18650765e-03, -6.11408817e-03]), array([-0.00404768])]\n",
      "Iteration 798, Cost: 0.2164738969897425\n",
      "gradient_weights:  [array([[-0.00093637, -0.00102489,  0.01984462],\n",
      "       [ 0.00035034,  0.0008858 ,  0.01390975]]), array([[ 0.00577925],\n",
      "       [-0.0077799 ],\n",
      "       [ 0.02225127]])]\n",
      "gradients_biases:  [array([ 2.12434776e-05, -3.19174053e-03, -6.12431110e-03]), array([-0.0040495])]\n",
      "Iteration 799, Cost: 0.2164117650427077\n",
      "gradient_weights:  [array([[-0.00094104, -0.0010202 ,  0.01984421],\n",
      "       [ 0.0003506 ,  0.00089264,  0.01391748]]), array([[ 0.0057882 ],\n",
      "       [-0.00779395],\n",
      "       [ 0.02225405]])]\n",
      "gradients_biases:  [array([ 2.17858045e-05, -3.19698400e-03, -6.13448207e-03]), array([-0.00405131])]\n",
      "Iteration 800, Cost: 0.21634959150741112\n",
      "gradient_weights:  [array([[-0.00094573, -0.0010155 ,  0.01984377],\n",
      "       [ 0.00035086,  0.00089951,  0.01392518]]), array([[ 0.00579716],\n",
      "       [-0.00780801],\n",
      "       [ 0.02225678]])]\n",
      "gradients_biases:  [array([ 2.23318665e-05, -3.20223807e-03, -6.14460105e-03]), array([-0.00405311])]\n",
      "Iteration 801, Cost: 0.21628737653549177\n",
      "gradient_weights:  [array([[-0.00095043, -0.00101079,  0.01984332],\n",
      "       [ 0.0003511 ,  0.00090639,  0.01393286]]), array([[ 0.00580614],\n",
      "       [-0.00782208],\n",
      "       [ 0.02225947]])]\n",
      "gradients_biases:  [array([ 2.28816706e-05, -3.20750276e-03, -6.15466802e-03]), array([-0.0040549])]\n",
      "Iteration 802, Cost: 0.2162251202787551\n",
      "gradient_weights:  [array([[-0.00095514, -0.00100605,  0.01984284],\n",
      "       [ 0.00035134,  0.00091329,  0.01394051]]), array([[ 0.00581512],\n",
      "       [-0.00783615],\n",
      "       [ 0.02226211]])]\n",
      "gradients_biases:  [array([ 2.34352237e-05, -3.21277808e-03, -6.16468295e-03]), array([-0.00405667])]\n",
      "Iteration 803, Cost: 0.2161628228891691\n",
      "gradient_weights:  [array([[-0.00095987, -0.00100131,  0.01984234],\n",
      "       [ 0.00035157,  0.00092021,  0.01394814]]), array([[ 0.00582412],\n",
      "       [-0.00785022],\n",
      "       [ 0.0222647 ]])]\n",
      "gradients_biases:  [array([ 2.39925329e-05, -3.21806404e-03, -6.17464580e-03]), array([-0.00405843])]\n",
      "Iteration 804, Cost: 0.21610048451885933\n",
      "gradient_weights:  [array([[-0.00096461, -0.00099655,  0.01984182],\n",
      "       [ 0.0003518 ,  0.00092715,  0.01395573]]), array([[ 0.00583314],\n",
      "       [-0.0078643 ],\n",
      "       [ 0.02226725]])]\n",
      "gradients_biases:  [array([ 2.45536052e-05, -3.22336065e-03, -6.18455656e-03]), array([-0.00406018])]\n",
      "Iteration 805, Cost: 0.21603810532010365\n",
      "gradient_weights:  [array([[-0.00096936, -0.00099177,  0.01984127],\n",
      "       [ 0.00035201,  0.00093411,  0.01396331]]), array([[ 0.00584216],\n",
      "       [-0.00787838],\n",
      "       [ 0.02226974]])]\n",
      "gradients_biases:  [array([ 2.51184474e-05, -3.22866793e-03, -6.19441520e-03]), array([-0.00406192])]\n",
      "Iteration 806, Cost: 0.2159756854453281\n",
      "gradient_weights:  [array([[-0.00097413, -0.00098698,  0.0198407 ],\n",
      "       [ 0.00035222,  0.00094108,  0.01397085]]), array([[ 0.0058512 ],\n",
      "       [-0.00789247],\n",
      "       [ 0.02227219]])]\n",
      "gradients_biases:  [array([ 2.56870667e-05, -3.23398589e-03, -6.20422170e-03]), array([-0.00406365])]\n",
      "Iteration 807, Cost: 0.21591322504710173\n",
      "gradient_weights:  [array([[-0.00097891, -0.00098217,  0.01984011],\n",
      "       [ 0.00035242,  0.00094808,  0.01397837]]), array([[ 0.00586025],\n",
      "       [-0.00790656],\n",
      "       [ 0.02227459]])]\n",
      "gradients_biases:  [array([ 2.62594700e-05, -3.23931454e-03, -6.21397603e-03]), array([-0.00406536])]\n",
      "Iteration 808, Cost: 0.21585072427813162\n",
      "gradient_weights:  [array([[-0.0009837 , -0.00097735,  0.0198395 ],\n",
      "       [ 0.00035262,  0.00095509,  0.01398587]]), array([[ 0.00586932],\n",
      "       [-0.00792065],\n",
      "       [ 0.02227695]])]\n",
      "gradients_biases:  [array([ 2.68356643e-05, -3.24465390e-03, -6.22367818e-03]), array([-0.00406706])]\n",
      "Iteration 809, Cost: 0.2157881832912584\n",
      "gradient_weights:  [array([[-0.00098851, -0.00097252,  0.01983886],\n",
      "       [ 0.00035281,  0.00096213,  0.01399334]]), array([[ 0.00587839],\n",
      "       [-0.00793475],\n",
      "       [ 0.02227925]])]\n",
      "gradients_biases:  [array([ 2.74156566e-05, -3.25000397e-03, -6.23332812e-03]), array([-0.00406875])]\n",
      "Iteration 810, Cost: 0.215725602239451\n",
      "gradient_weights:  [array([[-0.00099333, -0.00096766,  0.0198382 ],\n",
      "       [ 0.00035299,  0.00096918,  0.01400078]]), array([[ 0.00588748],\n",
      "       [-0.00794885],\n",
      "       [ 0.02228151]])]\n",
      "gradients_biases:  [array([ 2.79994539e-05, -3.25536477e-03, -6.24292585e-03]), array([-0.00407043])]\n",
      "Iteration 811, Cost: 0.215662981275802\n",
      "gradient_weights:  [array([[-0.00099817, -0.0009628 ,  0.01983752],\n",
      "       [ 0.00035316,  0.00097625,  0.0140082 ]]), array([[ 0.00589658],\n",
      "       [-0.00796296],\n",
      "       [ 0.02228372]])]\n",
      "gradients_biases:  [array([ 2.85870632e-05, -3.26073632e-03, -6.25247134e-03]), array([-0.00407209])]\n",
      "Iteration 812, Cost: 0.21560032055352257\n",
      "gradient_weights:  [array([[-0.00100302, -0.00095792,  0.01983681],\n",
      "       [ 0.00035332,  0.00098334,  0.01401559]]), array([[ 0.0059057 ],\n",
      "       [-0.00797707],\n",
      "       [ 0.02228589]])]\n",
      "gradients_biases:  [array([ 2.91784915e-05, -3.26611862e-03, -6.26196459e-03]), array([-0.00407375])]\n",
      "Iteration 813, Cost: 0.21553762022593753\n",
      "gradient_weights:  [array([[-0.00100788, -0.00095302,  0.01983608],\n",
      "       [ 0.00035348,  0.00099045,  0.01402295]]), array([[ 0.00591483],\n",
      "       [-0.00799118],\n",
      "       [ 0.022288  ]])]\n",
      "gradients_biases:  [array([ 2.97737459e-05, -3.27151170e-03, -6.27140557e-03]), array([-0.00407539])]\n",
      "Iteration 814, Cost: 0.2154748804464806\n",
      "gradient_weights:  [array([[-0.00101276, -0.00094811,  0.01983533],\n",
      "       [ 0.00035363,  0.00099758,  0.01403029]]), array([[ 0.00592397],\n",
      "       [-0.0080053 ],\n",
      "       [ 0.02229007]])]\n",
      "gradients_biases:  [array([ 3.03728334e-05, -3.27691555e-03, -6.28079428e-03]), array([-0.00407702])]\n",
      "Iteration 815, Cost: 0.21541210136868896\n",
      "gradient_weights:  [array([[-0.00101765, -0.00094318,  0.01983455],\n",
      "       [ 0.00035377,  0.00100473,  0.0140376 ]]), array([[ 0.00593312],\n",
      "       [-0.00801942],\n",
      "       [ 0.0222921 ]])]\n",
      "gradients_biases:  [array([ 3.09757609e-05, -3.28233020e-03, -6.29013071e-03]), array([-0.00407863])]\n",
      "Iteration 816, Cost: 0.21534928314619886\n",
      "gradient_weights:  [array([[-0.00102255, -0.00093824,  0.01983376],\n",
      "       [ 0.00035391,  0.0010119 ,  0.01404488]]), array([[ 0.00594228],\n",
      "       [-0.00803355],\n",
      "       [ 0.02229407]])]\n",
      "gradients_biases:  [array([ 3.15825356e-05, -3.28775566e-03, -6.29941485e-03]), array([-0.00408023])]\n",
      "Iteration 817, Cost: 0.21528642593273997\n",
      "gradient_weights:  [array([[-0.00102747, -0.00093328,  0.01983294],\n",
      "       [ 0.00035404,  0.00101909,  0.01405214]]), array([[ 0.00595146],\n",
      "       [-0.00804768],\n",
      "       [ 0.022296  ]])]\n",
      "gradients_biases:  [array([ 3.21931644e-05, -3.29319195e-03, -6.30864669e-03]), array([-0.00408183])]\n",
      "Iteration 818, Cost: 0.21522352988213111\n",
      "gradient_weights:  [array([[-0.0010324 , -0.00092831,  0.01983209],\n",
      "       [ 0.00035416,  0.00102629,  0.01405937]]), array([[ 0.00596065],\n",
      "       [-0.00806181],\n",
      "       [ 0.02229788]])]\n",
      "gradients_biases:  [array([ 3.28076544e-05, -3.29863907e-03, -6.31782623e-03]), array([-0.0040834])]\n",
      "Iteration 819, Cost: 0.21516059514827424\n",
      "gradient_weights:  [array([[-0.00103734, -0.00092333,  0.01983123],\n",
      "       [ 0.00035427,  0.00103352,  0.01406658]]), array([[ 0.00596986],\n",
      "       [-0.00807595],\n",
      "       [ 0.02229971]])]\n",
      "gradients_biases:  [array([ 3.34260126e-05, -3.30409704e-03, -6.32695347e-03]), array([-0.00408497])]\n",
      "Iteration 820, Cost: 0.2150976218851503\n",
      "gradient_weights:  [array([[-0.0010423 , -0.00091832,  0.01983034],\n",
      "       [ 0.00035438,  0.00104076,  0.01407376]]), array([[ 0.00597907],\n",
      "       [-0.00809009],\n",
      "       [ 0.0223015 ]])]\n",
      "gradients_biases:  [array([ 3.40482462e-05, -3.30956587e-03, -6.33602839e-03]), array([-0.00408652])]\n",
      "Iteration 821, Cost: 0.21503461024681364\n",
      "gradient_weights:  [array([[-0.00104727, -0.00091331,  0.01982942],\n",
      "       [ 0.00035447,  0.00104803,  0.01408091]]), array([[ 0.0059883 ],\n",
      "       [-0.00810424],\n",
      "       [ 0.02230324]])]\n",
      "gradients_biases:  [array([ 3.46743620e-05, -3.31504559e-03, -6.34505101e-03]), array([-0.00408806])]\n",
      "Iteration 822, Cost: 0.21497156038738707\n",
      "gradient_weights:  [array([[-0.00105226, -0.00090828,  0.01982849],\n",
      "       [ 0.00035457,  0.00105531,  0.01408804]]), array([[ 0.00599754],\n",
      "       [-0.00811839],\n",
      "       [ 0.02230493]])]\n",
      "gradients_biases:  [array([ 3.53043673e-05, -3.32053619e-03, -6.35402132e-03]), array([-0.00408959])]\n",
      "Iteration 823, Cost: 0.21490847246105657\n",
      "gradient_weights:  [array([[-0.00105726, -0.00090323,  0.01982753],\n",
      "       [ 0.00035465,  0.00106261,  0.01409514]]), array([[ 0.00600679],\n",
      "       [-0.00813254],\n",
      "       [ 0.02230658]])]\n",
      "gradients_biases:  [array([ 3.59382690e-05, -3.32603770e-03, -6.36293932e-03]), array([-0.00409111])]\n",
      "Iteration 824, Cost: 0.21484534662206664\n",
      "gradient_weights:  [array([[-0.00106227, -0.00089817,  0.01982655],\n",
      "       [ 0.00035472,  0.00106993,  0.01410221]]), array([[ 0.00601606],\n",
      "       [-0.0081467 ],\n",
      "       [ 0.02230818]])]\n",
      "gradients_biases:  [array([ 3.65760743e-05, -3.33155013e-03, -6.37180501e-03]), array([-0.00409261])]\n",
      "Iteration 825, Cost: 0.2147821830247144\n",
      "gradient_weights:  [array([[-0.0010673 , -0.00089309,  0.01982554],\n",
      "       [ 0.00035479,  0.00107727,  0.01410925]]), array([[ 0.00602534],\n",
      "       [-0.00816086],\n",
      "       [ 0.02230973]])]\n",
      "gradients_biases:  [array([ 3.72177902e-05, -3.33707349e-03, -6.38061841e-03]), array([-0.0040941])]\n",
      "Iteration 826, Cost: 0.21471898182334528\n",
      "gradient_weights:  [array([[-0.00107234, -0.000888  ,  0.01982452],\n",
      "       [ 0.00035485,  0.00108463,  0.01411627]]), array([[ 0.00603463],\n",
      "       [-0.00817503],\n",
      "       [ 0.02231124]])]\n",
      "gradients_biases:  [array([ 3.78634238e-05, -3.34260779e-03, -6.38937951e-03]), array([-0.00409557])]\n",
      "Iteration 827, Cost: 0.2146557431723471\n",
      "gradient_weights:  [array([[-0.0010774 , -0.00088289,  0.01982347],\n",
      "       [ 0.0003549 ,  0.00109201,  0.01412326]]), array([[ 0.00604393],\n",
      "       [-0.00818919],\n",
      "       [ 0.0223127 ]])]\n",
      "gradients_biases:  [array([ 3.85129821e-05, -3.34815306e-03, -6.39808833e-03]), array([-0.00409704])]\n",
      "Iteration 828, Cost: 0.21459246722614542\n",
      "gradient_weights:  [array([[-0.00108247, -0.00087777,  0.01982239],\n",
      "       [ 0.00035495,  0.00109941,  0.01413023]]), array([[ 0.00605325],\n",
      "       [-0.00820336],\n",
      "       [ 0.02231411]])]\n",
      "gradients_biases:  [array([ 3.91664724e-05, -3.35370931e-03, -6.40674487e-03]), array([-0.00409849])]\n",
      "Iteration 829, Cost: 0.21452915413919815\n",
      "gradient_weights:  [array([[-0.00108755, -0.00087263,  0.0198213 ],\n",
      "       [ 0.00035499,  0.00110683,  0.01413716]]), array([[ 0.00606257],\n",
      "       [-0.00821754],\n",
      "       [ 0.02231548]])]\n",
      "gradients_biases:  [array([ 3.98239016e-05, -3.35927654e-03, -6.41534914e-03]), array([-0.00409993])]\n",
      "Iteration 830, Cost: 0.21446580406599014\n",
      "gradient_weights:  [array([[-0.00109265, -0.00086748,  0.01982018],\n",
      "       [ 0.00035502,  0.00111426,  0.01414408]]), array([[ 0.00607191],\n",
      "       [-0.00823172],\n",
      "       [ 0.0223168 ]])]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradients_biases:  [array([ 4.04852769e-05, -3.36485478e-03, -6.42390116e-03]), array([-0.00410135])]\n",
      "Iteration 831, Cost: 0.21440241716102842\n",
      "gradient_weights:  [array([[-0.00109776, -0.00086231,  0.01981904],\n",
      "       [ 0.00035504,  0.00112172,  0.01415096]]), array([[ 0.00608127],\n",
      "       [-0.0082459 ],\n",
      "       [ 0.02231807]])]\n",
      "gradients_biases:  [array([ 4.11506054e-05, -3.37044404e-03, -6.43240094e-03]), array([-0.00410276])]\n",
      "Iteration 832, Cost: 0.2143389935788365\n",
      "gradient_weights:  [array([[-0.00110288, -0.00085713,  0.01981787],\n",
      "       [ 0.00035505,  0.00112919,  0.01415782]]), array([[ 0.00609063],\n",
      "       [-0.00826009],\n",
      "       [ 0.0223193 ]])]\n",
      "gradients_biases:  [array([ 4.18198942e-05, -3.37604433e-03, -6.44084848e-03]), array([-0.00410416])]\n",
      "Iteration 833, Cost: 0.21427553347394945\n",
      "gradient_weights:  [array([[-0.00110802, -0.00085193,  0.01981669],\n",
      "       [ 0.00035506,  0.00113669,  0.01416464]]), array([[ 0.00610001],\n",
      "       [-0.00827428],\n",
      "       [ 0.02232048]])]\n",
      "gradients_biases:  [array([ 4.24931505e-05, -3.38165568e-03, -6.44924382e-03]), array([-0.00410555])]\n",
      "Iteration 834, Cost: 0.2142120370009084\n",
      "gradient_weights:  [array([[-0.00111317, -0.00084672,  0.01981548],\n",
      "       [ 0.00035506,  0.0011442 ,  0.01417145]]), array([[ 0.0061094 ],\n",
      "       [-0.00828847],\n",
      "       [ 0.02232162]])]\n",
      "gradients_biases:  [array([ 4.31703813e-05, -3.38727808e-03, -6.45758697e-03]), array([-0.00410692])]\n",
      "Iteration 835, Cost: 0.21414850431425553\n",
      "gradient_weights:  [array([[-0.00111834, -0.00084149,  0.01981424],\n",
      "       [ 0.00035505,  0.00115173,  0.01417822]]), array([[ 0.0061188 ],\n",
      "       [-0.00830267],\n",
      "       [ 0.02232271]])]\n",
      "gradients_biases:  [array([ 4.38515939e-05, -3.39291156e-03, -6.46587793e-03]), array([-0.00410828])]\n",
      "Iteration 836, Cost: 0.21408493556852842\n",
      "gradient_weights:  [array([[-0.00112352, -0.00083625,  0.01981299],\n",
      "       [ 0.00035504,  0.00115929,  0.01418497]]), array([[ 0.00612822],\n",
      "       [-0.00831686],\n",
      "       [ 0.02232375]])]\n",
      "gradients_biases:  [array([ 4.45367953e-05, -3.39855614e-03, -6.47411675e-03]), array([-0.00410962])]\n",
      "Iteration 837, Cost: 0.2140213309182551\n",
      "gradient_weights:  [array([[-0.00112872, -0.00083099,  0.01981171],\n",
      "       [ 0.00035501,  0.00116686,  0.01419169]]), array([[ 0.00613764],\n",
      "       [-0.00833107],\n",
      "       [ 0.02232474]])]\n",
      "gradients_biases:  [array([ 4.52259928e-05, -3.40421182e-03, -6.48230343e-03]), array([-0.00411096])]\n",
      "Iteration 838, Cost: 0.21395769051794866\n",
      "gradient_weights:  [array([[-0.00113393, -0.00082572,  0.0198104 ],\n",
      "       [ 0.00035498,  0.00117445,  0.01419838]]), array([[ 0.00614708],\n",
      "       [-0.00834527],\n",
      "       [ 0.0223257 ]])]\n",
      "gradients_biases:  [array([ 4.59191935e-05, -3.40987863e-03, -6.49043799e-03]), array([-0.00411228])]\n",
      "Iteration 839, Cost: 0.21389401452210197\n",
      "gradient_weights:  [array([[-0.00113915, -0.00082043,  0.01980908],\n",
      "       [ 0.00035494,  0.00118206,  0.01420505]]), array([[ 0.00615653],\n",
      "       [-0.00835948],\n",
      "       [ 0.0223266 ]])]\n",
      "gradients_biases:  [array([ 4.66164045e-05, -3.41555657e-03, -6.49852048e-03]), array([-0.00411358])]\n",
      "Iteration 840, Cost: 0.213830303085182\n",
      "gradient_weights:  [array([[-0.00114439, -0.00081513,  0.01980773],\n",
      "       [ 0.0003549 ,  0.00118969,  0.01421169]]), array([[ 0.006166  ],\n",
      "       [-0.0083737 ],\n",
      "       [ 0.02232746]])]\n",
      "gradients_biases:  [array([ 4.73176331e-05, -3.42124567e-03, -6.50655090e-03]), array([-0.00411487])]\n",
      "Iteration 841, Cost: 0.21376655636162506\n",
      "gradient_weights:  [array([[-0.00114964, -0.00080981,  0.01980636],\n",
      "       [ 0.00035484,  0.00119734,  0.0142183 ]]), array([[ 0.00617547],\n",
      "       [-0.00838791],\n",
      "       [ 0.02232827]])]\n",
      "gradients_biases:  [array([ 4.80228864e-05, -3.42694594e-03, -6.51452928e-03]), array([-0.00411615])]\n",
      "Iteration 842, Cost: 0.21370277450583103\n",
      "gradient_weights:  [array([[-0.0011549 , -0.00080448,  0.01980497],\n",
      "       [ 0.00035478,  0.00120501,  0.01422488]]), array([[ 0.00618496],\n",
      "       [-0.00840213],\n",
      "       [ 0.02232904]])]\n",
      "gradients_biases:  [array([ 4.87321716e-05, -3.43265739e-03, -6.52245566e-03]), array([-0.00411742])]\n",
      "Iteration 843, Cost: 0.2136389576721582\n",
      "gradient_weights:  [array([[-0.00116018, -0.00079913,  0.01980355],\n",
      "       [ 0.00035471,  0.00121269,  0.01423144]]), array([[ 0.00619446],\n",
      "       [-0.00841635],\n",
      "       [ 0.02232976]])]\n",
      "gradients_biases:  [array([ 4.94454959e-05, -3.43838004e-03, -6.53033007e-03]), array([-0.00411867])]\n",
      "Iteration 844, Cost: 0.21357510601491791\n",
      "gradient_weights:  [array([[-0.00116548, -0.00079376,  0.01980211],\n",
      "       [ 0.00035463,  0.0012204 ,  0.01423797]]), array([[ 0.00620397],\n",
      "       [-0.00843058],\n",
      "       [ 0.02233044]])]\n",
      "gradients_biases:  [array([ 5.01628666e-05, -3.44411390e-03, -6.53815253e-03]), array([-0.00411991])]\n",
      "Iteration 845, Cost: 0.2135112196883691\n",
      "gradient_weights:  [array([[-0.00117078, -0.00078839,  0.01980065],\n",
      "       [ 0.00035455,  0.00122813,  0.01424447]]), array([[ 0.00621349],\n",
      "       [-0.00844481],\n",
      "       [ 0.02233107]])]\n",
      "gradients_biases:  [array([ 5.08842909e-05, -3.44985900e-03, -6.54592308e-03]), array([-0.00412114])]\n",
      "Iteration 846, Cost: 0.21344729884671287\n",
      "gradient_weights:  [array([[-0.00117611, -0.00078299,  0.01979916],\n",
      "       [ 0.00035445,  0.00123587,  0.01425094]]), array([[ 0.00622303],\n",
      "       [-0.00845904],\n",
      "       [ 0.02233165]])]\n",
      "gradients_biases:  [array([ 5.16097760e-05, -3.45561535e-03, -6.55364175e-03]), array([-0.00412235])]\n",
      "Iteration 847, Cost: 0.21338334364408737\n",
      "gradient_weights:  [array([[-0.00118144, -0.00077758,  0.01979765],\n",
      "       [ 0.00035435,  0.00124364,  0.01425739]]), array([[ 0.00623258],\n",
      "       [-0.00847328],\n",
      "       [ 0.02233219]])]\n",
      "gradients_biases:  [array([ 5.23393291e-05, -3.46138296e-03, -6.56130858e-03]), array([-0.00412355])]\n",
      "Iteration 848, Cost: 0.21331935423456228\n",
      "gradient_weights:  [array([[-0.00118679, -0.00077216,  0.01979612],\n",
      "       [ 0.00035424,  0.00125142,  0.01426381]]), array([[ 0.00624214],\n",
      "       [-0.00848751],\n",
      "       [ 0.02233269]])]\n",
      "gradients_biases:  [array([ 5.30729574e-05, -3.46716184e-03, -6.56892360e-03]), array([-0.00412473])]\n",
      "Iteration 849, Cost: 0.2132553307721331\n",
      "gradient_weights:  [array([[-0.00119216, -0.00076672,  0.01979457],\n",
      "       [ 0.00035413,  0.00125923,  0.0142702 ]]), array([[ 0.00625171],\n",
      "       [-0.00850176],\n",
      "       [ 0.02233314]])]\n",
      "gradients_biases:  [array([ 5.38106684e-05, -3.47295203e-03, -6.57648685e-03]), array([-0.0041259])]\n",
      "Iteration 850, Cost: 0.21319127341071642\n",
      "gradient_weights:  [array([[-0.00119754, -0.00076127,  0.01979299],\n",
      "       [ 0.000354  ,  0.00126705,  0.01427656]]), array([[ 0.0062613 ],\n",
      "       [-0.008516  ],\n",
      "       [ 0.02233354]])]\n",
      "gradients_biases:  [array([ 5.45524691e-05, -3.47875352e-03, -6.58399837e-03]), array([-0.00412706])]\n",
      "Iteration 851, Cost: 0.21312718230414388\n",
      "gradient_weights:  [array([[-0.00120293, -0.0007558 ,  0.01979139],\n",
      "       [ 0.00035387,  0.00127489,  0.01428289]]), array([[ 0.00627089],\n",
      "       [-0.00853025],\n",
      "       [ 0.0223339 ]])]\n",
      "gradients_biases:  [array([ 5.52983669e-05, -3.48456634e-03, -6.59145820e-03]), array([-0.0041282])]\n",
      "Iteration 852, Cost: 0.21306305760615707\n",
      "gradient_weights:  [array([[-0.00120834, -0.00075032,  0.01978977],\n",
      "       [ 0.00035373,  0.00128276,  0.0142892 ]]), array([[ 0.0062805 ],\n",
      "       [-0.0085445 ],\n",
      "       [ 0.02233421]])]\n",
      "gradients_biases:  [array([ 5.60483691e-05, -3.49039051e-03, -6.59886639e-03]), array([-0.00412933])]\n",
      "Iteration 853, Cost: 0.21299889947040207\n",
      "gradient_weights:  [array([[-0.00121376, -0.00074482,  0.01978812],\n",
      "       [ 0.00035359,  0.00129064,  0.01429548]]), array([[ 0.00629012],\n",
      "       [-0.00855875],\n",
      "       [ 0.02233448]])]\n",
      "gradients_biases:  [array([ 5.68024829e-05, -3.49622603e-03, -6.60622297e-03]), array([-0.00413045])]\n",
      "Iteration 854, Cost: 0.21293470805042392\n",
      "gradient_weights:  [array([[-0.0012192 , -0.00073931,  0.01978646],\n",
      "       [ 0.00035343,  0.00129854,  0.01430173]]), array([[ 0.00629975],\n",
      "       [-0.00857301],\n",
      "       [ 0.0223347 ]])]\n",
      "gradients_biases:  [array([ 5.75607157e-05, -3.50207294e-03, -6.61352800e-03]), array([-0.00413155])]\n",
      "Iteration 855, Cost: 0.21287048349966142\n",
      "gradient_weights:  [array([[-0.00122465, -0.00073378,  0.01978476],\n",
      "       [ 0.00035327,  0.00130646,  0.01430796]]), array([[ 0.0063094 ],\n",
      "       [-0.00858727],\n",
      "       [ 0.02233488]])]\n",
      "gradients_biases:  [array([ 5.83230748e-05, -3.50793123e-03, -6.62078151e-03]), array([-0.00413264])]\n",
      "Iteration 856, Cost: 0.2128062259714416\n",
      "gradient_weights:  [array([[-0.00123011, -0.00072823,  0.01978305],\n",
      "       [ 0.0003531 ,  0.0013144 ,  0.01431415]]), array([[ 0.00631905],\n",
      "       [-0.00860153],\n",
      "       [ 0.02233502]])]\n",
      "gradients_biases:  [array([ 5.90895675e-05, -3.51380094e-03, -6.62798355e-03]), array([-0.00413372])]\n",
      "Iteration 857, Cost: 0.21274193561897403\n",
      "gradient_weights:  [array([[-0.00123559, -0.00072268,  0.01978131],\n",
      "       [ 0.00035292,  0.00132236,  0.01432032]]), array([[ 0.00632872],\n",
      "       [-0.00861579],\n",
      "       [ 0.02233511]])]\n",
      "gradients_biases:  [array([ 5.98602012e-05, -3.51968208e-03, -6.63513418e-03]), array([-0.00413478])]\n",
      "Iteration 858, Cost: 0.21267761259534587\n",
      "gradient_weights:  [array([[-0.00124109, -0.0007171 ,  0.01977955],\n",
      "       [ 0.00035273,  0.00133033,  0.01432646]]), array([[ 0.0063384 ],\n",
      "       [-0.00863006],\n",
      "       [ 0.02233515]])]\n",
      "gradients_biases:  [array([ 6.06349832e-05, -3.52557466e-03, -6.64223344e-03]), array([-0.00413582])]\n",
      "Iteration 859, Cost: 0.21261325705351602\n",
      "gradient_weights:  [array([[-0.0012466 , -0.00071151,  0.01977777],\n",
      "       [ 0.00035254,  0.00133833,  0.01433257]]), array([[ 0.00634809],\n",
      "       [-0.00864433],\n",
      "       [ 0.02233515]])]\n",
      "gradients_biases:  [array([ 6.14139208e-05, -3.53147870e-03, -6.64928138e-03]), array([-0.00413686])]\n",
      "Iteration 860, Cost: 0.21254886914630983\n",
      "gradient_weights:  [array([[-0.00125212, -0.00070591,  0.01977596],\n",
      "       [ 0.00035234,  0.00134635,  0.01433865]]), array([[ 0.0063578 ],\n",
      "       [-0.00865861],\n",
      "       [ 0.02233511]])]\n",
      "gradients_biases:  [array([ 6.21970215e-05, -3.53739422e-03, -6.65627806e-03]), array([-0.00413788])]\n",
      "Iteration 861, Cost: 0.2124844490264136\n",
      "gradient_weights:  [array([[-0.00125766, -0.00070029,  0.01977414],\n",
      "       [ 0.00035213,  0.00135439,  0.01434471]]), array([[ 0.00636751],\n",
      "       [-0.00867288],\n",
      "       [ 0.02233502]])]\n",
      "gradients_biases:  [array([ 6.29842925e-05, -3.54332123e-03, -6.66322353e-03]), array([-0.00413888])]\n",
      "Iteration 862, Cost: 0.21241999684636942\n",
      "gradient_weights:  [array([[-0.00126321, -0.00069466,  0.01977228],\n",
      "       [ 0.00035191,  0.00136244,  0.01435074]]), array([[ 0.00637724],\n",
      "       [-0.00868716],\n",
      "       [ 0.02233488]])]\n",
      "gradients_biases:  [array([ 6.37757414e-05, -3.54925976e-03, -6.67011784e-03]), array([-0.00413987])]\n",
      "Iteration 863, Cost: 0.21235551275856923\n",
      "gradient_weights:  [array([[-0.00126877, -0.00068901,  0.01977041],\n",
      "       [ 0.00035168,  0.00137052,  0.01435674]]), array([[ 0.00638698],\n",
      "       [-0.00870144],\n",
      "       [ 0.02233471]])]\n",
      "gradients_biases:  [array([ 6.45713755e-05, -3.55520982e-03, -6.67696106e-03]), array([-0.00414085])]\n",
      "Iteration 864, Cost: 0.21229099691524994\n",
      "gradient_weights:  [array([[-0.00127435, -0.00068335,  0.01976851],\n",
      "       [ 0.00035145,  0.00137861,  0.01436271]]), array([[ 0.00639673],\n",
      "       [-0.00871573],\n",
      "       [ 0.02233448]])]\n",
      "gradients_biases:  [array([ 6.53712022e-05, -3.56117143e-03, -6.68375323e-03]), array([-0.00414181])]\n",
      "Iteration 865, Cost: 0.2122264494684873\n",
      "gradient_weights:  [array([[-0.00127995, -0.00067767,  0.01976659],\n",
      "       [ 0.00035121,  0.00138673,  0.01436865]]), array([[ 0.00640649],\n",
      "       [-0.00873002],\n",
      "       [ 0.02233422]])]\n",
      "gradients_biases:  [array([ 6.61752290e-05, -3.56714460e-03, -6.69049442e-03]), array([-0.00414276])]\n",
      "Iteration 866, Cost: 0.21216187057019104\n",
      "gradient_weights:  [array([[-0.00128556, -0.00067198,  0.01976465],\n",
      "       [ 0.00035096,  0.00139486,  0.01437457]]), array([[ 0.00641626],\n",
      "       [-0.00874431],\n",
      "       [ 0.02233391]])]\n",
      "gradients_biases:  [array([ 6.69834632e-05, -3.57312936e-03, -6.69718469e-03]), array([-0.0041437])]\n",
      "Iteration 867, Cost: 0.21209726037209922\n",
      "gradient_weights:  [array([[-0.00129118, -0.00066627,  0.01976269],\n",
      "       [ 0.0003507 ,  0.00140301,  0.01438045]]), array([[ 0.00642605],\n",
      "       [-0.0087586 ],\n",
      "       [ 0.02233356]])]\n",
      "gradients_biases:  [array([ 6.77959124e-05, -3.57912572e-03, -6.70382410e-03]), array([-0.00414462])]\n",
      "Iteration 868, Cost: 0.21203261902577283\n",
      "gradient_weights:  [array([[-0.00129682, -0.00066055,  0.0197607 ],\n",
      "       [ 0.00035044,  0.00141118,  0.01438631]]), array([[ 0.00643585],\n",
      "       [-0.00877289],\n",
      "       [ 0.02233316]])]\n",
      "gradients_biases:  [array([ 6.86125840e-05, -3.58513370e-03, -6.71041271e-03]), array([-0.00414552])]\n",
      "Iteration 869, Cost: 0.21196794668259\n",
      "gradient_weights:  [array([[-0.00130247, -0.00065482,  0.01975869],\n",
      "       [ 0.00035016,  0.00141938,  0.01439214]]), array([[ 0.00644565],\n",
      "       [-0.00878719],\n",
      "       [ 0.02233272]])]\n",
      "gradients_biases:  [array([ 6.94334855e-05, -3.59115332e-03, -6.71695059e-03]), array([-0.00414642])]\n",
      "Iteration 870, Cost: 0.21190324349374123\n",
      "gradient_weights:  [array([[-0.00130814, -0.00064906,  0.01975665],\n",
      "       [ 0.00034988,  0.00142759,  0.01439794]]), array([[ 0.00645547],\n",
      "       [-0.00880149],\n",
      "       [ 0.02233223]])]\n",
      "gradients_biases:  [array([ 7.02586243e-05, -3.59718459e-03, -6.72343780e-03]), array([-0.00414729])]\n",
      "Iteration 871, Cost: 0.21183850961022327\n",
      "gradient_weights:  [array([[-0.00131382, -0.0006433 ,  0.0197546 ],\n",
      "       [ 0.00034959,  0.00143582,  0.01440371]]), array([[ 0.00646531],\n",
      "       [-0.0088158 ],\n",
      "       [ 0.0223317 ]])]\n",
      "gradients_biases:  [array([ 7.10880080e-05, -3.60322754e-03, -6.72987440e-03]), array([-0.00414816])]\n",
      "Iteration 872, Cost: 0.21177374518283398\n",
      "gradient_weights:  [array([[-0.00131952, -0.00063752,  0.01975252],\n",
      "       [ 0.0003493 ,  0.00144407,  0.01440946]]), array([[ 0.00647515],\n",
      "       [-0.0088301 ],\n",
      "       [ 0.02233113]])]\n",
      "gradients_biases:  [array([ 7.19216442e-05, -3.60928218e-03, -6.73626047e-03]), array([-0.00414901])]\n",
      "Iteration 873, Cost: 0.21170895036216703\n",
      "gradient_weights:  [array([[-0.00132523, -0.00063172,  0.01975042],\n",
      "       [ 0.00034899,  0.00145233,  0.01441518]]), array([[ 0.006485  ],\n",
      "       [-0.00884441],\n",
      "       [ 0.02233051]])]\n",
      "gradients_biases:  [array([ 7.27595402e-05, -3.61534853e-03, -6.74259608e-03]), array([-0.00414984])]\n",
      "Iteration 874, Cost: 0.21164412529860616\n",
      "gradient_weights:  [array([[-0.00133096, -0.00062591,  0.01974829],\n",
      "       [ 0.00034868,  0.00146062,  0.01442086]]), array([[ 0.00649487],\n",
      "       [-0.00885872],\n",
      "       [ 0.02232985]])]\n",
      "gradients_biases:  [array([ 7.36017037e-05, -3.62142661e-03, -6.74888129e-03]), array([-0.00415066])]\n",
      "Iteration 875, Cost: 0.21157927014232003\n",
      "gradient_weights:  [array([[-0.0013367 , -0.00062008,  0.01974615],\n",
      "       [ 0.00034836,  0.00146893,  0.01442652]]), array([[ 0.00650475],\n",
      "       [-0.00887303],\n",
      "       [ 0.02232915]])]\n",
      "gradients_biases:  [array([ 7.44481422e-05, -3.62751644e-03, -6.75511617e-03]), array([-0.00415147])]\n",
      "Iteration 876, Cost: 0.21151438504325643\n",
      "gradient_weights:  [array([[-0.00134245, -0.00061424,  0.01974398],\n",
      "       [ 0.00034803,  0.00147726,  0.01443215]]), array([[ 0.00651464],\n",
      "       [-0.00888735],\n",
      "       [ 0.02232841]])]\n",
      "gradients_biases:  [array([ 7.52988632e-05, -3.63361803e-03, -6.76130081e-03]), array([-0.00415226])]\n",
      "Iteration 877, Cost: 0.2114494701511372\n",
      "gradient_weights:  [array([[-0.00134822, -0.00060839,  0.01974178],\n",
      "       [ 0.0003477 ,  0.0014856 ,  0.01443776]]), array([[ 0.00652454],\n",
      "       [-0.00890167],\n",
      "       [ 0.02232762]])]\n",
      "gradients_biases:  [array([ 7.61538745e-05, -3.63973142e-03, -6.76743526e-03]), array([-0.00415304])]\n",
      "Iteration 878, Cost: 0.21138452561545262\n",
      "gradient_weights:  [array([[-0.001354  , -0.00060252,  0.01973957],\n",
      "       [ 0.00034735,  0.00149397,  0.01444333]]), array([[ 0.00653445],\n",
      "       [-0.00891599],\n",
      "       [ 0.02232679]])]\n",
      "gradients_biases:  [array([ 7.70131835e-05, -3.64585661e-03, -6.77351962e-03]), array([-0.0041538])]\n",
      "Iteration 879, Cost: 0.21131955158545618\n",
      "gradient_weights:  [array([[-0.0013598 , -0.00059664,  0.01973733],\n",
      "       [ 0.000347  ,  0.00150235,  0.01444887]]), array([[ 0.00654437],\n",
      "       [-0.00893031],\n",
      "       [ 0.02232591]])]\n",
      "gradients_biases:  [array([ 7.78767978e-05, -3.65199362e-03, -6.77955395e-03]), array([-0.00415455])]\n",
      "Iteration 880, Cost: 0.21125454821015888\n",
      "gradient_weights:  [array([[-0.00136561, -0.00059074,  0.01973507],\n",
      "       [ 0.00034664,  0.00151076,  0.01445439]]), array([[ 0.0065543 ],\n",
      "       [-0.00894464],\n",
      "       [ 0.02232499]])]\n",
      "gradients_biases:  [array([ 7.87447252e-05, -3.65814248e-03, -6.78553833e-03]), array([-0.00415529])]\n",
      "Iteration 881, Cost: 0.211189515638324\n",
      "gradient_weights:  [array([[-0.00137144, -0.00058482,  0.01973279],\n",
      "       [ 0.00034628,  0.00151918,  0.01445988]]), array([[ 0.00656425],\n",
      "       [-0.00895896],\n",
      "       [ 0.02232403]])]\n",
      "gradients_biases:  [array([ 7.96169732e-05, -3.66430320e-03, -6.79147284e-03]), array([-0.00415601])]\n",
      "Iteration 882, Cost: 0.21112445401846164\n",
      "gradient_weights:  [array([[-0.00137728, -0.0005789 ,  0.01973048],\n",
      "       [ 0.0003459 ,  0.00152762,  0.01446534]]), array([[ 0.00657421],\n",
      "       [-0.00897329],\n",
      "       [ 0.02232303]])]\n",
      "gradients_biases:  [array([ 8.04935494e-05, -3.67047580e-03, -6.79735756e-03]), array([-0.00415671])]\n",
      "Iteration 883, Cost: 0.21105936349882337\n",
      "gradient_weights:  [array([[-0.00138314, -0.00057295,  0.01972815],\n",
      "       [ 0.00034552,  0.00153609,  0.01447077]]), array([[ 0.00658417],\n",
      "       [-0.00898763],\n",
      "       [ 0.02232198]])]\n",
      "gradients_biases:  [array([ 8.13744617e-05, -3.67666031e-03, -6.80319257e-03]), array([-0.00415741])]\n",
      "Iteration 884, Cost: 0.21099424422739693\n",
      "gradient_weights:  [array([[-0.00138901, -0.000567  ,  0.0197258 ],\n",
      "       [ 0.00034513,  0.00154457,  0.01447617]]), array([[ 0.00659415],\n",
      "       [-0.00900196],\n",
      "       [ 0.02232089]])]\n",
      "gradients_biases:  [array([ 8.22597175e-05, -3.68285674e-03, -6.80897795e-03]), array([-0.00415808])]\n",
      "Iteration 885, Cost: 0.21092909635190055\n",
      "gradient_weights:  [array([[-0.0013949 , -0.00056102,  0.01972343],\n",
      "       [ 0.00034473,  0.00155307,  0.01448154]]), array([[ 0.00660414],\n",
      "       [-0.0090163 ],\n",
      "       [ 0.02231976]])]\n",
      "gradients_biases:  [array([ 8.31493247e-05, -3.68906511e-03, -6.81471379e-03]), array([-0.00415875])]\n",
      "Iteration 886, Cost: 0.21086392001977772\n",
      "gradient_weights:  [array([[-0.0014008 , -0.00055504,  0.01972103],\n",
      "       [ 0.00034432,  0.00156159,  0.01448689]]), array([[ 0.00661414],\n",
      "       [-0.00903064],\n",
      "       [ 0.02231859]])]\n",
      "gradients_biases:  [array([ 8.40432909e-05, -3.69528544e-03, -6.82040017e-03]), array([-0.00415939])]\n",
      "Iteration 887, Cost: 0.2107987153781921\n",
      "gradient_weights:  [array([[-0.00140672, -0.00054904,  0.01971861],\n",
      "       [ 0.0003439 ,  0.00157013,  0.01449221]]), array([[ 0.00662416],\n",
      "       [-0.00904498],\n",
      "       [ 0.02231737]])]\n",
      "gradients_biases:  [array([ 8.49416239e-05, -3.70151775e-03, -6.82603718e-03]), array([-0.00416003])]\n",
      "Iteration 888, Cost: 0.21073348257402152\n",
      "gradient_weights:  [array([[-0.00141265, -0.00054302,  0.01971617],\n",
      "       [ 0.00034348,  0.00157869,  0.01449749]]), array([[ 0.00663418],\n",
      "       [-0.00905932],\n",
      "       [ 0.02231612]])]\n",
      "gradients_biases:  [array([ 8.58443314e-05, -3.70776207e-03, -6.83162491e-03]), array([-0.00416065])]\n",
      "Iteration 889, Cost: 0.21066822175385339\n",
      "gradient_weights:  [array([[-0.00141859, -0.00053699,  0.0197137 ],\n",
      "       [ 0.00034305,  0.00158726,  0.01450275]]), array([[ 0.00664421],\n",
      "       [-0.00907367],\n",
      "       [ 0.02231482]])]\n",
      "gradients_biases:  [array([ 8.67514212e-05, -3.71401841e-03, -6.83716343e-03]), array([-0.00416125])]\n",
      "Iteration 890, Cost: 0.21060293306397881\n",
      "gradient_weights:  [array([[-0.00142455, -0.00053095,  0.01971122],\n",
      "       [ 0.00034261,  0.00159586,  0.01450798]]), array([[ 0.00665426],\n",
      "       [-0.00908802],\n",
      "       [ 0.02231347]])]\n",
      "gradients_biases:  [array([ 8.76629010e-05, -3.72028679e-03, -6.84265285e-03]), array([-0.00416184])]\n",
      "Iteration 891, Cost: 0.21053761665038723\n",
      "gradient_weights:  [array([[-0.00143053, -0.00052489,  0.01970871],\n",
      "       [ 0.00034217,  0.00160448,  0.01451318]]), array([[ 0.00666432],\n",
      "       [-0.00910237],\n",
      "       [ 0.02231209]])]\n",
      "gradients_biases:  [array([ 8.85787787e-05, -3.72656724e-03, -6.84809325e-03]), array([-0.00416241])]\n",
      "Iteration 892, Cost: 0.21047227265876156\n",
      "gradient_weights:  [array([[-0.00143652, -0.00051882,  0.01970618],\n",
      "       [ 0.00034171,  0.00161311,  0.01451835]]), array([[ 0.00667438],\n",
      "       [-0.00911672],\n",
      "       [ 0.02231067]])]\n",
      "gradients_biases:  [array([ 8.94990620e-05, -3.73285977e-03, -6.85348472e-03]), array([-0.00416297])]\n",
      "Iteration 893, Cost: 0.21040690123447234\n",
      "gradient_weights:  [array([[-0.00144252, -0.00051273,  0.01970362],\n",
      "       [ 0.00034125,  0.00162177,  0.0145235 ]]), array([[ 0.00668446],\n",
      "       [-0.00913108],\n",
      "       [ 0.0223092 ]])]\n",
      "gradients_biases:  [array([ 9.04237587e-05, -3.73916441e-03, -6.85882736e-03]), array([-0.00416352])]\n",
      "Iteration 894, Cost: 0.21034150252257278\n",
      "gradient_weights:  [array([[-0.00144854, -0.00050663,  0.01970105],\n",
      "       [ 0.00034078,  0.00163044,  0.01452861]]), array([[ 0.00669455],\n",
      "       [-0.00914543],\n",
      "       [ 0.02230769]])]\n",
      "gradients_biases:  [array([ 9.13528768e-05, -3.74548117e-03, -6.86412126e-03]), array([-0.00416405])]\n",
      "Iteration 895, Cost: 0.2102760766677933\n",
      "gradient_weights:  [array([[-0.00145457, -0.00050051,  0.01969845],\n",
      "       [ 0.0003403 ,  0.00163913,  0.01453369]]), array([[ 0.00670465],\n",
      "       [-0.00915979],\n",
      "       [ 0.02230614]])]\n",
      "gradients_biases:  [array([ 9.22864241e-05, -3.75181008e-03, -6.86936651e-03]), array([-0.00416457])]\n",
      "Iteration 896, Cost: 0.21021062381453626\n",
      "gradient_weights:  [array([[-0.00146062, -0.00049438,  0.01969583],\n",
      "       [ 0.00033981,  0.00164784,  0.01453875]]), array([[ 0.00671476],\n",
      "       [-0.00917415],\n",
      "       [ 0.02230455]])]\n",
      "gradients_biases:  [array([ 9.32244084e-05, -3.75815115e-03, -6.87456321e-03]), array([-0.00416507])]\n",
      "Iteration 897, Cost: 0.21014514410687063\n",
      "gradient_weights:  [array([[-0.00146668, -0.00048824,  0.01969318],\n",
      "       [ 0.00033932,  0.00165657,  0.01454378]]), array([[ 0.00672488],\n",
      "       [-0.00918852],\n",
      "       [ 0.02230292]])]\n",
      "gradients_biases:  [array([ 9.41668376e-05, -3.76450442e-03, -6.87971146e-03]), array([-0.00416556])]\n",
      "Iteration 898, Cost: 0.21007963768852694\n",
      "gradient_weights:  [array([[-0.00147276, -0.00048208,  0.01969052],\n",
      "       [ 0.00033882,  0.00166532,  0.01454878]]), array([[ 0.00673501],\n",
      "       [-0.00920288],\n",
      "       [ 0.02230124]])]\n",
      "gradients_biases:  [array([ 9.51137196e-05, -3.77086990e-03, -6.88481136e-03]), array([-0.00416603])]\n",
      "Iteration 899, Cost: 0.21001410470289175\n",
      "gradient_weights:  [array([[-0.00147885, -0.0004759 ,  0.01968783],\n",
      "       [ 0.00033831,  0.00167409,  0.01455375]]), array([[ 0.00674516],\n",
      "       [-0.00921725],\n",
      "       [ 0.02229953]])]\n",
      "gradients_biases:  [array([ 9.60650624e-05, -3.77724761e-03, -6.88986300e-03]), array([-0.00416649])]\n",
      "Iteration 900, Cost: 0.20994854529300253\n",
      "gradient_weights:  [array([[-0.00148496, -0.00046972,  0.01968512],\n",
      "       [ 0.00033779,  0.00168288,  0.01455869]]), array([[ 0.00675531],\n",
      "       [-0.00923162],\n",
      "       [ 0.02229777]])]\n",
      "gradients_biases:  [array([ 9.70208739e-05, -3.78363757e-03, -6.89486649e-03]), array([-0.00416693])]\n",
      "Iteration 901, Cost: 0.20988295960154246\n",
      "gradient_weights:  [array([[-0.00149108, -0.00046351,  0.01968238],\n",
      "       [ 0.00033726,  0.00169169,  0.0145636 ]]), array([[ 0.00676547],\n",
      "       [-0.00924599],\n",
      "       [ 0.02229597]])]\n",
      "gradients_biases:  [array([ 9.79811621e-05, -3.79003981e-03, -6.89982192e-03]), array([-0.00416736])]\n",
      "Iteration 902, Cost: 0.20981734777083513\n",
      "gradient_weights:  [array([[-0.00149722, -0.0004573 ,  0.01967963],\n",
      "       [ 0.00033673,  0.00170052,  0.01456848]]), array([[ 0.00677565],\n",
      "       [-0.00926037],\n",
      "       [ 0.02229413]])]\n",
      "gradients_biases:  [array([ 9.89459349e-05, -3.79645435e-03, -6.90472941e-03]), array([-0.00416777])]\n",
      "Iteration 903, Cost: 0.20975170994283943\n",
      "gradient_weights:  [array([[-0.00150337, -0.00045107,  0.01967685],\n",
      "       [ 0.00033619,  0.00170936,  0.01457333]]), array([[ 0.00678583],\n",
      "       [-0.00927474],\n",
      "       [ 0.02229226]])]\n",
      "gradients_biases:  [array([ 9.99152003e-05, -3.80288120e-03, -6.90958904e-03]), array([-0.00416817])]\n",
      "Iteration 904, Cost: 0.20968604625914417\n",
      "gradient_weights:  [array([[-0.00150954, -0.00044482,  0.01967405],\n",
      "       [ 0.00033564,  0.00171823,  0.01457816]]), array([[ 0.00679603],\n",
      "       [-0.00928912],\n",
      "       [ 0.02229034]])]\n",
      "gradients_biases:  [array([ 0.00010089, -0.00380932, -0.0069144 ]), array([-0.00416855])]\n",
      "Iteration 905, Cost: 0.20962035686096306\n",
      "gradient_weights:  [array([[-0.00151572, -0.00043857,  0.01967122],\n",
      "       [ 0.00033508,  0.00172711,  0.01458295]]), array([[ 0.00680624],\n",
      "       [-0.0093035 ],\n",
      "       [ 0.02228838]])]\n",
      "gradients_biases:  [array([ 0.00010187, -0.00381577, -0.00691917]), array([-0.00416892])]\n",
      "Iteration 906, Cost: 0.20955464188912964\n",
      "gradient_weights:  [array([[-0.00152191, -0.00043229,  0.01966838],\n",
      "       [ 0.00033452,  0.00173601,  0.01458772]]), array([[ 0.00681645],\n",
      "       [-0.00931788],\n",
      "       [ 0.02228638]])]\n",
      "gradients_biases:  [array([ 0.00010285, -0.00382224, -0.00692388]), array([-0.00416928])]\n",
      "Iteration 907, Cost: 0.20948890148409166\n",
      "gradient_weights:  [array([[-0.00152813, -0.00042601,  0.01966551],\n",
      "       [ 0.00033394,  0.00174493,  0.01459246]]), array([[ 0.00682668],\n",
      "       [-0.00933227],\n",
      "       [ 0.02228433]])]\n",
      "gradients_biases:  [array([ 0.00010384, -0.00382871, -0.00692855]), array([-0.00416962])]\n",
      "Iteration 908, Cost: 0.20942313578590674\n",
      "gradient_weights:  [array([[-0.00153435, -0.00041971,  0.01966262],\n",
      "       [ 0.00033336,  0.00175387,  0.01459716]]), array([[ 0.00683692],\n",
      "       [-0.00934665],\n",
      "       [ 0.02228225]])]\n",
      "gradients_biases:  [array([ 0.00010483, -0.0038352 , -0.00693317]), array([-0.00416994])]\n",
      "Iteration 909, Cost: 0.20935734493423636\n",
      "gradient_weights:  [array([[-0.00154059, -0.00041339,  0.01965971],\n",
      "       [ 0.00033278,  0.00176283,  0.01460184]]), array([[ 0.00684717],\n",
      "       [-0.00936104],\n",
      "       [ 0.02228013]])]\n",
      "gradients_biases:  [array([ 0.00010583, -0.0038417 , -0.00693775]), array([-0.00417025])]\n",
      "Iteration 910, Cost: 0.20929152906834134\n",
      "gradient_weights:  [array([[-0.00154685, -0.00040706,  0.01965678],\n",
      "       [ 0.00033218,  0.00177181,  0.01460649]]), array([[ 0.00685742],\n",
      "       [-0.00937543],\n",
      "       [ 0.02227797]])]\n",
      "gradients_biases:  [array([ 0.00010683, -0.00384822, -0.00694228]), array([-0.00417054])]\n",
      "Iteration 911, Cost: 0.2092256883270765\n",
      "gradient_weights:  [array([[-0.00155312, -0.00040072,  0.01965382],\n",
      "       [ 0.00033157,  0.00178081,  0.01461111]]), array([[ 0.00686769],\n",
      "       [-0.00938982],\n",
      "       [ 0.02227577]])]\n",
      "gradients_biases:  [array([ 0.00010783, -0.00385474, -0.00694676]), array([-0.00417082])]\n",
      "Iteration 912, Cost: 0.20915982284888568\n",
      "gradient_weights:  [array([[-0.0015594 , -0.00039437,  0.01965084],\n",
      "       [ 0.00033096,  0.00178983,  0.01461571]]), array([[ 0.00687797],\n",
      "       [-0.00940421],\n",
      "       [ 0.02227353]])]\n",
      "gradients_biases:  [array([ 0.00010884, -0.00386128, -0.00695119]), array([-0.00417109])]\n",
      "Iteration 913, Cost: 0.20909393277179666\n",
      "gradient_weights:  [array([[-0.0015657 , -0.000388  ,  0.01964784],\n",
      "       [ 0.00033034,  0.00179886,  0.01462027]]), array([[ 0.00688826],\n",
      "       [-0.00941861],\n",
      "       [ 0.02227125]])]\n",
      "gradients_biases:  [array([ 0.00010986, -0.00386783, -0.00695558]), array([-0.00417134])]\n",
      "Iteration 914, Cost: 0.20902801823341605\n",
      "gradient_weights:  [array([[-0.00157202, -0.00038161,  0.01964482],\n",
      "       [ 0.00032971,  0.00180791,  0.0146248 ]]), array([[ 0.00689856],\n",
      "       [-0.00943301],\n",
      "       [ 0.02226893]])]\n",
      "gradients_biases:  [array([ 0.00011088, -0.0038744 , -0.00695992]), array([-0.00417158])]\n",
      "Iteration 915, Cost: 0.20896207937092443\n",
      "gradient_weights:  [array([[-0.00157835, -0.00037521,  0.01964177],\n",
      "       [ 0.00032908,  0.00181699,  0.0146293 ]]), array([[ 0.00690887],\n",
      "       [-0.00944741],\n",
      "       [ 0.02226657]])]\n",
      "gradients_biases:  [array([ 0.0001119 , -0.00388097, -0.00696421]), array([-0.0041718])]\n",
      "Iteration 916, Cost: 0.20889611632107097\n",
      "gradient_weights:  [array([[-0.00158469, -0.0003688 ,  0.01963871],\n",
      "       [ 0.00032843,  0.00182608,  0.01463378]]), array([[ 0.00691919],\n",
      "       [-0.00946181],\n",
      "       [ 0.02226417]])]\n",
      "gradients_biases:  [array([ 0.00011293, -0.00388756, -0.00696846]), array([-0.004172])]\n",
      "Iteration 917, Cost: 0.20883012922016897\n",
      "gradient_weights:  [array([[-0.00159105, -0.00036238,  0.01963562],\n",
      "       [ 0.00032778,  0.00183519,  0.01463823]]), array([[ 0.00692952],\n",
      "       [-0.00947621],\n",
      "       [ 0.02226173]])]\n",
      "gradients_biases:  [array([ 0.00011396, -0.00389416, -0.00697266]), array([-0.00417219])]\n",
      "Iteration 918, Cost: 0.20876411820409038\n",
      "gradient_weights:  [array([[-0.00159742, -0.00035594,  0.01963251],\n",
      "       [ 0.00032712,  0.00184432,  0.01464264]]), array([[ 0.00693986],\n",
      "       [-0.00949061],\n",
      "       [ 0.02225926]])]\n",
      "gradients_biases:  [array([ 0.000115  , -0.00390078, -0.00697682]), array([-0.00417237])]\n",
      "Iteration 919, Cost: 0.20869808340826107\n",
      "gradient_weights:  [array([[-0.00160381, -0.00034949,  0.01962937],\n",
      "       [ 0.00032645,  0.00185347,  0.01464703]]), array([[ 0.00695021],\n",
      "       [-0.00950502],\n",
      "       [ 0.02225674]])]\n",
      "gradients_biases:  [array([ 0.00011604, -0.0039074 , -0.00698092]), array([-0.00417253])]\n",
      "Iteration 920, Cost: 0.20863202496765598\n",
      "gradient_weights:  [array([[-0.00161022, -0.00034302,  0.01962622],\n",
      "       [ 0.00032578,  0.00186263,  0.01465139]]), array([[ 0.00696057],\n",
      "       [-0.00951943],\n",
      "       [ 0.02225418]])]\n",
      "gradients_biases:  [array([ 0.00011709, -0.00391404, -0.00698499]), array([-0.00417267])]\n",
      "Iteration 921, Cost: 0.20856594301679393\n",
      "gradient_weights:  [array([[-0.00161664, -0.00033654,  0.01962304],\n",
      "       [ 0.0003251 ,  0.00187182,  0.01465572]]), array([[ 0.00697094],\n",
      "       [-0.00953384],\n",
      "       [ 0.02225159]])]\n",
      "gradients_biases:  [array([ 0.00011814, -0.0039207 , -0.00698901]), array([-0.00417281])]\n",
      "Iteration 922, Cost: 0.20849983768973296\n",
      "gradient_weights:  [array([[-0.00162307, -0.00033005,  0.01961984],\n",
      "       [ 0.00032441,  0.00188102,  0.01466002]]), array([[ 0.00698132],\n",
      "       [-0.00954825],\n",
      "       [ 0.02224896]])]\n",
      "gradients_biases:  [array([ 0.0001192 , -0.00392736, -0.00699298]), array([-0.00417292])]\n",
      "Iteration 923, Cost: 0.20843370912006529\n",
      "gradient_weights:  [array([[-0.00162952, -0.00032354,  0.01961662],\n",
      "       [ 0.00032371,  0.00189025,  0.01466429]]), array([[ 0.00699171],\n",
      "       [-0.00956267],\n",
      "       [ 0.02224629]])]\n",
      "gradients_biases:  [array([ 0.00012026, -0.00393404, -0.0069969 ]), array([-0.00417302])]\n",
      "Iteration 924, Cost: 0.20836755744091268\n",
      "gradient_weights:  [array([[-0.00163598, -0.00031702,  0.01961338],\n",
      "       [ 0.000323  ,  0.00189949,  0.01466853]]), array([[ 0.00700211],\n",
      "       [-0.00957708],\n",
      "       [ 0.02224358]])]\n",
      "gradients_biases:  [array([ 0.00012132, -0.00394073, -0.00700078]), array([-0.00417311])]\n",
      "Iteration 925, Cost: 0.2083013827849212\n",
      "gradient_weights:  [array([[-0.00164246, -0.00031048,  0.01961011],\n",
      "       [ 0.00032229,  0.00190875,  0.01467274]]), array([[ 0.00701252],\n",
      "       [-0.0095915 ],\n",
      "       [ 0.02224083]])]\n",
      "gradients_biases:  [array([ 0.00012239, -0.00394743, -0.00700462]), array([-0.00417318])]\n",
      "Iteration 926, Cost: 0.20823518528425666\n",
      "gradient_weights:  [array([[-0.00164895, -0.00030394,  0.01960683],\n",
      "       [ 0.00032156,  0.00191803,  0.01467692]]), array([[ 0.00702294],\n",
      "       [-0.00960592],\n",
      "       [ 0.02223804]])]\n",
      "gradients_biases:  [array([ 0.00012347, -0.00395415, -0.00700841]), array([-0.00417324])]\n",
      "Iteration 927, Cost: 0.20816896507059998\n",
      "gradient_weights:  [array([[-0.00165546, -0.00029737,  0.01960352],\n",
      "       [ 0.00032083,  0.00192733,  0.01468108]]), array([[ 0.00703337],\n",
      "       [-0.00962034],\n",
      "       [ 0.02223522]])]\n",
      "gradients_biases:  [array([ 0.00012455, -0.00396088, -0.00701215]), array([-0.00417328])]\n",
      "Iteration 928, Cost: 0.20810272227514182\n",
      "gradient_weights:  [array([[-0.00166198, -0.0002908 ,  0.01960019],\n",
      "       [ 0.0003201 ,  0.00193664,  0.0146852 ]]), array([[ 0.00704381],\n",
      "       [-0.00963476],\n",
      "       [ 0.02223236]])]\n",
      "gradients_biases:  [array([ 0.00012563, -0.00396762, -0.00701585]), array([-0.0041733])]\n",
      "Iteration 929, Cost: 0.20803645702857845\n",
      "gradient_weights:  [array([[-0.00166852, -0.00028421,  0.01959684],\n",
      "       [ 0.00031935,  0.00194598,  0.01468929]]), array([[ 0.00705425],\n",
      "       [-0.00964918],\n",
      "       [ 0.02222945]])]\n",
      "gradients_biases:  [array([ 0.00012672, -0.00397438, -0.00701951]), array([-0.00417332])]\n",
      "Iteration 930, Cost: 0.2079701694611067\n",
      "gradient_weights:  [array([[-0.00167507, -0.00027761,  0.01959346],\n",
      "       [ 0.0003186 ,  0.00195533,  0.01469336]]), array([[ 0.00706471],\n",
      "       [-0.00966361],\n",
      "       [ 0.02222652]])]\n",
      "gradients_biases:  [array([ 0.00012781, -0.00398115, -0.00702312]), array([-0.00417331])]\n",
      "Iteration 931, Cost: 0.20790385970241917\n",
      "gradient_weights:  [array([[-0.00168164, -0.000271  ,  0.01959007],\n",
      "       [ 0.00031784,  0.0019647 ,  0.0146974 ]]), array([[ 0.00707518],\n",
      "       [-0.00967804],\n",
      "       [ 0.02222354]])]\n",
      "gradients_biases:  [array([ 0.00012891, -0.00398793, -0.00702669]), array([-0.00417329])]\n",
      "Iteration 932, Cost: 0.20783752788169968\n",
      "gradient_weights:  [array([[-0.00168822, -0.00026437,  0.01958665],\n",
      "       [ 0.00031707,  0.00197409,  0.0147014 ]]), array([[ 0.00708566],\n",
      "       [-0.00969247],\n",
      "       [ 0.02222053]])]\n",
      "gradients_biases:  [array([ 0.00013001, -0.00399472, -0.00703021]), array([-0.00417326])]\n",
      "Iteration 933, Cost: 0.2077711741276186\n",
      "gradient_weights:  [array([[-0.00169482, -0.00025773,  0.01958321],\n",
      "       [ 0.00031629,  0.0019835 ,  0.01470538]]), array([[ 0.00709614],\n",
      "       [-0.0097069 ],\n",
      "       [ 0.02221748]])]\n",
      "gradients_biases:  [array([ 0.00013112, -0.00400153, -0.00703369]), array([-0.00417321])]\n",
      "Iteration 934, Cost: 0.20770479856832805\n",
      "gradient_weights:  [array([[-0.00170143, -0.00025107,  0.01957975],\n",
      "       [ 0.00031551,  0.00199293,  0.01470933]]), array([[ 0.00710664],\n",
      "       [-0.00972133],\n",
      "       [ 0.02221439]])]\n",
      "gradients_biases:  [array([ 0.00013224, -0.00400835, -0.00703712]), array([-0.00417315])]\n",
      "Iteration 935, Cost: 0.20763840133145733\n",
      "gradient_weights:  [array([[-0.00170806, -0.00024441,  0.01957627],\n",
      "       [ 0.00031472,  0.00200238,  0.01471325]]), array([[ 0.00711715],\n",
      "       [-0.00973577],\n",
      "       [ 0.02221126]])]\n",
      "gradients_biases:  [array([ 0.00013335, -0.00401518, -0.00704051]), array([-0.00417307])]\n",
      "Iteration 936, Cost: 0.20757198254410858\n",
      "gradient_weights:  [array([[-0.0017147 , -0.00023773,  0.01957276],\n",
      "       [ 0.00031392,  0.00201184,  0.01471714]]), array([[ 0.00712766],\n",
      "       [-0.0097502 ],\n",
      "       [ 0.0222081 ]])]\n",
      "gradients_biases:  [array([ 0.00013447, -0.00402203, -0.00704385]), array([-0.00417298])]\n",
      "Iteration 937, Cost: 0.20750554233285146\n",
      "gradient_weights:  [array([[-0.00172136, -0.00023103,  0.01956924],\n",
      "       [ 0.00031311,  0.00202132,  0.014721  ]]), array([[ 0.00713818],\n",
      "       [-0.00976464],\n",
      "       [ 0.0222049 ]])]\n",
      "gradients_biases:  [array([ 0.0001356 , -0.00402889, -0.00704715]), array([-0.00417287])]\n",
      "Iteration 938, Cost: 0.20743908082371948\n",
      "gradient_weights:  [array([[-0.00172803, -0.00022433,  0.01956569],\n",
      "       [ 0.00031229,  0.00203082,  0.01472483]]), array([[ 0.00714872],\n",
      "       [-0.00977908],\n",
      "       [ 0.02220166]])]\n",
      "gradients_biases:  [array([ 0.00013673, -0.00403576, -0.00705041]), array([-0.00417275])]\n",
      "Iteration 939, Cost: 0.20737259814220488\n",
      "gradient_weights:  [array([[-0.00173472, -0.00021761,  0.01956212],\n",
      "       [ 0.00031147,  0.00204034,  0.01472863]]), array([[ 0.00715926],\n",
      "       [-0.00979352],\n",
      "       [ 0.02219839]])]\n",
      "gradients_biases:  [array([ 0.00013787, -0.00404265, -0.00705363]), array([-0.00417261])]\n",
      "Iteration 940, Cost: 0.20730609441325423\n",
      "gradient_weights:  [array([[-0.00174142, -0.00021088,  0.01955853],\n",
      "       [ 0.00031064,  0.00204988,  0.01473241]]), array([[ 0.00716981],\n",
      "       [-0.00980796],\n",
      "       [ 0.02219508]])]\n",
      "gradients_biases:  [array([ 0.00013901, -0.00404955, -0.0070568 ]), array([-0.00417246])]\n",
      "Iteration 941, Cost: 0.2072395697612641\n",
      "gradient_weights:  [array([[-0.00174813, -0.00020413,  0.01955492],\n",
      "       [ 0.0003098 ,  0.00205944,  0.01473615]]), array([[ 0.00718038],\n",
      "       [-0.00982241],\n",
      "       [ 0.02219173]])]\n",
      "gradients_biases:  [array([ 0.00014016, -0.00405646, -0.00705993]), array([-0.00417229])]\n",
      "Iteration 942, Cost: 0.20717302431007625\n",
      "gradient_weights:  [array([[-0.00175486, -0.00019737,  0.01955129],\n",
      "       [ 0.00030896,  0.00206901,  0.01473987]]), array([[ 0.00719095],\n",
      "       [-0.00983685],\n",
      "       [ 0.02218835]])]\n",
      "gradients_biases:  [array([ 0.00014131, -0.00406339, -0.00706301]), array([-0.00417211])]\n",
      "Iteration 943, Cost: 0.20710645818297363\n",
      "gradient_weights:  [array([[-0.00176161, -0.0001906 ,  0.01954764],\n",
      "       [ 0.0003081 ,  0.0020786 ,  0.01474355]]), array([[ 0.00720153],\n",
      "       [-0.0098513 ],\n",
      "       [ 0.02218493]])]\n",
      "gradients_biases:  [array([ 0.00014246, -0.00407032, -0.00706605]), array([-0.00417191])]\n",
      "Iteration 944, Cost: 0.20703987150267564\n",
      "gradient_weights:  [array([[-0.00176837, -0.00018382,  0.01954396],\n",
      "       [ 0.00030724,  0.00208821,  0.01474721]]), array([[ 0.00721211],\n",
      "       [-0.00986575],\n",
      "       [ 0.02218148]])]\n",
      "gradients_biases:  [array([ 0.00014362, -0.00407728, -0.00706905]), array([-0.0041717])]\n",
      "Iteration 945, Cost: 0.2069732643913338\n",
      "gradient_weights:  [array([[-0.00177515, -0.00017702,  0.01954026],\n",
      "       [ 0.00030638,  0.00209784,  0.01475083]]), array([[ 0.00722271],\n",
      "       [-0.0098802 ],\n",
      "       [ 0.02217798]])]\n",
      "gradients_biases:  [array([ 0.00014479, -0.00408424, -0.00707201]), array([-0.00417147])]\n",
      "Iteration 946, Cost: 0.2069066369705274\n",
      "gradient_weights:  [array([[-0.00178194, -0.00017021,  0.01953654],\n",
      "       [ 0.0003055 ,  0.00210749,  0.01475443]]), array([[ 0.00723332],\n",
      "       [-0.00989465],\n",
      "       [ 0.02217446]])]\n",
      "gradients_biases:  [array([ 0.00014596, -0.00409122, -0.00707492]), array([-0.00417123])]\n",
      "Iteration 947, Cost: 0.20683998936125914\n",
      "gradient_weights:  [array([[-0.00178874, -0.00016339,  0.01953281],\n",
      "       [ 0.00030462,  0.00211715,  0.014758  ]]), array([[ 0.00724393],\n",
      "       [-0.00990911],\n",
      "       [ 0.02217089]])]\n",
      "gradients_biases:  [array([ 0.00014714, -0.00409821, -0.0070778 ]), array([-0.00417097])]\n",
      "Iteration 948, Cost: 0.20677332168395085\n",
      "gradient_weights:  [array([[-0.00179556, -0.00015655,  0.01952904],\n",
      "       [ 0.00030372,  0.00212684,  0.01476154]]), array([[ 0.00725456],\n",
      "       [-0.00992356],\n",
      "       [ 0.0221673 ]])]\n",
      "gradients_biases:  [array([ 0.00014832, -0.00410522, -0.00708063]), array([-0.0041707])]\n",
      "Iteration 949, Cost: 0.20670663405843911\n",
      "gradient_weights:  [array([[-0.0018024 , -0.00014971,  0.01952526],\n",
      "       [ 0.00030283,  0.00213654,  0.01476505]]), array([[ 0.00726519],\n",
      "       [-0.00993802],\n",
      "       [ 0.02216366]])]\n",
      "gradients_biases:  [array([ 0.0001495 , -0.00411224, -0.00708341]), array([-0.00417041])]\n",
      "Iteration 950, Cost: 0.2066399266039713\n",
      "gradient_weights:  [array([[-0.00180925, -0.00014285,  0.01952146],\n",
      "       [ 0.00030192,  0.00214626,  0.01476853]]), array([[ 0.00727584],\n",
      "       [-0.00995248],\n",
      "       [ 0.02215999]])]\n",
      "gradients_biases:  [array([ 0.00015069, -0.00411927, -0.00708616]), array([-0.00417011])]\n",
      "Iteration 951, Cost: 0.2065731994392007\n",
      "gradient_weights:  [array([[-0.00181611, -0.00013597,  0.01951764],\n",
      "       [ 0.00030101,  0.00215599,  0.01477198]]), array([[ 0.00728649],\n",
      "       [-0.00996694],\n",
      "       [ 0.02215629]])]\n",
      "gradients_biases:  [array([ 0.00015188, -0.00412632, -0.00708886]), array([-0.00416979])]\n",
      "Iteration 952, Cost: 0.2065064526821829\n",
      "gradient_weights:  [array([[-0.00182299, -0.00012909,  0.01951379],\n",
      "       [ 0.00030008,  0.00216575,  0.0147754 ]]), array([[ 0.00729715],\n",
      "       [-0.0099814 ],\n",
      "       [ 0.02215255]])]\n",
      "gradients_biases:  [array([ 0.00015308, -0.00413338, -0.00709153]), array([-0.00416946])]\n",
      "Iteration 953, Cost: 0.20643968645037147\n",
      "gradient_weights:  [array([[-0.00182989, -0.00012219,  0.01950992],\n",
      "       [ 0.00029915,  0.00217552,  0.01477879]]), array([[ 0.00730782],\n",
      "       [-0.00999586],\n",
      "       [ 0.02214877]])]\n",
      "gradients_biases:  [array([ 0.00015429, -0.00414045, -0.00709415]), array([-0.00416911])]\n",
      "Iteration 954, Cost: 0.2063729008606136\n",
      "gradient_weights:  [array([[-0.00183679, -0.00011528,  0.01950604],\n",
      "       [ 0.00029822,  0.00218531,  0.01478216]]), array([[ 0.00731849],\n",
      "       [-0.01001033],\n",
      "       [ 0.02214496]])]\n",
      "gradients_biases:  [array([ 0.0001555 , -0.00414754, -0.00709673]), array([-0.00416875])]\n",
      "Iteration 955, Cost: 0.206306096029146\n",
      "gradient_weights:  [array([[-0.00184372, -0.00010836,  0.01950213],\n",
      "       [ 0.00029727,  0.00219512,  0.01478549]]), array([[ 0.00732918],\n",
      "       [-0.0100248 ],\n",
      "       [ 0.02214112]])]\n",
      "gradients_biases:  [array([ 0.00015671, -0.00415464, -0.00709927]), array([-0.00416837])]\n",
      "Iteration 956, Cost: 0.2062392720715912\n",
      "gradient_weights:  [array([[-0.00185066, -0.00010143,  0.0194982 ],\n",
      "       [ 0.00029632,  0.00220495,  0.0147888 ]]), array([[ 0.00733987],\n",
      "       [-0.01003926],\n",
      "       [ 0.02213724]])]\n",
      "gradients_biases:  [array([ 0.00015793, -0.00416176, -0.00710176]), array([-0.00416798])]\n",
      "Iteration 957, Cost: 0.20617242910295275\n",
      "gradient_weights:  [array([[-1.85761025e-03, -9.44806235e-05,  1.94942463e-02],\n",
      "       [ 2.95364462e-04,  2.21479117e-03,  1.47920724e-02]]), array([[ 0.00735058],\n",
      "       [-0.01005373],\n",
      "       [ 0.02213333]])]\n",
      "gradients_biases:  [array([ 0.00015916, -0.00416888, -0.00710422]), array([-0.00416757])]\n",
      "Iteration 958, Cost: 0.2061055672376118\n",
      "gradient_weights:  [array([[-1.86457845e-03, -8.75219391e-05,  1.94902747e-02],\n",
      "       [ 2.94398179e-04,  2.22465328e-03,  1.47953192e-02]]), array([[ 0.00736129],\n",
      "       [-0.0100682 ],\n",
      "       [ 0.02212938]])]\n",
      "gradients_biases:  [array([ 0.00016039, -0.00417603, -0.00710664]), array([-0.00416715])]\n",
      "Iteration 959, Cost: 0.2060386865893229\n",
      "gradient_weights:  [array([[-1.87156144e-03, -8.05510909e-05,  1.94862822e-02],\n",
      "       [ 2.93424464e-04,  2.23453313e-03,  1.47985368e-02]]), array([[ 0.00737201],\n",
      "       [-0.01008268],\n",
      "       [ 0.0221254 ]])]\n",
      "gradients_biases:  [array([ 0.00016162, -0.00418318, -0.00710901]), array([-0.00416672])]\n",
      "Iteration 960, Cost: 0.2059717872712098\n",
      "gradient_weights:  [array([[-1.87855921e-03, -7.35681237e-05,  1.94822688e-02],\n",
      "       [ 2.92443332e-04,  2.24443069e-03,  1.48017252e-02]]), array([[ 0.00738274],\n",
      "       [-0.01009715],\n",
      "       [ 0.02212138]])]\n",
      "gradients_biases:  [array([ 0.00016286, -0.00419035, -0.00711135]), array([-0.00416627])]\n",
      "Iteration 961, Cost: 0.20590486939576175\n",
      "gradient_weights:  [array([[-1.88557175e-03, -6.65730829e-05,  1.94782346e-02],\n",
      "       [ 2.91454798e-04,  2.25434592e-03,  1.48048845e-02]]), array([[ 0.00739348],\n",
      "       [-0.01011163],\n",
      "       [ 0.02211733]])]\n",
      "gradients_biases:  [array([ 0.0001641 , -0.00419753, -0.00711364]), array([-0.0041658])]\n",
      "Iteration 962, Cost: 0.20583793307482948\n",
      "gradient_weights:  [array([[-1.89259905e-03, -5.95660141e-05,  1.94741797e-02],\n",
      "       [ 2.90458877e-04,  2.26427880e-03,  1.48080146e-02]]), array([[ 0.00740422],\n",
      "       [-0.01012611],\n",
      "       [ 0.02211324]])]\n",
      "gradients_biases:  [array([ 0.00016535, -0.00420473, -0.0071159 ]), array([-0.00416532])]\n",
      "Iteration 963, Cost: 0.2057709784196212\n",
      "gradient_weights:  [array([[-1.89964112e-03, -5.25469632e-05,  1.94701039e-02],\n",
      "       [ 2.89455585e-04,  2.27422928e-03,  1.48111157e-02]]), array([[ 0.00741498],\n",
      "       [-0.01014058],\n",
      "       [ 0.02210913]])]\n",
      "gradients_biases:  [array([ 0.00016661, -0.00421194, -0.00711811]), array([-0.00416482])]\n",
      "Iteration 964, Cost: 0.20570400554069868\n",
      "gradient_weights:  [array([[-1.90669793e-03, -4.55159764e-05,  1.94660074e-02],\n",
      "       [ 2.88444937e-04,  2.28419735e-03,  1.48141876e-02]]), array([[ 0.00742574],\n",
      "       [-0.01015507],\n",
      "       [ 0.02210497]])]\n",
      "gradients_biases:  [array([ 0.00016787, -0.00421917, -0.00712029]), array([-0.00416431])]\n",
      "Iteration 965, Cost: 0.2056370145479739\n",
      "gradient_weights:  [array([[-1.91376949e-03, -3.84731004e-05,  1.94618902e-02],\n",
      "       [ 2.87426950e-04,  2.29418295e-03,  1.48172304e-02]]), array([[ 0.00743651],\n",
      "       [-0.01016955],\n",
      "       [ 0.02210079]])]\n",
      "gradients_biases:  [array([ 0.00016913, -0.00422641, -0.00712242]), array([-0.00416379])]\n",
      "Iteration 966, Cost: 0.20557000555070437\n",
      "gradient_weights:  [array([[-1.92085578e-03, -3.14183819e-05,  1.94577524e-02],\n",
      "       [ 2.86401638e-04,  2.30418607e-03,  1.48202440e-02]]), array([[ 0.00744729],\n",
      "       [-0.01018403],\n",
      "       [ 0.02209657]])]\n",
      "gradients_biases:  [array([ 0.0001704 , -0.00423366, -0.00712452]), array([-0.00416325])]\n",
      "Iteration 967, Cost: 0.20550297865749018\n",
      "gradient_weights:  [array([[-1.92795680e-03, -2.43518681e-05,  1.94535939e-02],\n",
      "       [ 2.85369019e-04,  2.31420665e-03,  1.48232287e-02]]), array([[ 0.00745807],\n",
      "       [-0.01019852],\n",
      "       [ 0.02209232]])]\n",
      "gradients_biases:  [array([ 0.00017167, -0.00424093, -0.00712657]), array([-0.00416269])]\n",
      "Iteration 968, Cost: 0.2054359339762697\n",
      "gradient_weights:  [array([[-1.93507255e-03, -1.72736064e-05,  1.94494149e-02],\n",
      "       [ 2.84329109e-04,  2.32424468e-03,  1.48261842e-02]]), array([[ 0.00746887],\n",
      "       [-0.010213  ],\n",
      "       [ 0.02208803]])]\n",
      "gradients_biases:  [array([ 0.00017295, -0.00424821, -0.00712859]), array([-0.00416212])]\n",
      "Iteration 969, Cost: 0.20536887161431608\n",
      "gradient_weights:  [array([[-1.94220300e-03, -1.01836447e-05,  1.94452153e-02],\n",
      "       [ 2.83281925e-04,  2.33430011e-03,  1.48291107e-02]]), array([[ 0.00747967],\n",
      "       [-0.01022749],\n",
      "       [ 0.02208372]])]\n",
      "gradients_biases:  [array([ 0.00017424, -0.0042555 , -0.00713057]), array([-0.00416154])]\n",
      "Iteration 970, Cost: 0.20530179167823343\n",
      "gradient_weights:  [array([[-1.94934816e-03, -3.08203094e-06,  1.94409952e-02],\n",
      "       [ 2.82227484e-04,  2.34437290e-03,  1.48320082e-02]]), array([[ 0.00749048],\n",
      "       [-0.01024198],\n",
      "       [ 0.02207937]])]\n",
      "gradients_biases:  [array([ 0.00017553, -0.00426281, -0.00713251]), array([-0.00416094])]\n",
      "Iteration 971, Cost: 0.20523469427395336\n",
      "gradient_weights:  [array([[-1.95650801e-03,  4.03118645e-06,  1.94367546e-02],\n",
      "       [ 2.81165803e-04,  2.35446303e-03,  1.48348766e-02]]), array([[ 0.0075013 ],\n",
      "       [-0.01025647],\n",
      "       [ 0.02207498]])]\n",
      "gradients_biases:  [array([ 0.00017682, -0.00427013, -0.00713441]), array([-0.00416033])]\n",
      "Iteration 972, Cost: 0.2051675795067311\n",
      "gradient_weights:  [array([[-1.96368255e-03,  1.11559588e-05,  1.94324935e-02],\n",
      "       [ 2.80096900e-04,  2.36457045e-03,  1.48377160e-02]]), array([[ 0.00751213],\n",
      "       [-0.01027097],\n",
      "       [ 0.02207057]])]\n",
      "gradients_biases:  [array([ 0.00017812, -0.00427747, -0.00713627]), array([-0.0041597])]\n",
      "Iteration 973, Cost: 0.205100447481142\n",
      "gradient_weights:  [array([[-1.97087177e-03,  1.82922373e-05,  1.94282121e-02],\n",
      "       [ 2.79020792e-04,  2.37469514e-03,  1.48405264e-02]]), array([[ 0.00752296],\n",
      "       [-0.01028546],\n",
      "       [ 0.02206612]])]\n",
      "gradients_biases:  [array([ 0.00017943, -0.00428482, -0.0071381 ]), array([-0.00415906])]\n",
      "Iteration 974, Cost: 0.20503329830107797\n",
      "gradient_weights:  [array([[-1.97807565e-03,  2.54399725e-05,  1.94239103e-02],\n",
      "       [ 2.77937497e-04,  2.38483704e-03,  1.48433078e-02]]), array([[ 0.00753381],\n",
      "       [-0.01029996],\n",
      "       [ 0.02206164]])]\n",
      "gradients_biases:  [array([ 0.00018074, -0.00429219, -0.00713988]), array([-0.0041584])]\n",
      "Iteration 975, Cost: 0.20496613206974396\n",
      "gradient_weights:  [array([[-1.98529419e-03,  3.25991149e-05,  1.94195882e-02],\n",
      "       [ 2.76847035e-04,  2.39499613e-03,  1.48460603e-02]]), array([[ 0.00754466],\n",
      "       [-0.01031445],\n",
      "       [ 0.02205713]])]\n",
      "gradients_biases:  [array([ 0.00018205, -0.00429957, -0.00714163]), array([-0.00415773])]\n",
      "Iteration 976, Cost: 0.20489894888965426\n",
      "gradient_weights:  [array([[-1.99252738e-03,  3.97696147e-05,  1.94152458e-02],\n",
      "       [ 2.75749422e-04,  2.40517237e-03,  1.48487838e-02]]), array([[ 0.00755552],\n",
      "       [-0.01032895],\n",
      "       [ 0.02205258]])]\n",
      "gradients_biases:  [array([ 0.00018337, -0.00430696, -0.00714334]), array([-0.00415704])]\n",
      "Iteration 977, Cost: 0.20483174886262928\n",
      "gradient_weights:  [array([[-1.99977521e-03,  4.69514218e-05,  1.94108832e-02],\n",
      "       [ 2.74644678e-04,  2.41536572e-03,  1.48514784e-02]]), array([[ 0.00756638],\n",
      "       [-0.01034345],\n",
      "       [ 0.02204801]])]\n",
      "gradients_biases:  [array([ 0.0001847 , -0.00431437, -0.00714502]), array([-0.00415634])]\n",
      "Iteration 978, Cost: 0.20476453208979203\n",
      "gradient_weights:  [array([[-2.00703766e-03,  5.41444857e-05,  1.94065003e-02],\n",
      "       [ 2.73532822e-04,  2.42557614e-03,  1.48541441e-02]]), array([[ 0.00757725],\n",
      "       [-0.01035795],\n",
      "       [ 0.0220434 ]])]\n",
      "gradients_biases:  [array([ 0.00018603, -0.00432179, -0.00714665]), array([-0.00415562])]\n",
      "Iteration 979, Cost: 0.2046972986715644\n",
      "gradient_weights:  [array([[-2.01431473e-03,  6.13487557e-05,  1.94020973e-02],\n",
      "       [ 2.72413872e-04,  2.43580360e-03,  1.48567809e-02]]), array([[ 0.00758814],\n",
      "       [-0.01037246],\n",
      "       [ 0.02203876]])]\n",
      "gradients_biases:  [array([ 0.00018736, -0.00432923, -0.00714825]), array([-0.00415489])]\n",
      "Iteration 980, Cost: 0.20463004870766427\n",
      "gradient_weights:  [array([[-2.02160641e-03,  6.85641809e-05,  1.93976741e-02],\n",
      "       [ 2.71287849e-04,  2.44604805e-03,  1.48593888e-02]]), array([[ 0.00759903],\n",
      "       [-0.01038696],\n",
      "       [ 0.02203409]])]\n",
      "gradients_biases:  [array([ 0.0001887 , -0.00433668, -0.00714981]), array([-0.00415415])]\n",
      "Iteration 981, Cost: 0.20456278229710184\n",
      "gradient_weights:  [array([[-2.02891268e-03,  7.57907099e-05,  1.93932309e-02],\n",
      "       [ 2.70154772e-04,  2.45630946e-03,  1.48619678e-02]]), array([[ 0.00760992],\n",
      "       [-0.01040147],\n",
      "       [ 0.02202938]])]\n",
      "gradients_biases:  [array([ 0.00019005, -0.00434415, -0.00715133]), array([-0.00415338])]\n",
      "Iteration 982, Cost: 0.20449549953817628\n",
      "gradient_weights:  [array([[-2.03623354e-03,  8.30282912e-05,  1.93887676e-02],\n",
      "       [ 2.69014660e-04,  2.46658778e-03,  1.48645180e-02]]), array([[ 0.00762083],\n",
      "       [-0.01041598],\n",
      "       [ 0.02202465]])]\n",
      "gradients_biases:  [array([ 0.0001914 , -0.00435163, -0.00715282]), array([-0.00415261])]\n",
      "Iteration 983, Cost: 0.20442820052847255\n",
      "gradient_weights:  [array([[-2.04356897e-03,  9.02768730e-05,  1.93842843e-02],\n",
      "       [ 2.67867535e-04,  2.47688299e-03,  1.48670393e-02]]), array([[ 0.00763174],\n",
      "       [-0.01043049],\n",
      "       [ 0.02201989]])]\n",
      "gradients_biases:  [array([ 0.00019276, -0.00435912, -0.00715427]), array([-0.00415182])]\n",
      "Iteration 984, Cost: 0.20436088536485825\n",
      "gradient_weights:  [array([[-2.05091896e-03,  9.75364031e-05,  1.93797811e-02],\n",
      "       [ 2.66713415e-04,  2.48719503e-03,  1.48695319e-02]]), array([[ 0.00764266],\n",
      "       [-0.010445  ],\n",
      "       [ 0.02201509]])]\n",
      "gradients_biases:  [array([ 0.00019412, -0.00436663, -0.00715568]), array([-0.00415102])]\n",
      "Iteration 985, Cost: 0.20429355414348013\n",
      "gradient_weights:  [array([[-0.00205828,  0.00010481,  0.01937526],\n",
      "       [ 0.00026555,  0.00249752,  0.014872  ]]), array([[ 0.00765358],\n",
      "       [-0.01045951],\n",
      "       [ 0.02201026]])]\n",
      "gradients_biases:  [array([ 0.00019549, -0.00437415, -0.00715706]), array([-0.0041502])]\n",
      "Iteration 986, Cost: 0.20422620695976107\n",
      "gradient_weights:  [array([[-0.00206566,  0.00011209,  0.01937071],\n",
      "       [ 0.00026438,  0.00250787,  0.01487443]]), array([[ 0.00766452],\n",
      "       [-0.01047403],\n",
      "       [ 0.02200541]])]\n",
      "gradients_biases:  [array([ 0.00019686, -0.00438169, -0.0071584 ]), array([-0.00414937])]\n",
      "Iteration 987, Cost: 0.2041588439083969\n",
      "gradient_weights:  [array([[-0.00207306,  0.00011938,  0.01936615],\n",
      "       [ 0.00026321,  0.00251823,  0.01487684]]), array([[ 0.00767546],\n",
      "       [-0.01048854],\n",
      "       [ 0.02200052]])]\n",
      "gradients_biases:  [array([ 0.00019823, -0.00438925, -0.0071597 ]), array([-0.00414852])]\n",
      "Iteration 988, Cost: 0.20409146508335313\n",
      "gradient_weights:  [array([[-0.00208046,  0.00012668,  0.01936157],\n",
      "       [ 0.00026203,  0.00252861,  0.01487921]]), array([[ 0.0076864 ],\n",
      "       [-0.01050306],\n",
      "       [ 0.0219956 ]])]\n",
      "gradients_biases:  [array([ 0.00019962, -0.00439681, -0.00716097]), array([-0.00414766])]\n",
      "Iteration 989, Cost: 0.20402407057786218\n",
      "gradient_weights:  [array([[-0.00208789,  0.000134  ,  0.01935697],\n",
      "       [ 0.00026084,  0.00253901,  0.01488156]]), array([[ 0.00769736],\n",
      "       [-0.01051758],\n",
      "       [ 0.02199065]])]\n",
      "gradients_biases:  [array([ 0.000201 , -0.0044044, -0.0071622]), array([-0.00414678])]\n",
      "Iteration 990, Cost: 0.20395666048441996\n",
      "gradient_weights:  [array([[-0.00209532,  0.00014132,  0.01935234],\n",
      "       [ 0.00025964,  0.00254942,  0.01488388]]), array([[ 0.00770832],\n",
      "       [-0.0105321 ],\n",
      "       [ 0.02198568]])]\n",
      "gradients_biases:  [array([ 0.0002024 , -0.00441199, -0.0071634 ]), array([-0.00414589])]\n",
      "Iteration 991, Cost: 0.20388923489478303\n",
      "gradient_weights:  [array([[-0.00210278,  0.00014866,  0.0193477 ],\n",
      "       [ 0.00025844,  0.00255985,  0.01488618]]), array([[ 0.00771929],\n",
      "       [-0.01054662],\n",
      "       [ 0.02198067]])]\n",
      "gradients_biases:  [array([ 0.0002038 , -0.0044196 , -0.00716456]), array([-0.00414498])]\n",
      "Iteration 992, Cost: 0.20382179389996558\n",
      "gradient_weights:  [array([[-0.00211024,  0.000156  ,  0.01934304],\n",
      "       [ 0.00025723,  0.00257029,  0.01488844]]), array([[ 0.00773027],\n",
      "       [-0.01056115],\n",
      "       [ 0.02197563]])]\n",
      "gradients_biases:  [array([ 0.0002052 , -0.00442723, -0.00716569]), array([-0.00414406])]\n",
      "Iteration 993, Cost: 0.20375433759023642\n",
      "gradient_weights:  [array([[-0.00211772,  0.00016336,  0.01933836],\n",
      "       [ 0.00025602,  0.00258075,  0.01489067]]), array([[ 0.00774125],\n",
      "       [-0.01057567],\n",
      "       [ 0.02197056]])]\n",
      "gradients_biases:  [array([ 0.00020661, -0.00443487, -0.00716678]), array([-0.00414313])]\n",
      "Iteration 994, Cost: 0.20368686605511616\n",
      "gradient_weights:  [array([[-0.00212522,  0.00017072,  0.01933366],\n",
      "       [ 0.00025479,  0.00259123,  0.01489288]]), array([[ 0.00775224],\n",
      "       [-0.0105902 ],\n",
      "       [ 0.02196547]])]\n",
      "gradients_biases:  [array([ 0.00020802, -0.00444253, -0.00716784]), array([-0.00414218])]\n",
      "Iteration 995, Cost: 0.2036193793833742\n",
      "gradient_weights:  [array([[-0.00213273,  0.0001781 ,  0.01932894],\n",
      "       [ 0.00025356,  0.00260173,  0.01489506]]), array([[ 0.00776324],\n",
      "       [-0.01060473],\n",
      "       [ 0.02196034]])]\n",
      "gradients_biases:  [array([ 0.00020944, -0.0044502 , -0.00716886]), array([-0.00414122])]\n",
      "Iteration 996, Cost: 0.2035518776630259\n",
      "gradient_weights:  [array([[-0.00214025,  0.00018549,  0.0193242 ],\n",
      "       [ 0.00025233,  0.00261224,  0.0148972 ]]), array([[ 0.00777425],\n",
      "       [-0.01061926],\n",
      "       [ 0.02195518]])]\n",
      "gradients_biases:  [array([ 0.00021087, -0.00445789, -0.00716985]), array([-0.00414024])]\n",
      "Iteration 997, Cost: 0.20348436098132974\n",
      "gradient_weights:  [array([[-0.00214779,  0.00019288,  0.01931944],\n",
      "       [ 0.00025108,  0.00262277,  0.01489932]]), array([[ 0.00778526],\n",
      "       [-0.01063379],\n",
      "       [ 0.02195   ]])]\n",
      "gradients_biases:  [array([ 0.0002123 , -0.00446559, -0.0071708 ]), array([-0.00413925])]\n",
      "Iteration 998, Cost: 0.20341682942478467\n",
      "gradient_weights:  [array([[-0.00215534,  0.00020029,  0.01931466],\n",
      "       [ 0.00024983,  0.00263331,  0.01490142]]), array([[ 0.00779627],\n",
      "       [-0.01064832],\n",
      "       [ 0.02194479]])]\n",
      "gradients_biases:  [array([ 0.00021373, -0.0044733 , -0.00717172]), array([-0.00413824])]\n",
      "Iteration 999, Cost: 0.20334928307912706\n",
      "gradient_weights:  [array([[-0.00216291,  0.00020771,  0.01930986],\n",
      "       [ 0.00024858,  0.00264387,  0.01490348]]), array([[ 0.0078073 ],\n",
      "       [-0.01066286],\n",
      "       [ 0.02193954]])]\n",
      "gradients_biases:  [array([ 0.00021517, -0.00448103, -0.00717261]), array([-0.00413722])]\n",
      "Iteration 1000, Cost: 0.2032817220293283\n",
      "Accuracy: 0.75\n",
      "F1 Score: 0.8\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "# Create a neural network with 2 input neurons, 3 hidden neurons, and 1 output neuron\n",
    "nn = NeuralNetwork([2, 3, 1])\n",
    "\n",
    "# Generate some dummy data for training\n",
    "X_train = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "Y_train = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# Train the neural network\n",
    "nn.train(X_train, Y_train, learning_rate=0.1, lam=0.0, max_iterations=1000, epsilon=0.005)\n",
    "\n",
    "# Generate some dummy data for testing\n",
    "X_test = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "Y_test = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# Evaluate the trained model\n",
    "accuracy, f1_score = nn.evaluate(X_test, Y_test)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1 Score:\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a512db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "253f7ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        self.weights = [np.random.randn(layers[i], layers[i+1]) for i in range(len(layers)-1)]\n",
    "        self.biases = [np.zeros((1, layers[i+1])) for i in range(len(layers)-1)]\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    def forward_pass(self, X):\n",
    "        activations = [X]\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(activations[-1], w) + b\n",
    "            activations.append(self.sigmoid(z))\n",
    "        return activations\n",
    "    \n",
    "    def backward_pass(self, X, Y, activations):\n",
    "        deltas = [(activations[-1] - Y) * self.sigmoid_derivative(activations[-1])]\n",
    "        for i in range(len(self.layers) - 2, 0, -1):\n",
    "            delta = np.dot(deltas[0], self.weights[i].T) * self.sigmoid_derivative(activations[i])\n",
    "            deltas.insert(0, delta)\n",
    "        return deltas\n",
    "    \n",
    "    def compute_gradients(self, activations, deltas):\n",
    "        gradients_weights = [np.dot(activations[i].T, deltas[i]) for i in range(len(self.layers) - 1)]\n",
    "        gradients_biases = [np.sum(deltas[i], axis=0) for i in range(len(self.layers) - 1)]\n",
    "        return gradients_weights, gradients_biases\n",
    "    \n",
    "    def update_weights(self, gradients_weights, gradients_biases, learning_rate):\n",
    "        self.weights = [w - learning_rate * gw for w, gw in zip(self.weights, gradients_weights)]\n",
    "        self.biases = [b - learning_rate * gb for b, gb in zip(self.biases, gradients_biases)]\n",
    "    \n",
    "    def train(self, X, Y, learning_rate, lam, max_iterations, epsilon):\n",
    "        for iteration in range(max_iterations):\n",
    "            activations = self.forward_pass(X)\n",
    "            deltas = self.backward_pass(X, Y, activations)\n",
    "            gradients_weights, gradients_biases = self.compute_gradients(activations, deltas)\n",
    "            self.update_weights(gradients_weights, gradients_biases, learning_rate)\n",
    "            # Compute cost function\n",
    "            J = np.mean(np.square(activations[-1] - Y))\n",
    "            print(f\"Iteration {iteration+1}, Cost: {J}\")\n",
    "            # Check for convergence\n",
    "            if J < epsilon:\n",
    "                print(\"Converged!\")\n",
    "                break\n",
    "\n",
    "    def accuracy(self, y_true, y_pred):\n",
    "        correct = np.sum(y_true == y_pred)\n",
    "        return correct / len(y_true)\n",
    "\n",
    "\n",
    "    def f1_score(self, y_true, y_pred):\n",
    "        tp = np.sum(np.logical_and(y_true, y_pred))\n",
    "        fp = np.sum(np.logical_and(np.logical_not(y_true), y_pred))\n",
    "        fn = np.sum(np.logical_and(y_true, np.logical_not(y_pred)))\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        return f1\n",
    "\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        activations = self.forward_pass(X_test)[-1]\n",
    "        y_pred = (activations > 0.5).astype(int)\n",
    "        acc = self.accuracy(y_test, y_pred)\n",
    "        f1 = self.f1_score(y_test, y_pred)\n",
    "        return acc, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3edb61b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, Cost: 0.3185688235920991\n",
      "Iteration 2, Cost: 0.22582855598996\n",
      "Iteration 3, Cost: 0.22141704327119757\n",
      "Iteration 4, Cost: 0.22341253665759406\n",
      "Iteration 5, Cost: 0.22515312045516978\n",
      "Iteration 6, Cost: 0.23128550793769212\n",
      "Iteration 7, Cost: 0.22664700055988482\n",
      "Iteration 8, Cost: 0.23044094099948711\n",
      "Iteration 9, Cost: 0.2247279060673223\n",
      "Iteration 10, Cost: 0.22650944453852895\n",
      "Iteration 11, Cost: 0.22370770213100724\n",
      "Iteration 12, Cost: 0.22487805716848464\n",
      "Iteration 13, Cost: 0.22287946793051072\n",
      "Iteration 14, Cost: 0.22353905222232942\n",
      "Iteration 15, Cost: 0.22216313301382512\n",
      "Iteration 16, Cost: 0.22251544213334615\n",
      "Iteration 17, Cost: 0.22157843734967922\n",
      "Iteration 18, Cost: 0.22175028493534435\n",
      "Iteration 19, Cost: 0.2211149931560793\n",
      "Iteration 20, Cost: 0.22118518368490442\n",
      "Iteration 21, Cost: 0.22075533827735155\n",
      "Iteration 22, Cost: 0.22077155233953522\n",
      "Iteration 23, Cost: 0.2204808226057023\n",
      "Iteration 24, Cost: 0.22047086325888682\n",
      "Iteration 25, Cost: 0.22027401782661266\n",
      "Iteration 26, Cost: 0.22025338964175642\n",
      "Iteration 27, Cost: 0.2201198141645434\n",
      "Iteration 28, Cost: 0.22009667144614337\n",
      "Iteration 29, Cost: 0.22000574799619027\n",
      "Iteration 30, Cost: 0.2199840135763548\n",
      "Iteration 31, Cost: 0.2199218932036685\n",
      "Iteration 32, Cost: 0.21990315718984668\n",
      "Iteration 33, Cost: 0.21986054292508045\n",
      "Iteration 34, Cost: 0.21984518031179814\n",
      "Iteration 35, Cost: 0.21981582385849108\n",
      "Iteration 36, Cost: 0.21980362969000036\n",
      "Iteration 37, Cost: 0.21978332132944042\n",
      "Iteration 38, Cost: 0.21977385724465068\n",
      "Iteration 39, Cost: 0.21975975098015138\n",
      "Iteration 40, Cost: 0.2197525242356543\n",
      "Iteration 41, Cost: 0.21974268815745615\n",
      "Iteration 42, Cost: 0.21973723646741153\n",
      "Iteration 43, Cost: 0.21973035330003868\n",
      "Iteration 44, Cost: 0.2197262787162841\n",
      "Iteration 45, Cost: 0.21972144610615518\n",
      "Iteration 46, Cost: 0.21971842274868814\n",
      "Iteration 47, Cost: 0.21971501970183474\n",
      "Iteration 48, Cost: 0.2197127891728179\n",
      "Iteration 49, Cost: 0.2197103863963725\n",
      "Iteration 50, Cost: 0.21970874831481968\n",
      "Iteration 51, Cost: 0.2197070477641424\n",
      "Iteration 52, Cost: 0.21970584921746142\n",
      "Iteration 53, Cost: 0.21970464313846513\n",
      "Iteration 54, Cost: 0.21970376882938128\n",
      "Iteration 55, Cost: 0.21970291186974977\n",
      "Iteration 56, Cost: 0.21970227565754108\n",
      "Iteration 57, Cost: 0.21970166578098804\n",
      "Iteration 58, Cost: 0.2197012037672019\n",
      "Iteration 59, Cost: 0.219700769127664\n",
      "Iteration 60, Cost: 0.2197004341805789\n",
      "Iteration 61, Cost: 0.21970012405257525\n",
      "Iteration 62, Cost: 0.2196998815649093\n",
      "Iteration 63, Cost: 0.2196996600487581\n",
      "Iteration 64, Cost: 0.21969948470246972\n",
      "Iteration 65, Cost: 0.21969932633738193\n",
      "Iteration 66, Cost: 0.21969919966548787\n",
      "Iteration 67, Cost: 0.21969908636073954\n",
      "Iteration 68, Cost: 0.21969899492636116\n",
      "Iteration 69, Cost: 0.21969891380717596\n",
      "Iteration 70, Cost: 0.21969884785313773\n",
      "Iteration 71, Cost: 0.21969878974399543\n",
      "Iteration 72, Cost: 0.21969874219696867\n",
      "Iteration 73, Cost: 0.2196987005507998\n",
      "Iteration 74, Cost: 0.21969866629019297\n",
      "Iteration 75, Cost: 0.21969863643054935\n",
      "Iteration 76, Cost: 0.21969861175370736\n",
      "Iteration 77, Cost: 0.21969859033729214\n",
      "Iteration 78, Cost: 0.2196985725694484\n",
      "Iteration 79, Cost: 0.21969855720422346\n",
      "Iteration 80, Cost: 0.2196985444147167\n",
      "Iteration 81, Cost: 0.2196985333881143\n",
      "Iteration 82, Cost: 0.21969852418433064\n",
      "Iteration 83, Cost: 0.21969851626955453\n",
      "Iteration 84, Cost: 0.2196985096475567\n",
      "Iteration 85, Cost: 0.21969850396536877\n",
      "Iteration 86, Cost: 0.21969849920176523\n",
      "Iteration 87, Cost: 0.21969849512176148\n",
      "Iteration 88, Cost: 0.21969849169552094\n",
      "Iteration 89, Cost: 0.21969848876554993\n",
      "Iteration 90, Cost: 0.2196984863015218\n",
      "Iteration 91, Cost: 0.2196984841971855\n",
      "Iteration 92, Cost: 0.2196984824253336\n",
      "Iteration 93, Cost: 0.21969848091383182\n",
      "Iteration 94, Cost: 0.2196984796398297\n",
      "Iteration 95, Cost: 0.21969847855406052\n",
      "Iteration 96, Cost: 0.21969847763809341\n",
      "Iteration 97, Cost: 0.21969847685809016\n",
      "Iteration 98, Cost: 0.2196984761995812\n",
      "Iteration 99, Cost: 0.21969847563920367\n",
      "Iteration 100, Cost: 0.21969847516581298\n",
      "Iteration 101, Cost: 0.21969847476320095\n",
      "Iteration 102, Cost: 0.21969847442290424\n",
      "Iteration 103, Cost: 0.2196984741336292\n",
      "Iteration 104, Cost: 0.2196984738890166\n",
      "Iteration 105, Cost: 0.21969847368116618\n",
      "Iteration 106, Cost: 0.21969847350533922\n",
      "Iteration 107, Cost: 0.21969847335598974\n",
      "Iteration 108, Cost: 0.21969847322960925\n",
      "Iteration 109, Cost: 0.21969847312229243\n",
      "Iteration 110, Cost: 0.2196984730314552\n",
      "Iteration 111, Cost: 0.21969847295433967\n",
      "Iteration 112, Cost: 0.21969847288905084\n",
      "Iteration 113, Cost: 0.21969847283363628\n",
      "Iteration 114, Cost: 0.21969847278671095\n",
      "Iteration 115, Cost: 0.21969847274688997\n",
      "Iteration 116, Cost: 0.21969847271316373\n",
      "Iteration 117, Cost: 0.21969847268454787\n",
      "Iteration 118, Cost: 0.21969847266030829\n",
      "Iteration 119, Cost: 0.21969847263974437\n",
      "Iteration 120, Cost: 0.21969847262232334\n",
      "Iteration 121, Cost: 0.21969847260754544\n",
      "Iteration 122, Cost: 0.21969847259502487\n",
      "Iteration 123, Cost: 0.2196984725844051\n",
      "Iteration 124, Cost: 0.21969847257540662\n",
      "Iteration 125, Cost: 0.21969847256777486\n",
      "Iteration 126, Cost: 0.21969847256130773\n",
      "Iteration 127, Cost: 0.21969847255582323\n",
      "Iteration 128, Cost: 0.2196984725511754\n",
      "Iteration 129, Cost: 0.21969847254723399\n",
      "Iteration 130, Cost: 0.21969847254389363\n",
      "Iteration 131, Cost: 0.21969847254106112\n",
      "Iteration 132, Cost: 0.21969847253866057\n",
      "Iteration 133, Cost: 0.21969847253662494\n",
      "Iteration 134, Cost: 0.21969847253489963\n",
      "Iteration 135, Cost: 0.2196984725334368\n",
      "Iteration 136, Cost: 0.21969847253219685\n",
      "Iteration 137, Cost: 0.21969847253114555\n",
      "Iteration 138, Cost: 0.21969847253025448\n",
      "Iteration 139, Cost: 0.21969847252949895\n",
      "Iteration 140, Cost: 0.21969847252885855\n",
      "Iteration 141, Cost: 0.2196984725283156\n",
      "Iteration 142, Cost: 0.21969847252785538\n",
      "Iteration 143, Cost: 0.21969847252746513\n",
      "Iteration 144, Cost: 0.21969847252713443\n",
      "Iteration 145, Cost: 0.21969847252685396\n",
      "Iteration 146, Cost: 0.21969847252661623\n",
      "Iteration 147, Cost: 0.21969847252641475\n",
      "Iteration 148, Cost: 0.21969847252624394\n",
      "Iteration 149, Cost: 0.21969847252609906\n",
      "Iteration 150, Cost: 0.2196984725259763\n",
      "Iteration 151, Cost: 0.21969847252587218\n",
      "Iteration 152, Cost: 0.21969847252578395\n",
      "Iteration 153, Cost: 0.21969847252570918\n",
      "Iteration 154, Cost: 0.21969847252564573\n",
      "Iteration 155, Cost: 0.219698472525592\n",
      "Iteration 156, Cost: 0.2196984725255464\n",
      "Iteration 157, Cost: 0.21969847252550778\n",
      "Iteration 158, Cost: 0.21969847252547503\n",
      "Iteration 159, Cost: 0.21969847252544727\n",
      "Iteration 160, Cost: 0.2196984725254237\n",
      "Iteration 161, Cost: 0.2196984725254038\n",
      "Iteration 162, Cost: 0.21969847252538688\n",
      "Iteration 163, Cost: 0.21969847252537256\n",
      "Iteration 164, Cost: 0.2196984725253604\n",
      "Iteration 165, Cost: 0.2196984725253501\n",
      "Iteration 166, Cost: 0.21969847252534136\n",
      "Iteration 167, Cost: 0.21969847252533395\n",
      "Iteration 168, Cost: 0.21969847252532762\n",
      "Iteration 169, Cost: 0.21969847252532232\n",
      "Iteration 170, Cost: 0.21969847252531785\n",
      "Iteration 171, Cost: 0.219698472525314\n",
      "Iteration 172, Cost: 0.21969847252531077\n",
      "Iteration 173, Cost: 0.219698472525308\n",
      "Iteration 174, Cost: 0.21969847252530567\n",
      "Iteration 175, Cost: 0.2196984725253037\n",
      "Iteration 176, Cost: 0.21969847252530206\n",
      "Iteration 177, Cost: 0.21969847252530056\n",
      "Iteration 178, Cost: 0.21969847252529937\n",
      "Iteration 179, Cost: 0.21969847252529837\n",
      "Iteration 180, Cost: 0.21969847252529748\n",
      "Iteration 181, Cost: 0.21969847252529676\n",
      "Iteration 182, Cost: 0.21969847252529615\n",
      "Iteration 183, Cost: 0.21969847252529567\n",
      "Iteration 184, Cost: 0.21969847252529515\n",
      "Iteration 185, Cost: 0.21969847252529479\n",
      "Iteration 186, Cost: 0.21969847252529448\n",
      "Iteration 187, Cost: 0.21969847252529423\n",
      "Iteration 188, Cost: 0.21969847252529395\n",
      "Iteration 189, Cost: 0.2196984725252938\n",
      "Iteration 190, Cost: 0.21969847252529362\n",
      "Iteration 191, Cost: 0.21969847252529345\n",
      "Iteration 192, Cost: 0.21969847252529334\n",
      "Iteration 193, Cost: 0.21969847252529323\n",
      "Iteration 194, Cost: 0.21969847252529315\n",
      "Iteration 195, Cost: 0.2196984725252931\n",
      "Iteration 196, Cost: 0.219698472525293\n",
      "Iteration 197, Cost: 0.21969847252529295\n",
      "Iteration 198, Cost: 0.21969847252529295\n",
      "Iteration 199, Cost: 0.2196984725252929\n",
      "Iteration 200, Cost: 0.21969847252529284\n",
      "Iteration 201, Cost: 0.21969847252529282\n",
      "Iteration 202, Cost: 0.21969847252529282\n",
      "Iteration 203, Cost: 0.21969847252529282\n",
      "Iteration 204, Cost: 0.21969847252529276\n",
      "Iteration 205, Cost: 0.21969847252529276\n",
      "Iteration 206, Cost: 0.21969847252529268\n",
      "Iteration 207, Cost: 0.21969847252529276\n",
      "Iteration 208, Cost: 0.2196984725252927\n",
      "Iteration 209, Cost: 0.2196984725252927\n",
      "Iteration 210, Cost: 0.21969847252529276\n",
      "Iteration 211, Cost: 0.21969847252529268\n",
      "Iteration 212, Cost: 0.21969847252529268\n",
      "Iteration 213, Cost: 0.2196984725252927\n",
      "Iteration 214, Cost: 0.2196984725252927\n",
      "Iteration 215, Cost: 0.21969847252529268\n",
      "Iteration 216, Cost: 0.21969847252529268\n",
      "Iteration 217, Cost: 0.2196984725252927\n",
      "Iteration 218, Cost: 0.21969847252529268\n",
      "Iteration 219, Cost: 0.21969847252529268\n",
      "Iteration 220, Cost: 0.21969847252529268\n",
      "Iteration 221, Cost: 0.21969847252529268\n",
      "Iteration 222, Cost: 0.21969847252529268\n",
      "Iteration 223, Cost: 0.21969847252529268\n",
      "Iteration 224, Cost: 0.21969847252529265\n",
      "Iteration 225, Cost: 0.21969847252529265\n",
      "Iteration 226, Cost: 0.21969847252529276\n",
      "Iteration 227, Cost: 0.21969847252529268\n",
      "Iteration 228, Cost: 0.21969847252529268\n",
      "Iteration 229, Cost: 0.21969847252529268\n",
      "Iteration 230, Cost: 0.21969847252529268\n",
      "Iteration 231, Cost: 0.21969847252529268\n",
      "Iteration 232, Cost: 0.21969847252529268\n",
      "Iteration 233, Cost: 0.21969847252529268\n",
      "Iteration 234, Cost: 0.21969847252529268\n",
      "Iteration 235, Cost: 0.21969847252529268\n",
      "Iteration 236, Cost: 0.21969847252529268\n",
      "Iteration 237, Cost: 0.21969847252529268\n",
      "Iteration 238, Cost: 0.21969847252529262\n",
      "Iteration 239, Cost: 0.21969847252529268\n",
      "Iteration 240, Cost: 0.21969847252529265\n",
      "Iteration 241, Cost: 0.2196984725252927\n",
      "Iteration 242, Cost: 0.21969847252529268\n",
      "Iteration 243, Cost: 0.21969847252529268\n",
      "Iteration 244, Cost: 0.21969847252529268\n",
      "Iteration 245, Cost: 0.21969847252529268\n",
      "Iteration 246, Cost: 0.21969847252529262\n",
      "Iteration 247, Cost: 0.21969847252529268\n",
      "Iteration 248, Cost: 0.21969847252529268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 249, Cost: 0.21969847252529268\n",
      "Iteration 250, Cost: 0.21969847252529268\n",
      "Iteration 251, Cost: 0.21969847252529268\n",
      "Iteration 252, Cost: 0.2196984725252927\n",
      "Iteration 253, Cost: 0.21969847252529268\n",
      "Iteration 254, Cost: 0.21969847252529268\n",
      "Iteration 255, Cost: 0.21969847252529268\n",
      "Iteration 256, Cost: 0.21969847252529268\n",
      "Iteration 257, Cost: 0.21969847252529262\n",
      "Iteration 258, Cost: 0.21969847252529268\n",
      "Iteration 259, Cost: 0.21969847252529268\n",
      "Iteration 260, Cost: 0.21969847252529268\n",
      "Iteration 261, Cost: 0.21969847252529268\n",
      "Iteration 262, Cost: 0.21969847252529268\n",
      "Iteration 263, Cost: 0.2196984725252927\n",
      "Iteration 264, Cost: 0.21969847252529268\n",
      "Iteration 265, Cost: 0.21969847252529268\n",
      "Iteration 266, Cost: 0.21969847252529262\n",
      "Iteration 267, Cost: 0.21969847252529268\n",
      "Iteration 268, Cost: 0.21969847252529268\n",
      "Iteration 269, Cost: 0.21969847252529268\n",
      "Iteration 270, Cost: 0.21969847252529268\n",
      "Iteration 271, Cost: 0.21969847252529268\n",
      "Iteration 272, Cost: 0.21969847252529268\n",
      "Iteration 273, Cost: 0.21969847252529265\n",
      "Iteration 274, Cost: 0.21969847252529268\n",
      "Iteration 275, Cost: 0.21969847252529268\n",
      "Iteration 276, Cost: 0.21969847252529268\n",
      "Iteration 277, Cost: 0.21969847252529268\n",
      "Iteration 278, Cost: 0.21969847252529268\n",
      "Iteration 279, Cost: 0.21969847252529265\n",
      "Iteration 280, Cost: 0.21969847252529262\n",
      "Iteration 281, Cost: 0.21969847252529268\n",
      "Iteration 282, Cost: 0.21969847252529268\n",
      "Iteration 283, Cost: 0.21969847252529268\n",
      "Iteration 284, Cost: 0.21969847252529268\n",
      "Iteration 285, Cost: 0.21969847252529265\n",
      "Iteration 286, Cost: 0.21969847252529265\n",
      "Iteration 287, Cost: 0.21969847252529268\n",
      "Iteration 288, Cost: 0.21969847252529268\n",
      "Iteration 289, Cost: 0.21969847252529268\n",
      "Iteration 290, Cost: 0.21969847252529268\n",
      "Iteration 291, Cost: 0.21969847252529265\n",
      "Iteration 292, Cost: 0.2196984725252927\n",
      "Iteration 293, Cost: 0.21969847252529268\n",
      "Iteration 294, Cost: 0.21969847252529268\n",
      "Iteration 295, Cost: 0.21969847252529268\n",
      "Iteration 296, Cost: 0.21969847252529268\n",
      "Iteration 297, Cost: 0.21969847252529268\n",
      "Iteration 298, Cost: 0.21969847252529268\n",
      "Iteration 299, Cost: 0.21969847252529268\n",
      "Iteration 300, Cost: 0.21969847252529268\n",
      "Iteration 301, Cost: 0.21969847252529268\n",
      "Iteration 302, Cost: 0.21969847252529268\n",
      "Iteration 303, Cost: 0.2196984725252927\n",
      "Iteration 304, Cost: 0.21969847252529268\n",
      "Iteration 305, Cost: 0.21969847252529268\n",
      "Iteration 306, Cost: 0.2196984725252927\n",
      "Iteration 307, Cost: 0.21969847252529268\n",
      "Iteration 308, Cost: 0.21969847252529268\n",
      "Iteration 309, Cost: 0.21969847252529268\n",
      "Iteration 310, Cost: 0.21969847252529268\n",
      "Iteration 311, Cost: 0.21969847252529268\n",
      "Iteration 312, Cost: 0.21969847252529268\n",
      "Iteration 313, Cost: 0.21969847252529268\n",
      "Iteration 314, Cost: 0.21969847252529268\n",
      "Iteration 315, Cost: 0.21969847252529268\n",
      "Iteration 316, Cost: 0.21969847252529268\n",
      "Iteration 317, Cost: 0.21969847252529268\n",
      "Iteration 318, Cost: 0.21969847252529268\n",
      "Iteration 319, Cost: 0.21969847252529268\n",
      "Iteration 320, Cost: 0.2196984725252927\n",
      "Iteration 321, Cost: 0.21969847252529268\n",
      "Iteration 322, Cost: 0.2196984725252927\n",
      "Iteration 323, Cost: 0.2196984725252927\n",
      "Iteration 324, Cost: 0.21969847252529268\n",
      "Iteration 325, Cost: 0.21969847252529268\n",
      "Iteration 326, Cost: 0.21969847252529265\n",
      "Iteration 327, Cost: 0.21969847252529268\n",
      "Iteration 328, Cost: 0.21969847252529265\n",
      "Iteration 329, Cost: 0.21969847252529268\n",
      "Iteration 330, Cost: 0.21969847252529268\n",
      "Iteration 331, Cost: 0.21969847252529268\n",
      "Iteration 332, Cost: 0.21969847252529268\n",
      "Iteration 333, Cost: 0.21969847252529265\n",
      "Iteration 334, Cost: 0.21969847252529268\n",
      "Iteration 335, Cost: 0.21969847252529268\n",
      "Iteration 336, Cost: 0.21969847252529265\n",
      "Iteration 337, Cost: 0.21969847252529265\n",
      "Iteration 338, Cost: 0.21969847252529268\n",
      "Iteration 339, Cost: 0.21969847252529268\n",
      "Iteration 340, Cost: 0.21969847252529268\n",
      "Iteration 341, Cost: 0.21969847252529268\n",
      "Iteration 342, Cost: 0.21969847252529268\n",
      "Iteration 343, Cost: 0.21969847252529268\n",
      "Iteration 344, Cost: 0.2196984725252927\n",
      "Iteration 345, Cost: 0.21969847252529268\n",
      "Iteration 346, Cost: 0.21969847252529268\n",
      "Iteration 347, Cost: 0.21969847252529276\n",
      "Iteration 348, Cost: 0.21969847252529265\n",
      "Iteration 349, Cost: 0.21969847252529262\n",
      "Iteration 350, Cost: 0.21969847252529268\n",
      "Iteration 351, Cost: 0.21969847252529265\n",
      "Iteration 352, Cost: 0.21969847252529268\n",
      "Iteration 353, Cost: 0.21969847252529268\n",
      "Iteration 354, Cost: 0.21969847252529268\n",
      "Iteration 355, Cost: 0.21969847252529265\n",
      "Iteration 356, Cost: 0.21969847252529265\n",
      "Iteration 357, Cost: 0.21969847252529265\n",
      "Iteration 358, Cost: 0.21969847252529268\n",
      "Iteration 359, Cost: 0.21969847252529268\n",
      "Iteration 360, Cost: 0.21969847252529268\n",
      "Iteration 361, Cost: 0.21969847252529268\n",
      "Iteration 362, Cost: 0.21969847252529268\n",
      "Iteration 363, Cost: 0.21969847252529268\n",
      "Iteration 364, Cost: 0.21969847252529268\n",
      "Iteration 365, Cost: 0.21969847252529268\n",
      "Iteration 366, Cost: 0.21969847252529268\n",
      "Iteration 367, Cost: 0.21969847252529268\n",
      "Iteration 368, Cost: 0.21969847252529268\n",
      "Iteration 369, Cost: 0.21969847252529268\n",
      "Iteration 370, Cost: 0.21969847252529268\n",
      "Iteration 371, Cost: 0.21969847252529268\n",
      "Iteration 372, Cost: 0.21969847252529268\n",
      "Iteration 373, Cost: 0.21969847252529268\n",
      "Iteration 374, Cost: 0.21969847252529268\n",
      "Iteration 375, Cost: 0.21969847252529268\n",
      "Iteration 376, Cost: 0.2196984725252927\n",
      "Iteration 377, Cost: 0.21969847252529268\n",
      "Iteration 378, Cost: 0.21969847252529268\n",
      "Iteration 379, Cost: 0.21969847252529265\n",
      "Iteration 380, Cost: 0.2196984725252927\n",
      "Iteration 381, Cost: 0.2196984725252927\n",
      "Iteration 382, Cost: 0.21969847252529268\n",
      "Iteration 383, Cost: 0.21969847252529268\n",
      "Iteration 384, Cost: 0.21969847252529268\n",
      "Iteration 385, Cost: 0.21969847252529268\n",
      "Iteration 386, Cost: 0.21969847252529268\n",
      "Iteration 387, Cost: 0.21969847252529268\n",
      "Iteration 388, Cost: 0.21969847252529268\n",
      "Iteration 389, Cost: 0.21969847252529268\n",
      "Iteration 390, Cost: 0.21969847252529268\n",
      "Iteration 391, Cost: 0.21969847252529268\n",
      "Iteration 392, Cost: 0.21969847252529268\n",
      "Iteration 393, Cost: 0.21969847252529268\n",
      "Iteration 394, Cost: 0.21969847252529268\n",
      "Iteration 395, Cost: 0.21969847252529268\n",
      "Iteration 396, Cost: 0.21969847252529268\n",
      "Iteration 397, Cost: 0.21969847252529268\n",
      "Iteration 398, Cost: 0.21969847252529265\n",
      "Iteration 399, Cost: 0.21969847252529268\n",
      "Iteration 400, Cost: 0.21969847252529268\n",
      "Iteration 401, Cost: 0.21969847252529268\n",
      "Iteration 402, Cost: 0.21969847252529268\n",
      "Iteration 403, Cost: 0.21969847252529268\n",
      "Iteration 404, Cost: 0.21969847252529268\n",
      "Iteration 405, Cost: 0.21969847252529268\n",
      "Iteration 406, Cost: 0.21969847252529268\n",
      "Iteration 407, Cost: 0.21969847252529268\n",
      "Iteration 408, Cost: 0.2196984725252927\n",
      "Iteration 409, Cost: 0.21969847252529268\n",
      "Iteration 410, Cost: 0.2196984725252927\n",
      "Iteration 411, Cost: 0.2196984725252927\n",
      "Iteration 412, Cost: 0.2196984725252927\n",
      "Iteration 413, Cost: 0.21969847252529268\n",
      "Iteration 414, Cost: 0.21969847252529268\n",
      "Iteration 415, Cost: 0.21969847252529268\n",
      "Iteration 416, Cost: 0.21969847252529268\n",
      "Iteration 417, Cost: 0.21969847252529268\n",
      "Iteration 418, Cost: 0.21969847252529265\n",
      "Iteration 419, Cost: 0.2196984725252927\n",
      "Iteration 420, Cost: 0.2196984725252927\n",
      "Iteration 421, Cost: 0.21969847252529268\n",
      "Iteration 422, Cost: 0.2196984725252927\n",
      "Iteration 423, Cost: 0.21969847252529265\n",
      "Iteration 424, Cost: 0.21969847252529268\n",
      "Iteration 425, Cost: 0.2196984725252927\n",
      "Iteration 426, Cost: 0.2196984725252927\n",
      "Iteration 427, Cost: 0.2196984725252927\n",
      "Iteration 428, Cost: 0.2196984725252927\n",
      "Iteration 429, Cost: 0.21969847252529268\n",
      "Iteration 430, Cost: 0.2196984725252927\n",
      "Iteration 431, Cost: 0.21969847252529268\n",
      "Iteration 432, Cost: 0.2196984725252927\n",
      "Iteration 433, Cost: 0.2196984725252927\n",
      "Iteration 434, Cost: 0.21969847252529265\n",
      "Iteration 435, Cost: 0.21969847252529268\n",
      "Iteration 436, Cost: 0.21969847252529268\n",
      "Iteration 437, Cost: 0.2196984725252927\n",
      "Iteration 438, Cost: 0.2196984725252927\n",
      "Iteration 439, Cost: 0.21969847252529268\n",
      "Iteration 440, Cost: 0.2196984725252927\n",
      "Iteration 441, Cost: 0.2196984725252927\n",
      "Iteration 442, Cost: 0.2196984725252927\n",
      "Iteration 443, Cost: 0.21969847252529268\n",
      "Iteration 444, Cost: 0.2196984725252927\n",
      "Iteration 445, Cost: 0.2196984725252927\n",
      "Iteration 446, Cost: 0.21969847252529268\n",
      "Iteration 447, Cost: 0.21969847252529268\n",
      "Iteration 448, Cost: 0.2196984725252927\n",
      "Iteration 449, Cost: 0.2196984725252927\n",
      "Iteration 450, Cost: 0.2196984725252927\n",
      "Iteration 451, Cost: 0.21969847252529268\n",
      "Iteration 452, Cost: 0.21969847252529268\n",
      "Iteration 453, Cost: 0.21969847252529268\n",
      "Iteration 454, Cost: 0.2196984725252927\n",
      "Iteration 455, Cost: 0.2196984725252927\n",
      "Iteration 456, Cost: 0.2196984725252927\n",
      "Iteration 457, Cost: 0.21969847252529265\n",
      "Iteration 458, Cost: 0.2196984725252927\n",
      "Iteration 459, Cost: 0.2196984725252927\n",
      "Iteration 460, Cost: 0.2196984725252927\n",
      "Iteration 461, Cost: 0.2196984725252927\n",
      "Iteration 462, Cost: 0.21969847252529268\n",
      "Iteration 463, Cost: 0.21969847252529268\n",
      "Iteration 464, Cost: 0.2196984725252927\n",
      "Iteration 465, Cost: 0.2196984725252927\n",
      "Iteration 466, Cost: 0.2196984725252927\n",
      "Iteration 467, Cost: 0.21969847252529268\n",
      "Iteration 468, Cost: 0.21969847252529268\n",
      "Iteration 469, Cost: 0.2196984725252927\n",
      "Iteration 470, Cost: 0.2196984725252927\n",
      "Iteration 471, Cost: 0.2196984725252927\n",
      "Iteration 472, Cost: 0.2196984725252927\n",
      "Iteration 473, Cost: 0.2196984725252927\n",
      "Iteration 474, Cost: 0.2196984725252927\n",
      "Iteration 475, Cost: 0.2196984725252927\n",
      "Iteration 476, Cost: 0.21969847252529268\n",
      "Iteration 477, Cost: 0.2196984725252927\n",
      "Iteration 478, Cost: 0.21969847252529268\n",
      "Iteration 479, Cost: 0.2196984725252927\n",
      "Iteration 480, Cost: 0.2196984725252927\n",
      "Iteration 481, Cost: 0.21969847252529268\n",
      "Iteration 482, Cost: 0.2196984725252927\n",
      "Iteration 483, Cost: 0.2196984725252927\n",
      "Iteration 484, Cost: 0.21969847252529268\n",
      "Iteration 485, Cost: 0.2196984725252927\n",
      "Iteration 486, Cost: 0.21969847252529268\n",
      "Iteration 487, Cost: 0.21969847252529268\n",
      "Iteration 488, Cost: 0.21969847252529268\n",
      "Iteration 489, Cost: 0.2196984725252927\n",
      "Iteration 490, Cost: 0.2196984725252927\n",
      "Iteration 491, Cost: 0.21969847252529265\n",
      "Iteration 492, Cost: 0.2196984725252927\n",
      "Iteration 493, Cost: 0.2196984725252927\n",
      "Iteration 494, Cost: 0.2196984725252927\n",
      "Iteration 495, Cost: 0.21969847252529268\n",
      "Iteration 496, Cost: 0.21969847252529268\n",
      "Iteration 497, Cost: 0.2196984725252927\n",
      "Iteration 498, Cost: 0.2196984725252927\n",
      "Iteration 499, Cost: 0.2196984725252927\n",
      "Iteration 500, Cost: 0.21969847252529268\n",
      "Iteration 501, Cost: 0.2196984725252927\n",
      "Iteration 502, Cost: 0.21969847252529268\n",
      "Iteration 503, Cost: 0.2196984725252927\n",
      "Iteration 504, Cost: 0.2196984725252927\n",
      "Iteration 505, Cost: 0.21969847252529268\n",
      "Iteration 506, Cost: 0.2196984725252927\n",
      "Iteration 507, Cost: 0.21969847252529268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 508, Cost: 0.2196984725252927\n",
      "Iteration 509, Cost: 0.2196984725252927\n",
      "Iteration 510, Cost: 0.21969847252529268\n",
      "Iteration 511, Cost: 0.2196984725252927\n",
      "Iteration 512, Cost: 0.2196984725252927\n",
      "Iteration 513, Cost: 0.2196984725252927\n",
      "Iteration 514, Cost: 0.21969847252529268\n",
      "Iteration 515, Cost: 0.2196984725252927\n",
      "Iteration 516, Cost: 0.2196984725252927\n",
      "Iteration 517, Cost: 0.2196984725252927\n",
      "Iteration 518, Cost: 0.21969847252529268\n",
      "Iteration 519, Cost: 0.2196984725252927\n",
      "Iteration 520, Cost: 0.21969847252529268\n",
      "Iteration 521, Cost: 0.2196984725252927\n",
      "Iteration 522, Cost: 0.21969847252529268\n",
      "Iteration 523, Cost: 0.2196984725252927\n",
      "Iteration 524, Cost: 0.2196984725252927\n",
      "Iteration 525, Cost: 0.21969847252529268\n",
      "Iteration 526, Cost: 0.2196984725252927\n",
      "Iteration 527, Cost: 0.2196984725252927\n",
      "Iteration 528, Cost: 0.21969847252529268\n",
      "Iteration 529, Cost: 0.2196984725252927\n",
      "Iteration 530, Cost: 0.21969847252529268\n",
      "Iteration 531, Cost: 0.21969847252529268\n",
      "Iteration 532, Cost: 0.21969847252529268\n",
      "Iteration 533, Cost: 0.2196984725252927\n",
      "Iteration 534, Cost: 0.2196984725252927\n",
      "Iteration 535, Cost: 0.2196984725252927\n",
      "Iteration 536, Cost: 0.2196984725252927\n",
      "Iteration 537, Cost: 0.21969847252529268\n",
      "Iteration 538, Cost: 0.2196984725252927\n",
      "Iteration 539, Cost: 0.2196984725252927\n",
      "Iteration 540, Cost: 0.21969847252529268\n",
      "Iteration 541, Cost: 0.2196984725252927\n",
      "Iteration 542, Cost: 0.21969847252529268\n",
      "Iteration 543, Cost: 0.21969847252529268\n",
      "Iteration 544, Cost: 0.2196984725252927\n",
      "Iteration 545, Cost: 0.2196984725252927\n",
      "Iteration 546, Cost: 0.21969847252529268\n",
      "Iteration 547, Cost: 0.2196984725252927\n",
      "Iteration 548, Cost: 0.21969847252529268\n",
      "Iteration 549, Cost: 0.2196984725252927\n",
      "Iteration 550, Cost: 0.21969847252529268\n",
      "Iteration 551, Cost: 0.2196984725252927\n",
      "Iteration 552, Cost: 0.21969847252529268\n",
      "Iteration 553, Cost: 0.2196984725252927\n",
      "Iteration 554, Cost: 0.21969847252529268\n",
      "Iteration 555, Cost: 0.2196984725252927\n",
      "Iteration 556, Cost: 0.2196984725252927\n",
      "Iteration 557, Cost: 0.21969847252529268\n",
      "Iteration 558, Cost: 0.21969847252529268\n",
      "Iteration 559, Cost: 0.21969847252529268\n",
      "Iteration 560, Cost: 0.2196984725252927\n",
      "Iteration 561, Cost: 0.21969847252529268\n",
      "Iteration 562, Cost: 0.21969847252529268\n",
      "Iteration 563, Cost: 0.2196984725252927\n",
      "Iteration 564, Cost: 0.2196984725252927\n",
      "Iteration 565, Cost: 0.2196984725252927\n",
      "Iteration 566, Cost: 0.2196984725252927\n",
      "Iteration 567, Cost: 0.2196984725252927\n",
      "Iteration 568, Cost: 0.21969847252529268\n",
      "Iteration 569, Cost: 0.21969847252529268\n",
      "Iteration 570, Cost: 0.2196984725252927\n",
      "Iteration 571, Cost: 0.2196984725252927\n",
      "Iteration 572, Cost: 0.2196984725252927\n",
      "Iteration 573, Cost: 0.21969847252529268\n",
      "Iteration 574, Cost: 0.21969847252529268\n",
      "Iteration 575, Cost: 0.2196984725252927\n",
      "Iteration 576, Cost: 0.21969847252529268\n",
      "Iteration 577, Cost: 0.2196984725252927\n",
      "Iteration 578, Cost: 0.2196984725252927\n",
      "Iteration 579, Cost: 0.2196984725252927\n",
      "Iteration 580, Cost: 0.21969847252529268\n",
      "Iteration 581, Cost: 0.2196984725252927\n",
      "Iteration 582, Cost: 0.21969847252529268\n",
      "Iteration 583, Cost: 0.2196984725252927\n",
      "Iteration 584, Cost: 0.2196984725252927\n",
      "Iteration 585, Cost: 0.2196984725252927\n",
      "Iteration 586, Cost: 0.21969847252529268\n",
      "Iteration 587, Cost: 0.2196984725252927\n",
      "Iteration 588, Cost: 0.2196984725252927\n",
      "Iteration 589, Cost: 0.2196984725252927\n",
      "Iteration 590, Cost: 0.21969847252529268\n",
      "Iteration 591, Cost: 0.21969847252529268\n",
      "Iteration 592, Cost: 0.2196984725252927\n",
      "Iteration 593, Cost: 0.21969847252529268\n",
      "Iteration 594, Cost: 0.21969847252529268\n",
      "Iteration 595, Cost: 0.2196984725252927\n",
      "Iteration 596, Cost: 0.2196984725252927\n",
      "Iteration 597, Cost: 0.2196984725252927\n",
      "Iteration 598, Cost: 0.2196984725252927\n",
      "Iteration 599, Cost: 0.2196984725252927\n",
      "Iteration 600, Cost: 0.2196984725252927\n",
      "Iteration 601, Cost: 0.21969847252529268\n",
      "Iteration 602, Cost: 0.21969847252529268\n",
      "Iteration 603, Cost: 0.2196984725252927\n",
      "Iteration 604, Cost: 0.2196984725252927\n",
      "Iteration 605, Cost: 0.2196984725252927\n",
      "Iteration 606, Cost: 0.2196984725252927\n",
      "Iteration 607, Cost: 0.21969847252529268\n",
      "Iteration 608, Cost: 0.2196984725252927\n",
      "Iteration 609, Cost: 0.21969847252529268\n",
      "Iteration 610, Cost: 0.21969847252529268\n",
      "Iteration 611, Cost: 0.2196984725252927\n",
      "Iteration 612, Cost: 0.21969847252529268\n",
      "Iteration 613, Cost: 0.2196984725252927\n",
      "Iteration 614, Cost: 0.2196984725252927\n",
      "Iteration 615, Cost: 0.21969847252529268\n",
      "Iteration 616, Cost: 0.21969847252529268\n",
      "Iteration 617, Cost: 0.21969847252529268\n",
      "Iteration 618, Cost: 0.2196984725252927\n",
      "Iteration 619, Cost: 0.21969847252529268\n",
      "Iteration 620, Cost: 0.21969847252529268\n",
      "Iteration 621, Cost: 0.2196984725252927\n",
      "Iteration 622, Cost: 0.2196984725252927\n",
      "Iteration 623, Cost: 0.2196984725252927\n",
      "Iteration 624, Cost: 0.21969847252529268\n",
      "Iteration 625, Cost: 0.2196984725252927\n",
      "Iteration 626, Cost: 0.2196984725252927\n",
      "Iteration 627, Cost: 0.2196984725252927\n",
      "Iteration 628, Cost: 0.2196984725252927\n",
      "Iteration 629, Cost: 0.21969847252529268\n",
      "Iteration 630, Cost: 0.21969847252529268\n",
      "Iteration 631, Cost: 0.2196984725252927\n",
      "Iteration 632, Cost: 0.21969847252529268\n",
      "Iteration 633, Cost: 0.21969847252529268\n",
      "Iteration 634, Cost: 0.21969847252529268\n",
      "Iteration 635, Cost: 0.21969847252529268\n",
      "Iteration 636, Cost: 0.2196984725252927\n",
      "Iteration 637, Cost: 0.21969847252529268\n",
      "Iteration 638, Cost: 0.2196984725252927\n",
      "Iteration 639, Cost: 0.2196984725252927\n",
      "Iteration 640, Cost: 0.2196984725252927\n",
      "Iteration 641, Cost: 0.21969847252529268\n",
      "Iteration 642, Cost: 0.21969847252529268\n",
      "Iteration 643, Cost: 0.2196984725252927\n",
      "Iteration 644, Cost: 0.21969847252529268\n",
      "Iteration 645, Cost: 0.21969847252529268\n",
      "Iteration 646, Cost: 0.2196984725252927\n",
      "Iteration 647, Cost: 0.2196984725252927\n",
      "Iteration 648, Cost: 0.21969847252529265\n",
      "Iteration 649, Cost: 0.2196984725252927\n",
      "Iteration 650, Cost: 0.21969847252529265\n",
      "Iteration 651, Cost: 0.21969847252529268\n",
      "Iteration 652, Cost: 0.2196984725252927\n",
      "Iteration 653, Cost: 0.2196984725252927\n",
      "Iteration 654, Cost: 0.2196984725252927\n",
      "Iteration 655, Cost: 0.21969847252529268\n",
      "Iteration 656, Cost: 0.2196984725252927\n",
      "Iteration 657, Cost: 0.2196984725252927\n",
      "Iteration 658, Cost: 0.21969847252529268\n",
      "Iteration 659, Cost: 0.21969847252529265\n",
      "Iteration 660, Cost: 0.21969847252529268\n",
      "Iteration 661, Cost: 0.21969847252529268\n",
      "Iteration 662, Cost: 0.2196984725252927\n",
      "Iteration 663, Cost: 0.21969847252529265\n",
      "Iteration 664, Cost: 0.2196984725252927\n",
      "Iteration 665, Cost: 0.2196984725252927\n",
      "Iteration 666, Cost: 0.21969847252529268\n",
      "Iteration 667, Cost: 0.2196984725252927\n",
      "Iteration 668, Cost: 0.21969847252529268\n",
      "Iteration 669, Cost: 0.21969847252529268\n",
      "Iteration 670, Cost: 0.21969847252529268\n",
      "Iteration 671, Cost: 0.2196984725252927\n",
      "Iteration 672, Cost: 0.2196984725252927\n",
      "Iteration 673, Cost: 0.21969847252529268\n",
      "Iteration 674, Cost: 0.2196984725252927\n",
      "Iteration 675, Cost: 0.2196984725252927\n",
      "Iteration 676, Cost: 0.21969847252529268\n",
      "Iteration 677, Cost: 0.2196984725252927\n",
      "Iteration 678, Cost: 0.21969847252529268\n",
      "Iteration 679, Cost: 0.21969847252529268\n",
      "Iteration 680, Cost: 0.2196984725252927\n",
      "Iteration 681, Cost: 0.2196984725252927\n",
      "Iteration 682, Cost: 0.21969847252529268\n",
      "Iteration 683, Cost: 0.2196984725252927\n",
      "Iteration 684, Cost: 0.2196984725252927\n",
      "Iteration 685, Cost: 0.21969847252529268\n",
      "Iteration 686, Cost: 0.21969847252529268\n",
      "Iteration 687, Cost: 0.2196984725252927\n",
      "Iteration 688, Cost: 0.21969847252529268\n",
      "Iteration 689, Cost: 0.2196984725252927\n",
      "Iteration 690, Cost: 0.2196984725252927\n",
      "Iteration 691, Cost: 0.2196984725252927\n",
      "Iteration 692, Cost: 0.21969847252529268\n",
      "Iteration 693, Cost: 0.2196984725252927\n",
      "Iteration 694, Cost: 0.2196984725252927\n",
      "Iteration 695, Cost: 0.21969847252529265\n",
      "Iteration 696, Cost: 0.2196984725252927\n",
      "Iteration 697, Cost: 0.2196984725252927\n",
      "Iteration 698, Cost: 0.21969847252529265\n",
      "Iteration 699, Cost: 0.2196984725252927\n",
      "Iteration 700, Cost: 0.2196984725252927\n",
      "Iteration 701, Cost: 0.2196984725252927\n",
      "Iteration 702, Cost: 0.2196984725252927\n",
      "Iteration 703, Cost: 0.21969847252529268\n",
      "Iteration 704, Cost: 0.2196984725252927\n",
      "Iteration 705, Cost: 0.21969847252529268\n",
      "Iteration 706, Cost: 0.2196984725252927\n",
      "Iteration 707, Cost: 0.2196984725252927\n",
      "Iteration 708, Cost: 0.2196984725252927\n",
      "Iteration 709, Cost: 0.2196984725252927\n",
      "Iteration 710, Cost: 0.2196984725252927\n",
      "Iteration 711, Cost: 0.2196984725252927\n",
      "Iteration 712, Cost: 0.2196984725252927\n",
      "Iteration 713, Cost: 0.2196984725252927\n",
      "Iteration 714, Cost: 0.2196984725252927\n",
      "Iteration 715, Cost: 0.21969847252529268\n",
      "Iteration 716, Cost: 0.2196984725252927\n",
      "Iteration 717, Cost: 0.2196984725252927\n",
      "Iteration 718, Cost: 0.21969847252529268\n",
      "Iteration 719, Cost: 0.2196984725252927\n",
      "Iteration 720, Cost: 0.21969847252529268\n",
      "Iteration 721, Cost: 0.2196984725252927\n",
      "Iteration 722, Cost: 0.21969847252529268\n",
      "Iteration 723, Cost: 0.21969847252529268\n",
      "Iteration 724, Cost: 0.2196984725252927\n",
      "Iteration 725, Cost: 0.2196984725252927\n",
      "Iteration 726, Cost: 0.2196984725252927\n",
      "Iteration 727, Cost: 0.2196984725252927\n",
      "Iteration 728, Cost: 0.21969847252529268\n",
      "Iteration 729, Cost: 0.21969847252529268\n",
      "Iteration 730, Cost: 0.2196984725252927\n",
      "Iteration 731, Cost: 0.2196984725252927\n",
      "Iteration 732, Cost: 0.2196984725252927\n",
      "Iteration 733, Cost: 0.2196984725252927\n",
      "Iteration 734, Cost: 0.2196984725252927\n",
      "Iteration 735, Cost: 0.2196984725252927\n",
      "Iteration 736, Cost: 0.2196984725252927\n",
      "Iteration 737, Cost: 0.2196984725252927\n",
      "Iteration 738, Cost: 0.21969847252529268\n",
      "Iteration 739, Cost: 0.21969847252529265\n",
      "Iteration 740, Cost: 0.21969847252529268\n",
      "Iteration 741, Cost: 0.2196984725252927\n",
      "Iteration 742, Cost: 0.2196984725252927\n",
      "Iteration 743, Cost: 0.2196984725252927\n",
      "Iteration 744, Cost: 0.2196984725252927\n",
      "Iteration 745, Cost: 0.2196984725252927\n",
      "Iteration 746, Cost: 0.2196984725252927\n",
      "Iteration 747, Cost: 0.21969847252529268\n",
      "Iteration 748, Cost: 0.2196984725252927\n",
      "Iteration 749, Cost: 0.2196984725252927\n",
      "Iteration 750, Cost: 0.21969847252529265\n",
      "Iteration 751, Cost: 0.2196984725252927\n",
      "Iteration 752, Cost: 0.2196984725252927\n",
      "Iteration 753, Cost: 0.2196984725252927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 754, Cost: 0.21969847252529268\n",
      "Iteration 755, Cost: 0.21969847252529268\n",
      "Iteration 756, Cost: 0.21969847252529268\n",
      "Iteration 757, Cost: 0.21969847252529268\n",
      "Iteration 758, Cost: 0.2196984725252927\n",
      "Iteration 759, Cost: 0.2196984725252927\n",
      "Iteration 760, Cost: 0.21969847252529268\n",
      "Iteration 761, Cost: 0.21969847252529268\n",
      "Iteration 762, Cost: 0.2196984725252927\n",
      "Iteration 763, Cost: 0.21969847252529268\n",
      "Iteration 764, Cost: 0.21969847252529265\n",
      "Iteration 765, Cost: 0.2196984725252927\n",
      "Iteration 766, Cost: 0.21969847252529268\n",
      "Iteration 767, Cost: 0.21969847252529268\n",
      "Iteration 768, Cost: 0.2196984725252927\n",
      "Iteration 769, Cost: 0.2196984725252927\n",
      "Iteration 770, Cost: 0.21969847252529268\n",
      "Iteration 771, Cost: 0.2196984725252927\n",
      "Iteration 772, Cost: 0.2196984725252927\n",
      "Iteration 773, Cost: 0.2196984725252927\n",
      "Iteration 774, Cost: 0.21969847252529268\n",
      "Iteration 775, Cost: 0.2196984725252927\n",
      "Iteration 776, Cost: 0.21969847252529268\n",
      "Iteration 777, Cost: 0.2196984725252927\n",
      "Iteration 778, Cost: 0.2196984725252927\n",
      "Iteration 779, Cost: 0.21969847252529268\n",
      "Iteration 780, Cost: 0.2196984725252927\n",
      "Iteration 781, Cost: 0.2196984725252927\n",
      "Iteration 782, Cost: 0.2196984725252927\n",
      "Iteration 783, Cost: 0.2196984725252927\n",
      "Iteration 784, Cost: 0.2196984725252927\n",
      "Iteration 785, Cost: 0.21969847252529268\n",
      "Iteration 786, Cost: 0.2196984725252927\n",
      "Iteration 787, Cost: 0.2196984725252927\n",
      "Iteration 788, Cost: 0.2196984725252927\n",
      "Iteration 789, Cost: 0.21969847252529268\n",
      "Iteration 790, Cost: 0.21969847252529265\n",
      "Iteration 791, Cost: 0.2196984725252927\n",
      "Iteration 792, Cost: 0.21969847252529268\n",
      "Iteration 793, Cost: 0.21969847252529268\n",
      "Iteration 794, Cost: 0.2196984725252927\n",
      "Iteration 795, Cost: 0.2196984725252927\n",
      "Iteration 796, Cost: 0.2196984725252927\n",
      "Iteration 797, Cost: 0.2196984725252927\n",
      "Iteration 798, Cost: 0.2196984725252927\n",
      "Iteration 799, Cost: 0.21969847252529268\n",
      "Iteration 800, Cost: 0.21969847252529268\n",
      "Iteration 801, Cost: 0.21969847252529268\n",
      "Iteration 802, Cost: 0.21969847252529268\n",
      "Iteration 803, Cost: 0.21969847252529268\n",
      "Iteration 804, Cost: 0.2196984725252927\n",
      "Iteration 805, Cost: 0.2196984725252927\n",
      "Iteration 806, Cost: 0.21969847252529268\n",
      "Iteration 807, Cost: 0.21969847252529268\n",
      "Iteration 808, Cost: 0.2196984725252927\n",
      "Iteration 809, Cost: 0.2196984725252927\n",
      "Iteration 810, Cost: 0.2196984725252927\n",
      "Iteration 811, Cost: 0.21969847252529268\n",
      "Iteration 812, Cost: 0.2196984725252927\n",
      "Iteration 813, Cost: 0.2196984725252927\n",
      "Iteration 814, Cost: 0.2196984725252927\n",
      "Iteration 815, Cost: 0.21969847252529268\n",
      "Iteration 816, Cost: 0.2196984725252927\n",
      "Iteration 817, Cost: 0.2196984725252927\n",
      "Iteration 818, Cost: 0.2196984725252927\n",
      "Iteration 819, Cost: 0.2196984725252927\n",
      "Iteration 820, Cost: 0.21969847252529268\n",
      "Iteration 821, Cost: 0.21969847252529268\n",
      "Iteration 822, Cost: 0.21969847252529268\n",
      "Iteration 823, Cost: 0.21969847252529268\n",
      "Iteration 824, Cost: 0.21969847252529268\n",
      "Iteration 825, Cost: 0.2196984725252927\n",
      "Iteration 826, Cost: 0.2196984725252927\n",
      "Iteration 827, Cost: 0.2196984725252927\n",
      "Iteration 828, Cost: 0.2196984725252927\n",
      "Iteration 829, Cost: 0.2196984725252927\n",
      "Iteration 830, Cost: 0.2196984725252927\n",
      "Iteration 831, Cost: 0.2196984725252927\n",
      "Iteration 832, Cost: 0.21969847252529268\n",
      "Iteration 833, Cost: 0.2196984725252927\n",
      "Iteration 834, Cost: 0.2196984725252927\n",
      "Iteration 835, Cost: 0.2196984725252927\n",
      "Iteration 836, Cost: 0.2196984725252927\n",
      "Iteration 837, Cost: 0.2196984725252927\n",
      "Iteration 838, Cost: 0.2196984725252927\n",
      "Iteration 839, Cost: 0.2196984725252927\n",
      "Iteration 840, Cost: 0.2196984725252927\n",
      "Iteration 841, Cost: 0.21969847252529268\n",
      "Iteration 842, Cost: 0.2196984725252927\n",
      "Iteration 843, Cost: 0.2196984725252927\n",
      "Iteration 844, Cost: 0.21969847252529268\n",
      "Iteration 845, Cost: 0.2196984725252927\n",
      "Iteration 846, Cost: 0.2196984725252927\n",
      "Iteration 847, Cost: 0.2196984725252927\n",
      "Iteration 848, Cost: 0.21969847252529268\n",
      "Iteration 849, Cost: 0.21969847252529265\n",
      "Iteration 850, Cost: 0.2196984725252927\n",
      "Iteration 851, Cost: 0.2196984725252927\n",
      "Iteration 852, Cost: 0.21969847252529265\n",
      "Iteration 853, Cost: 0.21969847252529268\n",
      "Iteration 854, Cost: 0.21969847252529268\n",
      "Iteration 855, Cost: 0.2196984725252927\n",
      "Iteration 856, Cost: 0.21969847252529268\n",
      "Iteration 857, Cost: 0.2196984725252927\n",
      "Iteration 858, Cost: 0.21969847252529268\n",
      "Iteration 859, Cost: 0.2196984725252927\n",
      "Iteration 860, Cost: 0.21969847252529268\n",
      "Iteration 861, Cost: 0.21969847252529268\n",
      "Iteration 862, Cost: 0.2196984725252927\n",
      "Iteration 863, Cost: 0.21969847252529268\n",
      "Iteration 864, Cost: 0.21969847252529268\n",
      "Iteration 865, Cost: 0.2196984725252927\n",
      "Iteration 866, Cost: 0.21969847252529268\n",
      "Iteration 867, Cost: 0.2196984725252927\n",
      "Iteration 868, Cost: 0.2196984725252927\n",
      "Iteration 869, Cost: 0.2196984725252927\n",
      "Iteration 870, Cost: 0.21969847252529268\n",
      "Iteration 871, Cost: 0.2196984725252927\n",
      "Iteration 872, Cost: 0.21969847252529268\n",
      "Iteration 873, Cost: 0.21969847252529268\n",
      "Iteration 874, Cost: 0.2196984725252927\n",
      "Iteration 875, Cost: 0.2196984725252927\n",
      "Iteration 876, Cost: 0.2196984725252927\n",
      "Iteration 877, Cost: 0.2196984725252927\n",
      "Iteration 878, Cost: 0.2196984725252927\n",
      "Iteration 879, Cost: 0.2196984725252927\n",
      "Iteration 880, Cost: 0.2196984725252927\n",
      "Iteration 881, Cost: 0.2196984725252927\n",
      "Iteration 882, Cost: 0.2196984725252927\n",
      "Iteration 883, Cost: 0.2196984725252927\n",
      "Iteration 884, Cost: 0.2196984725252927\n",
      "Iteration 885, Cost: 0.21969847252529268\n",
      "Iteration 886, Cost: 0.2196984725252927\n",
      "Iteration 887, Cost: 0.2196984725252927\n",
      "Iteration 888, Cost: 0.2196984725252927\n",
      "Iteration 889, Cost: 0.21969847252529268\n",
      "Iteration 890, Cost: 0.21969847252529268\n",
      "Iteration 891, Cost: 0.2196984725252927\n",
      "Iteration 892, Cost: 0.2196984725252927\n",
      "Iteration 893, Cost: 0.2196984725252927\n",
      "Iteration 894, Cost: 0.2196984725252927\n",
      "Iteration 895, Cost: 0.2196984725252927\n",
      "Iteration 896, Cost: 0.2196984725252927\n",
      "Iteration 897, Cost: 0.2196984725252927\n",
      "Iteration 898, Cost: 0.21969847252529268\n",
      "Iteration 899, Cost: 0.21969847252529268\n",
      "Iteration 900, Cost: 0.2196984725252927\n",
      "Iteration 901, Cost: 0.21969847252529268\n",
      "Iteration 902, Cost: 0.2196984725252927\n",
      "Iteration 903, Cost: 0.21969847252529268\n",
      "Iteration 904, Cost: 0.2196984725252927\n",
      "Iteration 905, Cost: 0.2196984725252927\n",
      "Iteration 906, Cost: 0.21969847252529268\n",
      "Iteration 907, Cost: 0.21969847252529265\n",
      "Iteration 908, Cost: 0.21969847252529265\n",
      "Iteration 909, Cost: 0.2196984725252927\n",
      "Iteration 910, Cost: 0.2196984725252927\n",
      "Iteration 911, Cost: 0.2196984725252927\n",
      "Iteration 912, Cost: 0.2196984725252927\n",
      "Iteration 913, Cost: 0.2196984725252927\n",
      "Iteration 914, Cost: 0.21969847252529268\n",
      "Iteration 915, Cost: 0.2196984725252927\n",
      "Iteration 916, Cost: 0.2196984725252927\n",
      "Iteration 917, Cost: 0.2196984725252927\n",
      "Iteration 918, Cost: 0.2196984725252927\n",
      "Iteration 919, Cost: 0.2196984725252927\n",
      "Iteration 920, Cost: 0.2196984725252927\n",
      "Iteration 921, Cost: 0.2196984725252927\n",
      "Iteration 922, Cost: 0.2196984725252927\n",
      "Iteration 923, Cost: 0.2196984725252927\n",
      "Iteration 924, Cost: 0.2196984725252927\n",
      "Iteration 925, Cost: 0.2196984725252927\n",
      "Iteration 926, Cost: 0.21969847252529268\n",
      "Iteration 927, Cost: 0.21969847252529268\n",
      "Iteration 928, Cost: 0.2196984725252927\n",
      "Iteration 929, Cost: 0.21969847252529268\n",
      "Iteration 930, Cost: 0.21969847252529268\n",
      "Iteration 931, Cost: 0.2196984725252927\n",
      "Iteration 932, Cost: 0.2196984725252927\n",
      "Iteration 933, Cost: 0.21969847252529268\n",
      "Iteration 934, Cost: 0.21969847252529268\n",
      "Iteration 935, Cost: 0.21969847252529268\n",
      "Iteration 936, Cost: 0.21969847252529268\n",
      "Iteration 937, Cost: 0.21969847252529268\n",
      "Iteration 938, Cost: 0.21969847252529268\n",
      "Iteration 939, Cost: 0.21969847252529268\n",
      "Iteration 940, Cost: 0.2196984725252927\n",
      "Iteration 941, Cost: 0.21969847252529268\n",
      "Iteration 942, Cost: 0.21969847252529268\n",
      "Iteration 943, Cost: 0.2196984725252927\n",
      "Iteration 944, Cost: 0.2196984725252927\n",
      "Iteration 945, Cost: 0.2196984725252927\n",
      "Iteration 946, Cost: 0.21969847252529268\n",
      "Iteration 947, Cost: 0.2196984725252927\n",
      "Iteration 948, Cost: 0.2196984725252927\n",
      "Iteration 949, Cost: 0.21969847252529268\n",
      "Iteration 950, Cost: 0.21969847252529268\n",
      "Iteration 951, Cost: 0.2196984725252927\n",
      "Iteration 952, Cost: 0.21969847252529268\n",
      "Iteration 953, Cost: 0.2196984725252927\n",
      "Iteration 954, Cost: 0.2196984725252927\n",
      "Iteration 955, Cost: 0.21969847252529268\n",
      "Iteration 956, Cost: 0.2196984725252927\n",
      "Iteration 957, Cost: 0.2196984725252927\n",
      "Iteration 958, Cost: 0.2196984725252927\n",
      "Iteration 959, Cost: 0.21969847252529268\n",
      "Iteration 960, Cost: 0.2196984725252927\n",
      "Iteration 961, Cost: 0.2196984725252927\n",
      "Iteration 962, Cost: 0.21969847252529268\n",
      "Iteration 963, Cost: 0.21969847252529268\n",
      "Iteration 964, Cost: 0.2196984725252927\n",
      "Iteration 965, Cost: 0.2196984725252927\n",
      "Iteration 966, Cost: 0.2196984725252927\n",
      "Iteration 967, Cost: 0.21969847252529265\n",
      "Iteration 968, Cost: 0.2196984725252927\n",
      "Iteration 969, Cost: 0.2196984725252927\n",
      "Iteration 970, Cost: 0.2196984725252927\n",
      "Iteration 971, Cost: 0.21969847252529268\n",
      "Iteration 972, Cost: 0.2196984725252927\n",
      "Iteration 973, Cost: 0.2196984725252927\n",
      "Iteration 974, Cost: 0.2196984725252927\n",
      "Iteration 975, Cost: 0.2196984725252927\n",
      "Iteration 976, Cost: 0.21969847252529268\n",
      "Iteration 977, Cost: 0.2196984725252927\n",
      "Iteration 978, Cost: 0.2196984725252927\n",
      "Iteration 979, Cost: 0.21969847252529268\n",
      "Iteration 980, Cost: 0.2196984725252927\n",
      "Iteration 981, Cost: 0.2196984725252927\n",
      "Iteration 982, Cost: 0.2196984725252927\n",
      "Iteration 983, Cost: 0.2196984725252927\n",
      "Iteration 984, Cost: 0.21969847252529265\n",
      "Iteration 985, Cost: 0.2196984725252927\n",
      "Iteration 986, Cost: 0.2196984725252927\n",
      "Iteration 987, Cost: 0.21969847252529265\n",
      "Iteration 988, Cost: 0.2196984725252927\n",
      "Iteration 989, Cost: 0.2196984725252927\n",
      "Iteration 990, Cost: 0.21969847252529268\n",
      "Iteration 991, Cost: 0.2196984725252927\n",
      "Iteration 992, Cost: 0.2196984725252927\n",
      "Iteration 993, Cost: 0.21969847252529268\n",
      "Iteration 994, Cost: 0.21969847252529265\n",
      "Iteration 995, Cost: 0.2196984725252927\n",
      "Iteration 996, Cost: 0.21969847252529268\n",
      "Iteration 997, Cost: 0.21969847252529268\n",
      "Iteration 998, Cost: 0.21969847252529268\n",
      "Iteration 999, Cost: 0.21969847252529268\n",
      "Iteration 1000, Cost: 0.21969847252529268\n",
      "Accuracy: 2.0\n",
      "F1 Score: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "df_wine = pd.read_csv(\"/Users/noshitha/Downloads/hw3/datasets/hw3_wine.csv\", delimiter=\"\\t\")\n",
    "\n",
    "# Preprocess data\n",
    "X_wine = pd.get_dummies(df_wine.iloc[:, 1:])\n",
    "y_wine = df_wine.iloc[:, 0]\n",
    "\n",
    "# Normalize data\n",
    "y_wine_resized = y_wine.values.reshape(-1, 1)\n",
    "\n",
    "# Initialize the OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Fit and transform the target variable\n",
    "y_encoded = encoder.fit_transform(y_wine_resized)\n",
    "\n",
    "# Define model architectures\n",
    "architectures = [\n",
    "    [X_wine.shape[1], 5, 3, 3],  # Example architecture\n",
    "]\n",
    "\n",
    "# Instantiate the NeuralNetwork class with the desired architecture\n",
    "model = NeuralNetwork(architectures[0])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_wine, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the neural network\n",
    "model.train(X_train, y_train, learning_rate=0.1, lam=0.0, max_iterations=1000, epsilon=0.005)\n",
    "\n",
    "# Evaluate the trained model\n",
    "accuracy, f1_score = model.evaluate(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1 Score:\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5de705",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dba14d79",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'layers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/bm/l2zpyjxn2gqg_1d20xgx5z0w0000gn/T/ipykernel_84064/2989320146.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;31m# Create an instance of the NeuralNetwork class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeuralNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;31m# Output the results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/bm/l2zpyjxn2gqg_1d20xgx5z0w0000gn/T/ipykernel_84064/2989320146.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, layers)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbiases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minitialize_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/bm/l2zpyjxn2gqg_1d20xgx5z0w0000gn/T/ipykernel_84064/2989320146.py\u001b[0m in \u001b[0;36minitialize_network\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minitialize_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbiases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'layers' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        self.weights = None\n",
    "        self.biases = None\n",
    "        self.initialize_network()\n",
    "\n",
    "    def initialize_network(self):\n",
    "        self.weights = [np.random.randn(layers[i], layers[i+1]) for i in range(len(layers)-1)]\n",
    "        self.biases = [np.zeros((1, layers[i+1])) for i in range(len(layers)-1)]\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    def forward_pass(self, X):\n",
    "        X = np.array(X)  # Ensure X is converted to numpy array\n",
    "        activations = [X]\n",
    "        for i, (w, b) in enumerate(zip(self.weights, self.biases)):\n",
    "            print(f\"\\nLayer {i+1}:\")\n",
    "            print(\"X:\", X.dtype)\n",
    "            print(\"Weights:\", w.dtype)\n",
    "            print(\"Biases:\", b.dtype)\n",
    "            z = np.dot(activations[-1], w) + b\n",
    "            print(\"Z:\", z.dtype)\n",
    "            activations.append(self.sigmoid(z))\n",
    "        return activations\n",
    "\n",
    "    \n",
    "    def backward_pass(self, X, Y, activations):\n",
    "        deltas = [(activations[-1] - Y) * self.sigmoid_derivative(activations[-1])]\n",
    "        for i in range(len(self.layers) - 2, 0, -1):\n",
    "            delta = np.dot(deltas[0], self.weights[i].T) * self.sigmoid_derivative(activations[i])\n",
    "            deltas.insert(0, delta)\n",
    "        return deltas\n",
    "    \n",
    "    def compute_gradients(self, activations, deltas):\n",
    "        gradients_weights = [np.dot(activations[i].T, deltas[i]) for i in range(len(self.layers) - 1)]\n",
    "        gradients_biases = [np.sum(deltas[i], axis=0) for i in range(len(self.layers) - 1)]\n",
    "        return gradients_weights, gradients_biases\n",
    "    \n",
    "    def update_weights(self, gradients_weights, gradients_biases, learning_rate):\n",
    "        self.weights = [w - learning_rate * gw for w, gw in zip(self.weights, gradients_weights)]\n",
    "        self.biases = [b - learning_rate * gb for b, gb in zip(self.biases, gradients_biases)]\n",
    "    \n",
    "    def train(self, X, Y, learning_rate, lam, max_iterations, epsilon):\n",
    "        for iteration in range(max_iterations):\n",
    "            activations = self.forward_pass(X)\n",
    "            deltas = self.backward_pass(X, Y, activations)\n",
    "            gradients_weights, gradients_biases = self.compute_gradients(activations, deltas)\n",
    "            self.update_weights(gradients_weights, gradients_biases, learning_rate)\n",
    "            # Compute cost function\n",
    "            J = np.mean(np.square(activations[-1] - Y))\n",
    "            #print(f\"Iteration {iteration+1}, Cost: {J}\")\n",
    "            # Check for convergence\n",
    "            if J < epsilon:\n",
    "                print(f\"Converged at cost :{J} while Epsilon:{epsilon} \")\n",
    "                return J\n",
    "        return J\n",
    "            \n",
    "\n",
    "    def accuracy(self, y_true, y_pred):\n",
    "        correct = np.sum(np.all(y_true == y_pred, axis=1))\n",
    "        return correct / len(y_true)\n",
    "\n",
    "\n",
    "    def f1_score(self, y_true, y_pred):\n",
    "        tp = np.sum(np.logical_and(y_true, y_pred))\n",
    "        fp = np.sum(np.logical_and(np.logical_not(y_true), y_pred))\n",
    "        fn = np.sum(np.logical_and(y_true, np.logical_not(y_pred)))\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        return f1\n",
    "\n",
    "\n",
    "    def evaluate(self, X_test, y_test, J):\n",
    "        activations = self.forward_pass(X_test)[-1]\n",
    "        y_pred = (activations > 0.5).astype(int)\n",
    "        acc = self.accuracy(y_test, y_pred)\n",
    "        f1 = self.f1_score(y_test, y_pred)\n",
    "        return J, acc, f1\n",
    "    \n",
    "    def k_fold_cross_validation(self, X, y, architectures, regularization_params, learning_rate, max_iterations, epsilon):\n",
    "        skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "        results_accuracy = {}\n",
    "        results_f1_score = {}\n",
    "\n",
    "        for arch in architectures:\n",
    "            for lam in regularization_params:\n",
    "                accuracy_list = []\n",
    "                f1_score_list = []\n",
    "                for train_index, test_index in skf.split(X, y):\n",
    "                    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "                    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "                    mean = np.mean(X_train, axis=0)\n",
    "                    std = np.std(X_train, axis=0)\n",
    "                    X_train_normalized = (X_train - mean) / std\n",
    "                    X_test_normalized = (X_test - mean) / std\n",
    "\n",
    "                    model = NeuralNetwork(arch)\n",
    "                    model.train(X_train_normalized, y_train, learning_rate=learning_rate, lam=lam, max_iterations=max_iterations, epsilon=epsilon)\n",
    "                    accuracy, f1_score = model.evaluate(X_test_normalized, y_test)\n",
    "                    accuracy_list.append(accuracy)\n",
    "                    f1_score_list.append(f1_score)\n",
    "\n",
    "                mean_accuracy = np.mean(accuracy_list)\n",
    "                mean_f1_score = np.mean(f1_score_list)\n",
    "\n",
    "                results_accuracy[(str(arch), lam)] = mean_accuracy\n",
    "                results_f1_score[(str(arch), lam)] = mean_f1_score\n",
    "\n",
    "        return results_accuracy, results_f1_score\n",
    "\n",
    "# Define the network architectures and regularization parameters\n",
    "architectures = [[1, 2, 1], [2, 4, 3, 2]]\n",
    "regularization_params = [0.000, 0.250]\n",
    "\n",
    "# Create an instance of the NeuralNetwork class\n",
    "model = NeuralNetwork([])\n",
    "\n",
    "# Output the results\n",
    "for i, arch in enumerate(architectures):\n",
    "    for lam in regularization_params:\n",
    "        print(f\"backprop_example_{i+1}: Regularization parameter lambda={lam}\\n\")\n",
    "\n",
    "        # Initialize the network with the given architecture\n",
    "        model.weights = [np.random.randn(arch[i], arch[i+1]) for i in range(len(arch)-1)]\n",
    "        model.biases = [np.zeros((1, arch[i+1])) for i in range(len(arch)-1)]\n",
    "\n",
    "        # Output initial weights\n",
    "        print(\"Initializing the network with the following structure (number of neurons per layer):\", arch)\n",
    "        for j in range(len(arch) - 1):\n",
    "            print(f\"\\nInitial Theta{j+1} (the weights of each neuron, including the bias weight, are stored in the rows):\")\n",
    "            for k in range(len(model.weights[j])):\n",
    "                print(\"\\t\", end=\"\")\n",
    "                print(*model.weights[j][k], sep=\"  \")\n",
    "\n",
    "        # Training set\n",
    "        print(\"\\nTraining set\")\n",
    "        for idx, instance in enumerate(X_train):\n",
    "            print(f\"\\tTraining instance {idx+1}\")\n",
    "            print(\"\\t\\tx:\", instance)\n",
    "            print(\"\\t\\ty:\", y_train[idx])\n",
    "\n",
    "        # Compute and output the error/cost J\n",
    "        print(\"\\n--------------------------------------------\")\n",
    "        print(\"Computing the error/cost, J, of the network\")\n",
    "        for idx, instance in enumerate(X_train):\n",
    "            print(f\"\\tProcessing training instance {idx+1}\")\n",
    "            activations = model.forward_pass(instance)\n",
    "            print(\"\\tForward propagating the input\", instance)\n",
    "            for l, activation in enumerate(activations[1:], start=1):\n",
    "                print(f\"\\t\\ta{l}:\", activation)\n",
    "            print(f\"\\n\\tf(x): {activations[-1][0]}\")\n",
    "            print(f\"Predicted output for instance {idx+1}:\", activations[-1][0])\n",
    "            print(f\"Expected output for instance {idx+1}:\", y_train[idx][0])\n",
    "            print(f\"Cost, J, associated with instance {idx+1}: {np.mean(np.square(activations[-1] - y_train[idx])):.3f}\")\n",
    "\n",
    "        # Final (regularized) cost J\n",
    "        print(f\"\\nFinal (regularized) cost, J, based on the complete training set: {J:.5f}\")\n",
    "\n",
    "        # Running backpropagation\n",
    "        print(\"\\n--------------------------------------------\")\n",
    "        print(\"Running backpropagation\")\n",
    "        for idx, instance in enumerate(X_train):\n",
    "            print(f\"\\tComputing gradients based on training instance {idx+1}\")\n",
    "            activations = model.forward_pass(instance)\n",
    "            deltas = model.backward_pass(instance, y_train[idx], activations)\n",
    "            for l, delta in enumerate(deltas[1:], start=2):\n",
    "                print(f\"\\t\\tdelta{l}:\", delta[0])\n",
    "            for l, gradient_weights in enumerate(model.compute_gradients(activations, deltas)[0], start=1):\n",
    "                print(f\"\\n\\t\\tGradients of Theta{l} based on training instance {idx+1}:\")\n",
    "                for k in range(len(gradient_weights)):\n",
    "                    print(\"\\t\\t\", end=\"\")\n",
    "                    print(*gradient_weights[k], sep=\"  \")\n",
    "        print(\"\\n--------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe93715",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
