{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2b3db68",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"/Users/noshitha/Downloads/contraceptive+method+choice/cmc.data\"\n",
    "attribute_names = [\n",
    "    \"Wife_age\", \"Wife_education\", \"Husband_education\", \"Number_of_children ever born\",\n",
    "    \"Wife_religion\", \"Wife_working\", \"Husband_occupation\", \"Standard-of-living_index\",\n",
    "    \"Media_exposure\", \"Contraceptive_method_used\"\n",
    "]\n",
    "data = pd.read_csv(data_file, header=None, names=attribute_names)\n",
    "\n",
    "# Convert \"Wife's age\" and \"Number of children ever born\" to object type\n",
    "data['Wife_age'] = data['Wife_age'].astype(str)\n",
    "data['Number_of_children ever born'] = data['Number_of_children ever born'].astype(str)\n",
    "\n",
    "# Separate features and target variable\n",
    "X = data.drop(\"Contraceptive_method_used\", axis=1)\n",
    "y = data[\"Contraceptive_method_used\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47ff1a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_trees:  1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/bm/l2zpyjxn2gqg_1d20xgx5z0w0000gn/T/ipykernel_1092/2060790644.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnum_trees\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mn_trees_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"num_trees: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_trees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         \u001b[0maccuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecisions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecalls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstratified_cross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_folds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_trees\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_subsample_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_subsample_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracies:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Precisions:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecisions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/bm/l2zpyjxn2gqg_1d20xgx5z0w0000gn/T/ipykernel_1092/2060790644.py\u001b[0m in \u001b[0;36mstratified_cross_validation\u001b[0;34m(X, y, n_folds, num_trees, max_depth, example_subsample_rate, attr_subsample_rate)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m         \u001b[0mrf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_trees\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_subsample_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_subsample_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m         \u001b[0mtrees\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubsampled_attributes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_random_forest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_fold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_fold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify_random_forest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubsampled_attributes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_validation_fold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class DecisionTreeClassifier:\n",
    "    def __init__(self, max_depth):\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def entropy(self, labels):\n",
    "        unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "        probabilities = counts / len(labels)\n",
    "        entropy_value = -np.sum(probabilities * np.log2(probabilities))\n",
    "        return entropy_value\n",
    "\n",
    "    def information_gain(self, y, x):\n",
    "        parent_entropy = self.entropy(y)\n",
    "        info_a = 0\n",
    "        if x.dtype == 'object':\n",
    "            unique_values = x.unique()\n",
    "            for value in unique_values:\n",
    "                partition_indices = x[x == value].index\n",
    "                partition_entropy = self.entropy(y[partition_indices])\n",
    "                info_a += len(partition_indices) / len(x) * partition_entropy\n",
    "        else:\n",
    "            attr_mean = x.mean()\n",
    "            partition_indices_left = x[x <= attr_mean].index\n",
    "            partition_indices_right = x[x > attr_mean].index\n",
    "            partition_entropy_left = self.entropy(y[partition_indices_left])\n",
    "            partition_entropy_right = self.entropy(y[partition_indices_right])\n",
    "            weight_left = len(partition_indices_left) / len(x)\n",
    "            weight_right = len(partition_indices_right) / len(x)\n",
    "            info_a = weight_left * partition_entropy_left + weight_right * partition_entropy_right\n",
    "        gain_a = parent_entropy - info_a\n",
    "        return gain_a\n",
    "\n",
    "    def decision_tree(self, X_train, y_train, current_depth=0):\n",
    "        if len(set(y_train)) == 1 or current_depth == self.max_depth or len(X_train.columns) == 0:\n",
    "            class_counts = Counter(y_train)\n",
    "            majority_class = class_counts.most_common(1)[0][0]\n",
    "            return {\"class_label\": majority_class}\n",
    "\n",
    "        gains = {}\n",
    "        for attr in X_train.columns:\n",
    "            gains[attr] = self.information_gain(y_train, X_train[attr])\n",
    "\n",
    "        best_attr = max(gains, key=gains.get)\n",
    "        node = {\"attribute\": best_attr, \"leaf\": {}}\n",
    "\n",
    "        if X_train[best_attr].dtype == 'object':\n",
    "            unique_values = X_train[best_attr].unique()\n",
    "            for value in unique_values:\n",
    "                partition_indices = X_train[X_train[best_attr] == value].index\n",
    "                if len(partition_indices) == 0:\n",
    "                    class_counts = Counter(y_train)\n",
    "                    majority_class = class_counts.most_common(1)[0][0]\n",
    "                    node[\"leaf\"][value] = {\"class_label\": majority_class}\n",
    "                else:\n",
    "                    node[\"leaf\"][value] = self.decision_tree(X_train.loc[partition_indices], y_train.loc[partition_indices], current_depth + 1)\n",
    "        else:\n",
    "            attr_mean = X_train[best_attr].mean()\n",
    "            partition_indices_left = X_train[X_train[best_attr] <= attr_mean].index\n",
    "            partition_indices_right = X_train[X_train[best_attr] > attr_mean].index\n",
    "            if len(partition_indices_left) == 0 or len(partition_indices_right) == 0:\n",
    "                class_counts = Counter(y_train)\n",
    "                majority_class = class_counts.most_common(1)[0][0]\n",
    "                return {\"class_label\": majority_class}\n",
    "            else:\n",
    "                node[\"split_value\"] = attr_mean\n",
    "                node[\"left\"] = self.decision_tree(X_train.loc[partition_indices_left], y_train.loc[partition_indices_left], current_depth + 1)\n",
    "                node[\"right\"] = self.decision_tree(X_train.loc[partition_indices_right], y_train.loc[partition_indices_right], current_depth + 1)\n",
    "\n",
    "        return node\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        self.tree = self.decision_tree(X_train, y_train)\n",
    "\n",
    "    def predict_instance(self, instance, tree):\n",
    "        if \"class_label\" in tree:\n",
    "            return tree[\"class_label\"]\n",
    "        else:\n",
    "            attr = tree[\"attribute\"]\n",
    "            if instance[attr] in tree[\"leaf\"]:\n",
    "                return self.predict_instance(instance, tree[\"leaf\"][instance[attr]])\n",
    "            else:\n",
    "                if instance[attr] <= tree[\"split_value\"]:\n",
    "                    return self.predict_instance(instance, tree[\"left\"])\n",
    "                else:\n",
    "                    return self.predict_instance(instance, tree[\"right\"])\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        predictions = []\n",
    "        for index, instance in X_test.iterrows():\n",
    "            predictions.append(self.predict_instance(instance, self.tree))\n",
    "        return predictions\n",
    "\n",
    "class RandomForestClassifier:\n",
    "    def __init__(self, num_trees, max_depth, example_subsample_rate, attr_subsample_rate):\n",
    "        self.num_trees = num_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.example_subsample_rate = example_subsample_rate\n",
    "        self.attr_subsample_rate = attr_subsample_rate\n",
    "    \n",
    "    def entropy(self, labels):\n",
    "        unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "        probabilities = counts / len(labels)\n",
    "        entropy_value = -np.sum(probabilities * np.log2(probabilities))\n",
    "        return entropy_value\n",
    "\n",
    "    def information_gain(self, y, x):\n",
    "        parent_entropy = self.entropy(y)\n",
    "        info_a = 0\n",
    "        if x.dtype == 'object':\n",
    "            unique_values = x.unique()\n",
    "            for value in unique_values:\n",
    "                partition_indices = x[x == value].index\n",
    "                partition_entropy = self.entropy(y[partition_indices])\n",
    "                info_a += len(partition_indices) / len(x) * partition_entropy\n",
    "        else:\n",
    "            attr_mean = x.mean()\n",
    "            partition_indices_left = x[x <= attr_mean].index\n",
    "            partition_indices_right = x[x > attr_mean].index\n",
    "            partition_entropy_left = self.entropy(y[partition_indices_left])\n",
    "            partition_entropy_right = self.entropy(y[partition_indices_right])\n",
    "            weight_left = len(partition_indices_left) / len(x)\n",
    "            weight_right = len(partition_indices_right) / len(x)\n",
    "            info_a = weight_left * partition_entropy_left + weight_right * partition_entropy_right\n",
    "        gain_a = parent_entropy - info_a\n",
    "        return gain_a\n",
    "    \n",
    "    def decision_tree(self, X_train, y_train, current_depth=0):\n",
    "        if len(set(y_train)) == 1 or current_depth == self.max_depth or len(X_train.columns) == 0:\n",
    "            class_counts = Counter(y_train)\n",
    "            majority_class = class_counts.most_common(1)[0][0]\n",
    "            return {\"class_label\": majority_class}\n",
    "\n",
    "        gains = {}\n",
    "        for attr in X_train.columns:\n",
    "            gains[attr] = self.information_gain(y_train, X_train[attr])\n",
    "\n",
    "        best_attr = max(gains, key=gains.get)\n",
    "        node = {\"attribute\": best_attr, \"leaf\": {}}\n",
    "\n",
    "        if X_train[best_attr].dtype == 'object':\n",
    "            unique_values = X_train[best_attr].unique()\n",
    "            for value in unique_values:\n",
    "                partition_indices = X_train[X_train[best_attr] == value].index\n",
    "                if len(partition_indices) == 0:\n",
    "                    class_counts = Counter(y_train)\n",
    "                    majority_class = class_counts.most_common(1)[0][0]\n",
    "                    node[\"leaf\"][value] = {\"class_label\": majority_class}\n",
    "                else:\n",
    "                    node[\"leaf\"][value] = self.decision_tree(X_train.loc[partition_indices], y_train.loc[partition_indices], current_depth + 1)\n",
    "        else:\n",
    "            attr_mean = X_train[best_attr].mean()\n",
    "            partition_indices_left = X_train[X_train[best_attr] <= attr_mean].index\n",
    "            partition_indices_right = X_train[X_train[best_attr] > attr_mean].index\n",
    "            if len(partition_indices_left) == 0 or len(partition_indices_right) == 0:\n",
    "                class_counts = Counter(y_train)\n",
    "                majority_class = class_counts.most_common(1)[0][0]\n",
    "                return {\"class_label\": majority_class}\n",
    "            else:\n",
    "                node[\"split_value\"] = attr_mean\n",
    "                node[\"left\"] = self.decision_tree(X_train.loc[partition_indices_left], y_train.loc[partition_indices_left], current_depth + 1)\n",
    "                node[\"right\"] = self.decision_tree(X_train.loc[partition_indices_right], y_train.loc[partition_indices_right], current_depth + 1)\n",
    "\n",
    "        return node\n",
    "\n",
    "    def bootstrap_sampling(self, X, y):\n",
    "        indices = np.random.choice(len(X), size=len(X), replace=True)  # Bootstrap sampling\n",
    "        return X.iloc[indices], y.iloc[indices]\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        trees = []\n",
    "        subsampled_attributes = []\n",
    "\n",
    "        for i in range(self.num_trees):\n",
    "            bootstrapped_X, bootstrapped_y = self.bootstrap_sampling(X_train, y_train)  # Bootstrap sampling\n",
    "            subsampled_attr_indexes = np.random.choice(range(X_train.shape[1]), int(X_train.shape[1] * self.attr_subsample_rate), replace=False)\n",
    "            subsampled_attributes.append(subsampled_attr_indexes.tolist())\n",
    "            subsampled_X = bootstrapped_X.iloc[:, subsampled_attr_indexes]\n",
    "            dt = DecisionTreeClassifier(self.max_depth)\n",
    "            tree = dt.decision_tree(subsampled_X, bootstrapped_y)\n",
    "            trees.append(tree)\n",
    "\n",
    "        return trees, subsampled_attributes\n",
    "\n",
    "    def predict_random_forest(self, trees, subsampled_attributes, X_test):\n",
    "        class_labels = []\n",
    "        dt = DecisionTreeClassifier(self.max_depth)  # Create instance for decision tree classification\n",
    "        for _, test_row in X_test.iterrows():\n",
    "            tree_votes = []\n",
    "            for tree, sub_attributes in zip(trees, subsampled_attributes):\n",
    "                test_features = test_row[sub_attributes]\n",
    "                predicted_label = dt.predict_instance(test_features, tree)  # Use decision tree predict_instance method\n",
    "                if predicted_label is None:\n",
    "                    print(\"None value predicted for features:\", test_features)\n",
    "                tree_votes.append(predicted_label)\n",
    "            class_labels.append(max(set(tree_votes), key=tree_votes.count))\n",
    "        return class_labels\n",
    "\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        predictions = []\n",
    "        for _, test_row in X_test.iterrows():\n",
    "            tree_votes = []\n",
    "            for tree, sub_attributes in zip(self.trees, self.subsampled_attributes):\n",
    "                test_features = test_row[sub_attributes]\n",
    "                predicted_label = self.predict_instance(test_features, tree)  # Use decision tree predict_instance method\n",
    "                if predicted_label is None:\n",
    "                    print(\"None value predicted for features:\", test_features)\n",
    "                tree_votes.append(predicted_label)\n",
    "            predictions.append(max(set(tree_votes), key=tree_votes.count))\n",
    "        return predictions\n",
    "\n",
    "    def fit_random_forest(self, X_train, y_train):\n",
    "        self.trees, self.subsampled_attributes = self.fit(X_train, y_train)\n",
    "    \n",
    "    \n",
    "# class RandomForestClassifier:\n",
    "#     def __init__(self, num_trees, max_depth, example_subsample_rate, attr_subsample_rate):\n",
    "#         self.num_trees = num_trees\n",
    "#         self.max_depth = max_depth\n",
    "#         self.example_subsample_rate = example_subsample_rate\n",
    "#         self.attr_subsample_rate = attr_subsample_rate\n",
    "\n",
    "#     def bootstrap_sampling(self, X, y):\n",
    "#         indices = np.random.choice(len(X), size=len(X), replace=True)\n",
    "#         return X.iloc[indices], y.iloc[indices]\n",
    "    \n",
    "#     def predict_random_forest(self, trees, subsampled_attributes, X_test):\n",
    "#         class_labels = []\n",
    "#         dt = DecisionTreeClassifier(self.max_depth)  # Create instance for decision tree classification\n",
    "#         for _, test_row in X_test.iterrows():\n",
    "#             tree_votes = []\n",
    "#             for tree, sub_attributes in zip(trees, subsampled_attributes):\n",
    "#                 test_features = test_row[sub_attributes]\n",
    "#                 predicted_label = dt.predict_instance(test_features, tree)  # Use decision tree predict_instance method\n",
    "#                 if predicted_label is None:\n",
    "#                     print(\"None value predicted for features:\", test_features)\n",
    "#                 tree_votes.append(predicted_label)\n",
    "#             class_labels.append(max(set(tree_votes), key=tree_votes.count))\n",
    "#         return class_labels\n",
    "\n",
    "#     def predict(self, X_test):\n",
    "#         predictions = []\n",
    "#         for _, test_row in X_test.iterrows():\n",
    "#             tree_votes = []\n",
    "#             for tree, sub_attributes in zip(self.trees, self.subsampled_attributes):\n",
    "#                 test_features = test_row[sub_attributes]\n",
    "#                 predicted_label = self.predict_instance(test_features, tree)  # Use decision tree predict_instance method\n",
    "#                 if predicted_label is None:\n",
    "#                     print(\"None value predicted for features:\", test_features)\n",
    "#                 tree_votes.append(predicted_label)\n",
    "#             predictions.append(max(set(tree_votes), key=tree_votes.count))\n",
    "#         return predictions\n",
    "\n",
    "#     def fit_random_forest(self, X_train, y_train):\n",
    "#         self.trees, self.subsampled_attributes = self.fit(X_train, y_train)\n",
    "    \n",
    "#     def fit(self, X_train, y_train):\n",
    "#         trees = []\n",
    "#         subsampled_attributes = []\n",
    "\n",
    "#         for i in range(self.num_trees):\n",
    "#             bootstrapped_X, bootstrapped_y = self.bootstrap_sampling(X_train, y_train)\n",
    "#             subsampled_attr_indexes = np.random.choice(range(X_train.shape[1]), int(X_train.shape[1] * self.attr_subsample_rate), replace=False)\n",
    "#             subsampled_attributes.append(subsampled_attr_indexes.tolist())\n",
    "#             subsampled_X = bootstrapped_X.iloc[:, subsampled_attr_indexes]\n",
    "#             dt = DecisionTreeClassifier(self.max_depth)\n",
    "#             tree = dt.decision_tree(subsampled_X, bootstrapped_y)\n",
    "#             trees.append(tree)\n",
    "\n",
    "#         return trees, subsampled_attributes\n",
    "\n",
    "#     def classify_random_forest(self, trees, subsampled_attributes, X_test):\n",
    "#         class_labels = []\n",
    "#         dt = DecisionTreeClassifier(self.max_depth)  # Create instance for decision tree classification\n",
    "#         for _, test_row in X_test.iterrows():\n",
    "#             tree_votes = []\n",
    "#             for tree, sub_attributes in zip(trees, subsampled_attributes):\n",
    "#                 test_features = test_row[sub_attributes]\n",
    "#                 predicted_label = dt.classify(tree, test_features)  # Use decision tree classify method\n",
    "#                 if predicted_label[0] is None:\n",
    "#                     print(\"None value predicted for features:\", test_features)\n",
    "#                 tree_votes.append(predicted_label[0])\n",
    "#             class_labels.append(max(set(tree_votes), key=tree_votes.count))\n",
    "#         return class_labels\n",
    "\n",
    "\n",
    "\n",
    "class EvaluationMetrics:\n",
    "    @staticmethod\n",
    "    def confusion_matrix(y_true, y_pred):\n",
    "        print(\"y_true: \",y_true,\"y_pred: \",y_pred)\n",
    "        classes = np.unique(np.concatenate([y_true, y_pred]))\n",
    "        n_classes = len(classes)\n",
    "        conf_matrix = np.zeros((n_classes, n_classes), dtype=int)\n",
    "\n",
    "        for i, true_label in enumerate(classes):\n",
    "            for j, pred_label in enumerate(classes):\n",
    "                conf_matrix[i, j] = np.sum((y_true == true_label) & (y_pred == pred_label))\n",
    "\n",
    "        return conf_matrix\n",
    "    \n",
    "#     @staticmethod\n",
    "#     def confusion_matrix(y_true, y_pred):\n",
    "#         # Filter out None values\n",
    "#         y_true_filtered = [x for x in y_true if x is not None]\n",
    "#         y_pred_filtered = [x for x in y_pred if x is not None]\n",
    "\n",
    "#         classes = np.unique(np.concatenate([y_true_filtered, y_pred_filtered]))\n",
    "#         n_classes = len(classes)\n",
    "#         conf_matrix = np.zeros((n_classes, n_classes), dtype=int)\n",
    "\n",
    "#         for i, true_label in enumerate(classes):\n",
    "#             for j, pred_label in enumerate(classes):\n",
    "#                 conf_matrix[i, j] = np.sum((np.array(y_true_filtered) == true_label) & (np.array(y_pred_filtered) == pred_label))\n",
    "\n",
    "#         return conf_matrix\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_metrics(conf_matrix):\n",
    "        TP = np.diag(conf_matrix)\n",
    "        FP = np.sum(conf_matrix, axis=0) - TP\n",
    "        FN = np.sum(conf_matrix, axis=1) - TP\n",
    "        TN = np.sum(conf_matrix) - (TP + FP + FN)\n",
    "\n",
    "        accuracy = np.sum(TP) / np.sum(conf_matrix)\n",
    "\n",
    "        precision = np.where(TP + FP == 0, 0, TP / (TP + FP))\n",
    "        recall = np.where(TP + FN == 0, 0, TP / (TP + FN))\n",
    "\n",
    "        f1_score = np.zeros_like(precision)\n",
    "        non_zero_denominator = (precision + recall) != 0\n",
    "        f1_score[non_zero_denominator] = 2 * (precision[non_zero_denominator] * recall[non_zero_denominator]) / (precision[non_zero_denominator] + recall[non_zero_denominator])\n",
    "\n",
    "        return accuracy, precision, recall, f1_score\n",
    "\n",
    "\n",
    "def stratified_cross_validation(X, y, n_folds, num_trees, max_depth, example_subsample_rate, attr_subsample_rate):\n",
    "    fold_size = len(X) // n_folds\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "\n",
    "    for i in range(n_folds):\n",
    "        start = i * fold_size\n",
    "        end = (i + 1) * fold_size\n",
    "\n",
    "        X_train_fold = pd.concat([X[:start], X[end:]])\n",
    "        y_train_fold = pd.concat([y[:start], y[end:]])\n",
    "\n",
    "        X_validation_fold = X[start:end]\n",
    "        y_validation_fold = y[start:end]\n",
    "\n",
    "        rf = RandomForestClassifier(num_trees, max_depth, example_subsample_rate, attr_subsample_rate)\n",
    "        trees, subsampled_attributes = rf.fit_random_forest(X_train_fold, y_train_fold)\n",
    "        predictions = rf.classify_random_forest(trees, subsampled_attributes, X_validation_fold)\n",
    "\n",
    "        y_validation_fold_list = y_validation_fold.tolist()\n",
    "        conf_matrix = EvaluationMetrics.confusion_matrix(y_validation_fold_list, predictions)\n",
    "        acc, prec, rec, f1 = EvaluationMetrics.calculate_metrics(conf_matrix)\n",
    "\n",
    "        accuracies.append(acc)\n",
    "        precisions.append(prec)\n",
    "        recalls.append(rec)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    mean_accuracy = np.mean(accuracies)\n",
    "    mean_precision = np.mean([np.mean(precision, axis=0) for precision in precisions], axis=0)\n",
    "    mean_recall = np.mean([np.mean(recall, axis=0) for recall in recalls], axis=0)\n",
    "    mean_f1_score = np.nanmean([np.nanmean(f1_score, axis=0) for f1_score in f1_scores], axis=0)\n",
    "\n",
    "    return mean_accuracy, mean_precision, mean_recall, mean_f1_score\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load dataset\n",
    "    data_file = \"/Users/noshitha/Downloads/contraceptive+method+choice/cmc.data\"\n",
    "\n",
    "    column_names = [\n",
    "        \"Wife_age\", \"Wife_education\", \"Husband_education\", \"Number_of_children_ever_born\",\n",
    "        \"Wife_religion\", \"Wife_working\", \"Husband_occupation\", \"Standard-of-living_index\",\n",
    "        \"Media_exposure\", \"Contraceptive_method_used\"\n",
    "    ]\n",
    "\n",
    "    data = pd.read_csv(data_file, names=column_names)\n",
    "\n",
    "    # Convert \"Wife_age\" and \"Number_of_children_ever_born\" to object type\n",
    "    data[\"Wife_age\"] = data[\"Wife_age\"].astype(object)\n",
    "    data[\"Number_of_children_ever_born\"] = data[\"Number_of_children_ever_born\"].astype(object)\n",
    "\n",
    "    # Split features and target variable\n",
    "    X = data.drop(\"Contraceptive_method_used\", axis=1)\n",
    "    y = data[\"Contraceptive_method_used\"]\n",
    "\n",
    "    n_trees_list = [1, 5, 10, 20, 30, 40, 50]\n",
    "    n_folds = 10\n",
    "    max_depth = 3\n",
    "    example_subsample_rate = 0.5\n",
    "    attr_subsample_rate = 0.5\n",
    "\n",
    "    for num_trees in n_trees_list:\n",
    "        print(\"num_trees: \", num_trees)\n",
    "        accuracies, precisions, recalls, f1_scores = stratified_cross_validation(X, y, n_folds, num_trees, max_depth, example_subsample_rate, attr_subsample_rate)\n",
    "        print(\"Accuracies:\", accuracies)\n",
    "        print(\"Precisions:\", precisions)\n",
    "        print(\"Recalls:\", recalls)\n",
    "        print(\"F1-scores:\", f1_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53421ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "13741e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_trees:  1\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/bm/l2zpyjxn2gqg_1d20xgx5z0w0000gn/T/ipykernel_38370/2568637676.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnum_trees\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mn_trees_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"num_trees: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_trees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m         \u001b[0maccuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecisions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecalls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstratified_cross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_folds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_trees\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_subsample_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_subsample_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracies:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Precisions:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecisions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/bm/l2zpyjxn2gqg_1d20xgx5z0w0000gn/T/ipykernel_38370/2568637676.py\u001b[0m in \u001b[0;36mstratified_cross_validation\u001b[0;34m(X, y, n_folds, num_trees, max_depth, example_subsample_rate, attr_subsample_rate)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mrf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_trees\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_subsample_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_subsample_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m         \u001b[0mtrees\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubsampled_attributes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_random_forest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_fold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_fold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify_random_forest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubsampled_attributes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_validation_fold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/bm/l2zpyjxn2gqg_1d20xgx5z0w0000gn/T/ipykernel_38370/2568637676.py\u001b[0m in \u001b[0;36mfit_random_forest\u001b[0;34m(self, X_train, y_train)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit_random_forest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0mtrees\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubsampled_attributes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrees\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubsampled_attributes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/bm/l2zpyjxn2gqg_1d20xgx5z0w0000gn/T/ipykernel_38370/2568637676.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_train, y_train)\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0msubsampled_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbootstrapped_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubsampled_attr_indexes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0mdt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubsampled_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbootstrapped_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m             \u001b[0mtrees\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/bm/l2zpyjxn2gqg_1d20xgx5z0w0000gn/T/ipykernel_38370/2568637676.py\u001b[0m in \u001b[0;36mdecision_tree\u001b[0;34m(self, X_train, y_train, current_depth)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munique_values\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0mpartition_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbest_attr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                 \u001b[0mnode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"leaf\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpartition_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpartition_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_depth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mattr_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbest_attr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/bm/l2zpyjxn2gqg_1d20xgx5z0w0000gn/T/ipykernel_38370/2568637676.py\u001b[0m in \u001b[0;36mdecision_tree\u001b[0;34m(self, X_train, y_train, current_depth)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mnode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"split_value\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_mean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mnode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"left\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpartition_indices_left\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpartition_indices_left\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_depth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0mnode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"right\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpartition_indices_right\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpartition_indices_right\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_depth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"leaf\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/bm/l2zpyjxn2gqg_1d20xgx5z0w0000gn/T/ipykernel_38370/2568637676.py\u001b[0m in \u001b[0;36mdecision_tree\u001b[0;34m(self, X_train, y_train, current_depth)\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0mpartition_entropy_left\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpartition_indices_left\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0mpartition_entropy_right\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpartition_indices_right\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                 \u001b[0mweight_left\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartition_indices_left\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m                 \u001b[0mweight_right\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartition_indices_right\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0minfo_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight_left\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpartition_entropy_left\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mweight_right\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpartition_entropy_right\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "class DecisionTreeClassifier:\n",
    "    def __init__(self, max_depth):\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def entropy(self, labels):\n",
    "        unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "        probabilities = counts / len(labels)\n",
    "        entropy_value = -np.sum(probabilities * np.log2(probabilities))\n",
    "        return entropy_value\n",
    "\n",
    "    def information_gain(self, y, x):\n",
    "        parent_entropy = self.entropy(y)\n",
    "        info_a = 0\n",
    "        for value in set(x):\n",
    "            partition_indices = x[x == value].index\n",
    "            partition_entropy = self.entropy(y[partition_indices])\n",
    "            info_a += len(partition_indices) / len(x) * partition_entropy\n",
    "        gain_a = parent_entropy - info_a\n",
    "        return gain_a\n",
    "\n",
    "    def decision_tree(self, X_train, y_train, current_depth=0):\n",
    "        if len(set(y_train)) == 1 or current_depth == self.max_depth or len(X_train.columns) == 0:\n",
    "            class_counts = Counter(y_train)\n",
    "            majority_class = class_counts.most_common(1)[0][0] if class_counts else None\n",
    "            return {\"class_label\": majority_class}\n",
    "\n",
    "        gains = {}\n",
    "        for attr in X_train.columns:\n",
    "            if X_train[attr].dtype == 'object':\n",
    "                gains[attr] = self.information_gain(y_train, X_train[attr])\n",
    "            else:\n",
    "                attr_mean = X_train[attr].mean()\n",
    "                partition_indices_left = X_train[X_train[attr] <= attr_mean].index\n",
    "                partition_indices_right = X_train[X_train[attr] > attr_mean].index\n",
    "                partition_entropy_left = self.entropy(y_train[partition_indices_left])\n",
    "                partition_entropy_right = self.entropy(y_train[partition_indices_right])\n",
    "                weight_left = len(partition_indices_left) / len(X_train)\n",
    "                weight_right = len(partition_indices_right) / len(X_train)\n",
    "                info_a = weight_left * partition_entropy_left + weight_right * partition_entropy_right\n",
    "                gains[attr] = self.entropy(y_train) - info_a\n",
    "\n",
    "        best_attr = max(gains, key=gains.get)\n",
    "        node = {\"attribute\": best_attr}\n",
    "\n",
    "        if X_train[best_attr].dtype == 'object':\n",
    "            unique_values = X_train[best_attr].unique()\n",
    "            node[\"leaf\"] = {}\n",
    "            for value in unique_values:\n",
    "                partition_indices = X_train[X_train[best_attr] == value].index\n",
    "                node[\"leaf\"][value] = self.decision_tree(X_train.loc[partition_indices], y_train.loc[partition_indices], current_depth + 1)\n",
    "        else:\n",
    "            attr_mean = X_train[best_attr].mean()\n",
    "            partition_indices_left = X_train[X_train[best_attr] <= attr_mean].index\n",
    "            partition_indices_right = X_train[X_train[best_attr] > attr_mean].index\n",
    "            node[\"split_value\"] = attr_mean\n",
    "            node[\"left\"] = self.decision_tree(X_train.loc[partition_indices_left], y_train.loc[partition_indices_left], current_depth + 1)\n",
    "            node[\"right\"] = self.decision_tree(X_train.loc[partition_indices_right], y_train.loc[partition_indices_right], current_depth + 1)\n",
    "\n",
    "        if \"leaf\" not in node:\n",
    "            class_counts = Counter(y_train)\n",
    "            majority_class = class_counts.most_common(1)[0][0]\n",
    "            node[\"class_label\"] = majority_class\n",
    "\n",
    "        return node\n",
    "    \n",
    "    def classify_mixed(self, tree, features):\n",
    "        class_labels = []\n",
    "\n",
    "        if isinstance(features, pd.Series):\n",
    "            node = tree\n",
    "            while \"class_label\" not in node:\n",
    "                if 'split_value' not in node:  # Check if the node is a leaf node\n",
    "                    class_labels.append(node.get('class_label', None))  # Return None if no class label found\n",
    "                    break\n",
    "\n",
    "                attr_value = features[node['attribute']]\n",
    "                if isinstance(attr_value, (int, float)):\n",
    "                    if attr_value <= node['split_value']:\n",
    "                        node = node['left']\n",
    "                    else:\n",
    "                        node = node['right']\n",
    "                else:\n",
    "                    # Handle categorical attributes\n",
    "                    node = node['leaf'].get(attr_value, None)  # Return None if no leaf node found for the attribute value\n",
    "\n",
    "        else:\n",
    "            for _, feature in features.iterrows():\n",
    "                node = tree\n",
    "                while \"class_label\" not in node:\n",
    "                    if 'split_value' not in node:  # Check if the node is a leaf node\n",
    "                        class_labels.append(node.get('class_label', None))  # Return None if no class label found\n",
    "                        break\n",
    "\n",
    "                    attr_value = feature[node['attribute']]\n",
    "                    if isinstance(attr_value, (int, float)):\n",
    "                        if attr_value <= node['split_value']:\n",
    "                            node = node['left']\n",
    "                        else:\n",
    "                            node = node['right']\n",
    "                    else:\n",
    "                        # Handle categorical attributes\n",
    "                        node = node['leaf'].get(attr_value, None)  # Return None if no leaf node found for the attribute value\n",
    "\n",
    "        return class_labels\n",
    "\n",
    "\n",
    "class RandomForestClassifier:\n",
    "    def __init__(self, num_trees, max_depth, example_subsample_rate, attr_subsample_rate):\n",
    "        self.num_trees = num_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.example_subsample_rate = example_subsample_rate\n",
    "        self.attr_subsample_rate = attr_subsample_rate\n",
    "\n",
    "    def bootstrap_sampling(self, X, y):\n",
    "        indices = np.random.choice(len(X), size=len(X), replace=True)\n",
    "        return X.iloc[indices], y.iloc[indices]\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        trees = []\n",
    "        subsampled_attributes = []\n",
    "\n",
    "        for i in range(self.num_trees):\n",
    "            bootstrapped_X, bootstrapped_y = self.bootstrap_sampling(X_train, y_train)\n",
    "            subsampled_attr_indexes = np.random.choice(range(X_train.shape[1]), int(X_train.shape[1] * self.attr_subsample_rate), replace=False)\n",
    "            subsampled_attributes.append(subsampled_attr_indexes.tolist())\n",
    "            subsampled_X = bootstrapped_X.iloc[:, subsampled_attr_indexes]\n",
    "            dt = DecisionTreeClassifier(self.max_depth)\n",
    "            tree = dt.decision_tree(subsampled_X, bootstrapped_y)\n",
    "            trees.append(tree)\n",
    "\n",
    "        return trees, subsampled_attributes\n",
    "    \n",
    "    def classify_random_forest(self, trees, subsampled_attributes, X_test):\n",
    "        class_labels = []\n",
    "        dt = DecisionTreeClassifier(self.max_depth)  # Create instance for decision tree classification\n",
    "        for i, test_row in X_test.iterrows():\n",
    "            tree_votes = []\n",
    "            for tree, sub_attributes in zip(trees, subsampled_attributes):\n",
    "                test_features = test_row[sub_attributes]\n",
    "                predicted_label = dt.classify_mixed(tree, test_features)\n",
    "                if predicted_label[0] is not None:  # Check if the prediction is not None\n",
    "                    tree_votes.append(predicted_label[0])\n",
    "            if tree_votes:  # Check if there are any votes\n",
    "                class_labels.append(max(set(tree_votes), key=tree_votes.count))\n",
    "            else:\n",
    "                # If all predictions are None, assign a default label (for example, the majority class)\n",
    "                class_labels.append(default_label)  # Replace default_label with the desired default value\n",
    "        return class_labels\n",
    "\n",
    "    def fit_random_forest(self, X_train, y_train):\n",
    "        trees, subsampled_attributes = self.fit(X_train, y_train)\n",
    "        return trees, subsampled_attributes\n",
    "\n",
    "\n",
    "class EvaluationMetrics:\n",
    "    @staticmethod\n",
    "    def confusion_matrix(y_true, y_pred):\n",
    "        classes = np.unique(np.concatenate([y_true, y_pred]))\n",
    "        n_classes = len(classes)\n",
    "        conf_matrix = np.zeros((n_classes, n_classes), dtype=int)\n",
    "\n",
    "        for i, true_label in enumerate(classes):\n",
    "            for j, pred_label in enumerate(classes):\n",
    "                conf_matrix[i, j] = np.sum((y_true == true_label) & (y_pred == pred_label))\n",
    "\n",
    "        return conf_matrix\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_metrics(conf_matrix):\n",
    "        TP = np.diag(conf_matrix)\n",
    "        FP = np.sum(conf_matrix, axis=0) - TP\n",
    "        FN = np.sum(conf_matrix, axis=1) - TP\n",
    "        TN = np.sum(conf_matrix) - (TP + FP + FN)\n",
    "\n",
    "        accuracy = np.sum(TP) / np.sum(conf_matrix)\n",
    "\n",
    "        precision = np.where(TP + FP == 0, 0, TP / (TP + FP))\n",
    "        recall = np.where(TP + FN == 0, 0, TP / (TP + FN))\n",
    "\n",
    "        f1_score = np.zeros_like(precision)\n",
    "        non_zero_denominator = (precision + recall) != 0\n",
    "        f1_score[non_zero_denominator] = 2 * (precision[non_zero_denominator] * recall[non_zero_denominator]) / (precision[non_zero_denominator] + recall[non_zero_denominator])\n",
    "\n",
    "        return accuracy, precision, recall, f1_score\n",
    "\n",
    "\n",
    "def stratified_cross_validation(X, y, n_folds, num_trees, max_depth, example_subsample_rate, attr_subsample_rate):\n",
    "    fold_size = len(X) // n_folds\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "\n",
    "    for i in range(n_folds):\n",
    "        start = i * fold_size\n",
    "        end = (i + 1) * fold_size\n",
    "\n",
    "        X_train_fold = pd.concat([X[:start], X[end:]])\n",
    "        y_train_fold = pd.concat([y[:start], y[end:]])\n",
    "\n",
    "        X_validation_fold = X[start:end]\n",
    "        y_validation_fold = y[start:end]\n",
    "\n",
    "        rf = RandomForestClassifier(num_trees, max_depth, example_subsample_rate, attr_subsample_rate)\n",
    "        trees, subsampled_attributes = rf.fit_random_forest(X_train_fold, y_train_fold)\n",
    "        predictions = rf.classify_random_forest(trees, subsampled_attributes, X_validation_fold)\n",
    "\n",
    "        y_validation_fold_list = y_validation_fold.tolist()\n",
    "        conf_matrix = EvaluationMetrics.confusion_matrix(y_validation_fold_list, predictions)\n",
    "        acc, prec, rec, f1 = EvaluationMetrics.calculate_metrics(conf_matrix)\n",
    "\n",
    "        accuracies.append(acc)\n",
    "        precisions.append(prec)\n",
    "        recalls.append(rec)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    mean_accuracy = np.mean(accuracies)\n",
    "    mean_precision = np.mean([np.mean(precision, axis=0) for precision in precisions], axis=0)\n",
    "    mean_recall = np.mean([np.mean(recall, axis=0) for recall in recalls], axis=0)\n",
    "    mean_f1_score = np.nanmean([np.nanmean(f1_score, axis=0) for f1_score in f1_scores], axis=0)\n",
    "\n",
    "    return mean_accuracy, mean_precision, mean_recall, mean_f1_score\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load dataset\n",
    "    data_file = \"/Users/noshitha/Downloads/contraceptive+method+choice/cmc.data\"\n",
    "\n",
    "    column_names = [\n",
    "        \"Wife_age\", \"Wife_education\", \"Husband_education\", \"Number_of_children_ever_born\",\n",
    "        \"Wife_religion\", \"Wife_working\", \"Husband_occupation\", \"Standard-of-living_index\",\n",
    "        \"Media_exposure\", \"Contraceptive_method_used\"\n",
    "    ]\n",
    "\n",
    "    data = pd.read_csv(data_file, names=column_names)\n",
    "\n",
    "    # Convert categorical to object type\n",
    "    data[\"Wife_education\"] = data[\"Wife_education\"].astype(object)\n",
    "    data[\"Husband_education\"] = data[\"Husband_education\"].astype(object)\n",
    "    data[\"Wife_religion\"] = data[\"Wife_religion\"].astype(object)\n",
    "    data[\"Wife_working\"] = data[\"Wife_working\"].astype(object)\n",
    "    data[\"Standard-of-living_index\"] = data[\"Standard-of-living_index\"].astype(object)\n",
    "    data[\"Media_exposure\"] = data[\"Media_exposure\"].astype(object)\n",
    "    data[\"Contraceptive_method_used\"] = data[\"Contraceptive_method_used\"].astype(object)\n",
    "    \n",
    "    \n",
    "\n",
    "    # Split features and target variable\n",
    "    X = data.drop(\"Contraceptive_method_used\", axis=1)\n",
    "    y = data[\"Contraceptive_method_used\"]\n",
    "\n",
    "    n_trees_list = [1, 5, 10, 20, 30, 40, 50]\n",
    "    n_folds = 10\n",
    "    max_depth = 3\n",
    "    example_subsample_rate = 0.5\n",
    "    attr_subsample_rate = 0.5\n",
    "\n",
    "    for num_trees in n_trees_list:\n",
    "        print(\"num_trees: \", num_trees)\n",
    "        accuracies, precisions, recalls, f1_scores = stratified_cross_validation(X, y, n_folds, num_trees, max_depth, example_subsample_rate, attr_subsample_rate)\n",
    "        print(\"Accuracies:\", accuracies)\n",
    "        print(\"Precisions:\", precisions)\n",
    "        print(\"Recalls:\", recalls)\n",
    "        print(\"F1-scores:\", f1_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24271203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wife_age                        object\n",
       "Wife_education                   int64\n",
       "Husband_education                int64\n",
       "Number_of_children_ever_born    object\n",
       "Wife_religion                    int64\n",
       "Wife_working                     int64\n",
       "Husband_occupation               int64\n",
       "Standard-of-living_index         int64\n",
       "Media_exposure                   int64\n",
       "Contraceptive_method_used        int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ede151",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "597d6dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_trees:  1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "max() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/bm/l2zpyjxn2gqg_1d20xgx5z0w0000gn/T/ipykernel_1092/1173636328.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnum_trees\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mn_trees_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"num_trees: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_trees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m         \u001b[0maccuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecisions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecalls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstratified_cross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_folds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_trees\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_subsample_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_subsample_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracies:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Precisions:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecisions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/bm/l2zpyjxn2gqg_1d20xgx5z0w0000gn/T/ipykernel_1092/1173636328.py\u001b[0m in \u001b[0;36mstratified_cross_validation\u001b[0;34m(X, y, n_folds, num_trees, max_depth, example_subsample_rate, attr_subsample_rate)\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0mrf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_trees\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_subsample_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_subsample_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mtrees\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubsampled_attributes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_random_forest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_fold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_fold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify_random_forest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubsampled_attributes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_validation_fold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0my_validation_fold_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_validation_fold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/bm/l2zpyjxn2gqg_1d20xgx5z0w0000gn/T/ipykernel_1092/1173636328.py\u001b[0m in \u001b[0;36mclassify_random_forest\u001b[0;34m(self, trees, subsampled_attributes, X_test)\u001b[0m\n\u001b[1;32m    126\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                     \u001b[0mtest_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_row\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msub_attributes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                 \u001b[0mpredicted_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_features\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Use decision tree classify method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m                 \u001b[0mtree_votes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mclass_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree_votes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtree_votes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/bm/l2zpyjxn2gqg_1d20xgx5z0w0000gn/T/ipykernel_1092/1173636328.py\u001b[0m in \u001b[0;36mclassify\u001b[0;34m(self, tree, features)\u001b[0m\n\u001b[1;32m     84\u001b[0m                         \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'leaf'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attribute'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                         \u001b[0mclass_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"leaf\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Return majority class label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: max() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "class DecisionTreeClassifier:\n",
    "    def __init__(self, max_depth):\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def entropy(self, labels):\n",
    "        unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "        probabilities = counts / len(labels)\n",
    "        entropy_value = -np.sum(probabilities * np.log2(probabilities))\n",
    "        return entropy_value\n",
    "\n",
    "    def information_gain(self, y, x):\n",
    "        parent_entropy = self.entropy(y)\n",
    "        info_a = 0\n",
    "        \n",
    "        if x.dtype == 'object':  # Check if the attribute is categorical\n",
    "            for value in set(x):\n",
    "                partition_indices = x[x == value].index\n",
    "                partition_entropy = self.entropy(y[partition_indices])\n",
    "                info_a += len(partition_indices) / len(x) * partition_entropy\n",
    "        else:  # If the attribute is numerical\n",
    "            # You can use the same numerical information gain calculation as before\n",
    "            attr_mean = x.mean()\n",
    "            partition_indices_left = x[x <= attr_mean].index\n",
    "            partition_indices_right = x[x > attr_mean].index\n",
    "            partition_entropy_left = self.entropy(y[partition_indices_left])\n",
    "            partition_entropy_right = self.entropy(y[partition_indices_right])\n",
    "            weight_left = len(partition_indices_left) / len(x)\n",
    "            weight_right = len(partition_indices_right) / len(x)\n",
    "            info_a = weight_left * partition_entropy_left + weight_right * partition_entropy_right\n",
    "\n",
    "        gain_a = parent_entropy - info_a\n",
    "        return gain_a\n",
    "\n",
    "    def decision_tree(self, X_train, y_train, current_depth=0):\n",
    "        if len(set(y_train)) == 1 or current_depth == self.max_depth or len(X_train.columns) == 0:\n",
    "            class_counts = Counter(y_train)\n",
    "            majority_class = class_counts.most_common(1)[0][0]\n",
    "            return {\"class_label\": majority_class}\n",
    "\n",
    "        gains = {}\n",
    "        for attr in X_train.columns:\n",
    "            gains[attr] = self.information_gain(y_train, X_train[attr])\n",
    "\n",
    "        best_attr = max(gains, key=gains.get)\n",
    "        node = {\"attribute\": best_attr, \"leaf\": {}}\n",
    "\n",
    "        if X_train[best_attr].dtype == 'object':\n",
    "            unique_values = X_train[best_attr].unique()\n",
    "            for value in unique_values:\n",
    "                partition_indices = X_train[X_train[best_attr] == value].index\n",
    "                node[\"leaf\"][value] = self.decision_tree(X_train.loc[partition_indices], y_train.loc[partition_indices], current_depth + 1)\n",
    "        else:\n",
    "            attr_mean = X_train[best_attr].mean()\n",
    "            partition_indices_left = X_train[X_train[best_attr] <= attr_mean].index\n",
    "            partition_indices_right = X_train[X_train[best_attr] > attr_mean].index\n",
    "            node[\"split_value\"] = attr_mean\n",
    "            node[\"left\"] = self.decision_tree(X_train.loc[partition_indices_left], y_train.loc[partition_indices_left], current_depth + 1)\n",
    "            node[\"right\"] = self.decision_tree(X_train.loc[partition_indices_right], y_train.loc[partition_indices_right], current_depth + 1)\n",
    "\n",
    "        return node\n",
    "\n",
    "    def classify(self, tree, features):\n",
    "        class_labels = []\n",
    "        if isinstance(features, pd.Series):\n",
    "            node = tree\n",
    "            while \"class_label\" not in node:\n",
    "                if features[node['attribute']] in node['leaf']:\n",
    "                    node = node['leaf'][features[node['attribute']]]\n",
    "                else:\n",
    "                    class_labels.append(max(node[\"leaf\"].items(), key=lambda x: len(x[1]))[0])  # Return majority class label\n",
    "                    break\n",
    "            else:\n",
    "                class_labels.append(node['class_label'])\n",
    "        else:\n",
    "            for _, feature in features.iterrows():\n",
    "                node = tree\n",
    "                while \"class_label\" not in node:\n",
    "                    if feature[node['attribute']] in node['leaf']:\n",
    "                        node = node['leaf'][feature[node['attribute']]]\n",
    "                    else:\n",
    "                        class_labels.append(max(node[\"leaf\"].items(), key=lambda x: len(x[1]))[0])  # Return majority class label\n",
    "                        break\n",
    "                else:\n",
    "                    class_labels.append(node['class_label'])\n",
    "        return class_labels\n",
    "    \n",
    "class RandomForestClassifier:\n",
    "    def __init__(self, num_trees, max_depth, example_subsample_rate, attr_subsample_rate):\n",
    "        self.num_trees = num_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.example_subsample_rate = example_subsample_rate\n",
    "        self.attr_subsample_rate = attr_subsample_rate\n",
    "\n",
    "    def bootstrap_sampling(self, X, y):\n",
    "        indices = np.random.choice(len(X), size=len(X), replace=True)\n",
    "        return X.iloc[indices], y.iloc[indices]\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        trees = []\n",
    "        subsampled_attributes = []\n",
    "\n",
    "        for i in range(self.num_trees):\n",
    "            bootstrapped_X, bootstrapped_y = self.bootstrap_sampling(X_train, y_train)\n",
    "            subsampled_attr_indexes = np.random.choice(range(X_train.shape[1]), int(X_train.shape[1] * self.attr_subsample_rate), replace=False)\n",
    "            subsampled_attributes.append(subsampled_attr_indexes.tolist())\n",
    "            subsampled_X = bootstrapped_X.iloc[:, subsampled_attr_indexes]\n",
    "            dt = DecisionTreeClassifier(self.max_depth)\n",
    "            tree = dt.decision_tree(subsampled_X, bootstrapped_y)\n",
    "            trees.append(tree)\n",
    "\n",
    "        return trees, subsampled_attributes\n",
    "\n",
    "    def classify_random_forest(self, trees, subsampled_attributes, X_test):\n",
    "        class_labels = []\n",
    "        dt = DecisionTreeClassifier(self.max_depth)  # Create instance for decision tree classification\n",
    "        for _, test_row in X_test.iterrows():\n",
    "            tree_votes = []\n",
    "            for tree, sub_attributes in zip(trees, subsampled_attributes):\n",
    "                if isinstance(test_row, pd.Series):\n",
    "                    test_features = pd.DataFrame([test_row[sub_attributes]])  # Ensure DataFrame format\n",
    "                else:\n",
    "                    test_features = test_row[sub_attributes]\n",
    "                predicted_label = dt.classify(tree, test_features)  # Use decision tree classify method\n",
    "                tree_votes.append(predicted_label[0])\n",
    "            class_labels.append(max(set(tree_votes), key=tree_votes.count))\n",
    "        return class_labels\n",
    "\n",
    "    def fit_random_forest(self, X_train, y_train):\n",
    "        trees, subsampled_attributes = self.fit(X_train, y_train)\n",
    "        return trees, subsampled_attributes\n",
    "\n",
    "\n",
    "class EvaluationMetrics:\n",
    "    @staticmethod\n",
    "    def confusion_matrix(y_true, y_pred):\n",
    "        classes = np.unique(np.concatenate([y_true, y_pred]))\n",
    "        n_classes = len(classes)\n",
    "        conf_matrix = np.zeros((n_classes, n_classes), dtype=int)\n",
    "\n",
    "        for i, true_label in enumerate(classes):\n",
    "            for j, pred_label in enumerate(classes):\n",
    "                conf_matrix[i, j] = np.sum((y_true == true_label) & (y_pred == pred_label))\n",
    "\n",
    "        return conf_matrix\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_metrics(conf_matrix):\n",
    "        TP = np.diag(conf_matrix)\n",
    "        FP = np.sum(conf_matrix, axis=0) - TP\n",
    "        FN = np.sum(conf_matrix, axis=1) - TP\n",
    "        TN = np.sum(conf_matrix) - (TP + FP + FN)\n",
    "\n",
    "        accuracy = np.sum(TP) / np.sum(conf_matrix)\n",
    "\n",
    "        precision = np.where(TP + FP == 0, 0, TP / (TP + FP))\n",
    "        recall = np.where(TP + FN == 0, 0, TP / (TP + FN))\n",
    "\n",
    "        f1_score = np.zeros_like(precision)\n",
    "        non_zero_denominator = (precision + recall) != 0\n",
    "        f1_score[non_zero_denominator] = 2 * (precision[non_zero_denominator] * recall[non_zero_denominator]) / (precision[non_zero_denominator] + recall[non_zero_denominator])\n",
    "\n",
    "        return accuracy, precision, recall, f1_score\n",
    "\n",
    "\n",
    "def stratified_cross_validation(X, y, n_folds, num_trees, max_depth, example_subsample_rate, attr_subsample_rate):\n",
    "    fold_size = len(X) // n_folds\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "\n",
    "    for i in range(n_folds):\n",
    "        start = i * fold_size\n",
    "        end = (i + 1) * fold_size\n",
    "\n",
    "        X_train_fold = pd.concat([X[:start], X[end:]])\n",
    "        y_train_fold = pd.concat([y[:start], y[end:]])\n",
    "\n",
    "        X_validation_fold = X[start:end]\n",
    "        y_validation_fold = y[start:end]\n",
    "\n",
    "        rf = RandomForestClassifier(num_trees, max_depth, example_subsample_rate, attr_subsample_rate)\n",
    "        trees, subsampled_attributes = rf.fit_random_forest(X_train_fold, y_train_fold)\n",
    "        predictions = rf.classify_random_forest(trees, subsampled_attributes, X_validation_fold)\n",
    "\n",
    "        y_validation_fold_list = y_validation_fold.tolist()\n",
    "        conf_matrix = EvaluationMetrics.confusion_matrix(y_validation_fold_list, predictions)\n",
    "        acc, prec, rec, f1 = EvaluationMetrics.calculate_metrics(conf_matrix)\n",
    "\n",
    "        accuracies.append(acc)\n",
    "        precisions.append(prec)\n",
    "        recalls.append(rec)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    mean_accuracy = np.mean(accuracies)\n",
    "    mean_precision = np.mean([np.mean(precision, axis=0) for precision in precisions], axis=0)\n",
    "    mean_recall = np.mean([np.mean(recall, axis=0) for recall in recalls], axis=0)\n",
    "    mean_f1_score = np.nanmean([np.nanmean(f1_score, axis=0) for f1_score in f1_scores], axis=0)\n",
    "\n",
    "    return mean_accuracy, mean_precision, mean_recall, mean_f1_score\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load dataset\n",
    "    data_file = \"/Users/noshitha/Downloads/contraceptive+method+choice/cmc.data\"\n",
    "\n",
    "    column_names = [\n",
    "        \"Wife_age\", \"Wife_education\", \"Husband_education\", \"Number_of_children_ever_born\",\n",
    "        \"Wife_religion\", \"Wife_working\", \"Husband_occupation\", \"Standard-of-living_index\",\n",
    "        \"Media_exposure\", \"Contraceptive_method_used\"\n",
    "    ]\n",
    "\n",
    "    data = pd.read_csv(data_file, names=column_names)\n",
    "\n",
    "    # Convert categorical to object type\n",
    "    data[\"Wife_education\"] = data[\"Wife_education\"].astype(object)\n",
    "    data[\"Husband_education\"] = data[\"Husband_education\"].astype(object)\n",
    "    data[\"Wife_religion\"] = data[\"Wife_religion\"].astype(object)\n",
    "    data[\"Wife_working\"] = data[\"Wife_working\"].astype(object)\n",
    "    data[\"Standard-of-living_index\"] = data[\"Standard-of-living_index\"].astype(object)\n",
    "    data[\"Media_exposure\"] = data[\"Media_exposure\"].astype(object)\n",
    "    data[\"Contraceptive_method_used\"] = data[\"Contraceptive_method_used\"].astype(object)\n",
    "    \n",
    "    \n",
    "\n",
    "    # Split features and target variable\n",
    "    X = data.drop(\"Contraceptive_method_used\", axis=1)\n",
    "    y = data[\"Contraceptive_method_used\"]\n",
    "\n",
    "    n_trees_list = [1]\n",
    "    n_folds = 10\n",
    "    max_depth = 3\n",
    "    example_subsample_rate = 0.5\n",
    "    attr_subsample_rate = 0.5\n",
    "\n",
    "    for num_trees in n_trees_list:\n",
    "        print(\"num_trees: \", num_trees)\n",
    "        accuracies, precisions, recalls, f1_scores = stratified_cross_validation(X, y, n_folds, num_trees, max_depth, example_subsample_rate, attr_subsample_rate)\n",
    "        print(\"Accuracies:\", accuracies)\n",
    "        print(\"Precisions:\", precisions)\n",
    "        print(\"Recalls:\", recalls)\n",
    "        print(\"F1-scores:\", f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0ab790",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
