{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208f31da",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_trees:  1\n",
      "Accuracies: 0.913953488372093\n",
      "Precisions: 0.9106066448974601\n",
      "Recalls: 0.9123723257904294\n",
      "F1-scores: 0.907530660390534\n",
      "num_trees:  5\n",
      "Accuracies: 0.9418604651162792\n",
      "Precisions: 0.9384663802753442\n",
      "Recalls: 0.9417481388343457\n",
      "F1-scores: 0.9374046531286119\n",
      "num_trees:  10\n",
      "Accuracies: 0.9325581395348838\n",
      "Precisions: 0.9254299594977343\n",
      "Recalls: 0.9263514836151753\n",
      "F1-scores: 0.9250004827560533\n",
      "num_trees:  20\n",
      "Accuracies: 0.9534883720930232\n",
      "Precisions: 0.9483449614204764\n",
      "Recalls: 0.9539370203551238\n",
      "F1-scores: 0.9500478599837099\n",
      "num_trees:  30\n",
      "Accuracies: 0.9511627906976743\n",
      "Precisions: 0.9469935167507761\n",
      "Recalls: 0.9510245624426659\n",
      "F1-scores: 0.9473115935260925\n",
      "num_trees:  40\n",
      "Accuracies: 0.944186046511628\n",
      "Precisions: 0.9406863979436574\n",
      "Recalls: 0.9436680809611844\n",
      "F1-scores: 0.9401828496981223\n",
      "num_trees:  50\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def entropy(labels):\n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "    probabilities = counts / len(labels)\n",
    "    entropy_value = -np.sum(probabilities * np.log2(probabilities))\n",
    "    return entropy_value\n",
    "\n",
    "def information_gain(y, x):\n",
    "    parent_entropy = entropy(y)\n",
    "    info_a = 0\n",
    "    for value in set(x):\n",
    "        partition_indices = x[x == value].index\n",
    "        partition_entropy = entropy(y[partition_indices])\n",
    "        info_a += len(partition_indices) / len(x) * partition_entropy\n",
    "    gain_a = parent_entropy - info_a\n",
    "    return gain_a\n",
    "\n",
    "def decision_tree(X_train, y_train, max_depth, current_depth=0):\n",
    "    if len(set(y_train)) == 1 or current_depth == max_depth or len(X_train.columns) == 0:\n",
    "        class_counts = Counter(y_train)\n",
    "        majority_class = class_counts.most_common(1)[0][0]\n",
    "        return {\"class_label\": majority_class}\n",
    "    \n",
    "    gains = {attr: information_gain(y_train, X_train[attr]) for attr in X_train.columns}\n",
    "    best_attr = max(gains, key=gains.get)\n",
    "    node = {\"attribute\": best_attr, \"leaf\": {}}\n",
    "    unique_values = X_train[best_attr].unique()\n",
    "    for value in unique_values:\n",
    "        partition_indices = X_train[X_train[best_attr] == value].index\n",
    "        node[\"leaf\"][value] = decision_tree(X_train.loc[partition_indices], y_train.loc[partition_indices], max_depth, current_depth + 1)\n",
    "    return node\n",
    "\n",
    "def classify_random_forest(trees, subsampled_attributes, X_test):\n",
    "    class_labels = []\n",
    "    for _, test_row in X_test.iterrows():\n",
    "        tree_votes = []\n",
    "        for tree, sub_attributes in zip(trees, subsampled_attributes):\n",
    "            test_features = pd.DataFrame(test_row[sub_attributes]).T\n",
    "            predicted_label = classify(tree, test_features)\n",
    "            tree_votes.append(predicted_label[0])  # Append predicted label\n",
    "        class_labels.append(max(set(tree_votes), key=tree_votes.count))  # Perform majority voting\n",
    "    return class_labels\n",
    "\n",
    "def classify(tree, features):\n",
    "    class_labels = []\n",
    "    for _, feature in features.iterrows():\n",
    "        node = tree\n",
    "        while \"class_label\" not in node:\n",
    "            split_attr = node[\"attribute\"]\n",
    "            feature_value = feature[split_attr]\n",
    "            if feature_value in node[\"leaf\"]:\n",
    "                node = node[\"leaf\"][feature_value]\n",
    "            else:\n",
    "                class_labels.append(max(node[\"leaf\"].items(), key=lambda x: len(x[1]))[0])\n",
    "                break\n",
    "        else:\n",
    "            class_labels.append(node[\"class_label\"])\n",
    "    return class_labels\n",
    "\n",
    "def bootstrap_sampling(X, y):\n",
    "    indices = np.random.choice(len(X), size=len(X), replace=True)\n",
    "    return X.iloc[indices], y.iloc[indices]\n",
    "\n",
    "def fit_random_forest(num_trees, max_depth, example_subsample_rate, attr_subsample_rate, X_train, y_train):\n",
    "    trees = []\n",
    "    subsampled_attributes = []\n",
    "\n",
    "    for i in range(num_trees):\n",
    "        # Bootstrap sampling to create a bootstrapped dataset\n",
    "        bootstrapped_X, bootstrapped_y = bootstrap_sampling(X_train, y_train)\n",
    "\n",
    "        # Subsample attributes\n",
    "        subsampled_attr_indexes = np.random.choice(range(X_train.shape[1]), int(X_train.shape[1] * attr_subsample_rate), replace=False)\n",
    "        subsampled_attributes.append(subsampled_attr_indexes.tolist())\n",
    "        subsampled_X = bootstrapped_X.iloc[:, subsampled_attr_indexes]\n",
    "\n",
    "        # Build decision tree using the bootstrapped and subsampled dataset\n",
    "        tree = decision_tree(subsampled_X, bootstrapped_y, max_depth)\n",
    "        trees.append(tree)\n",
    "\n",
    "    return trees, subsampled_attributes\n",
    "\n",
    "def confusion_matrix(y_true, y_pred):\n",
    "    classes = np.unique(np.concatenate([y_true, y_pred]))\n",
    "    n_classes = len(classes)\n",
    "    conf_matrix = np.zeros((n_classes, n_classes), dtype=int)\n",
    "\n",
    "    for i, true_label in enumerate(classes):\n",
    "        for j, pred_label in enumerate(classes):\n",
    "            conf_matrix[i, j] = np.sum((y_true == true_label) & (y_pred == pred_label))\n",
    "\n",
    "    return conf_matrix\n",
    "\n",
    "\n",
    "def calculate_metrics(conf_matrix):\n",
    "    TP = np.diag(conf_matrix)\n",
    "    FP = np.sum(conf_matrix, axis=0) - TP\n",
    "    FN = np.sum(conf_matrix, axis=1) - TP\n",
    "    TN = np.sum(conf_matrix) - (TP + FP + FN)\n",
    "\n",
    "    accuracy = np.sum(TP) / np.sum(conf_matrix)\n",
    "    \n",
    "    precision = np.where(TP + FP == 0, 0, TP / (TP + FP))\n",
    "    recall = np.where(TP + FN == 0, 0, TP / (TP + FN))\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    return accuracy, precision, recall, f1_score\n",
    "\n",
    "\n",
    "def stratified_cross_validation(X, y, n_folds, num_trees, max_depth, example_subsample_rate, attr_subsample_rate):\n",
    "    fold_size = len(X) // n_folds\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    conf_matrices = []\n",
    "    all_predictions = []\n",
    "\n",
    "    for i in range(n_folds):\n",
    "        start = i * fold_size\n",
    "        end = (i + 1) * fold_size\n",
    "\n",
    "        X_train_fold = pd.concat([X[:start], X[end:]])\n",
    "        y_train_fold = pd.concat([y[:start], y[end:]])\n",
    "\n",
    "        X_validation_fold = X[start:end]\n",
    "        y_validation_fold = y[start:end]\n",
    "        \n",
    "        trees, subsampled_attributes = fit_random_forest(num_trees, max_depth, example_subsample_rate, attr_subsample_rate, X_train_fold, y_train_fold)\n",
    "        predictions = classify_random_forest(trees, subsampled_attributes, X_validation_fold)\n",
    "        \n",
    "        all_predictions.extend(predictions)\n",
    "        \n",
    "        # Convert y_validation_fold to list\n",
    "        y_validation_fold_list = y_validation_fold.tolist()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        conf_matrix = confusion_matrix(y_validation_fold_list, predictions)\n",
    "        acc, prec, rec, f1 = calculate_metrics(conf_matrix)\n",
    "        \n",
    "        accuracies.append(acc)\n",
    "        precisions.append(prec)\n",
    "        recalls.append(rec)\n",
    "        f1_scores.append(f1)\n",
    "        \n",
    "        conf_matrices.append(conf_matrix)\n",
    "\n",
    "        mean_accuracy = np.mean(accuracies)\n",
    "        mean_precision = np.mean([np.mean(precision, axis=0) for precision in precisions], axis=0)\n",
    "        mean_recall = np.mean([np.mean(recall, axis=0) for recall in recalls], axis=0)\n",
    "        mean_f1_score = np.nanmean([np.nanmean(f1_score, axis=0) for f1_score in f1_scores], axis=0)\n",
    "\n",
    "    return mean_accuracy, mean_precision, mean_recall, mean_f1_score\n",
    "        \n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Read the dataset\n",
    "    df_voting = pd.read_csv(\"/Users/noshitha/Downloads/hw3/datasets/hw3_house_votes_84.csv\")\n",
    "\n",
    "    # Shuffle the dataset\n",
    "    df_voting_shuffle = shuffle(df_voting)\n",
    "\n",
    "    # Split the dataset into features and target variable\n",
    "    X = df_voting_shuffle.iloc[:, :-1]\n",
    "    y = df_voting_shuffle.iloc[:, -1]\n",
    "    \n",
    "    n_trees_list = [1, 5, 10, 20, 30, 40, 50]\n",
    "    n_folds = 10\n",
    "    max_depth = 3\n",
    "    example_subsample_rate = 0.5\n",
    "    attr_subsample_rate = 0.5\n",
    "    \n",
    "    accuracy  = []\n",
    "    precision = []\n",
    "    recall    = []\n",
    "    f1_score  = []\n",
    "    \n",
    "    for num_trees in n_trees_list:\n",
    "        print(\"num_trees: \",num_trees)\n",
    "        accuracies, precisions, recalls, f1_scores = stratified_cross_validation(X, y, n_folds, num_trees, max_depth, example_subsample_rate, attr_subsample_rate)\n",
    "        print(\"Accuracies:\", accuracies)\n",
    "        print(\"Precisions:\", precisions)\n",
    "        print(\"Recalls:\", recalls)\n",
    "        print(\"F1-scores:\", f1_scores)\n",
    "        accuracy.append(accuracies)\n",
    "        precision.append(precisions)\n",
    "        recall.append(recalls)\n",
    "        f1_score.append(f1_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53388a15",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ff8083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(n_trees_list, accuracy, marker='o')\n",
    "plt.xlabel('Number of Trees')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy vs Number of Trees - House Votes Dataset')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2768e9b",
   "metadata": {},
   "source": [
    "### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47722e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(n_trees_list, precision, marker='o')\n",
    "plt.xlabel('Number of Trees')\n",
    "plt.ylabel('Precision'))\n",
    "plt.title('Precision vs Number of Trees - House Votes Dataset')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b1c74c",
   "metadata": {},
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce2e12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(n_trees_list, precision, marker='o')\n",
    "plt.xlabel('Number of Trees')\n",
    "plt.ylabel('Recall'))\n",
    "plt.title('Recall vs Number of Trees - House Votes Dataset')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aefa111",
   "metadata": {},
   "source": [
    "### F1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e379d901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(n_trees_list, precision, marker='o')\n",
    "plt.xlabel('Number of Trees')\n",
    "plt.ylabel('F1_score'))\n",
    "plt.title('F1_score vs Number of Trees - House Votes Dataset')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90d339c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8ad0bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3dadeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd2bdb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9c58af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6ac599",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
